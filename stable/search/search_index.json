{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Welcome to the PyPSA-China-PIK documentation!","text":"<p>This is the documentation for the Potsdam Institute for Climate Impact Studies' (PIK) Python Power System Analysis for China (<code>PyPSA-China-PIK</code>) model, maintained by the Energy Transition Lab at PIK. <code>PyPSA-China-PIK</code> is an open model to simulate the future of energy in China at provincial level. Currently, electricity and heat are covered as energy sectors.  The model can be partially coupled to the REMIND Integrated Assesment Model to obtain multi-sectoral demand pathways. In this mode, battery electric vehicles can also be modelled.</p>"},{"location":"#what-is-pypsa-china-pik","title":"What is PyPSA-China-PIK?","text":"<p><code>PyPSA-China-PIK</code> is a highly configurable, open-source and open data power system model that co-optimises investments (capacity expansion) and dispatch of future energy systems in China. The model covers several end-use sectors, including electricity, battery electric vehicles and heat. It comes with default data for costs, technical parameters, loads and existing infrastructure that can easily be replaced. The execution options are controlled by configuration files, meaning the code can be run with limited or even no coding expertise. A highly modular structure also makes it easy to extend the model.</p> <p><code>PyPSA-China-PIK</code> is built around the PyPSA \"Python for Power System Analysis\" (pronounced \"pipes-ah\") energy system modelling toolbox. PyPSA provides python objects that represent mixed AC and DC electricity networks, generators with optional unit commitment or variable generation, storage units and transformers as well as efficient bindings to open-source and commercial solvers.</p> <p>Currently, hourly (and lower) time resolutions and provincial level demand and transmission are supported. It is possible to improve the spatial resolution as for other <code>PyPSA</code> workflows (EUR, USA, earth) if open data is available.  </p>"},{"location":"#capabilities","title":"Capabilities","text":"Capability Description Sectors Models both electricity and heat demand and supply Open Data &amp; Code Fully open-source model and datasets Provincial Resolution Simulates energy systems at the provincial level across China Renewables &amp; Hydro Models renewables and hydroelectricity availability based on historic data using Atlite. Renewables can be split by capacity factor grades Storage Long and short-term duration storage with hydrogen, pumped hydro and batteries. Coupling with REMIND Can be partially coupled to the REMIND IAM for multi-sectoral analysis Cost Optimization Minimizes system costs using a solver of your choice Flexible Workflow Managed by Snakemake for reproducible and automated analysis Customizable Modular structure can easily be customized Post-processing Tools Detailed results analysis and visualization <p>The model has been validated against short term energy trends. Todo: add figure.</p>"},{"location":"#learning","title":"Learning","text":"<p>The model comes with an installation guide, model overview basic tutorials and references for the code and configuration options. <code>PyPSA-China-PIK</code> is best understood as workflow, managed by the low-code snakemake tool. It is possible to run the workflow with minimal knowledge of <code>snakemake</code> and we have listed a few useful tricks but we recommend going over the snakemake documentation.  </p> <p>The workflow consists of gathering and preparing relevant data, formulating the problem as a PyPSA network object, minimising the system costs using a solver and post-processing the data. The <code>atlite</code> package is used to compute renewable generator availability and potentials. You may want to look into the PyPSA documentation and the atlite documentation. </p> <p></p>"},{"location":"#authors-and-credits","title":"Authors and Credits","text":"<p>This version has is maintained by the PIK RD3-ETL team. It is not yet published, please contact us in case you are interested in using the model.</p> <p>The model is based on the <code>PyPSA-EUR</code> work by the Tom Brown Group, originally adapted to China by Hailiang Liu et al for their study of hydro-power in china and extended by Xiaowei Zhou et al for their  \"Multi-energy system horizon planning: Early decarbonisation in China avoids stranded assets\" paper. It has received significant upgrades.</p> <p> </p>"},{"location":"configuration/","title":"Configuration Reference","text":"<p>This is documentation for the PyPSA-China configuration (<code>config/default_config.yaml</code> &amp; <code>config/technology_config.yaml</code>). The configuration file controls various aspects of the PyPSA-China energy system modeling workflow.</p>"},{"location":"configuration/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Run Configuration</li> <li>File Paths</li> <li>Grid Topology</li> <li>Scenarios</li> <li>CO2 Scenarios</li> <li>Time Snapshots</li> <li>Logging</li> <li>Data Fetch Toggles</li> <li>Atlite Weather Settings</li> <li>Renewable Energy Technologies</li> <li>Heat Demand</li> <li>Reporting</li> <li>Bus and Carriers</li> <li>Technology Categories</li> <li>Sectors &amp; Component</li> <li>Hydro Dams</li> <li>Hydrogen Storage</li> <li>Solving</li> <li>Transmission Lines</li> <li>Security Constraints</li> <li>Existing Capacities</li> <li>Regions</li> <li>Input/Output Settings</li> <li>Transmission Efficiency</li> <li>Combined Heat and Power (CHP) Parameters</li> <li>Solar Technology Parameters</li> <li>Heat Pump Configuration</li> <li>Thermal Energy Storage</li> <li>Electricity Sector Configuration</li> <li>Hydroelectric Power</li> <li>Fossil Fuel Ramping Constraints</li> <li>Nuclear Reactor Parameters</li> <li>Economic Parameters</li> </ul>"},{"location":"configuration/#usage-notes","title":"Usage Notes","text":"<ol> <li> <p>File Organization: Configuration files should be placed in the <code>config/</code> directory.</p> </li> <li> <p>Customization: Do not edit <code>default_config.yaml</code>. Overwrite the variables you need in <code>my_config.yaml</code>. See running section. Then run <code>snakemake --configfile config/my_config.yaml</code></p> </li> <li> <p>Technology Configuration: Additional technology parameters are defined in separate files in  <code>config/technology_config.yaml</code> </p> </li> <li> <p>Solver Selection: Remember to select a solver that is installed.</p> </li> </ol>"},{"location":"configuration/#run-configuration","title":"Run Configuration","text":"<pre><code>run:\n  name: unamed_run\n  is_remind_coupled: false\nforesight: \"overnight\"\n</code></pre> <ul> <li><code>run.name</code>: Identifier for the model run. Used for organizing results and logs.</li> <li><code>run.is_remind_coupled</code>: Boolean indicating whether the model run is coupled with REMIND. Set to <code>false</code> for standalone PyPSA-China runs. <code>True</code> overwrites numerous settings and affects the behaviour of the path manager.</li> <li><code>foresight</code>: [\"overnight\"] Set to <code>\"overnight\"</code> for perfect foresight within one time horizon. Each horizon is solved independently. The <code>myopic</code> mode is currently unsupported.</li> </ul>"},{"location":"configuration/#file-paths","title":"File Paths","text":"<p><pre><code>paths:\n  results_dir: \"results/\"\n  costs_dir: \"resources/data/costs/default\"\n  yearly_regional_load: \n    ac: \"resources/data/load/Provincial_Load_2020_2060_MWh.csv\"\n    ac_to_mwh: 1\n</code></pre> - <code>results_dir</code>: Directory where model results are stored - <code>costs_dir</code>: Directory containing technology &amp; cost data - <code>yearly_regional_load.ac</code>: Path to provincial electricity load data file - <code>yearly_regional_load.ac_to_mwh</code>: Conversion factor to MWh (set to 1 if load file is already in MWh)</p>"},{"location":"configuration/#grid-topology","title":"Grid Topology","text":"<p><pre><code>edge_paths:\n  current: \"resources/data/grids/edges_current.csv\"\n  current+FCG: \"resources/data/grids/edges_current_FCG.csv\"\n  current+Neighbor: \"resources/data/grids/edges_current_neighbor.csv\"\n</code></pre> Paths to named transmission line topology files for different scenarios. Predefined   - <code>\"current\"</code>: Existing transmission network   - <code>\"current+FCG\"</code>: Fully connected grid (represents current expansion plans)   - <code>\"current+Neighbor\"</code>: current + all neighbours connected</p>"},{"location":"configuration/#scenario-configuration","title":"Scenario Configuration","text":"<p>A scenario is a set of time horizons with a carbon reduction pathway. The variations become snakemake wildcards.</p> <pre><code>scenario: \n  co2_pathway: [\"exp175default\"]\n  topology: \"current+FCG\"\n  planning_horizons: [year_list]\n  heating_demand: [\"positive\"]\n</code></pre> <ul> <li><code>co2_pathway</code>: List of named CO2 emission scenarios to model</li> <li><code>topology</code>: Grid topology configuration. Must correspond to an edge_path</li> <li><code>planning_horizons</code>: Years to model</li> <li><code>heating_demand</code>: Heating demand scenarios (will be overhauled)</li> </ul>"},{"location":"configuration/#co2-scenarios","title":"CO2 Scenarios","text":"<p>Emission reduction pathways. <code>scenario.co2_pathway</code> entries must be defined here.</p> <pre><code>co2_scenarios: \n  exp175default: # pathway name\n    control: \"reduction\"\n    pathway:\n      '2020': 0.0\n      '2025': 0.22623418\n      # ... additional years\n</code></pre> <p>Defines CO2 emission reduction scenarios: - <code>control</code>: Control mechanism (<code>\"price\"</code>, <code>\"reduction\"</code>, <code>\"budget\"</code>, or <code>None</code>) - <code>pathway</code>: Yearly values for the price, reduction or budget. Ignored if <code>control=None</code>.</p>"},{"location":"configuration/#snapshots","title":"Snapshots","text":"<p>Snapshots are the modelled timestamps</p> <p><pre><code>snapshots:\n  start: \"01-01 00:00\"\n  end: \"12-31 23:00\"\n  bounds: 'both'\n  freq: '5h'\n  frequency: 5.\n  end_year_plus1: false\n</code></pre> Controls the temporal resolution of the model: - <code>start</code>: Start date and time (MM-DD HH:MM format) - <code>end</code>: End date and time - <code>bounds</code>: Include start/end points (<code>'both'</code>, <code>'left'</code>, <code>'right'</code>) - <code>freq</code>: Frequency string for pandas (e.g., <code>'5h'</code> for 5-hour intervals) - <code>frequency</code>: weight of a snapshot (eg 5 means that the generation for that timestamp will be multiplifed by 5 to get actual MWh values &amp; costs) - <code>end_year_plus1</code>: bool: not single year?</p> <p>Time sampling</p> <p>Lower resolutions are currently dumbly sampled as 1 in n. Nonetheless it is possible to get mixes and costs close to those of the 1hr resolution.</p>"},{"location":"configuration/#logging","title":"Logging","text":"<pre><code>logging_level: INFO\nlogging:\n  level: INFO\n  format: '%(levelname)s:%(name)s:%(message)s'\n</code></pre> <ul> <li><code>logging_level</code>: Global logging level</li> <li><code>logging.level</code>: Detailed logging level</li> <li><code>logging.format</code>: Log message format string</li> </ul>"},{"location":"configuration/#data-fetch-toggles","title":"Data fetch toggles","text":"<pre><code>enable:\n  build_cutout: false\n  retrieve_cutout: false\n  retrieve_raster: false\n</code></pre> <p>Controls whether the data fetch workflow steps are enabled: - <code>build_cutout</code>: Build new weather data cutouts - <code>retrieve_cutout</code>: Download existing cutouts (not in correct file structure, must be manually copied) - <code>retrieve_raster</code>: Download existing raster data</p>"},{"location":"configuration/#atlite-weather","title":"Atlite Weather","text":"<pre><code>atlite:\n  cutout_name: \"China-2020c\"\n  freq: \"h\"\n  nprocesses: 1\n  show_progress: true\n  monthly_requests: true\n  cutouts:\n    China-2020c:\n      module: era5\n      dx: 0.25\n      dy: 0.25\n      weather_year: 2020\n</code></pre> <p>Configuration for weather data processing: - <code>cutout_name</code>: Name of cutout (searched for in <code>resources/cutouts/</code>) - <code>freq</code>: Temporal frequency for weather data - <code>nprocesses</code>: Number of parallel processes - <code>show_progress</code>: Display progress bars - <code>monthly_requests</code>: Split requests by month - <code>cutouts</code>: config(s) for one or more cutout names   - <code>module</code>: Weather data source (e.g., <code>era5</code>)   - <code>dx</code>/<code>dy</code>: Spatial resolution in degrees   - <code>weather_year</code>: weather year to fetch/use</p>"},{"location":"configuration/#renewable-energy-technologies-atlite","title":"Renewable Energy Technologies Atlite","text":""},{"location":"configuration/#wind","title":"Wind","text":"<pre><code>renewable:\n  onwind | offwind: \n    cutout: cutout-name\n    resource:\n      method: wind\n      turbine: model\n    resource_classes:\n      min_cf_delta: 0.05\n      n: 3\n    capacity_per_sqkm: 3\n    potential: simple\n    natura: false\n    clip_p_max_pu: 1.e-2\n    min_p_nom_max: 1.e-2\n</code></pre>"},{"location":"configuration/#solar-pv","title":"Solar PV","text":"<pre><code>  solar:\n    cutout: cutout-name\n    resource:\n      method: pv\n      panel: model\n      orientation:\n        slope: 35.\n        azimuth: 180.\n    resource_classes:\n      min_cf_delta: 0.02\n      n: 2\n    capacity_per_sqkm: 5.1\n    potential: simple\n    correction_factor: 0.85\n    natura: false\n    clip_p_max_pu: 1.e-2\n    min_p_nom_max: 1.e-2\n</code></pre> <p>Common renewable parameters: - <code>cutout</code>: Weather data cutout name to use - <code>resource.method</code>: Resource calculation method - <code>resource.turbine | panel</code>: Technology specification - <code>resource_classes</code>: Capacity factor binning to reduce provincial aggregation effects   - <code>n</code>: Number of resource bins   - <code>min_cf_delta</code>: Minimum capacity factor difference between bins - <code>capacity_per_sqkm</code>: Max installable Power density (MW/km\u00b2) - <code>potential</code>: Potential calculation method (<code>simple</code> or <code>conservative</code>) - <code>correction_factor</code>: Technology-specific correction factor - <code>natural_reserves</code>: Consider nature protection areas - <code>max_depth</code>: Maximum water depth for offshore wind (m) - <code>max_altitude</code>: Maximum altitude at which technolgoy can be built. - <code>max_slope</code>: Maximum terrain slope for renewables (%) - <code>clip_p_max_pu</code>: Minimum capacity factor threshold - <code>min_p_nom_max</code>: Minimum installable capacity threshold - <code>land_cover_codes</code>: Copernicus LC100 land cover codes for allowed areas</p> <p><pre><code>renewable_potential_cutoff: 200  # MW\n</code></pre> Skip locations with potential below this threshold to reduce problem size.</p>"},{"location":"configuration/#heat-demand","title":"Heat Demand","text":"<pre><code>heat_demand:\n  start_day: \"01-04\"\n  end_day: \"30-09\"\n  heating_start_temp: 15.0\n  heating_lin_slope: 1\n  heating_offet: 0\nsolar_thermal_angle: 45\n</code></pre> <ul> <li><code>start_day</code>/<code>end_day</code>: Heating season dates (DD-MM format)</li> <li><code>heating_start_temp</code>: Temperature threshold for heating demand (\u00b0C)</li> <li><code>heating_lin_slope</code>: Linear relationship slope</li> <li><code>heating_offset</code>: Linear model offset</li> <li><code>solar_thermal_angle</code>: Solar thermal collector angle (degrees)</li> </ul>"},{"location":"configuration/#reporting","title":"Reporting","text":"<pre><code>reporting:\n  adjust_link_capacities_by_efficiency: true\n</code></pre> <ul> <li><code>adjust_link_capacities_by_efficiency</code>: PyPSA links capacities are in input. Typical reporting for AC is in output capacitiy. If true Adjust link capacities by efficiency for consistent AC-side reporting.</li> </ul>"},{"location":"configuration/#bus-and-carrier-configuration","title":"Bus and Carrier Configuration","text":"<pre><code>bus_suffix: [\"\",\" central heat\",\" decentral heat\",\" gas\",\" coal\"]\nbus_carrier: {\n    \"\": \"AC\",\n    \" central heat\": \"heat\",\n    \" decentral heat\": \"heat\",\n    \" gas\": \"gas\",\n    \" coal\": \"coal\",\n}\n</code></pre> <p>Defines bus types and their corresponding energy carriers: - <code>bus_suffix</code>: List of bus name suffixes - <code>bus_carrier</code>: Mapping of suffixes to carrier types</p>"},{"location":"configuration/#technology-categories","title":"Technology Categories","text":"<pre><code>Techs:\n  vre_techs: [\"onwind\",\"offwind\",\"solar\",\"solar thermal\",\"hydroelectricity\", \"nuclear\",\"biomass\",\"beccs\",\"heat pump\",\"resistive heater\",\"Sabatier\", \"fuel cell\", \"H2 CHP\"]\n  non_dispatchable: [\"onwind\", \"offwind\", \"solar\", \"solar thermal\"]\n  conv_techs: [\"CCGT\", \"CHP gas\", \"CHP OCGT gas\", \"gas boiler\",\"coal boiler\",\"coal power plant\",\"CHP coal\", \"OCGT\"]\n  store_techs: [\"H2\",\"battery\",\"water tanks\",\"PHS\"]\n  coal_ccs_retrofit: false\n  hydrogen_lines: true\n</code></pre> <p>Technology categorization: - <code>vre_techs</code>: Variable renewable energy and clean technologies - <code>non_dispatchable</code>: Technologies without dispatch control (weather-dependent) - <code>conv_techs</code>: Conventional generation technologies (includes CHP OCGT gas) - <code>store_techs</code>: Storage technologies - <code>coal_ccs_retrofit</code>: Enable coal with carbon capture retrofit (currently unsupported) - <code>hydrogen_lines</code>: Enable hydrogen transmission lines</p>"},{"location":"configuration/#nuclear-reactor-growth-limits","title":"Nuclear Reactor Growth Limits","text":"<pre><code>nuclear_reactors:\n  enable_growth_limit: true\n  max_annual_capacity_addition: 5000  # MW\n  base_year: 2020\n  # base_capacity: 50000  # (optional) MW\n</code></pre> <p>Controls nuclear capacity expansion: - <code>enable_growth_limit</code>: Enable/disable annual growth constraints on nuclear capacity - <code>max_annual_capacity_addition</code>: Maximum MW that can be added per year - <code>base_year</code>: Reference year for capacity calculations (should be \u2264 first planning year) - <code>base_capacity</code>: (Optional) Base year total capacity in MW. If not set, auto-detected from base_year network</p>"},{"location":"configuration/#sector-and-component-switches","title":"Sector and component Switches","text":"<pre><code>heat_coupling: false\nadd_biomass: True\nadd_hydro: True\nadd_H2: True\nadd_methanation: True\nline_losses: True\nno_lines: False\n</code></pre> <p>Control which components to include in the model: - <code>heat_coupling</code>: Enable heat sector coupling - <code>add_biomass</code>: Include biomass technologies - <code>add_hydro</code>: Include hydroelectric power - <code>add_H2</code>: Include hydrogen technologies (and pipelines) - <code>add_methanation</code>: Include methanation processes - <code>line_losses</code>: Model transmission line losses - <code>no_lines</code>: Disable transmission lines (autartik)</p>"},{"location":"configuration/#operational-reserves","title":"Operational Reserves","text":"<pre><code>operational_reserve:\n  activate: false\n  epsilon_load: 0.02\n  epsilon_vres: 0.02\n  contingency: 300000  # MW\n</code></pre> <ul> <li><code>activate</code>: Enable operational reserve constraints</li> <li><code>epsilon_load</code>: Reserve fraction relative to load (2%)</li> <li><code>epsilon_vres</code>: Reserve fraction relative to variable renewable energy sources (2%)</li> <li><code>contingency</code>: Additional fixed reserve requirement (MW)</li> </ul>"},{"location":"configuration/#hydro-dams","title":"Hydro Dams","text":"<pre><code>hydro_dams:\n  dams_path: \"resources/data/hydro/dams_large.csv\"\n  inflow_path: \"resources/data/hydro/daily_hydro_inflow_per_dam_1979_2016_m3.pickle\"\n  inflow_date_start: \"1979-01-01\"\n  inflow_date_end: \"2017-01-01\"\n  reservoir_initial_capacity_path: \"resources/data/hydro/reservoir_initial_capacity.pickle\"\n  reservoir_effective_capacity_path: \"resources/data/hydro/reservoir_effective_capacity.pickle\"\n  river_links_stations: \"\"\n  p_nom_path: \"resources/data/p_nom/hydro_p_nom.h5\"\n  p_max_pu_path: \"resources/data/p_nom/hydro_p_max_pu.h5\"\n  p_max_pu_key: \"hydro_p_max_pu\"\n  damn_flows_path: \"resources/data/hydro/dam_flow_links.csv\"\n</code></pre> <p>Hydroelectric dam configuration: - <code>dams_path</code>: CSV file with dam locations and characteristics - <code>inflow_path</code>: Historical inflow data - <code>inflow_date_start</code>/<code>inflow_date_end</code>: Date range for inflow data - <code>reservoir_*_capacity_path</code>: Reservoir capacity data files - <code>p_nom_path</code>: Installed capacity data - <code>p_max_pu_path</code>: Maximum capacity factor time series - <code>damn_flows_path</code>: Dam flow connections</p>"},{"location":"configuration/#hydrogen-storage","title":"Hydrogen Storage","text":"<pre><code>H2:\n  geo_storage_nodes: [\"Sichuan\", \"Chongqing\", \"Hubei\", \"Jiangxi\", \"Anhui\", \"Jiangsu\", \"Shandong\", \"Guangdong\"]\n</code></pre> <ul> <li><code>geo_storage_nodes</code>: Provinces with geological hydrogen storage potential</li> </ul>"},{"location":"configuration/#solving-configuration","title":"Solving Configuration","text":""},{"location":"configuration/#general-options","title":"General Options","text":"<pre><code>solving:\n  options:\n    formulation: kirchhoff\n    load_shedding: false\n    voll: 1e5\n    noisy_costs: false\n    min_iterations: 4\n    max_iterations: 6\n    clip_p_max_pu: 0.01\n    skip_iterations: false\n    track_iterations: false\n    export_duals: true\n  solver:\n    name: gurobi\n    options: gurobi-default\n  mem: 80000  # MB\n</code></pre> <ul> <li><code>formulation</code>: Network formulation (<code>kirchhoff</code> or <code>angles</code>)</li> <li><code>load_shedding</code>: Allow unserved energy</li> <li><code>voll</code>: Value of lost load (EUR/MWh)</li> <li><code>noisy_costs</code>: Add noise to costs for degeneracy handling</li> <li><code>min_iterations</code>/<code>max_iterations</code>: Iteration bounds for iterative solving</li> <li><code>clip_p_max_pu</code>: Minimum capacity factor threshold</li> <li><code>skip_iterations</code>/<code>track_iterations</code>: Iteration control</li> <li><code>export_duals</code>: Export dual variables for post-processing</li> <li><code>solver.name</code>: Solver to use (gurobi, highs, cplex, cbc, glpk)</li> <li><code>solver.options</code>: Solver option preset name</li> <li><code>mem</code>: Memory allocation in MB</li> </ul>"},{"location":"configuration/#solver-options","title":"Solver Options","text":"<p>Multiple solver presets are available: - <code>gurobi-default</code>: Standard barrier method with optimized settings - <code>gurobi-numeric-focus</code>: Prioritizes numeric stability over speed - <code>gurobi-fallback</code>: Uses Gurobi defaults with homogeneous barrier - <code>highs-default</code>: Open-source HiGHS solver configuration - <code>cplex-default</code>: IBM CPLEX barrier method - <code>cbc-default</code>/<code>glpk-default</code>: Used in CI testing</p>"},{"location":"configuration/#transmission-lines","title":"Transmission Lines","text":"<pre><code>lines:\n  line_length_factor: 1.25\n  expansion:\n    transmission_limit: vopt\n    base_year: 2020\n</code></pre> <ul> <li><code>line_length_factor</code>: Line length increase factor, representing impossibility to build perfectly straight</li> <li><code>expansion.transmission_limit</code>: Transmission expansion constraint</li> <li><code>[v]opt</code>: Optimal (unconstrained)</li> <li><code>v1.03</code>: Volume-constrained (3% increase limit PER YEAR not horizon)</li> <li><code>c1.03</code>: Cost-constrained (3% increase limit PER YEAR not horizon)</li> <li><code>expansion.base_year</code>: Reference year for expansion limits (to calculate max exp). Should be year of input network topology file (which has capacities)</li> </ul>"},{"location":"configuration/#security-constraints","title":"Security Constraints","text":"<pre><code>security:\n  line_security_margin: 70  # Max percent of line capacity\n</code></pre> <ul> <li><code>line_security_margin</code>: Security margin for transmission lines (% of capacity)</li> </ul>"},{"location":"configuration/#existing-capacities","title":"Existing Capacities","text":"<pre><code>existing_capacities:\n  add: True\n  grouping_years: [1985, 1990, 1995, 2000, 2005, 2010, 2015, 2020, 2025, 2030, 2035, 2040, 2045, 2050, 2055, 2060]\n  collapse_years: True\n  threshold_capacity: 80\n  techs: ['coal','CHP coal', 'CHP gas', 'OCGT', 'CCGT', 'solar', 'onwind', 'offwind', 'nuclear', \"PHS\", \"biomass\"]\n  node_assignment: simple\n</code></pre> <p>Configuration for incorporating existing power plant capacities: - <code>add</code>: Include existing GEM capacities in the model. These are retired only if they reach end of life (determined based on the costs tech config) - <code>grouping_years</code>: Years for capacity grouping - <code>collapse_years</code>: Treat grouped capacities as a single unit when preparing &amp; solving network - <code>threshold_capacity</code>: Minimum capacity threshold (MW) - <code>techs</code>: Technologies to include from existing capacity data - <code>node_assignment</code>: Method for assigning plants to nodes (<code>simple</code> or <code>gps</code>)</p>"},{"location":"configuration/#region-configuration","title":"Region Configuration","text":"<pre><code>fetch_regions:\n  simplify_tol:\n    eez: 0.5\n    land: 0.05\n</code></pre> <ul> <li><code>simplify_tol.eez</code>: Tolerance for EEZ (Exclusive Economic Zone) boundary simplification</li> <li><code>simplify_tol.land</code>: Tolerance for land region boundary simplification (lower value for better GPS plant assignment)</li> </ul>"},{"location":"configuration/#node-configuration","title":"Node Configuration","text":"<pre><code>nodes:\n  split_provinces: False\n  exclude_provinces: [\"Macau\", \"HongKong\"]\n  splits:\n    InnerMongolia:\n      West: [\"Hulunbuir\", \"Xing'an\", \"Tongliao\", \"Chifeng\", \"Xilin Gol\"]\n      East: [\"Alxa\", \"Baotou\", \"Baynnur\", \"Hohhot\", \"Ordos\", \"Ulaan Chab\", \"Wuhai\"]\n</code></pre> <ul> <li><code>split_provinces</code>: Split provinces (admin level 1) using admin level 2 (counties/prefectures). Currently not fully supported by build_loads and reporting.</li> <li><code>exclude_provinces</code>: List of provinces to exclude from splitting</li> <li><code>splits</code>: Custom groupings of admin level 2 regions within provinces</li> </ul>"},{"location":"configuration/#fuel-subsidies","title":"Fuel Subsidies","text":"<pre><code>subsidies:\n  enabled: false\n  gas:\n    Guangdong: -10\n    Jiangsu: -10\n    Zhejiang: -10\n    Beijing: -11\n    Tianjin: -11\n    Shanghai: -11\n</code></pre> <p>Provincial fuel subsidies configuration: - <code>enabled</code>: Enable/disable fuel subsidy system - <code>gas</code>: Gas subsidies by province (EUR/MWh, negative values represent subsidies)</p> <p>Subsidies reduce the effective marginal cost of fuel in specific provinces.</p>"},{"location":"configuration/#inputoutput-settings","title":"Input/Output Settings","text":"<pre><code>io:\n  nc_compression:\n    level: 4\n    zlib: True\n</code></pre> <p>Controls compression settings for NetCDF output files: - <code>level</code>: Compression level (0-9, higher = more compression) - <code>zlib</code>: Enable zlib compression</p>"},{"location":"configuration/#technology-config","title":"TECHNOLOGY CONFIG","text":"<p>The below are in <code>config/technology_config.yaml</code></p>"},{"location":"configuration/#transmission-efficiency","title":"Transmission Efficiency","text":"<pre><code>transmission_efficiency:\n  DC:\n    efficiency_static: 0.98\n    efficiency_per_1000km: 0.977\n  H2 pipeline:\n    efficiency_static: 1\n    efficiency_per_1000km: 0.979\n    compression_per_1000km: 0.019\n</code></pre> <p>Defines transmission efficiency parameters for different carriers &amp; technologies:</p>"},{"location":"configuration/#dc-transmission","title":"DC Transmission","text":"<ul> <li><code>efficiency_static</code>: Base efficiency for DC transmission lines (98%)</li> <li><code>efficiency_per_1000km</code>: Distance-dependent efficiency factor per 1000 km (97.7% per 1000 km)</li> </ul>"},{"location":"configuration/#hydrogen-pipeline","title":"Hydrogen Pipeline","text":"<ul> <li><code>efficiency_static</code>: Base efficiency for hydrogen pipelines (100% - no static losses)</li> <li><code>efficiency_per_1000km</code>: Distance-dependent efficiency factor per 1000 km (97.9% per 1000 km)</li> <li><code>compression_per_1000km</code>: Energy required for compression per 1000 km (1.9% of transported energy)</li> </ul> <p>The total efficiency for transmission links is calculated as: <pre><code>Total efficiency = efficiency_static \u00d7 (efficiency_per_1000km)^(distance_km/1000)\n</code></pre></p>"},{"location":"configuration/#combined-heat-and-power-chp-parameters","title":"Combined Heat and Power (CHP) Parameters","text":"<pre><code>chp_parameters:\n  coal:\n    total_eff: 0.9\n    heat_to_power: 1.5\n  gas:\n    total_eff: 0.9\n    heat_to_power: 0.9\n  OCGT:\n    heat_to_power: 1\n    total_eff: 0.82\n\nuse_historical_efficiency:\n  coal_boiler: True\n</code></pre> <p>CHP efficiency parameters by technology: - <code>coal.total_eff</code>: Total combined efficiency for coal CHP (90%, from Danish Energy Tech Catalogue) - <code>coal.heat_to_power</code>: Heat-to-power ratio for extraction CHP (1.5, typical for China from Nature Energy 2024) - <code>gas.total_eff</code>: Total combined efficiency for CCGT CHP (90%) - <code>gas.heat_to_power</code>: Heat-to-power ratio for gas CHP (0.9) - <code>OCGT.total_eff</code>: Total efficiency for OCGT CHP (82%, from GE aeroderivative data) - <code>OCGT.heat_to_power</code>: Heat-to-power ratio for simple cycle gas turbine (1.0) - <code>use_historical_efficiency.coal_boiler</code>: Use historical efficiency values when building new coal boilers</p> <p>Note: Efficiency may decrease with reduced power output, which is not currently captured in the model.</p>"},{"location":"configuration/#solar-technology-parameters","title":"Solar Technology Parameters","text":"<pre><code>solar_cf_correction: 0.85\n</code></pre> <ul> <li><code>solar_cf_correction</code>: Correction factor applied to solar capacity factors (85%)</li> </ul> <p>This factor accounts for various real-world effects that reduce solar Thermal performance compared to theoretical values. NOT APPLIED TO PV</p>"},{"location":"configuration/#heat-pump-configuration","title":"Heat Pump Configuration","text":"<pre><code>time_dep_hp_cop: True\n</code></pre> <ul> <li><code>time_dep_hp_cop</code>: Enable time-dependent coefficient of performance (COP) for heat pumps</li> </ul> <p>When enabled, heat pump efficiency varies with ambient temperature conditions throughout the year.</p>"},{"location":"configuration/#thermal-energy-storage","title":"Thermal Energy Storage","text":"<pre><code>water_tanks:\n  tes_tau:\n    decentral: 3.  # days\n    central: 180  # days\n  p_nom_over_e_nom: 0.2\n</code></pre> <p>Standing loss parameters for thermal energy storage (water tanks): - <code>decentral</code>: Time constant for decentralized thermal storage (3 days) - <code>central</code>: Time constant for centralized thermal storage (180 days) - <code>p_nom_over_e_nom</code>: Maximum power-to-energy ratio (0.2 = 5-hour discharge time)</p> <p>The time constant (tau) determines the rate of thermal losses.</p>"},{"location":"configuration/#electricity-sector-configuration","title":"Electricity Sector Configuration","text":"<p>Partially legacy and to be revised</p> <pre><code>electricity:\n  max_hours:\n    battery: 6\n    H2: 168\n  min_charge:\n    battery: 0.1 # fraction of e_nom\n</code></pre>"},{"location":"configuration/#storage-parameters","title":"Storage Parameters","text":"<ul> <li><code>max_hours.battery</code>: Maximum storage duration for batteries (6 hours)</li> <li><code>max_hours.H2</code>: Maximum storage duration for hydrogen storage (168 hours = 1 week)</li> <li><code>min_charge.battery</code>: Minimum state of charge for batteries (10% of nominal energy capacity)</li> </ul>"},{"location":"configuration/#hydroelectric-power","title":"Hydroelectric Power","text":"<pre><code>hydro:\n  hydro_capital_cost: True\n  marginal_cost:\n    reservoir: 0.  # EUR/MWh\n  PHS_max_hours: 24  # hours\n</code></pre> <ul> <li><code>hydro_capital_cost</code>: Include capital costs for hydroelectric plants</li> <li><code>marginal_cost.reservoir</code>: Marginal cost of reservoir operation (0 EUR/MWh)</li> <li><code>PHS_max_hours</code>: Maximum storage duration for pumped hydro storage (24 hours)</li> </ul>"},{"location":"configuration/#fossil-fuel-ramping-constraints","title":"Fossil Fuel Ramping Constraints","text":"<p>Operational ramping constraints for fossil fuel power plants:</p> <pre><code>fossil_ramps:\n  tech:\n    ramp_limit_up: 0.5 # fraction of p_nom per hour\n    ramp_limit_down: 0.5 # fraction of p_nom per hour\n</code></pre> <ul> <li><code>ramp_limit_up</code>: Maximum upward ramping rate (50% of nominal capacity per hour)</li> <li><code>ramp_limit_down</code>: Maximum downward ramping rate (50% of nominal capacity per hour)</li> </ul>"},{"location":"configuration/#nuclear-reactor-parameters","title":"Nuclear Reactor Parameters","text":"<p>Instead of solving unit commitment problem, which is computationally expensive, can stylise baseload generation. The upper limit reflects planned and unplanned outages</p> <p><pre><code>nuclear_reactors:\n  p_max_pu: 0.88 # fraction of p_nom, after IEAE\n  p_min_pu: 0.7 # fraction of p_nom\n</code></pre> Operational constraints for nuclear power plants: - <code>p_max_pu</code>: Maximum power output (% of nominal capacity) - <code>p_min_pu</code>: Minimum power output (% of nominal capacity)</p>"},{"location":"configuration/#economic-parameters","title":"Economic Parameters","text":"<pre><code>costs:\n  discountrate: 0.06\n  social_discount_rate: 0.02\n  USD2013_to_EUR2013: 0.9189\n  marginal_cost:\n    hydro: 0.\n  pv_utility_fraction: 1\n</code></pre>"},{"location":"configuration/#discount-rates","title":"Discount Rates","text":"<ul> <li><code>discountrate</code>: Financial discount rate for investment decisions (6%)</li> <li><code>social_discount_rate</code>: Social discount rate for welfare analysis (2%)</li> </ul> <p>The financial discount rate is used for technology investment decisions, while the social discount rate is applied for broader economic impact assessments.</p>"},{"location":"configuration/#currency-conversion","title":"Currency Conversion","text":"<ul> <li><code>USD2013_to_EUR2013</code>: Exchange rate from USD to EUR for 2013 prices (0.9189 EUR/USD) - used for technoeconomic conversion. Weakpoint, only one currency. Also tech costs are now in Euro2015 from DK EA</li> </ul>"},{"location":"configuration/#solar-pv-configuration","title":"Solar PV Configuration","text":"<ul> <li><code>pv_utility_fraction</code>: Fraction of solar PV that is utility-scale (100%)</li> </ul> <p>This parameter distinguishes between utility-scale and residential/distributed solar installations, affecting cost assumptions and grid integration characteristics.</p>"},{"location":"data/","title":"Data","text":"<p>The user is responsible for checking the license of the original data, their validity and applicability. Below is a list of data sources</p>"},{"location":"data/#land-classification-copernicus-global-land-cover","title":"Land classification (copernicus global land cover)","text":"<p>Marcel Buchhorn, Bruno Smets, Luc Bertels, Bert De Roo, Myroslava Lesiv, Nandin-Erdene Tsendbazar, Martin Herold, &amp; Steffen Fritz. (2020). Copernicus Global Land Service: Land Cover 100m: collection 3: epoch 2019: Globe (V3.0.1) [Data set]. Zenodo. https://doi.org/10.5281/zenodo.3939050</p> <ul> <li>Source: Copernicus Global Land Service:</li> <li>Link: https://land.copernicus.eu/en/products/global-dynamic-land-cover/copernicus-global-land-service-land-cover-100m-collection-3-epoch-2019-globe</li> <li>License: Creative Commons Attribution 4.0 International</li> <li>Description: 100m resolution land classification</li> </ul>"},{"location":"data/#administrative-regions","title":"Administrative regions","text":"<p>GADM Global adminstrative map - Source: GADM - Link: https://gadm.org/ - License: GADM license - Description: poylgons of administrative areas</p>"},{"location":"data/#protected-areas","title":"Protected areas","text":"<p>China Nature Reserve Specimen Resource Sharing Platform. (2024). List and Vector Boundaries of Nature Reserves in China [Data set].</p> <ul> <li>Source: China Nature Reserve Specimen Resource Sharing Platform</li> <li>Link: https://doi.org/10.5281/zenodo.14875797</li> <li>License: MIT</li> <li>Description: polygons of protected area</li> </ul>"},{"location":"data/#bathymetry","title":"Bathymetry","text":"<p>Elevation and depth data</p> <ul> <li>Source: GEBCO</li> <li>Link: https://www.gebco.net/data-products-gridded-bathymetry-data/gebco2025-grid</li> <li>License: CC0 (reference)</li> <li>Description: Bathymetric dataset (2025 version).</li> </ul>"},{"location":"data/#power-infrastruture","title":"Power infrastruture","text":"<p>Historical brownfield capacities</p> <ul> <li>Source: Glboal Energy Monitor</li> <li>Link: https://globalenergymonitor.org/projects/global-integrated-power-tracker/</li> <li>License: Creative Commons Attribution 4.0 International Public License</li> <li>Description: powerplant, renewable and transmission data</li> </ul>"},{"location":"data/#population","title":"Population","text":"<p>Gridded population data</p> <ul> <li>Source: WorldPop (www.worldpop.org - School of Geography and Environmental Science, University of Southampton; Department of Geography and Geosciences, University of Louisville; Departement de Geographie, Universite de Namur) and Center for International Earth Science Information Network (CIESIN), Columbia University (2018). Global High Resolution Population Denominators Project - Funded by The Bill and Melinda Gates Foundation (OPP1134076). https://dx.doi.org/10.5258/SOTON/WP00647</li> <li>Link: https://hub.worldpop.org/doi/10.5258/SOTON/WP00647</li> <li>License: CC-BY 4.0 (reference)</li> <li>Description: Gridded population data.</li> </ul>"},{"location":"data/#ev-charging-load","title":"EV charging load","text":"<p>Weipeng Zhan, Yuan Liao, Junjun Deng, Zhenpo Wang, &amp; Sonia Yeh. (2025). Large-scale empirical study of electric vehicle usage patterns and charging infrastructure needs. npj Sustainable Mobility and Transport. - Source: Nature Portfolio - Link: https://www.nature.com/articles/s44333-024-00023-3 - License: Nature Portfolio license - Description: Empirical data on electric vehicle usage patterns and charging infrastructure needs, including charging ratio profiles for passenger and freight vehicles used to generate time-series charging demand.</p>"},{"location":"misc/","title":"Good to know (misc info to be sorted later)","text":""},{"location":"misc/#time-resolution","title":"Time resolution","text":"<ul> <li>the time step can be chosen in discrete hours. The time step applies to all snapshots, it is not currently possible to have different time meshes for subsets of snapshots.</li> <li>the time step simply choses 1 from every . For example 4 hours will take 0:00,4:00,8:00,12:00,16:00,20:00</li> <li>the time step weight should be chosen such that  </li> <li>leap days are currently dropped</li> </ul> <p>A better time meshing should be developed at some point.</p>"},{"location":"misc/#links-vs-generators","title":"Links vs Generators","text":"<p>Conventional Gas plants, CHP &amp; BECSS are implemented as links. For example, OCGT consists of a generator with carrier gas that produces gas at the gas bus, with the fuel costs as marginal costs &amp; a link from the gas bus to AC. CHP gas follows a similar approach with heat as output in addition. </p> <p>Other thermal powerplants such as coal and gas+ccs are implemented as simple generators. The mixture of both leads to complications in the post-processing.</p>"},{"location":"misc/#reporting","title":"Reporting","text":""},{"location":"misc/#link-capacities","title":"Link capacities","text":"<p>In pypsa the capacities for a link <code>bus0-&gt;bus1</code> correspond to the max power at <code>bus0</code>. Reported nameplate capacities for AC, typically refer to the AC power. An option has been added to use the AC power in the statistics.</p> <p>Similarly, the pypsa lossy transport link implementation (e.g. H2 pipelines or HVDC) requires a forward and reversed link. The reversed link is typically fictitious and only added for convenience. </p>"},{"location":"misc/#carrier-groups","title":"Carrier groups","text":""},{"location":"model/","title":"Model description","text":"<p>The model minimizes system costs over the full 8760 hours of a year. Optionally, the resolution can be decreased (sample every n hours) to speed up calculations.</p>"},{"location":"model/#objective-function","title":"Objective function","text":"<p>The objective function of the model is to minimize the total yearly system costs:</p> <p> </p> <p>subject to - meeting energy demand for each region r and time t - transmission constraints between nodes (volume or cost expansion) - wind, solar and hydro availability for all r, t - respecting geographical potentials for renewables - emission budget (or pricing added to OPEX) - flexibility (optional load shedding, storage, etc)</p>"},{"location":"model/#renewable-resources","title":"Renewable resources","text":"<p>Renewable resources are computed using the <code>atlite</code> package which comptues potentials and availability series from ERA5 weather data (0.25x0.25 degree<sup>2</sup> cells).  Renewable supply is described at subprovincial resolution by default, with user-set binning of cells. Each bin is aggregated into a generator with it's own potential and availability curve.</p>"},{"location":"model/#emission-budgets-prices","title":"Emission budgets &amp; prices","text":"<p>There are two options to penalise emissions. </p> <ol> <li>A global constraint can be added (as reduction vs a reference year or budget). This is a hard bound.</li> <li>An emission price can be added.</li> </ol> <p>Currently, only CO2 is controlled but it is straightforward to add new GHGs or harmful emissions.</p>"},{"location":"model/#electricity-demand","title":"Electricity Demand","text":"<p>Historical hourly electricity demand for each province is scaled according to future demand projections/scenarios. The default historical data is NDRC data for the year 2018, which includes peak/valley daily demands and typical hourly profiles. It is possible to split projections by sectors.</p>"},{"location":"model/#remind-coupling","title":"REMIND coupling","text":"<p>In coupled runs, sectoral or total demands are provided by the REMIND IAM. Hourly profiles for each sector are scaled by the REMIND total</p>"},{"location":"model/#ev","title":"EV","text":"<p>In REMIND coupled mode, Electric Vehicle demand can be separated from the main AC demand. Details to be added.</p>"},{"location":"model/#heat-demand","title":"Heat demand","text":"<p>Heating demand is split into domestic hot water, centralised space heating and decentralised space heating. In a future release centralised heating will include demand from light industry. Space heating is considered to be seasonal and switched off outside a user-set heating window. Within this window, the profile is determined by the heating degree day (HDD) model. All HDD parameters can be set by the user in the configuration.</p> <ul> <li>centralised vs de-centralised fraction: is an exogenous input. By default, this data comes from the 2020 Statistics year book.</li> <li>SPH provincial demand: is an exogenous input. By default, this data comes from the 2020 Statistics year book.</li> <li>SPH national projections: from the PyPSA-China paper \"positive or constant\". To be reworked</li> <li>DHW: demand is exogenous. By default it is based on iea demand projections (An Energy Sector Roadmap to Carbon Neutrality in China, IEA, 2021). Demand is considered constant after 2060</li> <li>Totals: the China totals are scaled according to (exogenous) future demand projections.</li> </ul>"},{"location":"model/#combined-heat-and-power","title":"Combined heat and power","text":"<ul> <li>Coal CHP plants are considered to be extraction mode, with a flexible but constrained heat to power ratio. The ratio at time $t$ and for region $r$ is a model decision variable.</li> <li>OCGT CHP is considered to be </li> </ul>"},{"location":"model/#brownfield-data","title":"Brownfield data","text":"<ul> <li>brownfield capacities can be toggled on or off in the configuration (default: on)</li> <li>brownfield capacities are derived from the Global Energy Monitor Integrated Plant tracker data set. The included operation/pipeline statuses can be selected in the <code>global_energy_monitor.yaml</code> config. </li> <li>only plants that have been built before the plan_year and not retired by the plan year are added to the model</li> </ul> <p>Technologies available for Brownfield:</p> <ul> <li>OCGT</li> <li>CCGT</li> <li>CHP CCGT</li> <li>CHP coal</li> <li>coal power</li> <li>hydro (always on)</li> <li>nuclear</li> <li>PHS</li> <li>solar &amp; wind (considered to have filled the best available capacity factor)</li> <li>inter provincial transmission</li> <li>&amp; more</li> </ul>"},{"location":"model/#transmission","title":"Transmission","text":"<ul> <li>due to resolution limitations, interprovincial transmission is simplified to dispatchable lossy HVDC</li> </ul>"},{"location":"model/#operational-reserves","title":"Operational reserves","text":"<p>It is possible to toggle operational reserves in the configuration. The reserves are determined by a contigency + a fraction of the load and VRE dispatch at time $t$ </p>"},{"location":"model/#workflow","title":"Workflow","text":"<p>The workflow consists of gathering and preparing relevant data, formulating the problem as a PyPSA network object, minimising the system costs using a solver and post-processing the data. An example workflow can be found below.</p> <p></p> <p>Each operation is represented by a snakemake <code>rule</code>. The <code>.yaml</code> configuration options determine which snakemake <code>rule</code>s will be executed. This means that the workflow graph will depend on your activated sectors (eg <code>heat_coupling=False</code>) and whether coupling to an IAM is selected.</p>"},{"location":"model/#snakemake","title":"snakemake","text":"<p>The <code>snakefile</code> and included <code>*.smk</code> blocks contain the rule declarations. The rules are linked by their inputs and outputs. These links determine the execution order.  All execution options are controlled by the setting files, therefore the <code>snakefile</code> does not need to be edited unless you want to change data inputs or extend the model. </p> <p>Data input sources are currently either hardcoded <code>snakefile</code> or in the config <code>paths</code> section. </p>"},{"location":"model/#key-strenghts-limitations","title":"Key strenghts &amp; limitations","text":""},{"location":"model/#strengths","title":"Strengths","text":"<ul> <li>multi sector, fully open model</li> <li>highly configurable code, easy to extend</li> <li>easy to replace data</li> </ul>"},{"location":"model/#limitations","title":"Limitations","text":"<ul> <li>The full electrical network data is not open and HVAC is there not described. This means operational planning is coarse.</li> <li>Connections of renewable generators: HV connections to load centers are not currently included.</li> <li>Default projections for demand do not account for future regional variations.</li> </ul>"},{"location":"release-guide/","title":"Release Guide for PyPSA-China (PIK)","text":"<p>This guide is for maintainers who are preparing and publishing releases.</p>"},{"location":"release-guide/#pre-release-checklist","title":"Pre-Release Checklist","text":"<p>Before creating a new release, ensure the following:</p> <ul> <li>[ ] All planned features and bug fixes are merged</li> <li>[ ] Tests are passing on the main branch</li> <li>[ ] Documentation is up to date</li> <li>[ ] Version number is decided (following semantic versioning)</li> </ul>"},{"location":"release-guide/#semantic-versioning","title":"Semantic Versioning","text":"<p>This project follows Semantic Versioning 2.0.0:</p> <ul> <li>MAJOR (X.0.0): Incompatible API changes or major restructuring</li> <li>MINOR (X.Y.0): New features, backward-compatible</li> <li>PATCH (X.Y.Z): Bug fixes, backward-compatible</li> </ul>"},{"location":"release-guide/#release-process","title":"Release Process","text":""},{"location":"release-guide/#1-update-version-number","title":"1. Update Version Number","text":"<p>Edit <code>workflow/__init__.py</code>:</p> <pre><code>__version__ = \"X.Y.Z\"  # Update to new version\n</code></pre>"},{"location":"release-guide/#2-update-changelogmd","title":"2. Update CHANGELOG.md","text":"<p>Update the CHANGELOG with the new version:</p> <pre><code>## [X.Y.Z] - YYYY-MM-DD\n\n### Added\n- List new features\n\n### Changed\n- List changes to existing functionality\n\n### Fixed\n- List bug fixes\n\n### Deprecated\n- List deprecated features\n\n### Removed\n- List removed features\n</code></pre> <p>Add the version link at the bottom: <pre><code>[X.Y.Z]: https://github.com/pik-piam/PyPSA-China-PIK/releases/tag/vX.Y.Z\n</code></pre></p>"},{"location":"release-guide/#3-commit-changes","title":"3. Commit Changes","text":"<pre><code>git add workflow/__init__.py CHANGELOG.md\ngit commit -m \"chore: prepare release vX.Y.Z\"\ngit push origin main\n</code></pre>"},{"location":"release-guide/#4-create-release-tag","title":"4. Create Release Tag","text":""},{"location":"release-guide/#option-a-using-github-actions-recommended","title":"Option A: Using GitHub Actions (Recommended)","text":"<ol> <li>Go to the repository on GitHub</li> <li>Click on \"Actions\"</li> <li>Select \"Create Release Tag\" workflow</li> <li>Click \"Run workflow\"</li> <li>Enter the version number (e.g., <code>1.3.0</code> without the 'v' prefix)</li> <li>Choose whether to create the GitHub release immediately</li> <li>Click \"Run workflow\"</li> </ol> <p>This will: - Validate the version format - Check if the tag already exists - Verify the version matches <code>workflow/__init__.py</code> - Verify CHANGELOG.md has an entry for this version - Create and push the tag - Optionally trigger the release workflow</p>"},{"location":"release-guide/#option-b-manual-tag-creation","title":"Option B: Manual Tag Creation","text":"<pre><code># Create an annotated tag\ngit tag -a vX.Y.Z -m \"Release version X.Y.Z\"\n\n# Push the tag to GitHub\ngit push origin vX.Y.Z\n</code></pre>"},{"location":"release-guide/#5-automated-release-process","title":"5. Automated Release Process","text":"<p>Once the tag is pushed, the Release workflow automatically:</p> <ol> <li>Creates a GitHub Release:</li> <li>Extracts release notes from CHANGELOG.md</li> <li>Creates the release with appropriate metadata</li> <li> <p>Marks it as the latest release</p> </li> <li> <p>Deploys Versioned Documentation:</p> </li> <li>Builds documentation with MkDocs</li> <li>Deploys to GitHub Pages using mike</li> <li>Creates version vX.Y.Z with alias 'stable'</li> <li>Sets 'stable' as the default version (releases take precedence over development)</li> </ol>"},{"location":"release-guide/#6-verify-the-release","title":"6. Verify the Release","text":"<p>After the workflows complete:</p> <ol> <li>Check the GitHub Release:</li> <li>Visit https://github.com/pik-piam/PyPSA-China-PIK/releases</li> <li> <p>Verify the release appears with correct version and notes</p> </li> <li> <p>Check the Documentation:</p> </li> <li>Visit https://pik-piam.github.io/PyPSA-China-PIK/</li> <li>Verify the version selector shows the new version</li> <li> <p>Check that content is correct for the new version</p> </li> <li> <p>Verify Version Links:</p> </li> <li>Ensure all links in CHANGELOG.md work correctly</li> <li>Test the documentation version switcher</li> </ol>"},{"location":"release-guide/#7-post-release","title":"7. Post-Release","text":"<ol> <li>Announce the Release:</li> <li>Consider posting about the release to relevant channels</li> <li> <p>Update any external documentation or websites</p> </li> <li> <p>Prepare for Next Development Cycle:</p> </li> <li>Consider updating version to next development version (e.g., X.Y.Z+1-dev)</li> <li>Add new <code>[Unreleased]</code> section to CHANGELOG.md</li> </ol>"},{"location":"release-guide/#hotfix-releases","title":"Hotfix Releases","text":"<p>For urgent bug fixes on a released version:</p> <ol> <li> <p>Create a hotfix or patch branch from the release tag:    <pre><code>git checkout -b hotfix/vX.Y.Z+1 vX.Y.Z\n</code></pre></p> </li> <li> <p>Make the necessary fixes</p> </li> <li> <p>Update version to X.Y.Z+1 in <code>workflow/__init__.py</code></p> </li> <li> <p>Update CHANGELOG.md with the patch notes</p> </li> <li> <p>Commit and create a PR to main</p> </li> <li> <p>After merging, follow the normal release process for the patch version</p> </li> </ol>"},{"location":"release-guide/#pre-releases-and-release-candidates","title":"Pre-releases and Release Candidates","text":"<p>For testing before a major release:</p> <ol> <li> <p>Create a pre-release version: <code>vX.Y.Z-rc1</code></p> </li> <li> <p>Mark the GitHub release as a \"pre-release\" (checkbox option)</p> </li> <li> <p>Deploy documentation to a separate version:    <pre><code>mike deploy X.Y.Z-rc1 --push\n</code></pre></p> </li> </ol>"},{"location":"release-guide/#troubleshooting","title":"Troubleshooting","text":""},{"location":"release-guide/#tag-already-exists","title":"Tag Already Exists","text":"<p>If you need to move a tag:</p> <pre><code># Delete local tag\ngit tag -d vX.Y.Z\n\n# Delete remote tag\ngit push origin :refs/tags/vX.Y.Z\n\n# Create new tag\ngit tag -a vX.Y.Z -m \"Release version X.Y.Z\"\ngit push origin vX.Y.Z\n</code></pre> <p>Warning: Only do this if the release hasn't been widely distributed.</p>"},{"location":"release-guide/#release-workflow-failed","title":"Release Workflow Failed","text":"<ol> <li>Check the Actions tab for error details</li> <li>Fix the issue (usually in CHANGELOG.md or version files)</li> <li>You can re-run the workflow from the Actions tab</li> <li>Or manually trigger using workflow_dispatch</li> </ol>"},{"location":"release-guide/#documentation-not-deploying","title":"Documentation Not Deploying","text":"<ol> <li>Ensure mike is installed with version &gt;=2.0.0</li> <li>Check that gh-pages branch exists</li> <li>Verify GitHub Pages is enabled in repository settings</li> <li>Check workflow logs for specific errors</li> </ol>"},{"location":"release-guide/#version-management-with-mike","title":"Version Management with Mike","text":"<p>Mike maintains versioned documentation in the <code>gh-pages</code> branch.</p>"},{"location":"release-guide/#useful-mike-commands","title":"Useful Mike Commands","text":"<pre><code># List all deployed versions\nmike list\n\n# Set a version as default\nmike set-default VERSION --push\n\n# Delete a version\nmike delete VERSION --push\n\n# Deploy without updating any alias\nmike deploy VERSION --push\n\n# Deploy release with stable alias (done by release workflow)\nmike deploy --update-aliases VERSION stable --push\n\n# Deploy development with latest alias (done by docs workflow on develop)\nmike deploy --update-aliases dev latest --push\n</code></pre>"},{"location":"release-guide/#local-testing","title":"Local Testing","text":"<p>Test documentation builds locally before releasing:</p> <pre><code># Build and serve locally\nmkdocs serve\n\n# Build with mike locally (without pushing)\nmike deploy VERSION\n\n# Serve with mike's version selector\nmike serve\n</code></pre>"},{"location":"release-guide/#release-checklist-template","title":"Release Checklist Template","text":"<p>Copy this checklist for each release:</p> <pre><code>## Release vX.Y.Z Checklist\n\n- [ ] Tests passing on main\n- [ ] Documentation updated\n- [ ] Version updated in workflow/__init__.py\n- [ ] CHANGELOG.md updated with version and date\n- [ ] Changes committed and pushed to main\n- [ ] Tag created (via GitHub Actions or manually)\n- [ ] Release workflow completed successfully\n- [ ] GitHub release created and verified\n- [ ] Documentation deployed and accessible\n- [ ] Version selector shows new version\n- [ ] Announcement prepared (if applicable)\n- [ ] Next development cycle prepared\n</code></pre>"},{"location":"release-guide/#questions","title":"Questions?","text":"<p>If you have questions about the release process, please: - Review this guide thoroughly - Check previous releases for examples - Contact the repository maintainers - Open a discussion on GitHub</p>"},{"location":"releases/","title":"Release Notes","text":"<p>This page provides an overview of releases and how to work with different versions of PyPSA-China (PIK).</p>"},{"location":"releases/#current-release","title":"Current Release","text":"<p>The current stable release is v1.3.2. See the CHANGELOG for detailed information about changes in this and previous versions.</p>"},{"location":"releases/#documentation","title":"Documentation","text":"<p>You can view documentation for different versions using the version selector in the navigation bar. Available versions:</p> <ul> <li>stable - The most recent stable release (points to latest version tag, e.g., v1.3.2)</li> <li>latest - Latest development version from the develop branch</li> <li>vX.Y.Z - Specific release versions (e.g., v1.3.2)</li> </ul>"},{"location":"releases/#release-schedule","title":"Release Schedule","text":"<p>PyPSA-China (PIK) follows semantic versioning:</p> <ul> <li>Major releases (X.0.0) - Significant changes, may include breaking changes</li> <li>Minor releases (X.Y.0) - New features, backward compatible</li> <li>Patch releases (X.Y.Z) - Bug fixes, backward compatible</li> </ul>"},{"location":"releases/#installation-of-specific-versions","title":"Installation of Specific Versions","text":""},{"location":"releases/#latest-release","title":"Latest Release","text":"<pre><code># Clone the repository\ngit clone https://github.com/pik-piam/PyPSA-China-PIK.git\ncd PyPSA-China-PIK\ngit checkout v1.3.2\n</code></pre>"},{"location":"releases/#development-version","title":"Development Version","text":"<pre><code># Clone the repository\ngit clone https://github.com/pik-piam/PyPSA-China-PIK.git\ncd PyPSA-China-PIK\n# Stay on main branch for latest development version\n</code></pre>"},{"location":"releases/#specific-version","title":"Specific Version","text":"<pre><code># Clone the repository\ngit clone https://github.com/pik-piam/PyPSA-China-PIK.git\ncd PyPSA-China-PIK\ngit checkout vX.Y.Z  # Replace with desired version\n</code></pre>"},{"location":"releases/#upgrading","title":"Upgrading","text":"<p>When upgrading between versions, please review the CHANGELOG for:</p> <ol> <li>Breaking changes - May require updates to your configuration or scripts</li> <li>New features - Opportunities to enhance your workflows</li> <li>Bug fixes - Issues that have been resolved</li> <li>Deprecations - Features that will be removed in future versions</li> </ol>"},{"location":"releases/#upgrade-checklist","title":"Upgrade Checklist","text":"<ul> <li>[ ] Review CHANGELOG for your target version</li> <li>[ ] Backup your current work and results</li> <li>[ ] Update your environment: <code>conda env update -f environment.yaml</code></li> <li>[ ] Test with a small example before running large studies</li> <li>[ ] Update configuration files if needed</li> <li>[ ] Check for deprecated features in your code</li> </ul>"},{"location":"releases/#getting-help","title":"Getting Help","text":"<p>If you encounter issues after upgrading or have questions about a specific release:</p> <ol> <li>Check the documentation</li> <li>Review closed issues for similar problems</li> <li>Open a new issue</li> </ol>"},{"location":"releases/#release-history","title":"Release History","text":"<p>For the complete release history and detailed change information, see the CHANGELOG.</p>"},{"location":"releases/#version-132-2025-12-02","title":"Version 1.3.2 (2025-12-02)","text":"<p>First official release with: - Versioned documentation support - Comprehensive documentation - Automated release workflow - Official tagging system - Model description submitted to JOSS Paper</p>"},{"location":"releases/#staying-updated","title":"Staying Updated","text":"<p>To stay informed about new releases:</p> <ol> <li>Watch the repository on GitHub for release notifications</li> <li>Star the repository to show support and get updates</li> <li>Check the CHANGELOG periodically for upcoming features in Unreleased section</li> </ol>"},{"location":"releases/#contributing-to-releases","title":"Contributing to Releases","text":"<p>Interested in contributing to the next release? See our Contributing Guide for information on:</p> <ul> <li>How to propose features</li> <li>Development workflow</li> <li>Testing requirements</li> <li>Documentation standards</li> </ul>"},{"location":"installation/pik_hpc/","title":"HPC setup at PIK","text":""},{"location":"installation/pik_hpc/#gurobi","title":"Gurobi","text":"<p>[!NOTE] Set-up on the PIK cluster    Gurobi license activation from the compute nodes requires internet access. The workaround is an ssh tunnel to the login nodes, which can be set-up on the compute nodes with <pre><code>    # interactive session on the compute nodes\n    srun --qos=priority --pty bash\n    # key pair gen (here ed25518 but can be rsa)\n    ssh-keygen -t ed25519 -f ~/.ssh/id_rsa.cluster_internal_exchange -C \"$USER@cluster_internal_exchange\"\n    # leave the compute nodes\n    exit\n</code></pre></p> <p>You will then need to add the contents of the public key <code>~/.ssh/id_rsa.cluster_internal_exchange.pub</code> to your authorised <code>~/.ssh/authorized_keys</code>, eg. with <code>cat &lt;key_name&gt; &gt;&gt; authorized_keys</code></p> <p>TROUBLE SHOOTING you may have some issues with the solver tunnel failing (permission denied). One of these two steps should solve it option 1: name the exchange key <code>id_rsa</code>. option 2: copy the contents to authorized_keys from the compute nodes (from the ssh_dir <code>srun --qos=priority --pty bash; cat &lt;key_name&gt; &gt;&gt; authorized_keys;exit</code>)</p> <p>In addition you should have your .profile &amp; .bashrc setup as per https://gitlab.pik-potsdam.de/rse/rsewiki/-/wikis/Cluster-Access and add <code>module load anaconda/2024.10</code> (or latest) to it</p>"},{"location":"installation/pik_hpc/#hpc-profile","title":"HPC profile","text":"<p>The model comes with a pre-defined slurm profile that has been tailored to the PIK HPC. It can be found in <code>configs/pik_hpc_profile/</code> </p>"},{"location":"installation/quick_start/","title":"Quick Start","text":"<p>System requirements</p> <p>With the low-resolution settings, PyPSA-China-PIK will run on a local machine or laptop. Solving a full year at hourly resolution will require a high performance cluster or server with around 50GB of RAM - depending on your settings.</p>"},{"location":"installation/quick_start/#installation","title":"Installation","text":"<ol> <li> <p>Install the conda package manager </p> <p>It may be possible to run the workflow with a better manager such as <code>uv</code> but it will not work out of the box</p> Unix/MacOs/WSLWindows <p>Conda tips</p> <p>Conda is a rather large install with a GUI and features not required by PyPSA-China. You may prefer to use the lighter miniconda, which is our recommendation. </p> <p>You can check whether you already have conda with <code>which anaconda</code> or <code>which conda</code>. Newer condas have a faster dependcy solver - as the package is rather large we strongly recommend you update to <code>v&gt; 2024.10</code>.</p> <p>Conda tips</p> <p>Conda is a rather large install with a GUI and features not required by PyPSA-China. You may prefer to use the lighter miniconda, which is our recommendation</p> <p>You can check whether you already have conda with <code>where anaconda</code> or <code>where conda</code>. Newer condas have a faster dependcy solver - as the package is rather large we strongly recommend you update to <code>v&gt; 2024.9</code>.</p> </li> <li> <p>Setup the environment: This can take a long time </p> Unix/MacOS/WSL install dependencies<pre><code>cd &lt;workflow_location&gt;\nconda env create --file=workflow/envs/environment.yaml\n</code></pre> </li> <li> <p>Activate environment</p> Unix/MacOS/WSLWindows activate environment<pre><code>source activate pypsa-china\n</code></pre> activate environment<pre><code>conda activate pypsa-china\n</code></pre> </li> <li> <p>Fetch data</p> Automatic download (slow)Manual download (faster) <p>Commands</p> <pre><code>cd my_repository_clone\nconda activate pypsa-china\nsnakemake --configfile examples/fetch_data.yaml\n</code></pre> <p>if running locally you may need to add <code>--cores 1</code></p> <p>Sources</p> <p>Manually fetch the data (faster but requires copy-pasting): https://zenodo.org/records/17719794</p> <p>Destinations are in <code>resources/data</code> folders (rasters, cutouts, existing infrastructure).</p> <p>A back-up data source is the PyPSA-China paper https://zenodo.org/records/13987282</p> </li> <li> <p>Install a Solver: e.g. gurobi HiGHS or cplex. The current default configuration uses gurobi.</p> </li> <li>Run locally</li> </ol>"},{"location":"installation/quick_start/#testing-the-installation","title":"Testing the installation","text":"Unix/MacOS/WSLWindows Test install<pre><code>source activate pypsa-china\ncd &lt;workflow_location&gt;\npytest tests/integration\n</code></pre> Test install<pre><code>conda activate pypsa-china\ncd &lt;workflow_location&gt;\npytest tests/integration\n</code></pre>"},{"location":"reference/SUMMARY/","title":"SUMMARY","text":""},{"location":"reference/SUMMARY/#reference-guide","title":"Reference guide","text":"<p>This is the code documentation extracted from docstrings.</p>"},{"location":"reference/add_brownfield/","title":"Add brownfield","text":"<p>Functions for myopic pathway network generation snakemake rules</p> <p>Add assets from previous planning horizon network solution to network to solve for the current planning horizon.</p> <p>Usage: - use via a snakemake rule - debug: use as standalone with mock_snakemake (reads snakefile)</p>"},{"location":"reference/add_brownfield/#add_brownfield.add_brownfield","title":"<code>add_brownfield(n, n_p, year)</code>","text":"<p>Add paid for assets as p_nom to the current network</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>Network</code> <p>next network to prep &amp; optimize in the planning horizon</p> required <code>n_p</code> <code>Network</code> <p>previous optimized network</p> required <code>year</code> <code>int</code> <p>the planning year</p> required Source code in <code>workflow/scripts/add_brownfield.py</code> <pre><code>def add_brownfield(n: pypsa.Network, n_p: pypsa.Network, year: int):\n    \"\"\"Add paid for assets as p_nom to the current network\n\n    Args:\n        n (pypsa.Network): next network to prep &amp; optimize in the planning horizon\n        n_p (pypsa.Network): previous optimized network\n        year (int): the planning year\n    \"\"\"\n\n    logger.info(\"Adding brownfield\")\n    # electric transmission grid set optimised capacities of previous as minimum\n    n.lines.s_nom_min = n_p.lines.s_nom_opt\n    # dc_i = n.links[n.links.carrier==\"DC\"].index\n    # n.links.loc[dc_i, \"p_nom_min\"] = n_p.links.loc[dc_i, \"p_nom_opt\"]\n    # update links\n    n.links.loc[(n.links.length &gt; 0) &amp; (n.links.lifetime == np.inf), \"p_nom\"] = n_p.links.loc[\n        (n_p.links.carrier == \"AC\") &amp; (n_p.links.build_year == 0), \"p_nom_opt\"\n    ]\n    n.links.loc[(n.links.length &gt; 0) &amp; (n.links.lifetime == np.inf), \"p_nom_min\"] = n_p.links.loc[\n        (n_p.links.carrier == \"AC\") &amp; (n_p.links.build_year == 0), \"p_nom_opt\"\n    ]\n\n    if year == 2025:\n        add_build_year_to_new_assets(n_p, 2020)\n\n    for c in n_p.iterate_components([\"Link\", \"Generator\", \"Store\"]):\n        attr = \"e\" if c.name == \"Store\" else \"p\"\n\n        # first, remove generators, links and stores that track\n        # CO2 or global EU values since these are already in n\n        n_p.mremove(c.name, c.df.index[c.df.lifetime == np.inf])\n        # remove assets whose build_year + lifetime &lt; year\n        n_p.mremove(c.name, c.df.index[c.df.build_year + c.df.lifetime &lt; year])\n        # remove assets if their optimized nominal capacity is lower than a threshold\n        # since CHP heat Link is proportional to CHP electric Link, ensure threshold is compatible\n        chp_heat = c.df.index[(c.df[attr + \"_nom_extendable\"] &amp; c.df.index.str.contains(\"CHP\"))]\n\n        threshold = snakemake.config[\"existing_capacities\"][\"threshold_capacity\"]\n\n        if not chp_heat.empty:\n            threshold_chp_heat = (\n                threshold * c.df.loc[chp_heat].efficiency2 / c.df.loc[chp_heat].efficiency\n            )\n            n_p.mremove(\n                c.name,\n                chp_heat[c.df.loc[chp_heat, attr + \"_nom_opt\"] &lt; threshold_chp_heat],\n            )\n\n        n_p.mremove(\n            c.name,\n            c.df.index[\n                c.df[attr + \"_nom_extendable\"]\n                &amp; ~c.df.index.isin(chp_heat)\n                &amp; (c.df[attr + \"_nom_opt\"] &lt; threshold)\n            ],\n        )\n\n        # copy over assets but fix their capacity\n        c.df[attr + \"_nom\"] = c.df[attr + \"_nom_opt\"]\n        c.df[attr + \"_nom_extendable\"] = False\n        c.df[attr + \"_nom_max\"] = np.inf\n\n        n.import_components_from_dataframe(c.df, c.name)\n\n        # copy time-dependent\n        selection = n.component_attrs[c.name].type.str.contains(\"series\") &amp; n.component_attrs[\n            c.name\n        ].status.str.contains(\"Input\")\n\n        for tattr in n.component_attrs[c.name].index[selection]:\n            n.import_series_from_dataframe(c.pnl[tattr].set_index(n.snapshots), c.name, tattr)\n\n    for tech in [\"onwind\", \"offwind\", \"solar\"]:\n        ds_tech = xr.open_dataset(snakemake.input[\"profile_\" + tech])\n        p_nom_max_initial = ds_tech[\"p_nom_max\"].to_pandas()\n\n        if tech == \"offwind\":\n            for node in OFFSHORE_WIND_NODES:\n                n.generators.loc[\n                    (n.generators.bus == node)\n                    &amp; (n.generators.carrier == tech)\n                    &amp; (n.generators.build_year == year),\n                    \"p_nom_max\",\n                ] = (\n                    p_nom_max_initial[node]\n                    - n_p.generators[\n                        (n_p.generators.bus == node) &amp; (n_p.generators.carrier == tech)\n                    ].p_nom_opt.sum()\n                )\n        else:\n            for node in PROV_NAMES:\n                n.generators.loc[\n                    (n.generators.bus == node)\n                    &amp; (n.generators.carrier == tech)\n                    &amp; (n.generators.build_year == year),\n                    \"p_nom_max\",\n                ] = (\n                    p_nom_max_initial[node]\n                    - n_p.generators[\n                        (n_p.generators.bus == node) &amp; (n_p.generators.carrier == tech)\n                    ].p_nom_opt.sum()\n                )\n\n    n.generators.loc[(n.generators.p_nom_max &lt; 0), \"p_nom_max\"] = 0\n\n    # retrofit coal power plant with carbon capture\n    n.generators.loc[n.generators.carrier == \"coal power plant\", \"p_nom_extendable\"] = True\n    n.generators.loc[\n        n.generators.index.str.contains(\"retrofit\") &amp; ~n.generators.index.str.contains(str(year)),\n        \"p_nom_extendable\",\n    ] = False\n</code></pre>"},{"location":"reference/add_electricity/","title":"Add electricity","text":"<p>Misc collection of functions supporting network prep     still to be cleaned up</p>"},{"location":"reference/add_electricity/#add_electricity.add_missing_carriers","title":"<code>add_missing_carriers(n, carriers)</code>","text":"<p>Function to add missing carriers to the network without raising errors.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>Network</code> <p>the pypsa network object</p> required <code>carriers</code> <code>list | set</code> <p>a list of carriers that should be included</p> required Source code in <code>workflow/scripts/add_electricity.py</code> <pre><code>def add_missing_carriers(n: pypsa.Network, carriers: list | set) -&gt; None:\n    \"\"\"Function to add missing carriers to the network without raising errors.\n\n    Args:\n        n (pypsa.Network): the pypsa network object\n        carriers (list | set): a list of carriers that should be included\n    \"\"\"\n    missing_carriers = set(carriers) - set(n.carriers.index)\n    if len(missing_carriers) &gt; 0:\n        n.add(\"Carrier\", missing_carriers)\n</code></pre>"},{"location":"reference/add_electricity/#add_electricity.calculate_annuity","title":"<code>calculate_annuity(lifetime, discount_rate)</code>","text":"<p>Calculate the annuity factor for an asset with lifetime n years and discount rate of r, e.g. annuity(20, 0.05) * 20 = 1.6</p> <p>Parameters:</p> Name Type Description Default <code>lifetime</code> <code>int</code> <p>ecomic asset lifetime for discounting/NPV calc</p> required <code>discount_rate</code> <code>float</code> <p>the WACC</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>the annuity factor</p> Source code in <code>workflow/scripts/add_electricity.py</code> <pre><code>def calculate_annuity(lifetime: int, discount_rate: float) -&gt; float:\n    \"\"\"Calculate the annuity factor for an asset with lifetime n years and\n    discount rate of r, e.g. annuity(20, 0.05) * 20 = 1.6\n\n    Args:\n        lifetime (int): ecomic asset lifetime for discounting/NPV calc\n        discount_rate (float): the WACC\n\n    Returns:\n        float: the annuity factor\n    \"\"\"\n    r = discount_rate\n    n = lifetime\n\n    if isinstance(r, pd.Series):\n        if r.any() &lt; 0:\n            raise ValueError(\"Discount rate must be positive\")\n        if r.any() &lt; 0:\n            raise ValueError(\"Discount rate must be positive\")\n        return pd.Series(1 / n, index=r.index).where(r == 0, r / (1.0 - 1.0 / (1.0 + r) ** n))\n    elif r &lt; 0:\n        raise ValueError(\"Discount rate must be positive\")\n    elif r &lt; 0:\n        raise ValueError(\"Discount rate must be positive\")\n    elif r &gt; 0:\n        return r / (1.0 - 1.0 / (1.0 + r) ** n)\n    else:\n        return 1 / n\n</code></pre>"},{"location":"reference/add_electricity/#add_electricity.load_costs","title":"<code>load_costs(tech_costs, cost_config, elec_config, cost_year, n_years, econ_lifetime=40)</code>","text":"<p>Calculate the anualised capex costs and OM costs for the technologies based on the input data</p> <p>Parameters:</p> Name Type Description Default <code>tech_costs</code> <code>PathLike</code> <p>the csv containing the costs</p> required <code>cost_config</code> <code>dict</code> <p>the snakemake pypsa-china cost config</p> required <code>elec_config</code> <code>dict</code> <p>the snakemake pypsa-china electricity config</p> required <code>cost_year</code> <code>int</code> <p>the year for which the costs are retrived</p> required <code>n_years</code> <code>int</code> <p>the # of years represented by the snapshots/investment period</p> required <code>econ_lifetime</code> <code>int</code> <p>the max lifetime over which to discount. Defaults to 40.</p> <code>40</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: costs dataframe in [CURRENCY] per MW_ ... or per MWh_ ...</p> Source code in <code>workflow/scripts/add_electricity.py</code> <pre><code>def load_costs(\n    tech_costs: PathLike,\n    cost_config: dict,\n    elec_config: dict,\n    cost_year: int,\n    n_years: int,\n    econ_lifetime=40,\n) -&gt; pd.DataFrame:\n    \"\"\"Calculate the anualised capex costs and OM costs for the technologies based on the input data\n\n    Args:\n        tech_costs (PathLike): the csv containing the costs\n        cost_config (dict): the snakemake pypsa-china cost config\n        elec_config (dict): the snakemake pypsa-china electricity config\n        cost_year (int): the year for which the costs are retrived\n        n_years (int): the # of years represented by the snapshots/investment period\n        econ_lifetime (int, optional): the max lifetime over which to discount. Defaults to 40.\n\n    Returns:\n        pd.DataFrame: costs dataframe in [CURRENCY] per MW_ ... or per MWh_ ...\n    \"\"\"\n\n    # set all asset costs and other parameters\n    costs = pd.read_csv(tech_costs, index_col=list(range(3))).sort_index()\n    costs.fillna(\" \", inplace=True)\n    # correct units to MW and EUR\n    costs.loc[costs.unit.str.contains(\"/kW\"), \"value\"] *= 1e3\n    costs.loc[costs.unit.str.contains(\"USD\"), \"value\"] *= cost_config[\"USD2013_to_EUR2013\"]\n    costs.loc[costs.unit.str.contains(\"USD\"), \"value\"] *= cost_config[\"USD2013_to_EUR2013\"]\n\n    cost_year = float(cost_year)\n    costs = (\n        costs.loc[idx[:, cost_year, :], \"value\"]\n        .unstack(level=2)\n        .groupby(\"technology\")\n        .sum(min_count=1)\n    )\n\n    # TODO set default lifetime as option\n    if \"discount rate\" not in costs.columns:\n        costs.loc[:, \"discount rate\"] = cost_config[\"discountrate\"]\n    costs = costs.fillna(\n        {\n            \"CO2 intensity\": 0,\n            \"FOM\": 0,\n            \"VOM\": 0,\n            \"discount rate\": cost_config[\"discountrate\"],\n            \"efficiency\": 1,\n            \"hist_efficiency\": 1,  # represents brownfield efficiency state, only useful for links\n            \"fuel\": 0,\n            \"investment\": 0,\n            \"lifetime\": 25,\n        }\n    )\n\n    discount_period = costs[\"lifetime\"].apply(lambda x: min(x, econ_lifetime))\n    costs[\"capital_cost\"] = (\n        (calculate_annuity(discount_period, costs[\"discount rate\"]) + costs[\"FOM\"] / 100.0)\n        * costs[\"investment\"]\n        * n_years\n    )\n\n    costs.at[\"OCGT\", \"fuel\"] = costs.at[\"gas\", \"fuel\"]\n    costs.at[\"CCGT\", \"fuel\"] = costs.at[\"gas\", \"fuel\"]\n    costs.at[\"CCGT-CCS\", \"fuel\"] = costs.at[\"gas\", \"fuel\"]\n\n    costs[\"marginal_cost\"] = costs[\"VOM\"] + costs[\"fuel\"] / costs[\"efficiency\"]\n\n    costs = costs.rename(columns={\"CO2 intensity\": \"co2_emissions\"})\n\n    costs.at[\"OCGT\", \"co2_emissions\"] = costs.at[\"gas\", \"co2_emissions\"]\n    costs.at[\"CCGT\", \"co2_emissions\"] = costs.at[\"gas\", \"co2_emissions\"]\n\n    def costs_for_storage(store, link1, link2=None, max_hours=1.0):\n        capital_cost = link1[\"capital_cost\"] + max_hours * store[\"capital_cost\"]\n        if link2 is not None:\n            capital_cost += link2[\"capital_cost\"]\n        return pd.Series(dict(capital_cost=capital_cost, marginal_cost=0.0, co2_emissions=0.0))\n\n    max_hours = elec_config[\"max_hours\"]\n    costs.loc[\"battery\"] = costs_for_storage(\n        costs.loc[\"battery storage\"], costs.loc[\"battery inverter\"], max_hours=max_hours[\"battery\"]\n    )\n\n    for attr in (\"marginal_cost\", \"capital_cost\"):\n        overwrites = cost_config.get(attr)\n        overwrites = cost_config.get(attr)\n        if overwrites is not None:\n            overwrites = pd.Series(overwrites)\n            costs.loc[overwrites.index, attr] = overwrites\n\n    return costs\n</code></pre>"},{"location":"reference/add_electricity/#add_electricity.sanitize_carriers","title":"<code>sanitize_carriers(n, config)</code>","text":"<p>Sanitize the carrier information in a PyPSA Network object.</p> <p>The function ensures that all unique carrier names are present in the network's carriers attribute, and adds nice names and colors for each carrier according to the provided configuration dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>Network</code> <p>PyPSA Network object representing the electrical power system.</p> required <code>config</code> <code>dict</code> <p>A dictionary containing configuration information, specifically the    \"plotting\" key with \"nice_names\" and \"tech_colors\" keys for carriers.</p> required Source code in <code>workflow/scripts/add_electricity.py</code> <pre><code>def sanitize_carriers(n: pypsa.Network, config: dict) -&gt; None:\n    \"\"\"Sanitize the carrier information in a PyPSA Network object.\n\n    The function ensures that all unique carrier names are present in the network's\n    carriers attribute, and adds nice names and colors for each carrier according\n    to the provided configuration dictionary.\n\n    Args:\n        n (pypsa.Network): PyPSA Network object representing the electrical power system.\n        config (dict): A dictionary containing configuration information, specifically the\n               \"plotting\" key with \"nice_names\" and \"tech_colors\" keys for carriers.\n    \"\"\"\n    # update default nice names w user settings\n    nice_names = NICE_NAMES_DEFAULT.update(config[\"plotting\"].get(\"nice_names\", {}))\n    for c in n.iterate_components():\n        if \"carrier\" in c.df:\n            add_missing_carriers(n, c.df.carrier)\n\n    # sort the nice names to match carriers and fill missing with \"ugly\" names\n    carrier_i = n.carriers.index\n    nice_names = pd.Series(nice_names).reindex(carrier_i).fillna(carrier_i.to_series())\n    # replace empty nice names with nice names\n    n.carriers.nice_name.where(n.carriers.nice_name != \"\", nice_names, inplace=True)\n\n    # TODO make less messy, avoid using map\n    tech_colors = config[\"plotting\"][\"tech_colors\"]\n    colors = pd.Series(tech_colors).reindex(carrier_i)\n    # try to fill missing colors with tech_colors after renaming\n    missing_colors_i = colors[colors.isna()].index\n    colors[missing_colors_i] = missing_colors_i.map(lambda x: rename_techs(x, nice_names)).map(\n        tech_colors\n    )\n    if colors.isna().any():\n        missing_i = list(colors.index[colors.isna()])\n        logger.warning(f\"tech_colors for carriers {missing_i} not defined in config.\")\n    n.carriers[\"color\"] = n.carriers.color.where(n.carriers.color != \"\", colors)\n</code></pre>"},{"location":"reference/add_electricity/#add_electricity.update_transmission_costs","title":"<code>update_transmission_costs(n, costs, length_factor=1.0)</code>","text":"<p>Update transmission line and link capital costs based on length and cost data.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <p>PyPSA Network object</p> required <code>costs</code> <p>Cost data DataFrame containing capital costs for transmission technologies</p> required <code>length_factor</code> <code>float</code> <p>Factor to scale line lengths. Defaults to 1.0.</p> <code>1.0</code> <p>Returns:</p> Name Type Description <code>None</code> <p>Modifies network transmission costs in place</p> Source code in <code>workflow/scripts/add_electricity.py</code> <pre><code>def update_transmission_costs(n, costs, length_factor=1.0):\n    \"\"\"Update transmission line and link capital costs based on length and cost data.\n\n    Args:\n        n: PyPSA Network object\n        costs: Cost data DataFrame containing capital costs for transmission technologies\n        length_factor (float, optional): Factor to scale line lengths. Defaults to 1.0.\n\n    Returns:\n        None: Modifies network transmission costs in place\n    \"\"\"\n    # TODO: line length factor of lines is applied to lines and links.\n    # Separate the function to distinguish.\n\n    n.lines[\"capital_cost\"] = (\n        n.lines[\"length\"] * length_factor * costs.at[\"HVAC overhead\", \"capital_cost\"]\n    )\n\n    if n.links.empty:\n        return\n\n    dc_b = n.links.carrier == \"DC\"\n\n    # If there are no dc links, then the 'underwater_fraction' column\n    # may be missing. Therefore we have to return here.\n    if n.links.loc[dc_b].empty:\n        return\n\n    costs = (\n        n.links.loc[dc_b, \"length\"]\n        * length_factor\n        * (\n            (1.0 - n.links.loc[dc_b, \"underwater_fraction\"])\n            * costs.at[\"HVDC overhead\", \"capital_cost\"]\n            + n.links.loc[dc_b, \"underwater_fraction\"] * costs.at[\"HVDC submarine\", \"capital_cost\"]\n        )\n        + costs.at[\"HVDC inverter pair\", \"capital_cost\"]\n    )\n    n.links.loc[dc_b, \"capital_cost\"] = costs\n</code></pre>"},{"location":"reference/add_existing_baseyear/","title":"Add existing baseyear","text":"<p>Functions to add brownfield capacities to the network for a reference year - adds VREs per grade and corrects technical potential. Best available grade is chosen</p>"},{"location":"reference/add_existing_baseyear/#add_existing_baseyear.add_base_year","title":"<code>add_base_year(n, plan_year)</code>","text":"<p>Add base year information to newly built components in the network.</p> <p>Sets the 'build_year' attribute for all extendable generators and links in the network to the specified planning year. This is used to track when infrastructure is added to the system.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>Network</code> <p>The PyPSA network object to modify.</p> required <code>plan_year</code> <code>int</code> <p>The planning year to assign as the build year for new components.</p> required Note <p>This function modifies the network in-place by updating the 'build_year' attribute for components marked as extendable (p_nom_extendable==True).</p> Example <p>network = pypsa.Network() add_base_year(network, 2030)</p> Source code in <code>workflow/scripts/add_existing_baseyear.py</code> <pre><code>def add_base_year(n: pypsa.Network, plan_year: int):\n    \"\"\"Add base year information to newly built components in the network.\n\n    Sets the 'build_year' attribute for all extendable generators and links\n    in the network to the specified planning year. This is used to track\n    when infrastructure is added to the system.\n\n    Args:\n        n: The PyPSA network object to modify.\n        plan_year: The planning year to assign as the build year for new components.\n\n    Note:\n        This function modifies the network in-place by updating the 'build_year'\n        attribute for components marked as extendable (p_nom_extendable==True).\n\n    Example:\n        &gt;&gt;&gt; network = pypsa.Network()\n        &gt;&gt;&gt; add_base_year(network, 2030)\n        # All extendable components now have build_year = 2030\n    \"\"\"\n\n    for component in [\"links\", \"generators\"]:\n        comp = getattr(n, component)\n        mask = comp.query(\"p_nom_extendable==True\").index\n        comp.loc[mask, \"build_year\"] = plan_year\n</code></pre>"},{"location":"reference/add_existing_baseyear/#add_existing_baseyear.add_base_year--all-extendable-components-now-have-build_year-2030","title":"All extendable components now have build_year = 2030","text":""},{"location":"reference/add_existing_baseyear/#add_existing_baseyear.add_existing_vre_capacities","title":"<code>add_existing_vre_capacities(n, costs, vre_caps, config)</code>","text":"<p>Add existing VRE capacities to the network and distribute them by vre grade potential. Adapted from pypsa-eur but the VRE capacities are province resolved.</p> <p>NOTE that using this function requires adding the land-use constraint in solve_network so   that the existing capacities are subtracted from the available potential</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>Network</code> <p>the network</p> required <code>costs</code> <code>DataFrame</code> <p>costs of the technologies</p> required <code>vre_caps</code> <code>DataFrame</code> <p>existing brownfield VRE capacities in MW</p> required <code>config</code> <code>dict</code> <p>snakemake configuration dictionary</p> required <p>Returns:     pd.DataFrame: DataFrame with existing VRE capacities distributed by CF grade</p> Source code in <code>workflow/scripts/add_existing_baseyear.py</code> <pre><code>def add_existing_vre_capacities(\n    n: pypsa.Network,\n    costs: pd.DataFrame,\n    vre_caps: pd.DataFrame,\n    config: dict,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Add existing VRE capacities to the network and distribute them by vre grade potential.\n    Adapted from pypsa-eur but the VRE capacities are province resolved.\n\n    NOTE that using this function requires adding the land-use constraint in solve_network so\n      that the existing capacities are subtracted from the available potential\n\n    Args:\n        n (pypsa.Network): the network\n        costs (pd.DataFrame): costs of the technologies\n        vre_caps (pd.DataFrame): existing brownfield VRE capacities in MW\n        config (dict): snakemake configuration dictionary\n    Returns:\n        pd.DataFrame: DataFrame with existing VRE capacities distributed by CF grade\n\n    \"\"\"\n\n    tech_map = {\"solar\": \"PV\", \"onwind\": \"Onshore\", \"offwind-ac\": \"Offshore\", \"offwind\": \"Offshore\"}\n    tech_map = {k: tech_map[k] for k in tech_map if k in config[\"Techs\"][\"vre_techs\"]}\n\n    # historical data by tech, location and build year\n    grouped_vre = vre_caps.groupby([\"Tech\", \"bus\", \"DateIn\"]).Capacity.sum()\n    vre_df = grouped_vre.unstack().reset_index()\n    df_agg = pd.DataFrame()\n\n    # iterate over vre carriers\n    for carrier in tech_map:\n        df = vre_df[vre_df.Tech == carrier].drop(columns=[\"Tech\"])\n        df.set_index(\"bus\", inplace=True)\n        df.columns = df.columns.astype(int)\n\n        # fetch existing vre generators (n grade bins per node)\n        gen_i = n.generators.query(\"carrier == @carrier\").index\n        carrier_gens = n.generators.loc[gen_i]\n        res_capacities = []\n        # for each bus, distribute the vre capacities by grade potential - best first\n        for bus, group in carrier_gens.groupby(\"bus\"):\n            if bus not in df.index:\n                continue\n            res_capacities.append(distribute_vre_by_grade(group.p_nom_max, df.loc[bus]))\n\n        if res_capacities:\n            res_capacities = pd.concat(res_capacities, axis=0)\n\n            for year in df.columns:\n                for gen in res_capacities.index:\n                    bus_bin = re.sub(f\" {carrier}.*\", \"\", gen)\n                    bus, bin_id = bus_bin.rsplit(\" \", maxsplit=1)\n                    name = f\"{bus_bin} {carrier}-{int(year)}\"\n                    capacity = res_capacities.loc[gen, year]\n                    if capacity &gt; 0.0:\n                        cost_key = carrier.split(\"-\", maxsplit=1)[0]\n                        df_agg.at[name, \"Fueltype\"] = carrier\n                        df_agg.at[name, \"Capacity\"] = capacity\n                        df_agg.at[name, \"DateIn\"] = int(year)\n                        df_agg.at[name, \"grouping_year\"] = int(year)\n                        df_agg.at[name, \"lifetime\"] = costs.at[cost_key, \"lifetime\"]\n                        df_agg.at[name, \"DateOut\"] = year + costs.at[cost_key, \"lifetime\"] - 1\n                        df_agg.at[name, \"bus\"] = bus\n                        df_agg.at[name, \"resource_class\"] = bin_id\n\n    if df_agg.empty:\n        return df_agg\n\n    df_agg.loc[:, \"Tech\"] = df_agg.Fueltype\n    return df_agg\n</code></pre>"},{"location":"reference/add_existing_baseyear/#add_existing_baseyear.add_paid_off_capacity","title":"<code>add_paid_off_capacity(network, paid_off_caps, costs, cutoff=100)</code>","text":"<p>Add capacities that have been paid off to the network. This is intended for REMIND coupling, where (some of) the REMIND investments can be freely allocated to the optimal node. NB: an additional constraing is needed to ensure that the capacity is not exceeded.</p> <p>Parameters:</p> Name Type Description Default <code>network</code> <code>Network</code> <p>the network to which the capacities are added.</p> required <code>paid_off_caps</code> <code>DataFrame</code> <p>DataFrame with paid off capacities &amp; columns [tech_group, Capacity, techs]</p> required <code>costs</code> <code>DataFrame</code> <p>techno-economic data for the technologies</p> required <code>cutoff</code> <code>int</code> <p>minimum capacity to be considered. Defaults to 100 MW.</p> <code>100</code> Source code in <code>workflow/scripts/add_existing_baseyear.py</code> <pre><code>def add_paid_off_capacity(\n    network: pypsa.Network, paid_off_caps: pd.DataFrame, costs: pd.DataFrame, cutoff=100\n):\n    \"\"\"\n    Add capacities that have been paid off to the network. This is intended\n    for REMIND coupling, where (some of) the REMIND investments can be freely allocated\n    to the optimal node. NB: an additional constraing is needed to ensure that\n    the capacity is not exceeded.\n\n    Args:\n        network (pypsa.Network): the network to which the capacities are added.\n        paid_off_caps (pd.DataFrame): DataFrame with paid off capacities &amp; columns\n            [tech_group, Capacity, techs]\n        costs (pd.DataFrame): techno-economic data for the technologies\n        cutoff (int, optional): minimum capacity to be considered. Defaults to 100 MW.\n    \"\"\"\n\n    paid_off = paid_off_caps.reset_index()\n\n    # explode tech list per tech group (constraint will apply to group)\n    paid_off.techs = paid_off.techs.apply(to_list)\n    paid_off = paid_off.explode(\"techs\")\n    paid_off[\"carrier\"] = paid_off.techs.str.replace(\"'\", \"\")\n    paid_off.set_index(\"carrier\", inplace=True)\n    # clip small capacities\n    paid_off[\"p_nom_max\"] = paid_off.Capacity.apply(lambda x: 0 if x &lt; cutoff else x)\n    paid_off.drop(columns=[\"Capacity\", \"techs\"], inplace=True)\n    paid_off = paid_off.query(\"p_nom_max &gt; 0\")\n\n    component_settings = {\n        \"Generator\": {\n            \"join_col\": \"carrier\",\n            \"attrs_to_fix\": [\"p_min_pu\", \"p_max_pu\"],\n        },\n        \"Link\": {\n            \"join_col\": \"carrier\",\n            \"attrs_to_fix\": [\"p_min_pu\", \"p_max_pu\", \"efficiency\", \"efficiency2\"],\n        },\n        \"Store\": {\n            \"join_col\": \"carrier\",\n            \"attrs_to_fix\": [],\n        },\n    }\n\n    # TODO make a centralised setting or update cpl config\n    rename_carriers = {\"OCGT\": \"gas OCGT\", \"CCGT\": \"gas CCGT\"}\n    paid_off.rename(rename_carriers, inplace=True)\n\n    for component, settings in component_settings.items():\n        prefix = \"e\" if component == \"Store\" else \"p\"\n        paid_off_comp = paid_off.rename(columns={\"p_nom_max\": f\"{prefix}_nom_max\"})\n\n        # exclude brownfield capacities\n        df = getattr(network, component.lower() + \"s\").query(f\"{prefix}_nom_extendable == True\")\n        # join will add the tech_group and p_nom_max_rcl columns, used later for constraints\n        # rcl is legacy name from Adrian for region country limit\n        paid = df.join(paid_off_comp, on=[settings[\"join_col\"]], how=\"right\", rsuffix=\"_rcl\")\n        paid.dropna(subset=[f\"{prefix}_nom_max\", f\"{prefix}_nom_max_rcl\"], inplace=True)\n        paid = paid.loc[paid.index.dropna()]\n        if paid.empty:\n            continue\n\n        # REMIND cap is in output, PyPSA link in input\n        if component == \"Link\":\n            paid.loc[:, \"p_nom_max_rcl\"] /= paid.loc[:, \"efficiency\"]\n\n        paid.index += \"_paid_off\"\n        # set permissive options for the paid-off capacities (constraint per group added to model later)\n        paid[\"capital_cost\"] = 0\n        paid[f\"{prefix}_nom_min\"] = 0.0\n        paid[f\"{prefix}_nom\"] = 0.0\n        paid[f\"{prefix}_nom_max\"] = np.inf\n\n        # add to the network\n        network.add(component, paid.index, **paid)\n        # now add the dynamic attributes not carried over by n.add (per unit avail etc)\n        for missing_attr in settings[\"attrs_to_fix\"]:\n            df_t = getattr(network, component.lower() + \"s_t\")[missing_attr]\n            if not df_t.empty:\n                base_cols = [\n                    x for x in paid.index.str.replace(\"_paid_off\", \"\") if x in df_t.columns\n                ]\n                df_t.loc[:, pd.Index(base_cols) + \"_paid_off\"] = df_t[base_cols].rename(\n                    columns=lambda x: x + \"_paid_off\"\n                )\n\n    if \"biomass\" in paid_off.index and \"biomass\" not in network.generators.carrier.unique():\n        _add_paidoff_biomass(\n            network,\n            costs,\n            paid_off.loc[\"biomass\", \"p_nom_max\"],\n            tech_group=paid_off.loc[\"biomass\", \"tech_group\"],\n        )\n</code></pre>"},{"location":"reference/add_existing_baseyear/#add_existing_baseyear.add_power_capacities_installed_before_baseyear","title":"<code>add_power_capacities_installed_before_baseyear(n, costs, config, installed_capacities, eff_penalty_hist=0.0)</code>","text":"<p>Add existing power capacities to the network. Note: hydro dams brownfield handled by prepare_network</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>Network</code> <p>the network</p> required <code>costs</code> <code>DataFrame</code> <p>techno-economic data</p> required <code>config</code> <code>dict</code> <p>configuration dictionary</p> required <code>installed_capacities</code> <code>DataFrame</code> <p>installed capacities in MW</p> required <code>eff_penalty_hist</code> <code>float</code> <p>efficiency penalty for historical plants (1-x)*current</p> <code>0.0</code> Source code in <code>workflow/scripts/add_existing_baseyear.py</code> <pre><code>def add_power_capacities_installed_before_baseyear(\n    n: pypsa.Network,\n    costs: pd.DataFrame,\n    config: dict,\n    installed_capacities: pd.DataFrame,\n    eff_penalty_hist=0.0,\n):\n    \"\"\"\n    Add existing power capacities to the network.\n    Note: hydro dams brownfield handled by prepare_network\n\n    Args:\n        n (pypsa.Network): the network\n        costs (pd.DataFrame): techno-economic data\n        config (dict): configuration dictionary\n        installed_capacities (pd.DataFrame): installed capacities in MW\n        eff_penalty_hist (float): efficiency penalty for historical plants (1-x)*current\n    \"\"\"\n\n    logger.info(\"adding power capacities installed before baseyear\")\n\n    df = installed_capacities.copy()\n    # fix fuel type CHP order to match network\n    df[\"tech_clean\"] = df[\"Fueltype\"].str.replace(r\"^CHP (.+)$\", r\"\\1 CHP\", regex=True)\n    df[\"tech_clean\"] = df[\"tech_clean\"].str.replace(\"central \", \"\")\n    df[\"tech_clean\"] = df[\"tech_clean\"].str.replace(\"decentral \", \"\")\n\n    # TODO fix this based on config / centralise / other\n    carrier_map = {\n        \"coal\": \"coal\",\n        \"coal power plant\": \"coal\",\n        \"CHP coal\": \"CHP coal\",\n        \"coal CHP\": \"CHP coal\",\n        \"CHP gas\": \"CHP gas\",\n        \"gas CHP\": \"CHP gas\",\n        \"gas OCGT\": \"gas OCGT\",\n        \"gas CCGT\": \"gas CCGT\",\n        \"solar\": \"solar\",\n        \"solar thermal\": \"solar thermal\",\n        \"onwind\": \"onwind\",\n        \"offwind\": \"offwind\",\n        \"coal boiler\": \"coal boiler central\",\n        \"ground-sourced heat pump\": \"heat pump\",\n        \"ground heat pump\": \"heat pump\",\n        \"air heat pump\": \"heat pump\",\n        \"nuclear\": \"nuclear\",\n        \"PHS\": \"PHS\",\n        \"biomass\": \"biomass\",\n    }\n    costs_map = {\n        \"coal power plant\": \"coal\",\n        \"coal CHP\": \"central coal CHP\",\n        \"gas CHP\": \"central gas CHP\",\n        \"gas OCGT\": \"OCGT\",\n        \"gas CCGT\": \"CCGT\",\n        \"solar\": \"solar\",\n        \"solar thermal\": \"central solar thermal\",\n        \"onwind\": \"onwind\",\n        \"offwind\": \"offwind\",\n        \"coal boiler\": \"central coal boiler\",\n        \"heat pump\": \"central ground-sourced heat pump\",\n        \"ground-sourced heat pump\": \"central ground-sourced heat pump\",\n        \"nuclear\": \"nuclear\",\n        \"biomass\": \"biomass\",\n    }\n\n    # add techs that may have a direct match to the technoecon data\n    missing_techs = {k: k for k in df.Fueltype.unique() if k not in costs_map}\n    costs_map.update(missing_techs)\n\n    if \"resource_class\" not in df.columns:\n        df[\"resource_class\"] = \"\"\n    else:\n        df.resource_class.fillna(\"\", inplace=True)\n    logger.info(df.grouping_year.unique())\n    # TODO: exclude collapse of coal &amp; coal CHP IF CCS retrofitting is enabled\n    if config[\"existing_capacities\"].get(\"collapse_years\", False):\n        df.grouping_year = 1  # 0 is default\n    df.grouping_year = df.grouping_year.astype(int, errors=\"ignore\")\n\n    df_ = df.pivot_table(\n        index=[\"grouping_year\", \"tech_clean\", \"resource_class\"],\n        columns=\"bus\",\n        values=\"Capacity\",\n        aggfunc=\"sum\",\n    )\n\n    df_.fillna(0, inplace=True)\n\n    defined_carriers = n.carriers.index.unique().to_list()\n    vre_carriers = [\"solar\", \"onwind\", \"offwind\"]\n\n    # TODO do we really need to loop over the years? / so many things?\n    # something like df_.unstack(level=0) would be more efficient\n    for grouping_year, generator, resource_grade in df_.index:\n        build_year = 1 if grouping_year == \"brownwfield\" else grouping_year\n        logger.info(f\"Adding existing generator {generator} with year grp {grouping_year}\")\n        if carrier_map.get(generator, \"missing\") not in defined_carriers:\n            logger.warning(\n                f\"Carrier {carrier_map.get(generator, None)} for {generator} not defined in network\"\n                \"Consider adding to the CARRIER_MAP\"\n            )\n        elif costs_map.get(generator) is None:\n            raise ValueError(f\"{generator} not defined in technoecon map - check costs_map\")\n\n        # capacity is the capacity in MW at each node for this\n        capacity = df_.loc[grouping_year, generator, resource_grade]\n        if capacity.values.max() == 0:\n            continue\n        capacity = capacity[capacity &gt; config[\"existing_capacities\"][\"threshold_capacity\"]].dropna()\n        buses = capacity.index\n        # fix index for network.add (merge grade to name)\n        if resource_grade:\n            capacity.index += \" \" + resource_grade\n        capacity.index += \" \" + costs_map[generator]\n\n        costs_key = costs_map[generator]\n\n        if generator in vre_carriers:\n            mask = n.generators_t.p_max_pu.columns.map(n.generators.carrier) == generator\n            p_max_pu = n.generators_t.p_max_pu.loc[:, mask]\n            n.add(\n                \"Generator\",\n                capacity.index,\n                suffix=f\"-{grouping_year}\",\n                bus=buses,\n                carrier=carrier_map[generator],\n                p_nom=capacity,\n                p_nom_min=capacity,\n                p_nom_extendable=False,\n                marginal_cost=costs.at[costs_key, \"marginal_cost\"],\n                efficiency=costs.at[costs_key, \"efficiency\"],\n                p_max_pu=p_max_pu[capacity.index],\n                build_year=build_year,\n                lifetime=costs.at[costs_key, \"lifetime\"],\n                location=buses,\n            )\n\n        elif generator in [\"nuclear\", \"coal power plant\", \"biomass\", \"oil\"]:\n            n.add(\n                \"Generator\",\n                capacity.index,\n                suffix=\"-\" + str(grouping_year),\n                bus=buses,\n                carrier=carrier_map[generator],\n                p_nom=capacity,\n                p_nom_max=capacity,\n                p_nom_min=capacity,\n                p_nom_extendable=False,\n                p_max_pu=config[\"nuclear_reactors\"][\"p_max_pu\"] if generator == \"nuclear\" else 1,\n                p_min_pu=config[\"nuclear_reactors\"][\"p_min_pu\"] if generator == \"nuclear\" else 0,\n                marginal_cost=costs.at[costs_key, \"marginal_cost\"],\n                efficiency=costs.at[costs_key, \"efficiency\"] * (1 - eff_penalty_hist),\n                build_year=build_year,\n                lifetime=costs.at[costs_key, \"lifetime\"],\n                location=buses,\n            )\n\n        # TODO this does not add the carrier to the list\n        elif generator in [\"gas CCGT\", \"gas OCGT\"]:\n            bus0 = buses + \" gas\"\n            carrier_ = carrier_map[generator]\n            # ugly fix to register the carrier. Emissions for sub carrier are 0: they are accounted for at gas bus\n            n.carriers.loc[carrier_] = {\n                \"co2_emissions\": 0,\n                \"color\": config[\"plotting\"][\"tech_colors\"][carrier_],\n                \"nice_name\": config[\"plotting\"][\"nice_names\"][carrier_],\n                \"max_growth\": np.inf,\n                \"max_relative_growth\": 0,\n            }\n            # now add link - carrier should exist\n            n.add(\n                \"Link\",\n                capacity.index,\n                suffix=\"-\" + str(grouping_year),\n                bus0=bus0,\n                bus1=buses,\n                carrier=carrier_map[generator],\n                marginal_cost=costs.at[costs_key, \"efficiency\"]\n                * costs.at[costs_key, \"VOM\"],  # NB: VOM is per MWel\n                # NB: fixed cost is per MWel\n                p_nom=capacity / costs.at[costs_key, \"efficiency\"],\n                p_nom_min=capacity / costs.at[costs_key, \"efficiency\"],\n                p_nom_max=capacity / costs.at[costs_key, \"efficiency\"],\n                p_nom_extendable=False,\n                efficiency=costs.at[costs_key, \"efficiency\"] * (1 - eff_penalty_hist),\n                build_year=build_year,\n                lifetime=costs.at[costs_key, \"lifetime\"],\n                location=buses,\n            )\n        elif generator in [\n            \"solar thermal\",\n            \"CHP coal\",\n            \"CHP gas\",\n            \"heat pump\",\n            \"coal boiler\",\n        ] and not config.get(\"heat_coupling\", False):\n            logger.info(f\"Skipped {generator} because heat coupling is not activated\")\n\n        elif generator == \"solar thermal\":\n            p_max_pu = n.generators_t.p_max_pu[capacity.index]\n            p_max_pu.columns = capacity.index\n            n.add(\n                \"Generator\",\n                capacity.index,\n                suffix=f\"-{str(grouping_year)}\",\n                bus=buses + \" central heat\",\n                carrier=carrier_map[generator],\n                p_nom=capacity,\n                p_nom_min=capacity,\n                p_nom_max=capacity,\n                p_nom_extendable=False,\n                marginal_cost=costs.at[\"central \" + generator, \"marginal_cost\"],\n                p_max_pu=p_max_pu,\n                build_year=build_year,\n                lifetime=costs.at[\"central \" + generator, \"lifetime\"],\n                location=buses,\n            )\n\n        elif generator in [\"CHP coal\", \"coal CHP\"]:\n            bus0 = buses + \" coal fuel\"\n            n.add(\n                \"Link\",\n                capacity.index,\n                suffix=f\" generator-{str(grouping_year)}\",\n                bus0=bus0,\n                bus1=buses,\n                carrier=carrier_map[generator],\n                marginal_cost=costs.at[\"central coal CHP\", \"efficiency\"]\n                * costs.at[\"central coal CHP\", \"VOM\"],  # NB: VOM is per MWel\n                p_nom=capacity / costs.at[\"central coal CHP\", \"efficiency\"],\n                p_nom_min=capacity / costs.at[\"central coal CHP\", \"efficiency\"],\n                p_nom_max=capacity / costs.at[\"central coal CHP\", \"efficiency\"],\n                p_nom_extendable=False,\n                efficiency=costs.at[\"central coal CHP\", \"efficiency\"] * (1 - eff_penalty_hist),\n                heat_to_power=config[\"chp_parameters\"][\"coal\"][\"heat_to_power\"],\n                build_year=build_year,\n                lifetime=costs.at[\"central coal CHP\", \"lifetime\"],\n                location=buses,\n            )\n            # simplified treatment based on a decrease with c_v and a max htpwr ratio\n            htpr = config[\"chp_parameters\"][\"coal\"][\"heat_to_power\"]\n\n            n.add(\n                \"Link\",\n                capacity.index,\n                suffix=f\" boiler-{str(grouping_year)}\",\n                bus0=bus0,\n                bus1=buses + \" central heat\",\n                carrier=carrier_map[generator],\n                marginal_cost=costs.at[\"central coal CHP\", \"efficiency\"]\n                * costs.at[\"central coal CHP\", \"VOM\"],  # NB: VOM is per MWel\n                # p_max will be constrained by chp constraints\n                p_nom=capacity * htpr,\n                p_nom_min=capacity * htpr,\n                p_nom_max=capacity * htpr,\n                p_nom_extendable=False,\n                # total eff will be fixed by CHP constraints\n                efficiency=config[\"chp_parameters\"][\"coal\"][\"total_eff\"],\n                build_year=build_year,\n                lifetime=costs.at[\"central coal CHP\", \"lifetime\"],\n                location=buses,\n            )\n\n        elif generator in [\"CHP gas\", \"gas CHP\"]:\n            bus0 = buses + \" gas\"\n            n.add(\n                \"Link\",\n                capacity.index,\n                suffix=f\" generator-{str(grouping_year)}\",\n                bus0=bus0,\n                bus1=buses,\n                carrier=carrier_map[generator],\n                marginal_cost=costs.at[\"central gas CHP CC\", \"efficiency\"]\n                * costs.at[\"central gas CHP CC\", \"VOM\"],  # NB: VOM is per MWel\n                capital_cost=costs.at[\"central gas CHP CC\", \"efficiency\"]\n                * costs.at[\"central gas CHP CC\", \"capital_cost\"],  # NB: fixed cost is per MWel,\n                p_nom=capacity / costs.at[\"central gas CHP CC\", \"efficiency\"],\n                p_nom_min=capacity / costs.at[\"central gas CHP CC\", \"efficiency\"],\n                p_nom_extendable=False,\n                efficiency=costs.at[\"central gas CHP CC\", \"efficiency\"] * (1 - eff_penalty_hist),\n                heat_to_power=config[\"chp_parameters\"][\"gas\"][\"heat_to_power\"],\n                build_year=build_year,\n                lifetime=costs.at[\"central gas CHP CC\", \"lifetime\"],\n                location=buses,\n            )\n            # simplified treatment based on a decrease with c_v and a max htpwr ratio\n            htpr = config[\"chp_parameters\"][\"gas\"][\"heat_to_power\"]\n\n            n.add(\n                \"Link\",\n                capacity.index,\n                suffix=f\" boiler-{str(grouping_year)}\",\n                bus0=bus0,\n                bus1=buses + \" central heat\",\n                carrier=carrier_map[generator],\n                marginal_cost=costs.at[\"central gas CHP CC\", \"efficiency\"]\n                * costs.at[\"central gas CHP CC\", \"VOM\"],  # NB: VOM is per MWel\n                # pmax will be constrained by chp constraints\n                p_nom=capacity * htpr,\n                p_nom_min=capacity * htpr,\n                p_nom_max=capacity * htpr,\n                p_nom_extendable=False,\n                # will be constrained by chp constraints\n                efficiency=config[\"chp_parameters\"][\"gas\"][\"total_eff\"],\n                build_year=build_year,\n                lifetime=costs.at[\"central gas CHP CC\", \"lifetime\"],\n                location=buses,\n            )\n\n        elif generator.find(\"coal boiler\") != -1:\n            bus0 = buses + \" coal\"\n            cat = \"central\" if generator.find(\"decentral\") == -1 else \"decentral\"\n            n.add(\n                \"Link\",\n                capacity.index,\n                suffix=\"\" + cat + generator + \"-\" + str(grouping_year),\n                bus0=bus0,\n                bus1=capacity.index + cat + \"heat\",\n                carrier=carrier_map[generator],\n                marginal_cost=costs.at[cat.lstrip() + generator, \"efficiency\"]\n                * costs.at[cat.lstrip() + generator, \"VOM\"],\n                capital_cost=costs.at[cat.lstrip() + generator, \"efficiency\"]\n                * costs.at[cat.lstrip() + generator, \"capital_cost\"],\n                p_nom=capacity / costs.at[cat.lstrip() + generator, \"efficiency\"],\n                p_nom_min=capacity / costs.at[cat.lstrip() + generator, \"efficiency\"],\n                p_nom_max=capacity / costs.at[cat.lstrip() + generator, \"efficiency\"],\n                p_nom_extendable=False,\n                efficiency=costs.at[cat.lstrip() + generator, \"efficiency\"],\n                build_year=build_year,\n                lifetime=costs.at[cat.lstrip() + generator, \"lifetime\"],\n                location=buses,\n            )\n\n        # TODO fix read operation in func, fix snakemake in function, make air pumps?\n        elif generator == \"heat pump\":\n            # TODO separate the read operation from the add operation\n            with pd.HDFStore(snakemake.input.cop_name, mode=\"r\") as store:\n                gshp_cop = store[\"gshp_cop_profiles\"]\n                gshp_cop.index = gshp_cop.index.tz_localize(None)\n                gshp_cop = shift_profile_to_planning_year(\n                    gshp_cop, snakemake.wildcards.planning_horizons\n                )\n                gshp_cop = gshp_cop.loc[n.snapshots]\n            n.add(\n                \"Link\",\n                capacity.index,\n                suffix=\"-\" + str(grouping_year),\n                bus0=capacity.index,\n                bus1=capacity.index + \" central heat\",\n                carrier=\"heat pump\",\n                efficiency=(\n                    gshp_cop[capacity.index]\n                    if config[\"time_dep_hp_cop\"]\n                    else costs.at[\"decentral ground-sourced heat pump\", \"efficiency\"]\n                ),\n                capital_cost=costs.at[\"decentral ground-sourced heat pump\", \"efficiency\"]\n                * costs.at[\"decentral ground-sourced heat pump\", \"capital_cost\"],\n                marginal_cost=costs.at[\"decentral ground-sourced heat pump\", \"efficiency\"]\n                * costs.at[\"decentral ground-sourced heat pump\", \"marginal_cost\"],\n                p_nom=capacity / costs.at[\"decentral ground-sourced heat pump\", \"efficiency\"],\n                p_nom_min=capacity / costs.at[\"decentral ground-sourced heat pump\", \"efficiency\"],\n                p_nom_max=capacity / costs.at[\"decentral ground-sourced heat pump\", \"efficiency\"],\n                p_nom_extendable=False,\n                build_year=build_year,\n                lifetime=costs.at[\"decentral ground-sourced heat pump\", \"lifetime\"],\n                location=buses,\n            )\n\n        elif generator == \"PHS\":\n            # pure pumped hydro storage, fixed, 6h energy by default, no inflow\n            n.add(\n                \"StorageUnit\",\n                capacity.index,\n                suffix=\"-\" + str(grouping_year),\n                bus=buses,\n                carrier=\"PHS\",\n                p_nom=capacity,\n                p_nom_min=capacity,\n                p_nom_max=capacity,\n                p_nom_extendable=False,\n                max_hours=config[\"hydro\"][\"PHS_max_hours\"],\n                efficiency_store=np.sqrt(costs.at[\"PHS\", \"efficiency\"]),\n                efficiency_dispatch=np.sqrt(costs.at[\"PHS\", \"efficiency\"]),\n                cyclic_state_of_charge=True,\n                marginal_cost=0.0,\n                location=buses,\n            )\n\n        else:\n            logger.warning(\n                f\"Skipped existing capacitity for '{generator}'\"\n                + \" - tech not implemented as existing capacity\"\n            )\n\n    return\n</code></pre>"},{"location":"reference/add_existing_baseyear/#add_existing_baseyear.distribute_vre_by_grade","title":"<code>distribute_vre_by_grade(cap_by_year, grade_capacities)</code>","text":"<p>Distribute VRE capacities by grade, ensuring potentials are respected and prioritizing better grades first</p> <p>Allocates variable renewable energy (VRE) capacity additions across different resource quality grades. The algorithm preferentially uses higher-quality grades before moving to lower-quality ones, implementing a \"fill-up\" strategy.</p> <p>Parameters:</p> Name Type Description Default <code>cap_by_year</code> <code>Series</code> <p>Annual VRE capacity additions indexed by year. Values represent the total capacity to be added in each year (MW or GW).</p> required <code>grade_capacities</code> <code>Series</code> <p>Available capacity potential by resource grade for a bus, indexed by grade identifier. Higher-quality grades should have lower indices for proper prioritization.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with distributed capacities where: - Rows are indexed by years (from cap_by_year) - Columns are indexed by grades (from grade_capacities) - Values represent allocated capacity for each year-grade combination</p> Example <p>cap_additions = pd.Series([100, 200], index=[2020, 2030]) grade_potentials = pd.Series([50, 75, 100], index=['grade_1', 'grade_2', 'grade_3']) result = distribute_vre_by_grade(cap_additions, grade_potentials) print(result.sum(axis=1))  # Should match original yearly totals</p> Note <p>The function assumes grade_capacities are ordered with best grades first. If total demand exceeds available capacity, the algorithm allocates as much as possible following the grade priority.</p> Source code in <code>workflow/scripts/add_existing_baseyear.py</code> <pre><code>def distribute_vre_by_grade(cap_by_year: pd.Series, grade_capacities: pd.Series) -&gt; pd.DataFrame:\n    \"\"\"Distribute VRE capacities by grade, ensuring potentials are respected and prioritizing better grades first\n\n    Allocates variable renewable energy (VRE) capacity additions across different\n    resource quality grades. The algorithm preferentially uses higher-quality\n    grades before moving to lower-quality ones, implementing a \"fill-up\" strategy.\n\n    Args:\n        cap_by_year (pd.Series): Annual VRE capacity additions indexed by year. Values represent\n            the total capacity to be added in each year (MW or GW).\n        grade_capacities (pd.Series): Available capacity potential by resource grade for a bus,\n            indexed by grade identifier. Higher-quality grades should have lower indices for\n            proper prioritization.\n\n    Returns:\n        DataFrame with distributed capacities where:\n            - Rows are indexed by years (from cap_by_year)\n            - Columns are indexed by grades (from grade_capacities)\n            - Values represent allocated capacity for each year-grade combination\n\n    Example:\n        &gt;&gt;&gt; cap_additions = pd.Series([100, 200], index=[2020, 2030])\n        &gt;&gt;&gt; grade_potentials = pd.Series([50, 75, 100], index=['grade_1', 'grade_2', 'grade_3'])\n        &gt;&gt;&gt; result = distribute_vre_by_grade(cap_additions, grade_potentials)\n        &gt;&gt;&gt; print(result.sum(axis=1))  # Should match original yearly totals\n\n    Note:\n        The function assumes grade_capacities are ordered with best grades first.\n        If total demand exceeds available capacity, the algorithm allocates as much\n        as possible following the grade priority.\n    \"\"\"\n\n    availability = cap_by_year.sort_index(ascending=False)\n    to_distribute = grade_capacities.fillna(0).sort_index()\n    n_years = len(to_distribute)\n    n_sources = len(availability)\n\n    # To store allocation per year per source (shape: sources x years)\n    allocation = np.zeros((n_sources, n_years), dtype=int)\n    remaining = availability.values\n\n    for j in range(n_years):\n        needed = to_distribute.values[j]\n        cumsum = np.cumsum(remaining)\n        used_up = cumsum &lt; needed\n        cutoff = np.argmax(cumsum &gt;= needed)\n\n        allocation[used_up, j] = remaining[used_up]\n\n        if needed &gt; (cumsum[cutoff - 1] if cutoff &gt; 0 else 0):\n            allocation[cutoff, j] = needed - (cumsum[cutoff - 1] if cutoff &gt; 0 else 0)\n\n        # Subtract what was used from availability\n        remaining -= allocation[:, j]\n\n    return pd.DataFrame(data=allocation, columns=grade_capacities.index, index=availability.index)\n</code></pre>"},{"location":"reference/add_existing_baseyear/#add_existing_baseyear.filter_brownfield_capacities","title":"<code>filter_brownfield_capacities(existing_df, plan_year)</code>","text":"<p>Filter brownfield capacities to remove retired/not yet built plants . Parameters:     existing_df (pd.DataFrame): DataFrame containing asset information with at least the columns 'DateOut', 'DateIn', 'grouping_year', and 'cluster_bus'.     plan_year (int): The year modelled year/horizon Returns:     pd.DataFrame: The filtered and updated DataFrame.</p> Source code in <code>workflow/scripts/add_existing_baseyear.py</code> <pre><code>def filter_brownfield_capacities(existing_df: pd.DataFrame, plan_year: int) -&gt; pd.DataFrame:\n    \"\"\"\n    Filter brownfield capacities to remove retired/not yet built plants .\n    Parameters:\n        existing_df (pd.DataFrame): DataFrame containing asset information with at least the columns 'DateOut', 'DateIn', 'grouping_year', and 'cluster_bus'.\n        plan_year (int): The year modelled year/horizon\n    Returns:\n        pd.DataFrame: The filtered and updated DataFrame.\n    \"\"\"\n\n    # drop assets which are already phased out / decommissioned\n    phased_out = existing_df[existing_df[\"DateOut\"] &lt; plan_year].index\n    existing_df.drop(phased_out, inplace=True)\n\n    to_drop = existing_df[existing_df.DateIn &gt; plan_year].index\n    existing_df.drop(to_drop, inplace=True)\n\n    existing_df.rename(columns={\"cluster_bus\": \"bus\"}, inplace=True)\n\n    return existing_df\n</code></pre>"},{"location":"reference/add_sectors/","title":"Add sectors","text":"<p>Add basic EV loads and chargers to a PyPSA network.</p>"},{"location":"reference/add_sectors/#add_sectors.add_carrier_if_missing","title":"<code>add_carrier_if_missing(n, carrier_name)</code>","text":"<p>Add a carrier to the network if it doesn't already exist.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>Network</code> <p>PyPSA network to modify.</p> required <code>carrier_name</code> <code>str</code> <p>Name of the carrier to add.</p> required Source code in <code>workflow/scripts/add_sectors.py</code> <pre><code>def add_carrier_if_missing(n: pypsa.Network, carrier_name: str):\n    \"\"\"Add a carrier to the network if it doesn't already exist.\n\n    Args:\n        n (pypsa.Network): PyPSA network to modify.\n        carrier_name (str): Name of the carrier to add.\n    \"\"\"\n    if carrier_name not in n.carriers.index:\n        n.add(\"Carrier\", carrier_name)\n        logger.debug(\"Carrier '%s' added to network.\", carrier_name)\n</code></pre>"},{"location":"reference/add_sectors/#add_sectors.attach_simple_ev","title":"<code>attach_simple_ev(n, p_set, nodes, options, ev_type)</code>","text":"<p>Attach electric vehicle demand and charging links to the PyPSA network.</p> <p>This function implements an EV demand model with a fixed charging profile. For each node, it creates:   \u2022 an EV load bus,   \u2022 a Load component representing the EV charging demand time series,   \u2022 and a charger Link connecting the AC bus to the EV load bus.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>Network</code> <p>Network to modify in place.</p> required <code>p_set</code> <code>DataFrame</code> <p>EV charging demand time series (MW), indexed by snapshots and with nodes as columns.</p> required <code>nodes</code> <code>Index</code> <p>AC buses where EV components are added.</p> required <code>options</code> <code>dict</code> <p>EV configuration parameters, including: - annual_consumption (float): annual energy demand per vehicle (MWh/year) - charge_rate (float): charger power rating per vehicle (MW) - share_charger (float): fraction of vehicles that can charge simultaneously</p> required <code>ev_type</code> <code>str</code> <p>EV category for component naming (e.g., \"passenger\", \"freight\").</p> required <p>Returns:</p> Name Type Description <code>None</code> <p>Modifies the network in place.</p> Source code in <code>workflow/scripts/add_sectors.py</code> <pre><code>def attach_simple_ev(\n    n: pypsa.Network, p_set: pd.DataFrame, nodes: pd.Index, options: dict, ev_type: str\n):\n    \"\"\"\n    Attach electric vehicle demand and charging links to the PyPSA network.\n\n    This function implements an EV demand model with a fixed charging profile.  \n    For each node, it creates:\n      \u2022 an EV load bus,\n      \u2022 a Load component representing the EV charging demand time series,\n      \u2022 and a charger Link connecting the AC bus to the EV load bus.\n\n    Args:\n        n (pypsa.Network): Network to modify in place.\n        p_set (pd.DataFrame): EV charging demand time series (MW), indexed by\n            snapshots and with nodes as columns.\n        nodes (pd.Index): AC buses where EV components are added.\n        options (dict): EV configuration parameters, including:\n            - annual_consumption (float): annual energy demand per vehicle (MWh/year)\n            - charge_rate (float): charger power rating per vehicle (MW)\n            - share_charger (float): fraction of vehicles that can charge simultaneously\n        ev_type (str): EV category for component naming (e.g., \"passenger\", \"freight\").\n\n    Returns:\n        None: Modifies the network in place.\n    \"\"\"\n\n    total_energy = p_set.sum().sum()\n    total_number_evs = total_energy / max(options[\"annual_consumption\"], 1e-6)\n    node_ratio = p_set.sum() / max(total_energy, 1e-6)\n    number_evs = node_ratio * total_number_evs\n    charge_power = (\n        number_evs * options[\"charge_rate\"] * options[\"share_charger\"]\n    ).clip(lower=0.001)\n\n    logger.info(\"EV %s: %s vehicles (direct charging)\", ev_type, f\"{int(total_number_evs):,}\")\n    logger.debug(\n        \"EV %s: total energy %.2f MWh, annual consumption %.3f MWh/veh\",\n        ev_type,\n        total_energy,\n        options[\"annual_consumption\"],\n    )\n    logger.debug(\n        \"EV %s: sample node power (MW)\\n%s\",\n        ev_type,\n        charge_power.head().to_string(),\n    )\n\n    ev_load_carrier = f\"EV_{ev_type}_load\"\n    add_carrier_if_missing(n, ev_load_carrier)\n    ev_load_bus = nodes + f\" EV_{ev_type}_load\"\n    n.add(\"Bus\", nodes, suffix=f\" EV_{ev_type}_load\", carrier=ev_load_carrier)\n\n    n.add(\n        \"Load\",\n        nodes,\n        suffix=f\" EV_{ev_type}_load\",\n        bus=ev_load_bus,\n        carrier=ev_load_carrier,\n        p_set=p_set.loc[n.snapshots, nodes],\n    )\n\n    charger_carrier = f\"EV_{ev_type}_charger\"\n    add_carrier_if_missing(n, charger_carrier)\n    n.add(\n        \"Link\",\n        nodes,\n        suffix=f\" EV_{ev_type}_charger\",\n        bus0=nodes,\n        bus1=ev_load_bus,\n        carrier=charger_carrier,\n        p_nom=charge_power,\n        efficiency=1.0,\n    )\n</code></pre>"},{"location":"reference/build_biomass_potential/","title":"Build biomass potential","text":"<p>Build biomass potential data for energy system modeling.</p> <p>This module processes biomass resource data and calculates biomass potential for different regions and technologies in the PyPSA-China model.</p>"},{"location":"reference/build_biomass_potential/#build_biomass_potential.build_biomass_potential_xing","title":"<code>build_biomass_potential_xing(biomass_potentials_path)</code>","text":"<p>Build potential from Xing et al. https://doi.org/10.1038/s41467-021-23282-x</p> <p>Parameters:</p> Name Type Description Default <code>biomass_potentials_path</code> <code>PathLike</code> <p>the path to the Xing SI data (xlsx).</p> required Source code in <code>workflow/scripts/build_biomass_potential.py</code> <pre><code>def build_biomass_potential_xing(biomass_potentials_path: PathLike):\n    \"\"\"Build potential from Xing et al. https://doi.org/10.1038/s41467-021-23282-x\n\n    Args:\n        biomass_potentials_path (PathLike, optional): the path to the Xing SI data (xlsx).\n    \"\"\"\n\n    df = read_xing_si_data(biomass_potentials_path)\n\n    # select only relevant part of potential\n    df = df[df.columns[df.columns.str.contains(\"Agricultural residues burnt as waste\")]].sum(axis=1)\n\n    # convert t biomass yr-1 to MWh, heat content is from paper reference 92\n    heat_content = 19  # GJ (t biomass\u22121)\n    heat_content *= 1000 / 3600  # GJ/t -&gt; MWh\n    df = df * heat_content\n\n    return df\n</code></pre>"},{"location":"reference/build_biomass_potential/#build_biomass_potential.estimate_co2_intensity_xing","title":"<code>estimate_co2_intensity_xing()</code>","text":"<p>Estimate the biomass Co2 intensity from the Xing paper</p> <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>the biomass co2 intensity in t/MWhth</p> Source code in <code>workflow/scripts/build_biomass_potential.py</code> <pre><code>def estimate_co2_intensity_xing() -&gt; float:\n    \"\"\"Estimate the biomass Co2 intensity from the Xing paper\n\n    Returns:\n        float: the biomass co2 intensity in t/MWhth\n    \"\"\"\n\n    biomass_potential_tot = 3.04  # Gt\n    embodied_co2_tot = 5.24  # Gt\n    heat_content = 19 * 1000 / 3600  # GJ/t -&gt; MWh_th/t\n    unit_co2 = embodied_co2_tot / biomass_potential_tot  # t CO2/t biomass\n    co2_intens = unit_co2 / heat_content  # t CO2/MWh_th\n\n    return co2_intens\n</code></pre>"},{"location":"reference/build_biomass_potential/#build_biomass_potential.read_xing_si_data","title":"<code>read_xing_si_data(biomass_potentials_path)</code>","text":"<p>Read and prepare the xing SI data</p> <p>Parameters:</p> Name Type Description Default <code>biomass_potentials_path</code> <code>PathLike</code> <p>the path to the Xing SI data (xlsx).</p> required Source code in <code>workflow/scripts/build_biomass_potential.py</code> <pre><code>def read_xing_si_data(biomass_potentials_path: PathLike):\n    \"\"\"Read and prepare the xing SI data\n\n    Args:\n        biomass_potentials_path (PathLike): the path to the Xing SI data (xlsx).\n    \"\"\"\n    # data is indexed by province and county\n    df = pd.read_excel(biomass_potentials_path, sheet_name=\"supplementary data 1\")\n    df = df.groupby(\"Province name\").sum()\n\n    df = df.rename(index={\"Inner-Monglia\": \"InnerMongolia\", \"Anhui \": \"Anhui\"})\n    df = df.add_suffix(\" biomass\")\n\n    return df\n</code></pre>"},{"location":"reference/build_cop_profiles/","title":"Build cop profiles","text":"<p>Snakemake rule script to calculate the heat pump coefficient of performance with atlite</p>"},{"location":"reference/build_cop_profiles/#build_cop_profiles.build_cop_profiles","title":"<code>build_cop_profiles(pop_map, cutout, temperature, output_path)</code>","text":"<p>Build COP time profiles with atlite and write outputs to output_path as hf5</p> <p>Parameters:</p> Name Type Description Default <code>pop_map</code> <code>DataFrame</code> <p>the population map (node resolution)</p> required <code>cutout</code> <code>cutout</code> <p>the atlite cutout (weather data)</p> required <code>temperature</code> <code>DataFrame</code> <p>the temperature data (node resolution)</p> required <code>output_path</code> <code>PathLike</code> <p>the path to write the output to as hdf5</p> required Source code in <code>workflow/scripts/build_cop_profiles.py</code> <pre><code>def build_cop_profiles(\n        pop_map: pd.DataFrame,\n        cutout: atlite.Cutout,\n        temperature: pd.DataFrame,\n        output_path: os.PathLike):\n    \"\"\"Build COP time profiles with atlite and write outputs to output_path as hf5\n\n    Args:\n        pop_map (pd.DataFrame): the population map (node resolution)\n        cutout (atlite.cutout): the atlite cutout (weather data)\n        temperature (pd.DataFrame): the temperature data (node resolution)\n        output_path (os.PathLike): the path to write the output to as hdf5\n    \"\"\"\n\n    pop_matrix = sp.sparse.csr_matrix(pop_map.T)\n    index = pop_map.columns\n    index.name = \"provinces\"\n\n    soil_temp = cutout.soil_temperature(matrix=pop_matrix, index=index)\n    soil_temp[\"time\"] = (\n        pd.DatetimeIndex(soil_temp[\"time\"].values, tz=\"UTC\")\n        .tz_convert(TIMEZONE)\n        .tz_localize(None)\n        .values\n    )\n\n\n    source_T = temperature\n    source_soil_T = soil_temp.to_pandas().divide(pop_map.sum())\n\n    # quadratic regression based on Staffell et al. (2012)\n    # https://doi.org/10.1039/C2EE22653G\n\n    sink_T = 55.0  # Based on DTU / large area radiators\n\n    delta_T = sink_T - source_T\n\n    # TODO make this user set and document\n    # For ASHP\n    def ashp_cop(d):\n        return 6.81 - 0.121 * d + 0.000630 * d**2\n\n    cop = ashp_cop(delta_T)\n\n    delta_soil_T = sink_T - source_soil_T\n\n    # TODO make this user set and document\n    # For GSHP\n    def gshp_cop(d):\n        return 8.77 - 0.150 * d + 0.000734 * d**2\n\n    cop_soil = gshp_cop(delta_soil_T)\n\n    with pd.HDFStore(output_path, mode=\"w\", complevel=4) as store:\n        store[\"ashp_cop_profiles\"] = cop\n        store[\"gshp_cop_profiles\"] = cop_soil\n</code></pre>"},{"location":"reference/build_cutout/","title":"Build cutout","text":"<p>Functions to download ERA5/SARAH data and build the atlite cutout for the atlite. These functions linked to the build_cutout rule.</p>"},{"location":"reference/build_cutout/#build_cutout.cutout_timespan","title":"<code>cutout_timespan(config, weather_year)</code>","text":"<p>Build the cutout timespan. Note that the coutout requests are in UTC (TBC)</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict</code> <p>the snakemake config</p> required <code>weather_year</code> <code>dict</code> <p>the coutout weather year</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <code>list</code> <p>end and start of the cutout timespan</p> Source code in <code>workflow/scripts/build_cutout.py</code> <pre><code>def cutout_timespan(config: dict, weather_year: int) -&gt; list:\n    \"\"\"Build the cutout timespan. Note that the coutout requests are in UTC (TBC)\n\n    Args:\n        config (dict): the snakemake config\n        weather_year (dict): the coutout weather year\n\n    Returns:\n        tuple: end and start of the cutout timespan\n    \"\"\"\n    snapshot_cfg = config[\"snapshots\"]\n    # make snapshots for TZ and then convert to naive UTC for atlite\n    snapshots = (\n        make_periodic_snapshots(\n            year=weather_year,\n            freq=snapshot_cfg[\"freq\"],\n            start_day_hour=snapshot_cfg[\"start\"],\n            end_day_hour=snapshot_cfg[\"end\"],\n            bounds=snapshot_cfg[\"bounds\"],\n            # here we need to convert UTC to local\n            tz=TIMEZONE,\n            end_year=(None if not snapshot_cfg[\"end_year_plus1\"] else weather_year + 1),\n        )\n        .tz_convert(\"UTC\")\n        .tz_localize(None)\n    )\n\n    return [snapshots[0], snapshots[-1]]\n</code></pre>"},{"location":"reference/build_load_profiles/","title":"Build load profiles","text":"<p>Functions for the rules to build the hourly heat and load demand profiles - electricity load profiles are based on scaling an hourly base year profile to yearly future     projections - daily heating demand is based on the degree day approx (from atlite) &amp; upscaled hourly based   on an intraday profile (for Denmark by default, see snakefile)</p>"},{"location":"reference/build_load_profiles/#build_load_profiles.build_daily_heat_demand_profiles","title":"<code>build_daily_heat_demand_profiles(cutout, pop_map, heat_demand_config, atlite_heating_hr_shift, switch_month_day=True)</code>","text":"<p>Build the heat demand profile according to forecast demands</p> <p>Parameters:</p> Name Type Description Default <code>cutout</code> <code>Cutout</code> <p>the weather cutout object</p> required <code>pop_map</code> <code>DataFrame</code> <p>the population raster map</p> required <code>heat_demand_config</code> <code>dict</code> <p>the heat demand configuration</p> required <code>atlite_heating_hr_shift</code> <code>int</code> <p>the hour shift for heating demand, needed due to imperfect timezone handling in atlite</p> required <code>switch_month_day</code> <code>bool</code> <p>whether to switch month &amp; day from heat_demand_config. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: regional daily heating demand with April to Sept forced to 0</p> Source code in <code>workflow/scripts/build_load_profiles.py</code> <pre><code>def build_daily_heat_demand_profiles(\n    cutout: atlite.Cutout,\n    pop_map: pd.DataFrame,\n    heat_demand_config: dict,\n    atlite_heating_hr_shift: int,\n    switch_month_day: bool = True,\n) -&gt; pd.DataFrame:\n    \"\"\"Build the heat demand profile according to forecast demands\n\n    Args:\n        cutout (atlite.Cutout): the weather cutout object\n        pop_map (pd.DataFrame): the population raster map\n        heat_demand_config (dict): the heat demand configuration\n        atlite_heating_hr_shift (int): the hour shift for heating demand, needed due to imperfect\n            timezone handling in atlite\n        switch_month_day (bool, optional): whether to switch month &amp; day from heat_demand_config.\n            Defaults to True.\n\n    Returns:\n        pd.DataFrame: regional daily heating demand with April to Sept forced to 0\n    \"\"\"\n    atlite_year = get_cutout_params(snakemake.config)[\"weather_year\"]\n\n    pop_matrix = sp.sparse.csr_matrix(pop_map.T)\n    index = pop_map.columns\n    index.name = \"provinces\"\n\n    # TODO clarify a bit here, maybe the po_matrix should be normalised earlier?\n    # unclear whether it's per cap or not\n    total_hd = cutout.heat_demand(\n        matrix=pop_matrix,\n        index=index,\n        threshold=heat_demand_config[\"heating_start_temp\"],\n        a=heat_demand_config[\"heating_lin_slope\"],\n        constant=heat_demand_config[\"heating_offet\"],\n        # hack to bring it back to local from UTC\n        hour_shift=atlite_heating_hr_shift,\n    )\n\n    regonal_daily_hd = total_hd.to_pandas().divide(pop_map.sum())\n    # input given as dd-mm but loc as yyyy-mm-dd\n    if switch_month_day:\n        start_day = \"{}-{}\".format(*heat_demand_config[\"start_day\"].split(\"-\")[::-1])\n        end_day = \"{}-{}\".format(*heat_demand_config[\"end_day\"].split(\"-\")[::-1])\n    else:\n        start_day = heat_demand_config[\"start_day\"]\n        end_day = heat_demand_config[\"end_day\"]\n    regonal_daily_hd.loc[f\"{atlite_year}-{start_day}\" : f\"{atlite_year}-{end_day}\"] = 0\n\n    # drop leap day\n    regonal_daily_hd.index = pd.to_datetime(regonal_daily_hd.index)\n    regonal_daily_hd = regonal_daily_hd[\n        ~((regonal_daily_hd.index.month == 2) &amp; (regonal_daily_hd.index.day == 29))\n    ]\n    return regonal_daily_hd\n</code></pre>"},{"location":"reference/build_load_profiles/#build_load_profiles.build_heat_demand_profile","title":"<code>build_heat_demand_profile(daily_hd, hot_water_per_day, snapshots, intraday_profiles, planning_horizons)</code>","text":"<p>Downscale the daily heat demand to hourly heat demand using pre-defined intraday profiles</p> <p>Parameters:</p> Name Type Description Default <code>daily_hd</code> <code>DataFrame</code> <p>the day resolved heat demand for each region (atlite time axis)</p> required <code>hot_water_per_day</code> <code>DataFrame</code> <p>the day resolved hot water demand for each region</p> required <code>snapshots</code> <code>date_range</code> <p>the snapshots for the planning year</p> required <code>planning_horizons</code> <code>int | str</code> <p>the planning year</p> required <p>Returns:</p> Type Description <code>tuple[DataFrame, DataFrame]</code> <p>tuple[pd.DataFrame, pd.DataFrame]: space_heat_demand, domestic hot water demand</p> Source code in <code>workflow/scripts/build_load_profiles.py</code> <pre><code>def build_heat_demand_profile(\n    daily_hd: pd.DataFrame,\n    hot_water_per_day: pd.DataFrame,\n    snapshots: pd.DatetimeIndex,\n    intraday_profiles: pd.Series,\n    planning_horizons: int | str,\n) -&gt; tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\"Downscale the daily heat demand to hourly heat demand using pre-defined intraday profiles\n\n    Args:\n        daily_hd (DataFrame): the day resolved heat demand for each region (atlite time axis)\n        hot_water_per_day (DataFrame): the day resolved hot water demand for each region\n        snapshots (pd.date_range): the snapshots for the planning year\n        planning_horizons (int | str): the planning year\n\n    Returns:\n        tuple[pd.DataFrame, pd.DataFrame]: space_heat_demand, domestic hot water demand\n    \"\"\"\n    # drop leap day\n    daily_hd = daily_hd[~((daily_hd.index.month == 2) &amp; (daily_hd.index.day == 29))]\n    # hourly resolution regional demand (but wrong data, it's just ffill)\n    heat_demand_hourly = shift_profile_to_planning_year(\n        daily_hd, planning_yr=planning_horizons\n    ).reindex(index=snapshots, method=\"ffill\")\n\n    # ===== downscale to hourly =======\n    intraday_year_profiles = (\n        downscale_time_data(\n            dt_index=heat_demand_hourly.index,\n            weekly_profile=(\n                list(intraday_profiles[\"weekday\"]) * 5 + list(intraday_profiles[\"weekend\"]) * 2\n            ),\n            regional_tzs=pd.Series(index=PROV_NAMES, data=list(REGIONAL_GEO_TIMEZONES.values())),\n        )\n        / 24\n    )\n    # intraday_year_profiles /= intraday_year_profiles.sum()\n\n    space_heat_demand = intraday_year_profiles.mul(heat_demand_hourly)\n    water_heat_demand = intraday_year_profiles.mul(hot_water_per_day)\n\n    return space_heat_demand, water_heat_demand\n</code></pre>"},{"location":"reference/build_load_profiles/#build_load_profiles.downscale_by_pop","title":"<code>downscale_by_pop(total, population)</code>","text":"<p>Simple downscale by population</p> <p>Parameters:</p> Name Type Description Default <code>total</code> <code>Series</code> <p>the value to downsacale</p> required <code>population</code> <code>Series</code> <p>population by node</p> required <p>Returns:</p> Type Description <code>Series</code> <p>pd.Series: downscaled to nodal resolution by population</p> Source code in <code>workflow/scripts/build_load_profiles.py</code> <pre><code>def downscale_by_pop(total: pd.Series, population: pd.Series) -&gt; pd.Series:\n    \"\"\"Simple downscale by population\n\n    Args:\n        total (pd.Series): the value to downsacale\n        population (pd.Series): population by node\n\n    Returns:\n        pd.Series: downscaled to nodal resolution by population\n    \"\"\"\n\n    return total * population / population.sum()\n</code></pre>"},{"location":"reference/build_load_profiles/#build_load_profiles.downscale_time_data","title":"<code>downscale_time_data(dt_index, weekly_profile, regional_tzs)</code>","text":"<p>Make hourly resolved data profiles based on exogenous weekdays and weekend profiles. This fn takes into account that the profiles are in local time and that regions may  have different timezones.</p> <p>Parameters:</p> Name Type Description Default <code>dt_index</code> <code>DatetimeIndex</code> <p>the snapshots (in network local naive time) but hourly res.</p> required <code>weekly_profile</code> <code>Iterable</code> <p>the weekly profile as a list of 7*24 entries.</p> required <code>regional_tzs</code> <code>Series</code> <p>regional geographical timezones for profiles. Defaults to pd.Series(index=PROV_NAMES, data=list(REGIONAL_GEO_TIMEZONES.values())).</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: Regionally resolved profiles for each snapshot hour rperesented by dt_index</p> Source code in <code>workflow/scripts/build_load_profiles.py</code> <pre><code>def downscale_time_data(\n    dt_index: pd.DatetimeIndex,\n    weekly_profile: Iterable,\n    regional_tzs: pd.Series,\n) -&gt; pd.DataFrame:\n    \"\"\"Make hourly resolved data profiles based on exogenous weekdays and weekend profiles.\n    This fn takes into account that the profiles are in local time and that regions may\n     have different timezones.\n\n    Args:\n        dt_index (DatetimeIndex): the snapshots (in network local naive time) but hourly res.\n        weekly_profile (Iterable): the weekly profile as a list of 7*24 entries.\n        regional_tzs (pd.Series, optional): regional geographical timezones for profiles.\n            Defaults to pd.Series(index=PROV_NAMES, data=list(REGIONAL_GEO_TIMEZONES.values())).\n\n    Returns:\n        pd.DataFrame: Regionally resolved profiles for each snapshot hour rperesented by dt_index\n    \"\"\"\n    weekly_profile = pd.Series(weekly_profile, range(24 * 7))\n    # make a dataframe with timestamps localized to the network TIMEZONE timestamps\n    all_times = pd.DataFrame(\n        dict(zip(PROV_NAMES, [dt_index.tz_localize(TIMEZONE)] * len(PROV_NAMES))),\n        index=dt_index.tz_localize(TIMEZONE),\n        columns=PROV_NAMES,\n    )\n    # then localize to regional time. _dt ensures index is not changed\n    week_hours = all_times.apply(\n        lambda col: col.dt.tz_convert(regional_tzs[col.name]).tz_localize(None)\n    )\n    # then convert into week hour &amp; map to the intraday heat demand profile (based on local time)\n    return week_hours.apply(lambda col: col.dt.weekday * 24 + col.dt.hour).apply(\n        lambda col: col.map(weekly_profile)\n    )\n</code></pre>"},{"location":"reference/build_load_profiles/#build_load_profiles.prepare_hourly_load_data","title":"<code>prepare_hourly_load_data(hourly_load_p, prov_codes_p)</code>","text":"<p>Read the hourly electricity demand data and prepare it for use in the model</p> <p>Parameters:</p> Name Type Description Default <code>hourly_load_p</code> <code>PathLike</code> <p>raw elec data from zenodo, see readme in data.</p> required <code>prov_codes_p</code> <code>PathLike</code> <p>province mapping for data.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: the hourly demand data with the right province names, in TWh/hr</p> Source code in <code>workflow/scripts/build_load_profiles.py</code> <pre><code>def prepare_hourly_load_data(\n    hourly_load_p: os.PathLike,\n    prov_codes_p: os.PathLike,\n) -&gt; pd.DataFrame:\n    \"\"\"Read the hourly electricity demand data and prepare it for use in the model\n\n    Args:\n        hourly_load_p (os.PathLike, optional): raw elec data from zenodo, see readme in data.\n        prov_codes_p (os.PathLike, optional): province mapping for data.\n\n    Returns:\n        pd.DataFrame: the hourly demand data with the right province names, in TWh/hr\n    \"\"\"\n    TO_TWh = 1e-6\n    hourly = pd.read_csv(hourly_load_p)\n    hourly_TWh = hourly.drop(columns=[\"Time Series\"]) * TO_TWh\n    prov_codes = pd.read_csv(prov_codes_p)\n    prov_codes.set_index(\"Code\", inplace=True)\n    hourly_TWh.columns = hourly_TWh.columns.map(prov_codes[\"Full name\"])\n    return hourly_TWh\n</code></pre>"},{"location":"reference/build_load_profiles/#build_load_profiles.project_elec_demand","title":"<code>project_elec_demand(hourly_demand_base_yr_MWh, yearly_projections_MWh, year=2020)</code>","text":"<p>Project the hourly demand to the future years</p> <p>Parameters:</p> Name Type Description Default <code>hourly_demand_base_yr_MWh</code> <code>DataFrame</code> <p>the hourly demand in the base year</p> required <code>yearly_projections_MWh</code> <code>DataFrame</code> <p>the yearly projections</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: the projected hourly demand</p> Source code in <code>workflow/scripts/build_load_profiles.py</code> <pre><code>def project_elec_demand(\n    hourly_demand_base_yr_MWh: pd.DataFrame,\n    yearly_projections_MWh: pd.DataFrame,\n    year=2020,\n) -&gt; pd.DataFrame:\n    \"\"\"Project the hourly demand to the future years\n\n    Args:\n        hourly_demand_base_yr_MWh (pd.DataFrame): the hourly demand in the base year\n        yearly_projections_MWh (pd.DataFrame): the yearly projections\n\n    Returns:\n        pd.DataFrame: the projected hourly demand\n    \"\"\"\n    hourly_load_profile = hourly_demand_base_yr_MWh.loc[:, PROV_NAMES]\n    # normalise the hourly load\n    hourly_load_profile /= hourly_load_profile.sum(axis=0)\n\n    yearly_projections_MWh = yearly_projections_MWh.T.loc[int(year), PROV_NAMES]\n    hourly_load_projected = yearly_projections_MWh.multiply(hourly_load_profile)\n\n    # TODO fix this to use timestamps\n    if len(hourly_load_projected) == 8784:\n        # rm feb 29th\n        hourly_load_projected.drop(hourly_load_projected.index[1416:1440], inplace=True)\n    elif len(hourly_load_projected) != 8760:\n        raise ValueError(\"The length of the hourly load is not 8760 or 8784 (leap year, dropped)\")\n\n    snapshots = make_periodic_snapshots(\n        year=year,\n        freq=\"1h\",\n        start_day_hour=\"01-01 00:00:00\",\n        end_day_hour=\"12-31 23:00\",\n        bounds=\"both\",\n    )\n\n    hourly_load_projected.index = snapshots\n    return hourly_load_projected\n</code></pre>"},{"location":"reference/build_load_profiles/#build_load_profiles.project_heat_demand","title":"<code>project_heat_demand(planning_year, projection_name, ref_year=REF_YEAR)</code>","text":"<p>Make projections for heating demand</p> <p>Parameters:</p> Name Type Description Default <code>projection_name</code> <code>str</code> <p>name of projection</p> required <code>planning_year</code> <code>int</code> <p>year to project to</p> required <code>ref_year</code> <code>int</code> <p>reference year. Defaults to REF_YEAR.</p> <code>REF_YEAR</code> <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>scaling factor relative to base year for heating demand</p> Source code in <code>workflow/scripts/build_load_profiles.py</code> <pre><code>def project_heat_demand(planning_year: int, projection_name: str, ref_year=REF_YEAR) -&gt; float:\n    \"\"\"Make projections for heating demand\n\n    Args:\n        projection_name (str): name of projection\n        planning_year (int): year to project to\n        ref_year (int, optional): reference year. Defaults to REF_YEAR.\n\n    Returns:\n        float: scaling factor relative to base year for heating demand\n    \"\"\"\n    years = np.array([1985, 1990, 1995, 2000, 2005, 2010, 2015, 2020, 2060])\n\n    if projection_name == \"positive\":\n\n        def func(x, a, b, c, d):\n            \"\"\"Cubic polynomial fit to proj\"\"\"\n            return a * x**3 + b * x**2 + c * x + d\n\n        # TODO soft code\n        # heated area projection in China\n        # 2060: 6.02 * 36.52 * 1e4 north population * floor_space_per_capita in city\n        heated_area = np.array(\n            [2742, 21263, 64645, 110766, 252056, 435668, 672205, 988209, 2198504]\n        )  # 10000 m2\n\n        # Perform curve fitting\n        popt, pcov = curve_fit(func, years, heated_area)\n        factor = func(int(planning_year), *popt) / func(REF_YEAR, *popt)\n\n    elif projection_name == \"constant\":\n        factor = 1.0\n\n    else:\n        raise ValueError(f\"Invalid heating demand projection {projection_name}\")\n    return factor\n</code></pre>"},{"location":"reference/build_load_profiles/#build_load_profiles.scale_degree_day_to_reference","title":"<code>scale_degree_day_to_reference(heating_dd, reg_yrly_tot)</code>","text":"<p>Scale the heating degree days to the reference region</p> <p>Parameters:</p> Name Type Description Default <code>heating_dd</code> <code>Series</code> <p>the heating degree days for each region (atlite)</p> required <code>reg_yrly_tot</code> <code>DataFrame</code> <p>the reference data per region</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.Series: the scaled heating degree days</p> Source code in <code>workflow/scripts/build_load_profiles.py</code> <pre><code>def scale_degree_day_to_reference(\n    heating_dd: pd.DataFrame, reg_yrly_tot: pd.DataFrame\n) -&gt; pd.DataFrame:\n    \"\"\"Scale the heating degree days to the reference region\n\n    Args:\n        heating_dd (pd.Series): the heating degree days for each region (atlite)\n        reg_yrly_tot (DataFrame): the reference data per region\n\n    Returns:\n        pd.Series: the scaled heating degree days\n    \"\"\"\n    norm_daily_hd = heating_dd / heating_dd.sum()\n    return norm_daily_hd.mul(reg_yrly_tot)\n</code></pre>"},{"location":"reference/build_population/","title":"Build population","text":"<p>Rules for building the population data by region</p>"},{"location":"reference/build_population/#build_population.build_population","title":"<code>build_population(data_path=None)</code>","text":"<p>Build the population data by region</p> <p>Parameters:</p> Name Type Description Default <code>data_path</code> <code>PathLike</code> <p>the path to the pop csv. Defaults to None.</p> <code>None</code> Source code in <code>workflow/scripts/build_population.py</code> <pre><code>def build_population(data_path: os.PathLike = None):\n    \"\"\"Build the population data by region\n\n    Args:\n        data_path (os.PathLike, optional): the path to the pop csv. Defaults to None.\n    \"\"\"\n\n    if data_path is None:\n        data_path = snakemake.input.population\n\n    population = YEARBOOK_DATA2POP * load_pop_csv(csv_path=data_path)\n    population.name = \"population\"\n    population.to_hdf(snakemake.output.population, key=population.name)\n</code></pre>"},{"location":"reference/build_population/#build_population.load_pop_csv","title":"<code>load_pop_csv(csv_path)</code>","text":"<p>Load the national bureau of statistics of China population.</p> <p>Supports both formats: - Yearbook format (2.5 pop at year end by Region) - Historical data format with comment lines</p> <p>Parameters:</p> Name Type Description Default <code>csv_path</code> <code>Pathlike</code> <p>Path to the CSV file</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: The population for constants.POP_YEAR by province</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the province names do not match expected names</p> Source code in <code>workflow/scripts/build_population.py</code> <pre><code>def load_pop_csv(csv_path: os.PathLike) -&gt; pd.DataFrame:\n    \"\"\"Load the national bureau of statistics of China population.\n\n    Supports both formats:\n    - Yearbook format (2.5 pop at year end by Region)\n    - Historical data format with comment lines\n\n    Args:\n        csv_path (os.Pathlike): Path to the CSV file\n\n    Returns:\n        pd.DataFrame: The population for constants.POP_YEAR by province\n\n    Raises:\n        ValueError: If the province names do not match expected names\n    \"\"\"\n    # Read CSV, skipping comment lines that start with #\n    df = pd.read_csv(csv_path, index_col=0, header=0, comment='#')\n    df = df.apply(pd.to_numeric)\n    df = df[POP_YEAR][df.index.isin(PROV_NAMES)]\n    if not sorted(df.index.to_list()) == sorted(PROV_NAMES):\n        raise ValueError(\n            f\"Province names do not match {sorted(df.index.to_list())} != {sorted(PROV_NAMES)}\"\n        )\n    return df\n</code></pre>"},{"location":"reference/build_population_gridcell_map/","title":"Build population gridcell map","text":"<p>Build population gridcell mapping for spatial analysis.</p> <p>This module creates population density maps and gridcell mappings for spatial disaggregation and analysis in the PyPSA-China energy system model.</p>"},{"location":"reference/build_population_gridcell_map/#build_population_gridcell_map.build_gridded_population","title":"<code>build_gridded_population(prov_pop_path, pop_density_raster_path, cutout_path, province_shape_path, gridded_pop_out)</code>","text":"<p>Build a gridded population DataFrame by matching population density to the cutout grid cells. This DataFrame is a sparse matrix of the population and shape BusesxCutout_gridcells   where buses are the provinces</p> <p>Parameters:</p> Name Type Description Default <code>prov_pop_path</code> <code>PathLike</code> <p>Path to the province population count file (hdf5).</p> required <code>pop_density_raster_path</code> <code>PathLike</code> <p>Path to the population density raster file.</p> required <code>cutout_path</code> <code>PathLike</code> <p>Path to the cutout file containing the grid.</p> required <code>province_shape_path</code> <code>PathLike</code> <p>Path to the province shape file.</p> required <code>gridded_pop_out</code> <code>PathLike</code> <p>output file path.</p> required Source code in <code>workflow/scripts/build_population_gridcell_map.py</code> <pre><code>def build_gridded_population(\n    prov_pop_path: PathLike,\n    pop_density_raster_path: PathLike,\n    cutout_path: PathLike,\n    province_shape_path: PathLike,\n    gridded_pop_out: PathLike,\n):\n    \"\"\"Build a gridded population DataFrame by matching population density to the cutout grid cells.\n    This DataFrame is a sparse matrix of the population and shape BusesxCutout_gridcells\n      where buses are the provinces\n\n    Args:\n        prov_pop_path (PathLike): Path to the province population count file (hdf5).\n        pop_density_raster_path (PathLike): Path to the population density raster file.\n        cutout_path (PathLike): Path to the cutout file containing the grid.\n        province_shape_path (PathLike): Path to the province shape file.\n        gridded_pop_out (PathLike): output file path.\n    \"\"\"\n\n    with pd.HDFStore(prov_pop_path, mode=\"r\") as store:\n        pop_province = store[\"population\"]\n\n    prov_poly = read_province_shapes(province_shape_path)\n    pop_density = read_pop_density(pop_density_raster_path, prov_poly, crs=CRS)\n\n    cutout = atlite.Cutout(cutout_path)\n    grid_points = cutout.grid\n    # this is in polygons but need points for sjoin with pop dnesity to work\n    grid_points.to_crs(3857, inplace=True)\n    grid_points[\"geometry\"] = grid_points.centroid\n    grid_points.to_crs(CRS, inplace=True)\n\n    # match cutout grid to province\n    # cutout_pts_in_prov = gpd.tools.sjoin(grid_points, prov_poly,\n    # how=\"left\", predicate=\"intersects\")\n    # TODO: do you want to dropna here?\n    cutout_pts_in_prov = gpd.tools.sjoin(\n        grid_points, prov_poly, how=\"left\", predicate=\"intersects\"\n    )  # .dropna()\n    cutout_pts_in_prov.rename(\n        columns={\"index_right\": \"province_index\", \"province\": \"province_name\"},\n        inplace=True,\n    )\n\n    # match cutout grid to province\n    cutout_pts_in_prov = gpd.tools.sjoin(grid_points, prov_poly, how=\"left\", predicate=\"intersects\")\n    cutout_pts_in_prov.rename(\n        columns={\"index_right\": \"province_index\", \"province\": \"province_name\"},\n        inplace=True,\n    )\n    # cutout_pts_in_prov.dropna(inplace=True)\n\n    # TODO CRS, think about whether this makes sense or need grid interp\n    merged = gpd.tools.sjoin_nearest(\n        cutout_pts_in_prov.to_crs(3857), pop_density.to_crs(3857), how=\"inner\"\n    )\n    merged = merged.to_crs(CRS)\n    # points outside china are NaN, need to rename to keep the index cutout after agg\n    # otherwise the spare matrix will not match the cutoutpoints\n    #  (smarter would be to change the cutout)\n    merged.fillna({\"province_name\": \"OutsideChina\"}, inplace=True)\n\n    points_in_provinces = pd.DataFrame(index=cutout_pts_in_prov.index)\n    # normalise pop per province and make a loc_id/province table\n    points_in_provinces = (\n        merged.groupby(\"province_name\")[\"pop_density\"]\n        .apply(lambda x: x / x.sum())\n        .unstack(fill_value=0.0)\n        .T\n    )\n    # now get rid of the outside china \"province\"\n    points_in_provinces.drop(columns=\"OutsideChina\", inplace=True)\n    points_in_provinces.index.name = \"\"\n    points_in_provinces.fillna(0.0, inplace=True)\n\n    points_in_provinces *= pop_province\n\n    with pd.HDFStore(gridded_pop_out, mode=\"w\", complevel=4) as store:\n        store[\"population_gridcell_map\"] = points_in_provinces\n</code></pre>"},{"location":"reference/build_population_gridcell_map/#build_population_gridcell_map.build_population_map","title":"<code>build_population_map(prov_pop_path, pop_density_raster_path, cutout_path, province_shape_path, gridded_pop_out)</code>","text":"<p>Build a gridded population DataFrame by matching population density to the cutout grid cells. This DataFrame is a sparse matrix of the population and shape BusesxCutout_gridcells   where buses are the provinces</p> <p>Parameters:</p> Name Type Description Default <code>prov_pop_path</code> <code>PathLike</code> <p>Path to the province population count file (hdf5).</p> required <code>pop_density_raster_path</code> <code>PathLike</code> <p>Path to the population density raster file.</p> required <code>cutout_path</code> <code>PathLike</code> <p>Path to the cutout file containing the grid.</p> required <code>province_shape_path</code> <code>PathLike</code> <p>Path to the province shape file.</p> required <code>gridded_pop_out</code> <code>PathLike</code> <p>output file path.</p> required Source code in <code>workflow/scripts/build_population_gridcell_map.py</code> <pre><code>def build_population_map(\n    prov_pop_path: PathLike,\n    pop_density_raster_path: PathLike,\n    cutout_path: PathLike,\n    province_shape_path: PathLike,\n    gridded_pop_out: PathLike,\n):\n    \"\"\"Build a gridded population DataFrame by matching population density to the cutout grid cells.\n    This DataFrame is a sparse matrix of the population and shape BusesxCutout_gridcells\n      where buses are the provinces\n\n    Args:\n        prov_pop_path (PathLike): Path to the province population count file (hdf5).\n        pop_density_raster_path (PathLike): Path to the population density raster file.\n        cutout_path (PathLike): Path to the cutout file containing the grid.\n        province_shape_path (PathLike): Path to the province shape file.\n        gridded_pop_out (PathLike): output file path.\n    \"\"\"\n\n    # =============== load data ===================\n    with pd.HDFStore(prov_pop_path, mode=\"r\") as store:\n        pop_province_count = store[\"population\"]\n\n    # CFSR points and Provinces\n    pop_ww = load_cfrs_data(pop_density_raster_path)\n\n    prov_poly = gpd.read_file(province_shape_path)[[\"province\", \"geometry\"]]\n    prov_poly.set_index(\"province\", inplace=True)\n    prov_poly = prov_poly.reindex(PROV_NAMES)\n    prov_poly.reset_index(inplace=True)\n\n    # load renewable profiles &amp; grid &amp; extract gridpoints\n    cutout = atlite.Cutout(cutout_path)\n    grid_points = cutout.grid\n    grid_points.to_crs(3857, inplace=True)\n    grid_points[\"geometry\"] = grid_points.centroid\n    grid_points.to_crs(CRS, inplace=True)\n\n    # match cutout grid to province\n    cutout_pts_in_prov = gpd.tools.sjoin(grid_points, prov_poly, how=\"left\", predicate=\"intersects\")\n    cutout_pts_in_prov.rename(\n        columns={\"index_right\": \"province_index\", \"province\": \"province_name\"},\n        inplace=True,\n    )\n\n    # Province masks merged with population density\n    # TODO: THIS REQUIRES EXPLANATION - can't just use random crs :||\n    cutout_pts_in_prov = cutout_pts_in_prov.to_crs(3857)\n    pop_ww = pop_ww.to_crs(3857)\n\n    merged = gpd.tools.sjoin_nearest(cutout_pts_in_prov, pop_ww, how=\"inner\")\n    merged = merged.to_crs(CRS)\n\n    # normalised pop distribution per province\n    # need an extra province for points not in the province, otherwise lose cutout grid index\n    merged.fillna({\"province_name\": \"OutsideChina\"}, inplace=True)\n    points_in_provinces = pd.DataFrame(index=cutout_pts_in_prov.index)\n    points_in_provinces = (\n        merged.groupby(\"province_name\")[\"pop_density\"]\n        .apply(lambda x: x / x.sum())\n        .unstack(fill_value=0.0)\n        .T\n    )\n    # Cleanup the matrix: get rid of the outside china \"province\" et\n    points_in_provinces.drop(columns=\"OutsideChina\", inplace=True)\n    points_in_provinces.index.name = \"\"\n    points_in_provinces.fillna(0.0, inplace=True)\n\n    # go from normalised distribution to head count\n    points_in_provinces *= pop_province_count\n    with pd.HDFStore(gridded_pop_out, mode=\"w\", complevel=4) as store:\n        store[\"population_gridcell_map\"] = points_in_provinces\n</code></pre>"},{"location":"reference/build_population_gridcell_map/#build_population_gridcell_map.load_cfrs_data","title":"<code>load_cfrs_data(target)</code>","text":"<p>Load  CFRS_grid.nc type files into a geodatafram</p> <p>Parameters:</p> Name Type Description Default <code>target</code> <code>PathLike</code> <p>the abs path</p> required <p>Returns:</p> Type Description <code>GeoDataFrame</code> <p>gpd.GeoDataFrame: the data in gdf</p> Source code in <code>workflow/scripts/build_population_gridcell_map.py</code> <pre><code>def load_cfrs_data(target: PathLike) -&gt; gpd.GeoDataFrame:\n    \"\"\"Load  CFRS_grid.nc type files into a geodatafram\n\n    Args:\n        target (PathLike): the abs path\n\n    Returns:\n        gpd.GeoDataFrame: the data in gdf\n    \"\"\"\n    pop_density = xr.open_dataarray(target).to_dataset(name=\"pop_density\")\n    pop_ww = xarr_to_gdf(pop_density, var_name=\"pop_density\")  # TODO is the CRS correct?\n\n    return pop_ww\n</code></pre>"},{"location":"reference/build_population_gridcell_map/#build_population_gridcell_map.xarr_to_gdf","title":"<code>xarr_to_gdf(xarr, var_name, x_var='x', y_var='y', crs=CRS)</code>","text":"<p>Convert an xarray to GDF</p> <p>Parameters:</p> Name Type Description Default <code>xarr</code> <code>DataArray</code> <p>the input array</p> required <code>var_name</code> <code>str</code> <p>the array variable to be converted.</p> required <code>x_var</code> <code>str</code> <p>the x dimension. Defaults to \"x\".</p> <code>'x'</code> <code>y_var</code> <code>str</code> <p>the y dimension. Defaults to \"y\".</p> <code>'y'</code> <code>crs</code> <code>_type_</code> <p>the crs. Defaults to CRS.</p> <code>CRS</code> <p>Returns:</p> Type Description <code>GeoDataFrame</code> <p>gpd.GeoDataFrame: geodata frame in chosen CRS</p> Source code in <code>workflow/scripts/build_population_gridcell_map.py</code> <pre><code>def xarr_to_gdf(\n    xarr: xr.DataArray, var_name: str, x_var=\"x\", y_var=\"y\", crs=CRS\n) -&gt; gpd.GeoDataFrame:\n    \"\"\"Convert an xarray to GDF\n\n    Args:\n        xarr (xr.DataArray): the input array\n        var_name (str): the array variable to be converted.\n        x_var (str, optional): the x dimension. Defaults to \"x\".\n        y_var (str, optional): the y dimension. Defaults to \"y\".\n        crs (_type_, optional): the crs. Defaults to CRS.\n\n    Returns:\n        gpd.GeoDataFrame: geodata frame in chosen CRS\n    \"\"\"\n    df = xarr.to_dataframe()\n    df.reset_index(inplace=True)\n    return gpd.GeoDataFrame(\n        df[var_name], geometry=gpd.points_from_xy(df[x_var], df[y_var]), crs=crs\n    )\n</code></pre>"},{"location":"reference/build_powerplants/","title":"Build powerplants","text":"<p>Build the existing capacities for each node from GEM (global energy monitor) tracker data. This script is intended for use as part of the Snakemake workflow.</p> <p>The GEM data has to be downloaded manually and placed in the source directory of the snakemake rule. download page: https://globalenergymonitor.org/projects/global-integrated-power-tracker/download-data/</p> <p>Nodes can be assigned to specific GEM IDs based on their GPS location or administrative region location.</p>"},{"location":"reference/build_powerplants/#build_powerplants.assign_node_from_gps","title":"<code>assign_node_from_gps(gem_data, nodes)</code>","text":"<p>Assign plant node based on GPS coordinates of the plant. Will cause issues if the nodes tolerance is too low</p> <p>Parameters:</p> Name Type Description Default <code>gem_data</code> <code>DataFrame</code> <p>GEM data</p> required <code>nodes</code> <code>GeoDataFrame</code> <p>node geometries (nodes as index).</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: DataFrame with assigned nodes.</p> Source code in <code>workflow/scripts/build_powerplants.py</code> <pre><code>def assign_node_from_gps(gem_data: pd.DataFrame, nodes: gpd.GeoDataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Assign plant node based on GPS coordinates of the plant.\n    Will cause issues if the nodes tolerance is too low\n\n    Args:\n        gem_data (pd.DataFrame): GEM data\n        nodes (gpd.GeoDataFrame): node geometries (nodes as index).\n\n    Returns:\n        pd.DataFrame: DataFrame with assigned nodes.\n    \"\"\"\n\n    gem_data[\"geometry\"] = gem_data.apply(\n        lambda row: Point(row[\"Longitude\"], row[\"Latitude\"]), axis=1\n    )\n    gem_gdf = gpd.GeoDataFrame(gem_data, geometry=\"geometry\", crs=\"EPSG:4326\")\n\n    joined = nodes.reset_index(names=\"node\").sjoin_nearest(gem_gdf, how=\"right\")\n    missing = joined[joined.node.isna()]\n    if not missing.empty:\n        logger.warning(\n            f\"Some GEM locations are not covered by the nodes at GPS: {missing['Plant name'].head()}\"\n        )\n    return joined\n</code></pre>"},{"location":"reference/build_powerplants/#build_powerplants.clean_gem_data","title":"<code>clean_gem_data(gem_data, gem_cfg)</code>","text":"<p>Clean the GEM data by  - mapping GEM types onto pypsa types  - filtering for relevant project statuses  - cleaning invalid entries (e.g \"not found\"-&gt;nan)</p> <p>Parameters:</p> Name Type Description Default <code>gem_data</code> <code>DataFrame</code> <p>GEM dataset.</p> required <code>gem_cfg</code> <code>dict</code> <p>Configuration dictionary, 'global_energy_monitor.yaml'</p> required <p>Returns:     pd.DataFrame: Cleaned GEM data.</p> Source code in <code>workflow/scripts/build_powerplants.py</code> <pre><code>def clean_gem_data(gem_data: pd.DataFrame, gem_cfg: dict) -&gt; pd.DataFrame:\n    \"\"\"\n    Clean the GEM data by\n     - mapping GEM types onto pypsa types\n     - filtering for relevant project statuses\n     - cleaning invalid entries (e.g \"not found\"-&gt;nan)\n\n    Args:\n        gem_data (pd.DataFrame): GEM dataset.\n        gem_cfg (dict): Configuration dictionary, 'global_energy_monitor.yaml'\n    Returns:\n        pd.DataFrame: Cleaned GEM data.\n    \"\"\"\n\n    _valid_project_states = gem_cfg[\"status\"]\n    GEM = gem_data.query(\"Status in @_valid_project_states\")\n    GEM.rename(columns={\"Plant _ Project name\": \"Plant name\"}, inplace=True)\n    GEM.loc[:, \"Retired year\"] = GEM[\"Retired year\"].replace(\"not found\", np.nan)\n    GEM.loc[:, \"Start year\"] = GEM[\"Start year\"].replace(\"not found\", np.nan)\n    GEM = GEM[gem_cfg[\"relevant_columns\"]]\n\n    # Remove whitespace from admin columns\n    # Remove all whitespace (including tabs, newlines) from admin columns\n    admin_cols = [col for col in ADM_COLS.values() if col in GEM.columns]\n    GEM[admin_cols] = GEM[admin_cols].apply(lambda x: x.str.replace(r\"\\s+\", \"\", regex=True))\n\n    # split oil and gas, rename bioenergy\n    gas_mask = GEM.query(\"Type == 'oil/gas' &amp; Fuel.str.contains('gas', case=False, na=False)\").index\n    GEM.loc[gas_mask, \"Type\"] = \"gas\"\n    GEM.Type = GEM.Type.str.replace(\"bioenergy\", \"biomass\")\n\n    # split CHP (potential issue: split before type split. After would be better)\n    if gem_cfg[\"CHP\"].get(\"split\", False):\n        GEM.loc[:, \"CHP_bool\"] = (\n            GEM.loc[:, \"CHP\"]\n            .map({\"not found\": False, \"yes\": True, \"no\": False, np.nan: False})\n            .fillna(False)\n        )\n        chp_mask = GEM[GEM[\"CHP_bool\"] == True].index\n\n        aliases = gem_cfg[\"CHP\"].get(\"aliases\", [])\n        for alias in aliases:\n            chp_mask = chp_mask.append(\n                GEM[GEM[\"Plant name\"].str.contains(alias, case=False, na=False)].index\n            )\n        chp_mask = chp_mask.unique()\n        GEM.loc[chp_mask, \"Type\"] = \"CHP \" + GEM.loc[chp_mask, \"Type\"]\n\n    GEM[\"tech\"] = \"\"\n    for tech, mapping in gem_cfg[\"tech_map\"].items():\n        if not isinstance(mapping, dict):\n            raise ValueError(\n                f\"Mapping for {tech} is a {type(mapping)} - expected dict. Check your config.\"\n            )\n\n        tech_mask = GEM.query(f\"Type == '{tech}'\").index\n        if tech_mask.empty:\n            continue\n        GEM.loc[tech_mask, \"Type\"] = GEM.loc[tech_mask, \"Technology\"].map(mapping)\n\n        # apply defaults if requested\n        if \"default\" not in mapping:\n            continue\n        fill_val = mapping[\"default\"]\n        if fill_val is not None:\n            GEM.loc[tech_mask, \"Type\"] = GEM.loc[tech_mask, \"Type\"].fillna(value=fill_val)\n        else:\n            GEM.loc[tech_mask, \"Type\"] = GEM.loc[tech_mask, \"Type\"].dropna()\n\n    return GEM.dropna(subset=[\"Type\"])\n</code></pre>"},{"location":"reference/build_powerplants/#build_powerplants.group_by_year","title":"<code>group_by_year(df, year_bins, base_year=2020)</code>","text":"<p>Group the DataFrame by year bins.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame with a 'Start year' column.</p> required <code>year_bins</code> <code>list</code> <p>List of year bins to group by.</p> required <code>base_year</code> <code>int</code> <p>cut-off for histirocal period. Default is 2020.</p> <code>2020</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: DataFrame with a new 'grouping_year' column.</p> Source code in <code>workflow/scripts/build_powerplants.py</code> <pre><code>def group_by_year(df: pd.DataFrame, year_bins: list, base_year=2020) -&gt; pd.DataFrame:\n    \"\"\"\n    Group the DataFrame by year bins.\n\n    Args:\n        df (pd.DataFrame): DataFrame with a 'Start year' column.\n        year_bins (list): List of year bins to group by.\n        base_year (int): cut-off for histirocal period. Default is 2020.\n\n    Returns:\n        pd.DataFrame: DataFrame with a new 'grouping_year' column.\n    \"\"\"\n    min_start_year = min(year_bins) - 2.5\n    base_year = 2020\n    df = df[df[\"Start year\"] &gt; min_start_year]\n    df = df[df[\"Retired year\"].isna() | (df[\"Retired year\"] &gt; base_year)].reset_index(drop=True)\n    df[\"grouping_year\"] = np.take(year_bins, np.digitize(df[\"Start year\"], year_bins, right=True))\n\n    return df\n</code></pre>"},{"location":"reference/build_powerplants/#build_powerplants.load_gem_excel","title":"<code>load_gem_excel(path, sheetname='Units', country_col='Country/area', country_names=['China'])</code>","text":"<p>Load a Global Energy monitor excel file as a dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>PathLike</code> <p>Path to the Excel file.</p> required <code>sheetname</code> <code>str</code> <p>Name of the sheet to load. Default is \"Units\".</p> <code>'Units'</code> <code>country_col</code> <code>str</code> <p>Column name for country names. Default is \"Country/area\".</p> <code>'Country/area'</code> <code>country_names</code> <code>list</code> <p>List of country names to filter by. Default is [\"China\"].</p> <code>['China']</code> Source code in <code>workflow/scripts/build_powerplants.py</code> <pre><code>def load_gem_excel(\n    path: os.PathLike, sheetname=\"Units\", country_col=\"Country/area\", country_names=[\"China\"]\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Load a Global Energy monitor excel file as a dataframe.\n\n    Args:\n        path (os.PathLike): Path to the Excel file.\n        sheetname (str): Name of the sheet to load. Default is \"Units\".\n        country_col (str): Column name for country names. Default is \"Country/area\".\n        country_names (list): List of country names to filter by. Default is [\"China\"].\n    \"\"\"\n\n    df = pd.read_excel(path, sheet_name=sheetname, engine=\"openpyxl\")\n    # replace problem characters in column names\n    df.columns = df.columns.str.replace(\"/\", \"_\")\n    country_col = country_col.replace(\"/\", \"_\")\n\n    if country_col not in df.columns:\n        logger.warning(f\"Column {country_col} not found in {path}. Returning unfiltered DataFrame.\")\n        return df\n\n    return df.query(f\"{country_col} in @country_names\")\n</code></pre>"},{"location":"reference/build_province_shapes/","title":"Build province shapes","text":"<p>Functions to get the province shapes.</p>"},{"location":"reference/build_province_shapes/#build_province_shapes.fetch_natural_earth_records","title":"<code>fetch_natural_earth_records(country_iso2_code='CN')</code>","text":"<p>Fetch the province/state level (1st admin level) from the         NATURAL_EARTH data store and make a file</p> <p>Parameters:</p> Name Type Description Default <code>country_iso2_code</code> <code>str</code> <p>the country code (iso_a2) for which  provincial records will be extracted. None will not filter (untestetd) Defaults to 'CN'</p> <code>'CN'</code> <p>Returns:     Records: the natural earth records</p> Source code in <code>workflow/scripts/build_province_shapes.py</code> <pre><code>def fetch_natural_earth_records(country_iso2_code=\"CN\") -&gt; object:\n    \"\"\"Fetch the province/state level (1st admin level) from the\n            NATURAL_EARTH data store and make a file\n\n    Args:\n        country_iso2_code (str, optional): the country code (iso_a2) for which\n             provincial records will be extracted. None will not filter (untestetd) Defaults to 'CN'\n    Returns:\n        Records: the natural earth records\n    \"\"\"\n\n    shpfilename = shpreader.natural_earth(\n        resolution=NATURAL_EARTH_RESOLUTION,\n        category=\"cultural\",\n        name=NATURAL_EARTH_DATA_SET,\n    )\n    reader = shpreader.Reader(shpfilename)\n    logger.info(\"Succesfully downloaded natural earth shapefiles\")\n    provinces_states = reader.records()\n\n    def filter_country_code(records: object, target_iso_a2_code=\"CN\") -&gt; list:\n        \"\"\"Filter provincial/state (admin level 1) records for one country\n\n        Args:\n            records (shpreader.Reader.records): the records object from cartopy\n                    shpreader for natural earth dataset\n            target_iso_a2_code (str, optional): the country code (iso_a2) for which\n                    provincial records will be extracted. Defaults to 'CN'.\n\n        Returns:\n            list: records list\n        \"\"\"\n        results = []\n        for rec in records:\n            if rec.attributes[\"iso_a2\"] == target_iso_a2_code:\n                results.append(rec)\n\n        return results\n\n    # TODO test with none\n    if country_iso2_code is not None:\n        provinces_states = filter_country_code(\n            provinces_states, target_iso_a2_code=country_iso2_code\n        )\n\n    return provinces_states\n</code></pre>"},{"location":"reference/build_province_shapes/#build_province_shapes.records_to_data_frame","title":"<code>records_to_data_frame(records)</code>","text":"<p>Dump irrelevant info and make records into a GeoDataFrame that matches the PROV_NAMES</p> <p>Parameters:</p> Name Type Description Default <code>records</code> <code>object</code> <p>the cartopy shpread records from natural earth</p> required <p>Returns:</p> Type Description <code>GeoDataFrame</code> <p>gpd.GeoDataFrame: the cleaned up &amp; sorted data in a format that can be saved</p> Source code in <code>workflow/scripts/build_province_shapes.py</code> <pre><code>def records_to_data_frame(records: object) -&gt; gpd.GeoDataFrame:\n    \"\"\"Dump irrelevant info and make records into a GeoDataFrame that matches the PROV_NAMES\n\n    Args:\n        records (object): the cartopy shpread records from natural earth\n\n    Returns:\n        gpd.GeoDataFrame: the cleaned up &amp; sorted data in a format that can be saved\n    \"\"\"\n\n    records[0].attributes[\"name\"]\n    d = {\"province\": [r.attributes[\"name_en\"] for r in records]}\n    geo = [r.geometry for r in records]\n    gdf = gpd.GeoDataFrame(d, geometry=geo)\n    gdf.sort_values(by=\"province\", inplace=True)\n    # remove white spaces\n    gdf[\"province\"] = gdf.province.str.replace(\" \", \"\")\n\n    filtered = gdf[gdf.province.isin(PROV_NAMES)]\n\n    if not filtered.province.to_list() == sorted(PROV_NAMES):\n        raise ValueError(\n            \"Built cut-out does not have the right provinces\"\n            + \"- do your province lists have white spaces?\"\n        )\n\n    return filtered\n</code></pre>"},{"location":"reference/build_province_shapes/#build_province_shapes.save_province_data","title":"<code>save_province_data(provinces_gdf, crs=CRS, output_file=DEFAULT_SHAPE_OUTPATH)</code>","text":"<p>Save to file</p> <p>Parameters:</p> Name Type Description Default <code>provinces_gdf</code> <code>GeoDataFrame</code> <p>the cleaned up province records</p> required <code>crs</code> <code>int</code> <p>the crs in epsg format. Defaults to CRS.</p> <code>CRS</code> <code>output_file</code> <code>pathlike</code> <p>the output path. defaults to DEFAULT_SHAPE_OUTPATH</p> <code>DEFAULT_SHAPE_OUTPATH</code> Source code in <code>workflow/scripts/build_province_shapes.py</code> <pre><code>def save_province_data(\n    provinces_gdf: gpd.GeoDataFrame,\n    crs: int = CRS,\n    output_file: os.PathLike = DEFAULT_SHAPE_OUTPATH,\n):\n    \"\"\"Save to file\n\n    Args:\n        provinces_gdf (GeoDataFrame): the cleaned up province records\n        crs (int, optional): the crs in epsg format. Defaults to CRS.\n        output_file (os.pathlike): the output path. defaults to DEFAULT_SHAPE_OUTPATH\n    \"\"\"\n    provinces_gdf.set_crs(epsg=crs, inplace=True)  # WGS84\n    provinces_gdf.to_file(os.path.abspath(output_file))\n</code></pre>"},{"location":"reference/build_renewable_profiles/","title":"Build renewable profiles","text":"<p>Adapted from pypsa-EUR by the pypsa China-PIK authors</p> <p>Calculates for each clustered region the (i) installable capacity (based on land-use from :mod:<code>determine_availability_matrix</code>) (ii) the available generation time series (based on weather data) (iii) the average distanc from the node for onshore wind, AC-connected offshore wind, DC-connected offshore wind and solar PV generators.</p>"},{"location":"reference/build_renewable_profiles/#build_renewable_profiles--outputs","title":"Outputs","text":"<ul> <li> <p><code>resources/profile_{technology}.nc</code> with the following structure</p> <p>===================  ====================  ===================================================== Field                Dimensions            Description ===================  ====================  ===================================================== profile              year, bus, bin, time  the per unit hourly availability factors for each bus</p> <p>p_nom_max            bus, bin              maximal installable capacity at the bus (in MW)</p> <p>average_distance     bus, bin              average distance of units in the region to the                                            grid bus for onshore techs and to the shoreline                                            for offshore technologies (in km) ===================  ====================  =====================================================</p> </li> </ul>"},{"location":"reference/build_renewable_profiles/#build_renewable_profiles--description","title":"Description","text":"<p>This script functions at two main spatial resolutions: the resolution of the clustered network regions, and the resolution of the cutout grid cells for the weather data. Typically the weather data grid is finer than the network regions, so we have to work out the distribution of generators across the grid cells within each region. This is done by taking account of a combination of the available land at each grid cell (computed in :mod:<code>determine_availability_matrix</code>) and the capacity factor there.</p> <p>Based on the availability matrix, the script first computes how much of the technology can be installed at each cutout grid cell. To compute the layout of generators in each clustered region, the installable potential in each grid cell is multiplied with the capacity factor at each grid cell. This is done since we assume more generators are installed at cells with a higher capacity factor.</p> <p>Based on the average capacity factor, the potentials are further divided into a configurable number of resource classes (bins).</p> <p>This layout is then used to compute the generation availability time series from the weather data cutout from <code>atlite</code>.</p> <p>The maximal installable potential for the node (<code>p_nom_max</code>) is computed by adding up the installable potentials of the individual grid cells.</p>"},{"location":"reference/build_renewable_profiles/#build_renewable_profiles.build_resource_classes","title":"<code>build_resource_classes(cutout, nbins, regions, capacity_factor, params)</code>","text":"<p>Bin resources based on their capacity factor The number of bins can be dynamically reduced based on a min delta cf</p> <p>Parameters:</p> Name Type Description Default <code>cutout</code> <code>Cutout</code> <p>the atlite cutout</p> required <code>nbins</code> <code>int</code> <p>the number of bins</p> required <code>regions</code> <code>GeoSeries</code> <p>the regions</p> required <code>capacity_factor</code> <code>(DataArray,)</code> <p>the capacity factor</p> required <code>params</code> <code>dict</code> <p>the config for VREs</p> required <p>Returns:</p> Type Description <code>DataArray</code> <p>xr.DataArray: the mask for the resource classes</p> <code>GeoSeries</code> <p>gpd.GeoSeries: multi-indexed series [bus, bin]: geometry</p> Source code in <code>workflow/scripts/build_renewable_profiles.py</code> <pre><code>def build_resource_classes(\n    cutout: Cutout,\n    nbins: int,\n    regions: gpd.GeoSeries,\n    capacity_factor: xr.DataArray,\n    params: dict,\n) -&gt; tuple[xr.DataArray, gpd.GeoSeries]:\n    \"\"\"Bin resources based on their capacity factor\n    The number of bins can be dynamically reduced based on a min delta cf\n\n    Args:\n        cutout (Cutout): the atlite cutout\n        nbins (int): the number of bins\n        regions (gpd.GeoSeries): the regions\n        capacity_factor (xr.DataArray,): the capacity factor\n        params (dict): the config for VREs\n\n    Returns:\n        xr.DataArray: the mask for the resource classes\n        gpd.GeoSeries: multi-indexed series [bus, bin]: geometry\n    \"\"\"\n    resource_classes = params.get(\"resource_classes\", {})\n    nbins = resource_classes.get(\"n\", 1)\n    min_cf_delta = resource_classes.get(\"min_cf_delta\", 0.0)\n    buses = regions.index\n\n    # indicator matrix for which cells touch which regions\n    IndMat = np.ceil(cutout.availabilitymatrix(regions, ExclusionContainer()))\n    cf_by_bus = capacity_factor * IndMat.where(IndMat &gt; 0)\n\n    epsilon = 1e-3\n    cf_min, cf_max = (\n        cf_by_bus.min(dim=[\"x\", \"y\"]) - epsilon,\n        cf_by_bus.max(dim=[\"x\", \"y\"]) + epsilon,\n    )\n\n    # avoid binning resources that are very similar\n    # If cf_range &lt; min_cf_delta, use at least 1 bin (instead of 0)\n    nbins_per_bus = [max(1, int(min(nbins, x))) for x in (cf_max - cf_min) // min_cf_delta]\n    normed_bins = xr.DataArray(\n        np.vstack(\n            [np.hstack([[0] * (nbins - n), np.linspace(0, 1, n + 1)]) for n in nbins_per_bus]\n        ),\n        dims=[\"bus\", \"bin\"],\n        coords={\"bus\": regions.index},\n    )\n    bins = cf_min + (cf_max - cf_min) * normed_bins\n\n    cf_by_bus_bin = cf_by_bus.expand_dims(bin=range(nbins))\n    lower_edges = bins[:, :-1]\n    upper_edges = bins[:, 1:]\n    class_masks = (cf_by_bus_bin &gt;= lower_edges) &amp; (cf_by_bus_bin &lt; upper_edges)\n\n    if nbins == 1:\n        bus_bin_mi = pd.MultiIndex.from_product([regions.index, [0]], names=[\"bus\", \"bin\"])\n        class_regions = regions.set_axis(bus_bin_mi)\n        class_regions[\"cf\"] = bins.to_series()\n    else:\n        grid = cutout.grid.set_index([\"y\", \"x\"])\n        class_regions = {}\n        for bus, bin_id in product(buses, range(nbins)):\n            bus_bin_mask = (\n                class_masks.sel(bus=bus, bin=bin_id).stack(spatial=[\"y\", \"x\"]).to_pandas()\n            )\n            grid_cells = grid.loc[bus_bin_mask]\n            geometry = grid_cells.intersection(regions.loc[bus, \"geometry\"]).union_all().buffer(0)\n            class_regions[(bus, bin_id)] = geometry\n\n        class_regions = gpd.GeoDataFrame(\n            {\"geometry\": class_regions.values()},\n            index=pd.MultiIndex.from_tuples(class_regions.keys(), names=[\"bus\", \"bin\"]),\n        )\n        class_regions[\"cf\"] = bins.to_series()\n\n    return class_masks, class_regions\n</code></pre>"},{"location":"reference/build_renewable_profiles/#build_renewable_profiles.localize_cutout_time","title":"<code>localize_cutout_time(cutout, drop_leap=True)</code>","text":"<p>Localize the time to the local timezone</p> <p>Parameters:</p> Name Type Description Default <code>cutout</code> <code>Cutout</code> <p>the atlite cutout object</p> required <code>drop_leap</code> <code>bool</code> <p>drop 29th Feb. Defaults to True.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>Cutout</code> <code>Cutout</code> <p>the updated cutout</p> Source code in <code>workflow/scripts/build_renewable_profiles.py</code> <pre><code>def localize_cutout_time(cutout: Cutout, drop_leap=True) -&gt; Cutout:\n    \"\"\"Localize the time to the local timezone\n\n    Args:\n        cutout (Cutout): the atlite cutout object\n        drop_leap (bool, optional): drop 29th Feb. Defaults to True.\n\n    Returns:\n        Cutout: the updated cutout\n    \"\"\"\n\n    data = cutout.data\n\n    timestamps = pd.DatetimeIndex(data.time)\n    # go from ECMWF/atlite UTC to local time\n    ts_naive = timestamps.tz_localize(\"UTC\").tz_convert(TIMEZONE).tz_localize(None)\n    cutout.data = cutout.data.assign_coords(time=ts_naive)\n\n    if drop_leap:\n        data = cutout.data\n        cutout.data = data.sel(time=~((data.time.dt.month == 2) &amp; (data.time.dt.day == 29)))\n\n    return cutout\n</code></pre>"},{"location":"reference/build_renewable_profiles/#build_renewable_profiles.prepare_resource_config","title":"<code>prepare_resource_config(params, nprocesses, noprogress=True)</code>","text":"<p>Parse the resource config (atlite calc config)</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>dict</code> <p>the renewable options</p> required <code>nprocesses</code> <code>int</code> <p>the number or processes</p> required <code>noprogress</code> <code>bool</code> <p>whether to show progress bars</p> <code>True</code> <p>Returns:</p> Type Description <code>(dict, dict)</code> <p>the resource config for the atlite calcs, the turbine/panel models</p> Source code in <code>workflow/scripts/build_renewable_profiles.py</code> <pre><code>def prepare_resource_config(params: dict, nprocesses: int, noprogress=True) -&gt; tuple[dict]:\n    \"\"\"Parse the resource config (atlite calc config)\n\n    Args:\n        params (dict): the renewable options\n        nprocesses (int): the number or processes\n        noprogress (bool): whether to show progress bars\n\n    Returns:\n        (dict, dict): the resource config for the atlite calcs, the turbine/panel models\n    \"\"\"\n\n    resource = params[\"resource\"]  # pv panel params / wind turbine params\n    resource[\"show_progress\"] = not noprogress\n    tech = \"panel\" if \"panel\" in resource else \"turbine\"\n\n    # in case of multiple years\n    models = resource[tech]\n    if not isinstance(models, dict):\n        models = {0: models}\n\n    if nprocesses &gt; 1:\n        client = Client(n_workers=nprocesses, threads_per_worker=1)\n        resource[\"dask_kwargs\"] = {\"scheduler\": client}\n\n    return resource, models\n</code></pre>"},{"location":"reference/build_solar_thermal_profiles/","title":"Build solar thermal profiles","text":"<p>Build solar thermal profiles for heating demand modeling.</p> <p>This module generates solar thermal collector profiles and heat demand time series for residential and commercial sectors in the PyPSA-China energy system model.</p>"},{"location":"reference/build_solar_thermal_profiles/#build_solar_thermal_profiles.build_solar_thermal_profiles","title":"<code>build_solar_thermal_profiles(pop_map, cutout, outp_path)</code>","text":"<p>Build per unit solar thermal time availability profiles and save them to a file</p> <p>Parameters:</p> Name Type Description Default <code>pop_map</code> <code>DataFrame</code> <p>DataFrame with the population map</p> required <code>cutout</code> <code>Cutout</code> <p>atlite cutout object with the weather data</p> required <code>outp_path</code> <code>PathLike</code> <p>Path to the output file</p> required Source code in <code>workflow/scripts/build_solar_thermal_profiles.py</code> <pre><code>def build_solar_thermal_profiles(\n    pop_map: pd.DataFrame, cutout: atlite.Cutout, outp_path: os.PathLike\n) -&gt; None:\n    \"\"\"Build per unit solar thermal time availability profiles and save them to a file\n\n    Args:\n        pop_map (pd.DataFrame): DataFrame with the population map\n        cutout (atlite.Cutout): atlite cutout object with the weather data\n        outp_path (os.PathLike): Path to the output file\n    \"\"\"\n    pop_matrix = sp.sparse.csr_matrix(pop_map.T)\n    index = pop_map.columns\n    index.name = \"provinces\"\n\n    st = cutout.solar_thermal(\n        orientation={\n            \"slope\": float(snakemake.config[\"solar_thermal_angle\"]),\n            \"azimuth\": 180.0,\n        },\n        matrix=pop_matrix,\n        index=index,\n    )\n\n    st[\"time\"] = (\n        pd.DatetimeIndex(st[\"time\"], tz=\"UTC\").tz_convert(TIMEZONE).tz_localize(None).values\n    )\n\n    with pd.HDFStore(outp_path, mode=\"w\", complevel=4) as store:\n        store[\"solar_thermal_profiles\"] = st.to_pandas().divide(pop_map.sum())\n</code></pre>"},{"location":"reference/build_temperature_profiles/","title":"Build temperature profiles","text":"<p>Functions associated with the build_temperature_profiles rule.</p>"},{"location":"reference/build_temperature_profiles/#build_temperature_profiles.build_temp_profiles","title":"<code>build_temp_profiles(pop_map, cutout, temperature_out)</code>","text":"<p>Build the temperature profiles in the cutout, this converts the atlite temperature &amp; weights the node building process by the population map</p> <p>Note that atlite only supports a single time zone shift</p> <p>Parameters:</p> Name Type Description Default <code>pop_map</code> <code>DataFrame</code> <p>the map to the pop density grid cell data (hdf5)</p> required <code>cutout</code> <code>Cutout</code> <p>the weather data cutout (atlite cutout)</p> required <code>temperature_out</code> <code>PathLike</code> <p>the output path (hdf5)</p> required Source code in <code>workflow/scripts/build_temperature_profiles.py</code> <pre><code>def build_temp_profiles(pop_map: pd.DataFrame, cutout: atlite.Cutout, temperature_out: PathLike):\n    \"\"\"Build the temperature profiles in the cutout, this converts the atlite temperature &amp; weights\n    the node building process by the population map\n\n    Note that atlite only supports a single time zone shift\n\n    Args:\n        pop_map (pd.DataFrame): the map to the pop density grid cell data (hdf5)\n        cutout (atlite.Cutout): the weather data cutout (atlite cutout)\n        temperature_out (PathLike): the output path (hdf5)\n    \"\"\"\n    # build a sparse matrix of BUSxCUTOUT_gridcells to weigh the cutout-&gt;bus aggregation process\n    pop_matrix = sp.sparse.csr_matrix(pop_map.T)\n    index = pop_map.columns\n    index.name = \"provinces\"\n\n    temperature = cutout.temperature(matrix=pop_matrix, index=index)\n    # convert the cutout UTC time to local time\n    temperature[\"time\"] = (\n        pd.DatetimeIndex(temperature[\"time\"], tz=\"UTC\")\n        .tz_convert(TIMEZONE)\n        .tz_localize(None)\n        .values\n    )\n\n    with pd.HDFStore(temperature_out, mode=\"w\", complevel=4) as store:\n        store[\"temperature\"] = temperature.to_pandas().divide(pop_map.sum())\n</code></pre>"},{"location":"reference/build_transport_demand/","title":"Build transport demand","text":"<p>Build transport charging demand per clustered model region. Simplified version that only generates charging demand time series.</p>"},{"location":"reference/build_transport_demand/#build_transport_demand.build_transport_demand","title":"<code>build_transport_demand(traffic_profile, nodes, sector_load_data, snapshots)</code>","text":"<p>Build transport charging demand time series (MWh) from annual totals.</p> <p>Parameters:</p> Name Type Description Default <code>traffic_profile</code> <code>Series</code> <p>Normalised weighting of weekday hours (<code>count</code> profile) defining the intraday pattern. The series length must match the expected weekly resolution of <code>generate_periodic_profiles</code>. Values do not need to be normalised; the function will normalise internally.</p> required <code>nodes</code> <code>list</code> <p>Provinces / nodes for which the time series are generated.</p> required <code>sector_load_data</code> <code>DataFrame</code> <p>Annual transport load by province (index) and year (columns) as provided by REMIND processing.</p> required <code>snapshots</code> <code>DatetimeIndex</code> <p>Target temporal resolution for the output.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: Provincial transport demand time series (MWh) whose annual</p> <code>DataFrame</code> <p>totals per node match <code>sector_load_data</code> for the target year.</p> Source code in <code>workflow/scripts/build_transport_demand.py</code> <pre><code>def build_transport_demand(\n    traffic_profile: pd.Series,\n    nodes: list,\n    sector_load_data: pd.DataFrame,\n    snapshots: pd.DatetimeIndex,\n) -&gt; pd.DataFrame:\n    \"\"\"Build transport charging demand time series (MWh) from annual totals.\n\n    Args:\n        traffic_profile (pd.Series): Normalised weighting of weekday hours (``count``\n            profile) defining the intraday pattern. The series length must match the\n            expected weekly resolution of ``generate_periodic_profiles``. Values do\n            not need to be normalised; the function will normalise internally.\n        nodes (list): Provinces / nodes for which the time series are generated.\n        sector_load_data (pd.DataFrame): Annual transport load by province (index)\n            and year (columns) as provided by REMIND processing.\n        snapshots (pd.DatetimeIndex): Target temporal resolution for the output.\n\n    Returns:\n        pd.DataFrame: Provincial transport demand time series (MWh) whose annual\n        totals per node match ``sector_load_data`` for the target year.\n    \"\"\"\n\n    if traffic_profile.empty:\n        raise ValueError(\"traffic_profile must contain at least one value\")\n\n    traffic = traffic_profile / traffic_profile.sum()\n\n    base_shape = generate_periodic_profiles(\n        dt_index=snapshots,\n        col_tzs=pd.Series(index=nodes, data=[TIMEZONE] * len(nodes)),\n        weekly_profile=traffic.values,\n    )\n\n    # 2. Get annual totals\n    target_year = str(snapshots[0].year)\n    nodal_totals = sector_load_data[target_year]\n\n    # 3. Build time series per province\n    result = pd.DataFrame(index=snapshots, columns=nodes, dtype=float)\n    for node in nodes:\n        total = nodal_totals[node]\n        shape = base_shape[node] / base_shape[node].sum()  # normalize\n        result[node] = shape * total\n\n    return result\n</code></pre>"},{"location":"reference/calculate_gebco_slope/","title":"Calculate gebco slope","text":"<p>Calculate slope from GEBCO bathymetry/altimetry data.</p> <p>This script: 1. Reprojects GEBCO data to Mollweide (ESRI:54009) for accurate slope calculation 2. Calculates slope in percent using gdaldem 3. Reprojects slope back to EPSG:4326 for compatibility with atlite</p> <p>Requires: GDAL (gdalwarp, gdaldem)</p>"},{"location":"reference/calculate_gebco_slope/#calculate_gebco_slope.calculate_slope","title":"<code>calculate_slope(input_gebco, output_slope, threads=4, log_file=None)</code>","text":"<p>Calculate slope from GEBCO data.</p> <p>The slope is calculated in an equal-area projection (Mollweide/ESRI:54009) to ensure accurate slope values, then reprojected to EPSG:4326 for use with atlite.</p> <p>Parameters:</p> Name Type Description Default <code>input_gebco</code> <p>Path to input GEBCO GeoTIFF file.</p> required <code>output_slope</code> <p>Path to output slope NetCDF file.</p> required <code>threads</code> <p>Number of threads for gdalwarp. Default is 4.</p> <code>4</code> <code>log_file</code> <p>Path to log file for appending command output (optional).</p> <code>None</code> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If input GEBCO file does not exist.</p> <code>RuntimeError</code> <p>If any GDAL command fails.</p> Source code in <code>workflow/scripts/calculate_gebco_slope.py</code> <pre><code>def calculate_slope(input_gebco, output_slope, threads=4, log_file=None):\n    \"\"\"Calculate slope from GEBCO data.\n\n    The slope is calculated in an equal-area projection (Mollweide/ESRI:54009)\n    to ensure accurate slope values, then reprojected to EPSG:4326 for use\n    with atlite.\n\n    Args:\n        input_gebco: Path to input GEBCO GeoTIFF file.\n        output_slope: Path to output slope NetCDF file.\n        threads: Number of threads for gdalwarp. Default is 4.\n        log_file: Path to log file for appending command output (optional).\n\n    Raises:\n        FileNotFoundError: If input GEBCO file does not exist.\n        RuntimeError: If any GDAL command fails.\n    \"\"\"\n    input_gebco = Path(input_gebco)\n    output_slope = Path(output_slope)\n\n    if not input_gebco.exists():\n        raise FileNotFoundError(f\"Input GEBCO file not found: {input_gebco}\")\n\n    # Create output directory if needed\n    output_slope.parent.mkdir(parents=True, exist_ok=True)\n\n    # Set up PROJ environment for GDAL\n    setup_proj_environment()\n\n    # Define temporary file paths\n    mollweide_file = output_slope.parent / (output_slope.stem + \"_mollweide.nc\")\n    slope_mollweide_file = output_slope.parent / (output_slope.stem + \"_mollweide_slope.nc\")\n\n    logger.info(\"=\" * 60)\n    logger.info(\"GEBCO Slope Calculation Pipeline\")\n    logger.info(\"=\" * 60)\n    logger.info(f\"Input:   {input_gebco}\")\n    logger.info(f\"Output:  {output_slope}\")\n    logger.info(f\"Threads: {threads}\")\n    logger.info(\"=\" * 60)\n\n    try:\n        # Step 1: Reproject to Mollweide (equal-area projection)\n        logger.info(\"Reprojecting to Mollweide...\")\n        cmd1 = (\n            f\"gdalwarp -multi -wo NUM_THREADS={threads} \"\n            \"-of netCDF -co FORMAT=NC4 \"\n            \"-s_srs 'EPSG:4326' -t_srs 'ESRI:54009'\"\n            f\" {input_gebco} {mollweide_file}\"\n        )\n        run_command(cmd1, \"Step 1: Reproject to Mollweide (ESRI:54009)\")\n        logger.info(\"Calculating slope...\")\n        # Step 2: Calculate slope in percent\n        cmd2 = (\n            f\"gdaldem slope -p \"\n            f\"-of netCDF -co FORMAT=NC4 \"\n            f\"{mollweide_file} {slope_mollweide_file}\"\n        )\n        run_command(cmd2, \"Step 2: Calculate slope (percent)\")\n\n        # Step 3: Reproject back to EPSG:4326 with compression\n        cmd3 = (\n            f\"gdalwarp -multi -wo NUM_THREADS={threads} \"\n            f\"-of netCDF -co FORMAT=NC4 \"\n            f\"-co COMPRESS=DEFLATE -co ZLEVEL=1 \"\n            f\"-s_srs 'ESRI:54009' -t_srs 'EPSG:4326' \"\n            f\"{slope_mollweide_file} {output_slope}\"\n        )\n        run_command(cmd3, \"Step 3: Reproject to EPSG:4326\")\n\n        logger.info(\"=\" * 60)\n        logger.info(\"\u2713 Slope calculation completed successfully!\")\n        logger.info(f\"Output: {output_slope}\")\n        logger.info(\"=\" * 60)\n\n    except Exception as e:\n        logger.error(f\"Pipeline failed: {e}\")\n        raise\n\n    finally:\n        # Clean up intermediate files\n        logger.info(\"Cleaning up intermediate files...\")\n        for temp_file in [mollweide_file, slope_mollweide_file]:\n            if temp_file.exists():\n                temp_file.unlink()\n                logger.info(f\"  Removed: {temp_file.name}\")\n</code></pre>"},{"location":"reference/calculate_gebco_slope/#calculate_gebco_slope.run_command","title":"<code>run_command(cmd, description)</code>","text":"<p>Run a shell command and log the output.</p> <p>Parameters:</p> Name Type Description Default <code>cmd</code> <code>str</code> <p>Shell command to execute.</p> required <code>description</code> <code>str</code> <p>Human-readable description of the command.</p> required <p>Returns:</p> Type Description <p>CompletedProcess instance from subprocess.run.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If the command fails (non-zero return code).</p> Source code in <code>workflow/scripts/calculate_gebco_slope.py</code> <pre><code>def run_command(cmd: str, description: str):\n    \"\"\"Run a shell command and log the output.\n\n    Args:\n        cmd: Shell command to execute.\n        description: Human-readable description of the command.\n\n    Returns:\n        CompletedProcess instance from subprocess.run.\n\n    Raises:\n        RuntimeError: If the command fails (non-zero return code).\n    \"\"\"\n    logger.info(f\"{description}...\")\n    logger.debug(f\"Command: {cmd}\")\n\n    result = subprocess.run(cmd, shell=True, capture_output=True, text=True, env=os.environ.copy())\n\n    if result.returncode != 0:\n        logger.error(f\"Command failed with return code {result.returncode}\")\n        logger.error(f\"STDERR: {result.stderr}\")\n        raise RuntimeError(f\"{description} failed\")\n\n    if result.stdout:\n        logger.debug(f\"STDOUT: {result.stdout}\")\n    if result.stderr:\n        logger.warning(f\"STDERR: {result.stderr}\")\n\n    logger.info(f\"\u2713 {description} completed successfully\")\n    return result\n</code></pre>"},{"location":"reference/calculate_gebco_slope/#calculate_gebco_slope.setup_proj_environment","title":"<code>setup_proj_environment()</code>","text":"<p>Set up PROJ_LIB environment variable for GDAL to find projection database.</p> Source code in <code>workflow/scripts/calculate_gebco_slope.py</code> <pre><code>def setup_proj_environment():\n    \"\"\"Set up PROJ_LIB environment variable for GDAL to find projection database.\"\"\"\n    if \"PROJ_LIB\" not in os.environ:\n        if \"CONDA_PREFIX\" in os.environ:\n            proj_lib = os.path.join(os.environ[\"CONDA_PREFIX\"], \"share\", \"proj\")\n            if os.path.isdir(proj_lib):\n                os.environ[\"PROJ_LIB\"] = proj_lib\n                logger.info(f\"Set PROJ_LIB to: {proj_lib}\")\n            else:\n                logger.warning(f\"PROJ data directory not found at: {proj_lib}\")\n        else:\n            logger.warning(\"CONDA_PREFIX not set, PROJ_LIB may not be configured\")\n    else:\n        logger.info(f\"PROJ_LIB already set to: {os.environ['PROJ_LIB']}\")\n</code></pre>"},{"location":"reference/constants/","title":"Constants","text":"<p>Soft coded centalized <code>constants</code></p>"},{"location":"reference/constants/#constants.filter_buses","title":"<code>filter_buses(names)</code>","text":"<p>Filter bus names to include only those in PROV_NAMES.</p> <p>Parameters:</p> Name Type Description Default <code>names</code> <p>Iterable of bus names to filter</p> required <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>List of names that are present in PROV_NAMES</p> Source code in <code>workflow/scripts/constants.py</code> <pre><code>def filter_buses(names) -&gt; list:\n    \"\"\"Filter bus names to include only those in PROV_NAMES.\n\n    Args:\n        names: Iterable of bus names to filter\n\n    Returns:\n        list: List of names that are present in PROV_NAMES\n    \"\"\"\n    return [name for name in names if name in PROV_NAMES]\n</code></pre>"},{"location":"reference/constants/#constants.get_province_names","title":"<code>get_province_names()</code>","text":"<p>HACK to make it possible for pytest to generate a smaller network</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>if the PROV_NAMES is not a list or str</p> <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>the province node names to build the network</p> Source code in <code>workflow/scripts/constants.py</code> <pre><code>def get_province_names() -&gt; list:\n    \"\"\"HACK to make it possible for pytest to generate a smaller network\n\n    Raises:\n        ValueError: if the PROV_NAMES is not a list or str\n\n    Returns:\n        list: the province node names to build the network\n    \"\"\"\n    default_prov_names = list(REGIONAL_GEO_TIMEZONES_DEFAULT)\n    _provs = os.getenv(\"PROV_NAMES\", default_prov_names)\n    if isinstance(_provs, str):\n        _provs = re.findall(r\"[\\w']+\", _provs)\n        if not _provs:\n            xpected = '[\"region1\", ...]'\n            err = f\"Environment var PROV_NAMES {_provs} for tests did not have expected format: \"\n            raise ValueError(err + xpected)\n    elif not isinstance(_provs, list):\n        raise ValueError(\"PROV_NAMES must be a list or str\")\n    return _provs\n</code></pre>"},{"location":"reference/determine_availability_matrix/","title":"Determine availability matrix","text":"<p>The script performs a land eligibility analysis of what share of land is availability for developing the selected technology at each cutout grid cell. The script uses the <code>atlite &lt;https://github.com/pypsa/atlite&gt;</code>_ library and several GIS datasets like the Copernicus land use data, GEBCO bathymetry data.</p> <p>Natural reserves are from https://zenodo.org/records/14875797</p> <p>The copernicus land monitoring data is/can be fetched by the pipeline. The GEBCO data is stored in the PyPSA-China-PIK zenodo bundle or must  be manually downloaded from the <code>General Bathymetric Chart of the Oceans (GEBCO)   &lt;https://www.gebco.net/data_and_products/gridded_bathymetry_data/&gt;</code>_.</p>"},{"location":"reference/fetch_rasters/","title":"Fetch rasters","text":"<p>Methods to fetch raster data from copernicus archive.Requires sentinelHub API creds Additional method to fetch bathymetry data from Cam Center for Environmental Data Analysis.</p> <p>Note that this is an EXAMPLE SCRIPT. The CLC daat works with type identifiers which sre not yet integrated into the workflow.</p>"},{"location":"reference/fetch_rasters/#fetch_rasters.authenticate","title":"<code>authenticate()</code>","text":"<p>Authenticae with the copernicus API</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>the access token dict</p> Source code in <code>workflow/scripts/fetch_rasters.py</code> <pre><code>def authenticate() -&gt; dict:\n    \"\"\"Authenticae with the copernicus API\n\n    Returns:\n        dict: the access token dict\n    \"\"\"\n    service_key = json.load(open(TOKEN_PATH, \"rb\"))\n\n    private_key = service_key[\"private_key\"].encode(\"utf-8\")\n\n    claim_set = {\n        \"iss\": service_key[\"client_id\"],\n        \"sub\": service_key[\"user_id\"],\n        \"aud\": service_key[\"token_uri\"],\n        \"iat\": int(time.time()),\n        \"exp\": int(time.time() + (60 * 60)),\n    }\n    grant = jwt.encode(claim_set, private_key, algorithm=\"RS256\")\n    base_url = claim_set[\"aud\"]\n    token_request = requests.post(\n        base_url,\n        headers={\n            \"Accept\": \"application/json\",\n            \"Content-Type\": \"application/x-www-form-urlencoded\",\n        },\n        data=f\"grant_type=urn:ietf:params:oauth:grant-type:jwt-bearer&amp;assertion={grant}\",\n    )\n    if token_request.status_code != 200:\n        print(token_request.text)\n        exit(1)\n    token = token_request.json()\n    return token\n</code></pre>"},{"location":"reference/fetch_rasters/#fetch_rasters.find_dataset_ids","title":"<code>find_dataset_ids(name='global-dynamic-land-cover')</code>","text":"<p>Find the catalogue ID of the dataset</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>dataset product name. Defaults to \"global-dynamic-land-cover\".</p> <code>'global-dynamic-land-cover'</code> <p>Returns:</p> Name Type Description <code>tuple</code> <code>tuple[str]</code> <p>the results (download_info_id, download_id)</p> Source code in <code>workflow/scripts/fetch_rasters.py</code> <pre><code>def find_dataset_ids(name=\"global-dynamic-land-cover\") -&gt; tuple[str]:\n    \"\"\"Find the catalogue ID of the dataset\n\n    Args:\n        name (str, optional): dataset product name. Defaults to \"global-dynamic-land-cover\".\n\n    Returns:\n        tuple: the results (download_info_id, download_id)\n    \"\"\"\n    bid = 0\n    # batches of 25\n    while True:\n        batch = \"\" if bid == 0 else f\"b_start={bid}&amp;\"\n        search_req = requests.get(\n            f\"https://land.copernicus.eu/api/@search?{batch}portal_type=DataSet&amp;metadata_fields=UID&amp;metadata_fields=dataset_full_format&amp;&amp;metadata_fields=dataset_download_information\",\n            headers={\"Accept\": \"application/json\"},\n        )\n        if search_req.status_code != 200:\n            logger.error(f\"failed request: {search_req.text}\")\n            exit(1)\n        search_results = search_req.json()\n        res = search_items(search_results, target_name=name)\n        if res == []:\n            bid += 25\n        else:\n            break\n\n    res_list = [r for r in search_results[\"items\"] if r[\"@id\"].find(\"2019\") != -1]\n    download_info_id = res_list[0][\"dataset_download_information\"][\"items\"][0][\"@id\"]\n    download_id = res_list[0][\"UID\"]\n    # dataset_url = res_list[0][\"@id\"]\n    return download_info_id, download_id\n</code></pre>"},{"location":"reference/fetch_rasters/#fetch_rasters.post_data_request","title":"<code>post_data_request(token, download_info_id, download_id)</code>","text":"<p>Post the request to the copernicus API</p> <p>Parameters:</p> Name Type Description Default <code>token</code> <code>dict</code> <p>the access token</p> required <code>download_info_id</code> <code>str</code> <p>output of find_dataset_ids</p> required <code>download_id</code> <code>str</code> <p>output of find_dataset_ids</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <code>tuple[dict, str]</code> <p>the data request (json dict) and task id (string?)</p> Source code in <code>workflow/scripts/fetch_rasters.py</code> <pre><code>def post_data_request(token: dict, download_info_id: str, download_id: str) -&gt; tuple[dict, str]:\n    \"\"\"Post the request to the copernicus API\n\n    Args:\n        token (dict): the access token\n        download_info_id (str): output of find_dataset_ids\n        download_id (str): output of find_dataset_ids\n\n    Returns:\n        tuple: the data request (json dict) and task id (string?)\n    \"\"\"\n    base_url = \"https://land.copernicus.eu\"\n    data_req = requests.post(\n        f\"{base_url}/api/@datarequest_post\",\n        headers={\n            \"Accept\": \"application/json\",\n            \"Content-Type\": \"application/json\",\n            \"Authorization\": f\"Bearer {token['access_token']}\",\n        },\n        json={\n            \"Datasets\": [\n                {\n                    \"DatasetID\": download_id,\n                    \"DatasetDownloadInformationID\": download_info_id,\n                    \"Layer\": \"Cover Fraction: Built-up \",\n                    \"NUTS\": \"CN\",\n                    \"OutputFormat\": \"Geotiff\",\n                    \"OutputGCS\": \"EPSG:4326\",\n                }\n            ]\n        },\n    )\n\n    return data_req.json(), data_req.json()[\"TaskIds\"][0][\"TaskID\"]\n</code></pre>"},{"location":"reference/fetch_rasters/#fetch_rasters.search_items","title":"<code>search_items(json_items, target_name='global-dynamic-land-cover')</code>","text":"<p>Filter items for target name</p> <p>Parameters:</p> Name Type Description Default <code>json_items</code> <code>dict</code> <p>the json response to the seach query</p> required <code>target_name</code> <code>str</code> <p>The items to find. Defaults to \"global-dynamic-land-cover\".</p> <code>'global-dynamic-land-cover'</code> <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>the items list</p> Source code in <code>workflow/scripts/fetch_rasters.py</code> <pre><code>def search_items(json_items: dict, target_name=\"global-dynamic-land-cover\") -&gt; list:\n    \"\"\"Filter items for target name\n\n    Args:\n        json_items (dict): the json response to the seach query\n        target_name (str, optional): The items to find. Defaults to \"global-dynamic-land-cover\".\n\n    Returns:\n        list: the items list\n    \"\"\"\n\n    return [itm[\"@id\"] for itm in json_items[\"items\"] if itm[\"@id\"].find(target_name) != -1]\n</code></pre>"},{"location":"reference/fetch_shapes/","title":"Fetch shapes","text":"<p>Data fetch operation for region/province/country shapes</p>"},{"location":"reference/fetch_shapes/#fetch_shapes.build_nodes","title":"<code>build_nodes(prefectures, nodes_cfg)</code>","text":"<p>Build the nodes, either directly at provincial (admin1) level or from adminlvk2 subregions</p> <p>Parameters:</p> Name Type Description Default <code>prefectures</code> <code>GeoDataFrame</code> required Source code in <code>workflow/scripts/fetch_shapes.py</code> <pre><code>def build_nodes(\n    prefectures: gpd.GeoDataFrame,\n    nodes_cfg: dict,\n) -&gt; gpd.GeoSeries:\n    \"\"\"Build the nodes, either directly at provincial (admin1) level or from adminlvk2 subregions\n\n    Args:\n      prefectures:\n    \"\"\"\n    gdf = prefectures.copy()\n    if nodes_cfg.get(\"split_provinces\", False):\n        validate_split_cfg(nodes_cfg[\"splits\"], gdf)\n        return split_provinces(gdf, nodes_cfg)\n    else:\n        provs = provs = gdf.dissolve(GDAM_LV1)\n        provs = provs.drop([nodes_cfg[\"exclude_provinces\"]])\n        return provs.rename_axis(\"node\")[\"geometry\"]\n</code></pre>"},{"location":"reference/fetch_shapes/#fetch_shapes.cut_smaller_from_larger","title":"<code>cut_smaller_from_larger(row, gdf, overlaps)</code>","text":"<p>Automatically assign overlapping area to the smaller region</p> Example <p>areas_gdf.apply(cut_smaller_from_larger, args=(areas_gdf, overlaps), axis=1)</p> <p>Parameters:</p> Name Type Description Default <code>row</code> <code>GeoSeries</code> <p>the row from pandas apply</p> required <code>gdf</code> <code>GeoDataFrame</code> <p>the geodataframe on which the operation is performed</p> required <code>overlaps</code> <code>DataFrame</code> <p>the boolean overlap table</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>in case areas are exactly equal</p> <p>Returns:</p> Type Description <code>GeoSeries</code> <p>gpd.GeoSeries: the row with overlaps removed or not</p> Source code in <code>workflow/scripts/fetch_shapes.py</code> <pre><code>def cut_smaller_from_larger(\n    row: gpd.GeoSeries, gdf: gpd.GeoDataFrame, overlaps: DataFrame\n) -&gt; gpd.GeoSeries:\n    \"\"\"Automatically assign overlapping area to the smaller region\n\n    Example:\n        areas_gdf.apply(cut_smaller_from_larger, args=(areas_gdf, overlaps), axis=1)\n\n    Args:\n        row (gpd.GeoSeries): the row from pandas apply\n        gdf (gpd.GeoDataFrame): the geodataframe on which the operation is performed\n        overlaps (DataFrame): the boolean overlap table\n\n    Raises:\n        ValueError: in case areas are exactly equal\n\n    Returns:\n        gpd.GeoSeries: the row with overlaps removed or not\n    \"\"\"\n    ovrlap_idx = np.where(overlaps.loc[row.name].values == True)[0].tolist()\n    for idx in ovrlap_idx:\n        geom = gdf.iloc[idx].geometry\n        if row.geometry.area &gt; geom.area:\n            row[\"geometry\"] = row[\"geometry\"].difference(geom)\n        elif row.geometry.area == geom.area:\n            raise ValueError(f\"Equal area overlap between {row.name} and {idx} - unhandled\")\n    return row\n</code></pre>"},{"location":"reference/fetch_shapes/#fetch_shapes.eez_by_region","title":"<code>eez_by_region(eez, province_shapes, prov_key='region', simplify_tol=0.5)</code>","text":"<p>Break up the eez by admin1 regions based on voronoi polygons of the centroids</p> <p>Parameters:</p> Name Type Description Default <code>eez</code> <code>GeoDataFrame</code> <p>description</p> required <code>province_shapes</code> <code>GeoDataFrame</code> <p>description</p> required <code>prov_key</code> <code>str</code> <p>name of the provinces col in province_shapes. Defaults to \"region\".</p> <code>'region'</code> <code>simplify_tol</code> <code>float</code> <p>tolerance for simplifying the voronoi polygons. Defaults to 0.5.</p> <code>0.5</code> <p>Returns:</p> Type Description <code>GeoDataFrame</code> <p>gpd.GeoDataFrame: description</p> Source code in <code>workflow/scripts/fetch_shapes.py</code> <pre><code>def eez_by_region(\n    eez: gpd.GeoDataFrame,\n    province_shapes: gpd.GeoDataFrame,\n    prov_key=\"region\",\n    simplify_tol=0.5,\n) -&gt; gpd.GeoDataFrame:\n    \"\"\"Break up the eez by admin1 regions based on voronoi polygons of the centroids\n\n    Args:\n        eez (gpd.GeoDataFrame): _description_\n        province_shapes (gpd.GeoDataFrame): _description_\n        prov_key (str, optional): name of the provinces col in province_shapes. Defaults to \"region\".\n        simplify_tol (float, optional): tolerance for simplifying the voronoi polygons. Defaults to 0.5.\n\n    Returns:\n        gpd.GeoDataFrame: _description_\n    \"\"\"\n    # generate voronoi cells (more than one per province &amp; can overlap)\n    voronois_simple = gpd.GeoDataFrame(\n        geometry=province_shapes.simplify(tolerance=simplify_tol).voronoi_polygons(),\n        crs=province_shapes.crs,\n    )\n    # assign region\n    prov_voronoi = (\n        voronois_simple.sjoin(province_shapes, predicate=\"intersects\")\n        .groupby(prov_key)\n        .apply(lambda x: x.union_all(\"unary\"))\n    )\n    prov_voronoi = gpd.GeoDataFrame(\n        geometry=prov_voronoi.values,\n        crs=province_shapes.crs,\n        data={prov_key: prov_voronoi.index},\n    )\n\n    # remove overlaps\n    gdf_ = remove_overlaps(prov_voronoi.set_index(prov_key))\n\n    eez_prov = (\n        gdf_.reset_index()\n        .overlay(eez, how=\"intersection\")[[prov_key, \"geometry\"]]\n        .groupby(prov_key)\n        .apply(lambda x: x.union_all(\"unary\"))\n    )\n    eez_prov = gpd.GeoDataFrame(\n        geometry=eez_prov.values,\n        crs=province_shapes.crs,\n        data={prov_key: eez_prov.index},\n    )\n\n    return eez_prov[eez_prov[prov_key].isin(OFFSHORE_WIND_NODES)].set_index(prov_key)\n</code></pre>"},{"location":"reference/fetch_shapes/#fetch_shapes.fetch_country_shape","title":"<code>fetch_country_shape(outp_path)</code>","text":"<p>Fetch the country shape from natural earth and save it to the outpath</p> <p>Parameters:</p> Name Type Description Default <code>outp_path</code> <code>PathLike</code> <p>the path to save the country shape (geojson)</p> required Source code in <code>workflow/scripts/fetch_shapes.py</code> <pre><code>def fetch_country_shape(outp_path: PathLike):\n    \"\"\"Fetch the country shape from natural earth and save it to the outpath\n\n    Args:\n        outp_path (PathLike): the path to save the country shape (geojson)\n    \"\"\"\n\n    country_shape = fetch_natural_earth_shape(\"admin_0_countries\", \"ADMIN\", COUNTRY_NAME)\n    country_shape.set_index(\"region\", inplace=True)\n    country_shape.to_file(outp_path, driver=\"GeoJSON\")\n</code></pre>"},{"location":"reference/fetch_shapes/#fetch_shapes.fetch_gadm","title":"<code>fetch_gadm(country_code='CHN', level=2)</code>","text":"<p>Fetch GADM shapefile for a given country and administrative level. https://gadm.org/download_country.html</p> <p>Parameters:</p> Name Type Description Default <code>country_code</code> <code>str</code> <p>ISO3 country code (e.g., 'CHN', 'USA').</p> <code>'CHN'</code> <code>level</code> <code>int</code> <p>Administrative level (0=country, 1=region, etc.).</p> <code>2</code> <p>Returns:</p> Type Description <p>geopandas.GeoDataFrame: Loaded shapefile as GeoDataFrame.</p> Source code in <code>workflow/scripts/fetch_shapes.py</code> <pre><code>def fetch_gadm(country_code=\"CHN\", level=2):\n    \"\"\"\n    Fetch GADM shapefile for a given country and administrative level.\n    https://gadm.org/download_country.html\n\n    Parameters:\n        country_code (str): ISO3 country code (e.g., 'CHN', 'USA').\n        level (int): Administrative level (0=country, 1=region, etc.).\n\n    Returns:\n        geopandas.GeoDataFrame: Loaded shapefile as GeoDataFrame.\n    \"\"\"\n    # Construct the URL\n    url = f\"https://geodata.ucdavis.edu/gadm/gadm4.1/shp/gadm41_{country_code}_shp.zip\"\n\n    # Download the zip file\n    response = requests.get(url)\n    if response.status_code != 200:\n        raise ValueError(\n            f\"Failed to download data for {country_code} - Status code: {response.status_code}\"\n        )\n\n    # Extract the zip file in memory\n    with zipfile.ZipFile(io.BytesIO(response.content)) as z:\n        # Filter to the desired level shapefile\n        level_filename = f\"gadm41_{country_code}_{level}.shp\"\n        if level_filename not in z.namelist():\n            raise ValueError(f\"Level {level} shapefile not found for {country_code}.\")\n\n        shp_dir = \"resources/data/province_shapes\"\n        z.extractall(shp_dir)\n        gdf = gpd.read_file(f\"{shp_dir}/{level_filename}\")\n\n    return gdf\n</code></pre>"},{"location":"reference/fetch_shapes/#fetch_shapes.fetch_maritime_eez","title":"<code>fetch_maritime_eez(zone_name)</code>","text":"<p>Fetch maritime data for a country from Maritime Gazette API# (Royal marine institute of Flanders data base)</p> <p>Parameters:</p> Name Type Description Default <code>zone_name</code> <code>str</code> <p>the country's zone name, e.g \"Chinese\" for china</p> required <p>Raises:</p> Type Description <code>HTTPError</code> <p>if the request fails</p> <p>Returns:     dict: the maritime data</p> Source code in <code>workflow/scripts/fetch_shapes.py</code> <pre><code>def fetch_maritime_eez(zone_name: str) -&gt; gpd.GeoDataFrame:\n    \"\"\"Fetch maritime data for a country from Maritime Gazette API#\n    (Royal marine institute of Flanders data base)\n\n    Args:\n        zone_name (str): the country's zone name, e.g \"Chinese\" for china\n\n    Raises:\n        requests.HTTPError: if the request fails\n    Returns:\n        dict: the maritime data\n    \"\"\"\n\n    def find_record_id(zone_name: str) -&gt; int:\n        # get Maritime Gazette record ID for the country\n        # eez ID is 70: see https://www.marineregions.org/gazetteer.php?p=webservices&amp;type=rest#/\n        url = f\"https://www.marineregions.org/rest/getGazetteerRecordsByName.json/{zone_name}/?like=true&amp;fuzzy=false&amp;typeID=70&amp;offset=0&amp;count=100\"\n        response = requests.get(url)\n        if response.status_code != 200:\n            raise requests.HTTPError(\n                f\"Failed to retrieve Maritime Gazette ID. Status code: {response.status_code}\"\n            )\n        record_data = response.json()\n        logger.debug(record_data)\n        return [\n            data\n            for data in record_data\n            if (data[\"status\"] == \"standard\")\n            and (data[\"preferredGazetteerName\"].lower().find(zone_name.lower()) != -1)\n        ][0][\"MRGID\"]\n\n    mgrid = find_record_id(zone_name)\n    logger.debug(f\"Found Maritime Gazette ID for {zone_name}: {mgrid}\")\n    #  URL of the WFS service\n    url = \"https://geo.vliz.be/geoserver/wfs\"\n    # WFS request parameters + record ID filter\n    base_filter_ = \"&lt;Filter&gt;&lt;PropertyIsEqualTo&gt;&lt;PropertyName&gt;mrgid_eez&lt;/PropertyName&gt;&lt;Literal&gt;\"\n    filter_ = base_filter_ + f\"{mgrid}&lt;/Literal&gt;&lt;/PropertyIsEqualTo&gt;&lt;/Filter&gt;\"\n    params = dict(\n        service=\"WFS\",\n        version=\"1.1.0\",\n        request=\"GetFeature\",\n        typeName=\"MarineRegions:eez\",\n        outputFormat=\"json\",\n        filter=filter_,\n    )\n\n    # Fetch data from WFS using requests\n    response_eez = requests.get(url, params=params)\n\n    # Check for successful request\n    if response_eez.status_code == 200:\n        data = response_eez.json()\n    else:\n        logger.error(f\"Error: {response_eez.status_code}\")\n        raise requests.HTTPError(\n            f\"Failed to retrieve Maritime Gazette data. Status code: {response_eez.status_code}\"\n        )\n    if data[\"totalFeatures\"] != 1:\n        raise ValueError(f\"Expected 1 feature, got {data['totalFeatures']}\\n: {data}\")\n    crs = data[\"crs\"][\"properties\"][\"name\"].split(\"EPSG::\")[-1]\n    eez = gpd.GeoDataFrame.from_features(data[\"features\"])\n    return eez.set_crs(epsg=crs)\n</code></pre>"},{"location":"reference/fetch_shapes/#fetch_shapes.fetch_natural_earth_shape","title":"<code>fetch_natural_earth_shape(dataset_name, filter_key, filter_value='China', region_key=None)</code>","text":"<p>Fetch region or country shape from natural earth dataset and filter</p> <p>Parameters:</p> Name Type Description Default <code>dataset_name</code> <code>str</code> <p>the name of the natural earth dataset to fetch</p> required <code>filter_key</code> <code>str</code> <p>key to filter the records by</p> required <code>filter_value</code> <code>str | list</code> <p>filter pass value. Defaults to \"China\".</p> <code>'China'</code> Example <p>china country: build_natural_earth_shape(\"admin_0_countries\", \"ADMIN\", \"China\") china provinces: build_natural_earth_shape(\"admin_1_states_provinces\",     \"iso_a2\", \"CN\", region_key=\"name_en\")</p> <p>Returns:</p> Type Description <code>GeoDataFrame</code> <p>gpd.GeoDataFrame: the filtered records</p> Source code in <code>workflow/scripts/fetch_shapes.py</code> <pre><code>def fetch_natural_earth_shape(\n    dataset_name: str, filter_key: str, filter_value=\"China\", region_key=None\n) -&gt; gpd.GeoDataFrame:\n    \"\"\"Fetch region or country shape from natural earth dataset and filter\n\n    Args:\n        dataset_name (str): the name of the natural earth dataset to fetch\n        filter_key (str): key to filter the records by\n        filter_value (str|list, optional): filter pass value. Defaults to \"China\".\n\n    Example:\n        china country: build_natural_earth_shape(\"admin_0_countries\", \"ADMIN\", \"China\")\n        china provinces: build_natural_earth_shape(\"admin_1_states_provinces\",\n            \"iso_a2\", \"CN\", region_key=\"name_en\")\n\n    Returns:\n        gpd.GeoDataFrame: the filtered records\n    \"\"\"\n    shpfilename = shpreader.natural_earth(\n        resolution=NATURAL_EARTH_RESOLUTION, category=\"cultural\", name=dataset_name\n    )\n    reader = shpreader.Reader(shpfilename)\n    records = list(reader.records())\n    if not region_key:\n        region_key = filter_key\n    if isinstance(filter_value, list):\n        gdf = gpd.GeoDataFrame(\n            [\n                {\"region\": c.attributes[region_key], \"geometry\": c.geometry}\n                for c in records\n                if c.attributes[filter_key] in filter_value\n            ]\n        )\n    else:\n        gdf = gpd.GeoDataFrame(\n            [\n                {\"region\": c.attributes[region_key], \"geometry\": c.geometry}\n                for c in records\n                if c.attributes[filter_key] == filter_value\n            ]\n        )\n    gdf.set_crs(epsg=CRS, inplace=True)\n    return gdf\n</code></pre>"},{"location":"reference/fetch_shapes/#fetch_shapes.fetch_prefecture_shapes","title":"<code>fetch_prefecture_shapes(fixes={GDAM_LV1: {'Nei Mongol': 'InnerMongolia', 'Xinjiang Uygur': 'Xinjiang', 'Hong Kong': 'HongKong', 'Ningxia Hui': 'Ningxia'}})</code>","text":"<p>Fetch county-level shapefiles for China.</p> <p>Parameters:</p> Name Type Description Default <code>fixes</code> <code>(dict, Optional)</code> <p>Dictionary mapping old names to new names for specific columns.</p> <code>{GDAM_LV1: {'Nei Mongol': 'InnerMongolia', 'Xinjiang Uygur': 'Xinjiang', 'Hong Kong': 'HongKong', 'Ningxia Hui': 'Ningxia'}}</code> Source code in <code>workflow/scripts/fetch_shapes.py</code> <pre><code>def fetch_prefecture_shapes(\n    fixes={\n        GDAM_LV1: {\n            \"Nei Mongol\": \"InnerMongolia\",\n            \"Xinjiang Uygur\": \"Xinjiang\",\n            \"Hong Kong\": \"HongKong\",\n            \"Ningxia Hui\": \"Ningxia\",\n        }\n    },\n):\n    \"\"\"\n    Fetch county-level shapefiles for China.\n\n    Args:\n        fixes (dict, Optional): Dictionary mapping old names to new names for specific columns.\n    \"\"\"\n    gdf = fetch_gadm(country_code=\"CHN\", level=2)\n    for col, fix_dict in fixes.items():\n        for old_name, new_name in fix_dict.items():\n            mask = gdf.query(f\"{col} == '{old_name}'\").index\n            gdf.loc[mask, col] = new_name\n    return gdf[[\"COUNTRY\", \"NAME_1\", \"NAME_2\", \"geometry\"]]\n</code></pre>"},{"location":"reference/fetch_shapes/#fetch_shapes.fetch_province_shapes","title":"<code>fetch_province_shapes()</code>","text":"<p>Fetch the province shapes from natural earth and save it to the outpath</p> <p>Returns:</p> Type Description <code>GeoDataFrame</code> <p>gpd.GeoDataFrame: the province shapes</p> Source code in <code>workflow/scripts/fetch_shapes.py</code> <pre><code>def fetch_province_shapes() -&gt; gpd.GeoDataFrame:\n    \"\"\"Fetch the province shapes from natural earth and save it to the outpath\n\n    Returns:\n        gpd.GeoDataFrame: the province shapes\n    \"\"\"\n\n    province_shapes = fetch_natural_earth_shape(\n        \"admin_1_states_provinces\", \"iso_a2\", COUNTRY_ISO, region_key=\"name_en\"\n    )\n    province_shapes.rename(columns={\"region\": \"province\"}, inplace=True)\n    province_shapes.province = province_shapes.province.str.replace(\" \", \"\")\n    province_shapes.sort_values(\"province\", inplace=True)\n    logger.debug(\"province shapes:\\n\", province_shapes)\n\n    filtered = province_shapes[province_shapes[\"province\"].isin(PROV_NAMES)]\n    if (filtered[\"province\"].unique() != sorted(PROV_NAMES)).all():\n        logger.warning(\n            f\"Missing provinces: {set(PROV_NAMES) - set(province_shapes['province'].unique())}\"\n        )\n    filtered.set_index(\"province\", inplace=True)\n\n    return filtered.sort_index()\n</code></pre>"},{"location":"reference/fetch_shapes/#fetch_shapes.has_overlap","title":"<code>has_overlap(gdf)</code>","text":"<p>Check for spatial overlaps across rows</p> <p>Parameters:</p> Name Type Description Default <code>gdf</code> <code>GeoDataFrame</code> <p>the geodataframe to check</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>Index x Index boolean dataframe</p> Source code in <code>workflow/scripts/fetch_shapes.py</code> <pre><code>def has_overlap(gdf: gpd.GeoDataFrame) -&gt; DataFrame:\n    \"\"\"Check for spatial overlaps across rows\n\n    Args:\n        gdf (gpd.GeoDataFrame): the geodataframe to check\n\n    Returns:\n        DataFrame: Index x Index boolean dataframe\n    \"\"\"\n    return gdf.apply(\n        lambda row: gdf[gdf.index != row.name].geometry.apply(\n            lambda geom: row.geometry.intersects(geom)\n        ),\n        axis=1,\n    )\n</code></pre>"},{"location":"reference/fetch_shapes/#fetch_shapes.remove_overlaps","title":"<code>remove_overlaps(gdf)</code>","text":"<p>Remove inter row overlaps from a GeoDataFrame, cutting out the smaller region from the larger one</p> <p>Parameters:</p> Name Type Description Default <code>gdf</code> <code>GeoDataFrame</code> <p>the geodataframe to be treated</p> required <p>Returns:</p> Type Description <code>GeoDataFrame</code> <p>gpd.GeoDataFrame: the treated geodataframe</p> Source code in <code>workflow/scripts/fetch_shapes.py</code> <pre><code>def remove_overlaps(gdf: gpd.GeoDataFrame) -&gt; gpd.GeoDataFrame:\n    \"\"\"Remove inter row overlaps from a GeoDataFrame, cutting out the smaller region from the larger one\n\n    Args:\n        gdf (gpd.GeoDataFrame): the geodataframe to be treated\n\n    Returns:\n        gpd.GeoDataFrame: the treated geodataframe\n    \"\"\"\n    overlaps = has_overlap(gdf)\n    return gdf.apply(cut_smaller_from_larger, args=(gdf, overlaps), axis=1)\n</code></pre>"},{"location":"reference/fetch_shapes/#fetch_shapes.split_provinces","title":"<code>split_provinces(prefectures, node_config)</code>","text":"<p>Split Inner Mongolia into East and West regions based on prefectures.</p> <p>Parameters:</p> Name Type Description Default <code>prefectures</code> <code>GeoDataFrame</code> <p>Gall chinese prefectures.</p> required <code>node_config</code> <code>dict</code> <p>the configuration for node build</p> required <p>Returns:     gpd.GeoDataFrame: Updated GeoDataFrame with Inner Mongolia split EAST/WEST.</p> Source code in <code>workflow/scripts/fetch_shapes.py</code> <pre><code>def split_provinces(prefectures: gpd.GeoDataFrame, node_config: dict) -&gt; gpd.GeoSeries:\n    \"\"\"\n    Split Inner Mongolia into East and West regions based on prefectures.\n\n    Args:\n        prefectures (gpd.GeoDataFrame): Gall chinese prefectures.\n        node_config (dict): the configuration for node build\n    Returns:\n        gpd.GeoDataFrame: Updated GeoDataFrame with Inner Mongolia split EAST/WEST.\n    \"\"\"\n    gdf = prefectures.copy()\n    for admin1, splits in node_config[\"splits\"].items():\n        mask = gdf.query(f\"{GDAM_LV1} == '{admin1}'\").index\n        splits_inv = {vv: admin1 + \"_\" + k for k, v in splits.items() for vv in v}\n        gdf.loc[mask, GDAM_LV1] = gdf.loc[mask, \"NAME_2\"].map(splits_inv)\n\n    # merge geometries by node\n    gdf.rename(columns={GDAM_LV1: \"node\"}, inplace=True)\n    return gdf[[\"node\", \"geometry\"]].dissolve(by=\"node\", aggfunc=\"sum\")\n</code></pre>"},{"location":"reference/fetch_shapes/#fetch_shapes.validate_split_cfg","title":"<code>validate_split_cfg(split_cfg, gdf)</code>","text":"<p>Validate the province split configuration. The province (admin level 1) is split by admin level 2 {subregion: [prefecture names],..}. The prefecture names must be unique and cover all admin2 in the admin1 level.</p> <p>Parameters:</p> Name Type Description Default <code>split_cfg</code> <code>dict</code> <p>the configuration for the prefecture split</p> required <code>gdf</code> <code>GeoDataFrame</code> <p>the geodataframe with prefecture shapes</p> required <p>Raises:     ValueError: if the prefectures are not unique or do not cover all admin2 in the admin1 level</p> Source code in <code>workflow/scripts/fetch_shapes.py</code> <pre><code>def validate_split_cfg(split_cfg: dict, gdf: gpd.GeoDataFrame):\n    \"\"\"Validate the province split configuration.\n    The province (admin level 1) is split by admin level 2 {subregion: [prefecture names],..}.\n    The prefecture names must be unique and cover all admin2 in the admin1 level.\n\n    Args:\n        split_cfg (dict): the configuration for the prefecture split\n        gdf (gpd.GeoDataFrame): the geodataframe with prefecture shapes\n    Raises:\n        ValueError: if the prefectures are not unique or do not cover all admin2 in the admin1 level\n    \"\"\"\n    # validate_settings\n    for admin1 in split_cfg:\n        if admin1 not in gdf[GDAM_LV1].unique():\n            err_ = f\"Invalid admin1 entry {admin1} not found in provinces {gdf[GDAM_LV1].unique()}\"\n            raise ValueError(err_)\n\n        # flatten values\n        admin2 = []\n        for names, v in split_cfg[admin1].items():\n            admin2 += v\n\n        # check completeness\n        all_admin2 = gdf.query(f'{GDAM_LV1} == \"{admin1}\"')[GDAM_LV2].unique().tolist()\n        if not sorted(admin2) == sorted(all_admin2):\n            raise ValueError(\n                f\"{admin1} prefectures do not match expected:\\ngot {admin2}\\nvs\\n {all_admin2}\"\n            )\n\n        # check uniqueness (pop -&gt; must be after completeness check)\n        duplicated = any([admin2.pop() in admin2 for i in range(len(admin2))])\n        if duplicated:\n            raise ValueError(f\"Duplicated prefecture names in {admin1}: {admin2}\")\n</code></pre>"},{"location":"reference/functions/","title":"Functions","text":"<p>Maths calculations used in the PyPSA-China workflow.</p>"},{"location":"reference/functions/#functions.HVAC_cost_curve","title":"<code>HVAC_cost_curve(distance)</code>","text":"<p>Calculate the cost of HVAC lines based on distance.</p> <p>Parameters:</p> Name Type Description Default <code>distance</code> <code>float</code> <p>distance in km</p> required <p>Returns:     float: cost in currency</p> Source code in <code>workflow/scripts/functions.py</code> <pre><code>def HVAC_cost_curve(distance):\n    \"\"\"Calculate the cost of HVAC lines based on distance.\n\n    Args:\n        distance (float): distance in km\n    Returns:\n        float: cost in currency\n    \"\"\"\n    raise DeprecationWarning(\"Function is invalid do not use\")\n    d = np.array([608, 656, 730, 780, 903, 920, 1300])\n    c = 1000 / 7.5 * np.array([5.5, 4.71, 5.5, 5.57, 5.5, 5.5, 5.51])\n\n    c_func = interpolate.interp1d(d, c, fill_value=\"extrapolate\")\n    c_results = c_func(distance)\n\n    return c_results\n</code></pre>"},{"location":"reference/functions/#functions.area_from_lon_lat_poly","title":"<code>area_from_lon_lat_poly(geometry)</code>","text":"<p>For shapely geometry in lon-lat coordinates, returns area in m^2.</p> <p>Parameters:</p> Name Type Description Default <code>geometry</code> <code>Polygon</code> <p>Polygon geometry in lon-lat coordinates.</p> required <p>Returns:</p> Name Type Description <code>float</code> <p>Area of the polygon in m^2.</p> Source code in <code>workflow/scripts/functions.py</code> <pre><code>def area_from_lon_lat_poly(geometry: Polygon):\n    \"\"\"For shapely geometry in lon-lat coordinates,\n    returns area in m^2.\n\n    Args:\n        geometry (Polygon): Polygon geometry in lon-lat coordinates.\n\n    Returns:\n        float: Area of the polygon in m^2.\n    \"\"\"\n\n    project = partial(\n        pyproj.transform,\n        pyproj.Proj(init=\"epsg:4326\"),\n        pyproj.Proj(proj=\"aea\"),  # Source: Lon-Lat\n    )  # Target: Albers Equal Area Conical https://en.wikipedia.org/wiki/Albers_projection\n    # TODO fix\n    new_geometry = transform(project, geometry)\n\n    # default area is in m^2\n    return new_geometry.area / 1e6\n</code></pre>"},{"location":"reference/functions/#functions.cartesian","title":"<code>cartesian(s1, s2)</code>","text":"<p>Compute the Cartesian product of two pandas Series.</p> <p>Parameters:</p> Name Type Description Default <code>s1</code> <code>Series</code> <p>first series</p> required <code>s2</code> <code>Series</code> <p>second series</p> required <p>Returns:     pd.DataFrame: A DataFrame representing the Cartesian product of s1 and s2.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; s1 = pd.Series([1, 2, 3], index=[\"a\", \"b\", \"c\"])\n&gt;&gt;&gt; s2 = pd.Series([4, 5, 6], index=[\"d\", \"e\", \"f\"])\n&gt;&gt;&gt; cartesian(s1, s2)\nd  e   f\na  4  5   6\nb  8 10  12\nc 12 15  18\n</code></pre> Source code in <code>workflow/scripts/functions.py</code> <pre><code>def cartesian(s1: pd.Series, s2: pd.Series) -&gt; pd.DataFrame:\n    \"\"\"\n    Compute the Cartesian product of two pandas Series.\n\n    Args:\n        s1 (pd.Series): first series\n        s2 (pd.Series): second series\n    Returns:\n        pd.DataFrame: A DataFrame representing the Cartesian product of s1 and s2.\n\n    Examples:\n        &gt;&gt;&gt; s1 = pd.Series([1, 2, 3], index=[\"a\", \"b\", \"c\"])\n        &gt;&gt;&gt; s2 = pd.Series([4, 5, 6], index=[\"d\", \"e\", \"f\"])\n        &gt;&gt;&gt; cartesian(s1, s2)\n        d  e   f\n        a  4  5   6\n        b  8 10  12\n        c 12 15  18\n    \"\"\"\n    return pd.DataFrame(np.outer(s1, s2), index=s1.index, columns=s2.index)\n</code></pre>"},{"location":"reference/functions/#functions.get_poly_center","title":"<code>get_poly_center(poly)</code>","text":"<p>Get the geographic centroid of a polygon geometry.</p> <p>Extracts the centroid coordinates from a polygon object, typically used for plotting and spatial analysis in geographic applications.</p> <p>Parameters:</p> Name Type Description Default <code>poly</code> <code>Polygon</code> <p>A (shapely) polygon geometry object with a  centroid attribute that has x and y coordinate arrays.</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <p>A tuple containing (x, y) coordinates of the polygon centroid as floating point numbers.</p> Example <p>from shapely.geometry import Polygon polygon = Polygon([(0, 0), (1, 0), (1, 1), (0, 1)]) center = get_poly_center(polygon) print(center) (0.5, 0.5)</p> Note <p>This function assumes the polygon object has a centroid attribute with xy arrays containing coordinate data.</p> Source code in <code>workflow/scripts/functions.py</code> <pre><code>def get_poly_center(poly: Polygon):\n    \"\"\"Get the geographic centroid of a polygon geometry.\n\n    Extracts the centroid coordinates from a polygon object, typically used\n    for plotting and spatial analysis in geographic applications.\n\n    Args:\n        poly (Polygon): A (shapely) polygon geometry object with a \n            centroid attribute that has x and y coordinate arrays.\n\n    Returns:\n        tuple: A tuple containing (x, y) coordinates of the polygon centroid\n            as floating point numbers.\n\n    Example:\n        &gt;&gt;&gt; from shapely.geometry import Polygon\n        &gt;&gt;&gt; polygon = Polygon([(0, 0), (1, 0), (1, 1), (0, 1)])\n        &gt;&gt;&gt; center = get_poly_center(polygon)\n        &gt;&gt;&gt; print(center)\n        (0.5, 0.5)\n\n    Note:\n        This function assumes the polygon object has a centroid attribute\n        with xy arrays containing coordinate data.\n    \"\"\"\n    return (poly.centroid.xy[0][0], poly.centroid.xy[1][0])\n</code></pre>"},{"location":"reference/functions/#functions.haversine","title":"<code>haversine(p1, p2)</code>","text":"<p>Calculate the great circle distance between two points on Earth.</p> <p>Uses the Haversine formula to compute the shortest distance over the Earth's surface between two points specified in decimal degrees latitude and longitude. This is useful for calculating distances between geographic locations.</p> <p>Parameters:</p> Name Type Description Default <code>p1</code> <code>Point</code> <p>location 1 in decimal deg</p> required <code>p2</code> <code>Point</code> <p>location 2 in decimal deg</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>Great circle distance between the two points in kilometers.</p> Example <p>from shapely.geometry import Point beijing = Point(116.4074, 39.9042)  # longitude, latitude shanghai = Point(121.4737, 31.2304) distance = haversine(beijing, shanghai) print(f\"Distance: {distance:.1f} km\") Distance: 1067.1 km</p> Note <p>The function assumes the Earth is a perfect sphere with radius 6371 km.</p> Source code in <code>workflow/scripts/functions.py</code> <pre><code>def haversine(p1, p2) -&gt; float:\n    \"\"\"Calculate the great circle distance between two points on Earth.\n\n    Uses the Haversine formula to compute the shortest distance over the Earth's\n    surface between two points specified in decimal degrees latitude and longitude.\n    This is useful for calculating distances between geographic locations.\n\n    Args:\n        p1 (shapely.Point): location 1 in decimal deg\n        p2 (shapely.Point): location 2 in decimal deg\n\n    Returns:\n        float: Great circle distance between the two points in kilometers.\n\n    Example:\n        &gt;&gt;&gt; from shapely.geometry import Point\n        &gt;&gt;&gt; beijing = Point(116.4074, 39.9042)  # longitude, latitude\n        &gt;&gt;&gt; shanghai = Point(121.4737, 31.2304)\n        &gt;&gt;&gt; distance = haversine(beijing, shanghai)\n        &gt;&gt;&gt; print(f\"Distance: {distance:.1f} km\")\n        Distance: 1067.1 km\n\n    Note:\n        The function assumes the Earth is a perfect sphere with radius 6371 km.\n    \"\"\"\n\n    # convert decimal degrees to radians\n    lon1, lat1, lon2, lat2 = map(radians, [p1[0], p1[1], p2[0], p2[1]])\n\n    # haversine formula\n    dlon = lon2 - lon1\n    dlat = lat2 - lat1\n    a = sin(dlat / 2) ** 2 + cos(lat1) * cos(lat2) * sin(dlon / 2) ** 2\n    c = 2 * asin(sqrt(a))\n    r = 6371  # Radius of earth in kilometers. Use 3956 for miles\n    return c * r\n</code></pre>"},{"location":"reference/make_summary/","title":"Make summary","text":"<p>Create summary CSV files for all scenario runs including costs, capacities, capacity factors, curtailment, energy balances, prices and other metrics.</p>"},{"location":"reference/make_summary/#make_summary.assign_carriers","title":"<code>assign_carriers(n)</code>","text":"<p>Assign AC where missing Args:     n (pypsa.Network): the network object to fix</p> Source code in <code>workflow/scripts/make_summary.py</code> <pre><code>def assign_carriers(n: pypsa.Network):\n    \"\"\"Assign AC where missing\n    Args:\n        n (pypsa.Network): the network object to fix\n    \"\"\"\n    if \"carrier\" not in n.lines:\n        n.lines[\"carrier\"] = \"AC\"\n</code></pre>"},{"location":"reference/make_summary/#make_summary.calculate_capacities","title":"<code>calculate_capacities(n, label, capacities, adjust_link_capacities=None)</code>","text":"<p>Calculate the optimal capacities by carrier and bus carrier</p> <p>For links that connect to AC buses (bus1=AC), the capacity can be multiplied by efficiency to report the actual capacity available at the AC side rather than the input side. This ensures consistent capacity reporting across the network.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>Network</code> <p>the network object</p> required <code>label</code> <code>str</code> <p>the label used by make summaries</p> required <code>capacities</code> <code>DataFrame</code> <p>the dataframe to fill/update</p> required <code>adjust_link_capacities</code> <code>bool</code> <p>Whether to adjust link capacities by efficiency. If None, reads from config. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: updated capacities</p> Source code in <code>workflow/scripts/make_summary.py</code> <pre><code>def calculate_capacities(\n    n: pypsa.Network, label: str, capacities: pd.DataFrame, adjust_link_capacities=None\n) -&gt; pd.DataFrame:\n    \"\"\"Calculate the optimal capacities by carrier and bus carrier\n\n    For links that connect to AC buses (bus1=AC), the capacity can be multiplied by efficiency\n    to report the actual capacity available at the AC side rather than the input side.\n    This ensures consistent capacity reporting across the network.\n\n    Args:\n        n (pypsa.Network): the network object\n        label (str): the label used by make summaries\n        capacities (pd.DataFrame): the dataframe to fill/update\n        adjust_link_capacities (bool, optional): Whether to adjust link capacities by efficiency.\n            If None, reads from config. Defaults to None.\n\n    Returns:\n        pd.DataFrame: updated capacities\n    \"\"\"\n\n    # Temporarily save original link capacities\n    original_p_nom_opt = n.links.p_nom_opt.copy()\n\n    # Drop reversed links &amp; report AC capacities for links from X to AC\n    if adjust_link_capacities:\n        # For links where bus1 is AC, multiply capacity by efficiency coefficient to get AC side capacity\n        ac_links = n.links[n.links.bus1.map(n.buses.carrier) == \"AC\"].index\n        n.links.loc[ac_links, \"p_nom_opt\"] *= n.links.loc[ac_links, \"efficiency\"]\n\n        # ignore lossy link dummies\n        pseudo_links = n.links.query(\"Link.str.contains('reversed') &amp; capital_cost ==0 \").index\n        n.links.loc[pseudo_links, \"p_nom_opt\"] = 0\n    # Calculate optimal capacity using default grouper\n    caps = n.statistics.optimal_capacity(\n        groupby=pypsa.statistics.get_carrier_and_bus_carrier, nice_names=False\n    )\n\n    # Restore original link capacities to avoid modifying the network object\n    n.links.p_nom_opt = original_p_nom_opt\n\n    if \"load shedding\" in caps.index.get_level_values(1):\n        caps.drop(\"load shedding\", level=1, inplace=True)\n    caps.rename(index={\"AC\": \"Transmission Lines\"}, inplace=True, level=1)\n\n    # track links that feed into AC\n    mask = (n.links.bus1.map(n.buses.carrier) == \"AC\") &amp; (n.links.carrier != \"stations\")\n    to_ac = n.links.loc[mask, \"carrier\"].unique()\n\n    caps_df = caps.reset_index()\n    ac_mask = caps_df[\"carrier\"].isin(to_ac)\n    caps_df.loc[ac_mask, \"end_carrier\"] = \"AC\"\n    caps = caps_df.fillna(\"-\").set_index([\"component\", \"carrier\", \"bus_carrier\", \"end_carrier\"])[0]\n\n    capacities[label] = caps.sort_index(level=0)\n    return capacities\n</code></pre>"},{"location":"reference/make_summary/#make_summary.calculate_cfs","title":"<code>calculate_cfs(n, label, cfs)</code>","text":"<p>Calculate the capacity factors by carrier</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>Network</code> <p>the network object</p> required <code>label</code> <code>str</code> <p>the label used by make summaries</p> required <code>cfs</code> <code>DataFrame</code> <p>the dataframe to fill/update</p> required <p>Returns:     pd.DataFrame: updated cfs</p> Source code in <code>workflow/scripts/make_summary.py</code> <pre><code>def calculate_cfs(n: pypsa.Network, label: str, cfs: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Calculate the capacity factors by carrier\n\n    Args:\n        n (pypsa.Network): the network object\n        label (str): the label used by make summaries\n        cfs (pd.DataFrame): the dataframe to fill/update\n    Returns:\n        pd.DataFrame: updated cfs\n    \"\"\"\n    for c in n.iterate_components(\n        n.branch_components | n.controllable_one_port_components ^ {\"Load\", \"StorageUnit\"}\n    ):\n        capacities_c = c.df[opt_name.get(c.name, \"p\") + \"_nom_opt\"].groupby(c.df.carrier).sum()\n\n        if c.name in [\"Link\", \"Line\", \"Transformer\"]:\n            p = c.pnl.p0.abs().mean()\n        elif c.name == \"Store\":\n            p = c.pnl.e.abs().mean()\n        else:\n            p = c.pnl.p.abs().mean()\n\n        p_c = p.groupby(c.df.carrier).sum()\n        cf_c = p_c / capacities_c\n        cf_c = pd.concat([cf_c], keys=[c.list_name])\n        cfs = cfs.reindex(cf_c.index.union(cfs.index))\n        cfs.loc[cf_c.index, label] = cf_c\n\n    return cfs\n</code></pre>"},{"location":"reference/make_summary/#make_summary.calculate_co2_balance","title":"<code>calculate_co2_balance(n, label, co2_balance, withdrawal_stores=['CO2 capture'])</code>","text":"<p>Calc the co2 balance [DOES NOT INCLUDE EMISSION GENERATING LINKSs]</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>Network</code> <p>the network object</p> required <code>withdrawal_stores</code> <code>list</code> <p>names of stores. Defaults to [\"CO2 capture\"].</p> <code>['CO2 capture']</code> <code>label</code> <code>str</code> <p>the label for the column</p> required <code>co2_balance</code> <code>DataFrame</code> <p>the df to update</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: updated co2_balance (bad style)</p> Source code in <code>workflow/scripts/make_summary.py</code> <pre><code>def calculate_co2_balance(\n    n: pypsa.Network,\n    label: str,\n    co2_balance: pd.DataFrame,\n    withdrawal_stores=[\"CO2 capture\"],\n) -&gt; pd.DataFrame:\n    \"\"\"Calc the co2 balance [DOES NOT INCLUDE EMISSION GENERATING LINKSs]\n\n    Args:\n        n (pypsa.Network): the network object\n        withdrawal_stores (list, optional): names of stores. Defaults to [\"CO2 capture\"].\n        label (str): the label for the column\n        co2_balance (pd.DataFrame): the df to update\n\n    Returns:\n       pd.DataFrame: updated co2_balance (bad style)\n    \"\"\"\n\n    # year *(assumes one planning year intended),\n    year = int(np.round(n.snapshots.year.values.mean(), 0))\n\n    # emissions from generators (from fneumann course)\n    emissions = (\n        n.generators_t.p\n        / n.generators.efficiency\n        * n.generators.carrier.map(n.carriers.co2_emissions)\n    )  # t/h\n    emissions_carrier = (\n        (n.snapshot_weightings.generators @ emissions).groupby(n.generators.carrier).sum()\n    )\n\n    # format and drop 0 values\n    emissions_carrier = emissions_carrier.where(emissions_carrier &gt; 0).dropna()\n    emissions_carrier.rename(year, inplace=True)\n    emissions_carrier = emissions_carrier.to_frame()\n    # CO2 withdrawal\n    stores = n.stores_t.e.T.groupby(n.stores.carrier).sum()\n    co2_stores = stores.index.intersection(withdrawal_stores)\n    co2_withdrawal = stores.iloc[:, -1].loc[co2_stores] * -1\n    co2_withdrawal.rename(year, inplace=True)\n    co2_withdrawal = co2_withdrawal.to_frame()\n    year_balance = pd.concat([emissions_carrier, co2_withdrawal])\n\n    #  combine with previous\n    co2_balance = co2_balance.reindex(year_balance.index.union(co2_balance.index))\n    co2_balance.loc[year_balance.index, label] = year_balance[year]\n\n    return co2_balance\n</code></pre>"},{"location":"reference/make_summary/#make_summary.calculate_costs","title":"<code>calculate_costs(n, label, costs)</code>","text":"<p>Calculate the costs by carrier Args:     n (pypsa.Network): the network object     label (str): the label used by make summaries     costs (pd.DataFrame): the dataframe to fill/update Returns:     pd.DataFrame: updated costs</p> Source code in <code>workflow/scripts/make_summary.py</code> <pre><code>def calculate_costs(n: pypsa.Network, label: str, costs: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Calculate the costs by carrier\n    Args:\n        n (pypsa.Network): the network object\n        label (str): the label used by make summaries\n        costs (pd.DataFrame): the dataframe to fill/update\n    Returns:\n        pd.DataFrame: updated costs\n    \"\"\"\n\n    for c in n.iterate_components(\n        n.branch_components | n.controllable_one_port_components ^ {\"Load\"}\n    ):\n        capital_costs = c.df.capital_cost * c.df[opt_name.get(c.name, \"p\") + \"_nom_opt\"]\n        capital_costs_grouped = capital_costs.groupby(c.df.carrier).sum()\n\n        capital_costs_grouped = pd.concat([capital_costs_grouped], keys=[\"capital\"])\n        capital_costs_grouped = pd.concat([capital_costs_grouped], keys=[c.list_name])\n\n        costs = costs.reindex(capital_costs_grouped.index.union(costs.index))\n\n        costs.loc[capital_costs_grouped.index, label] = capital_costs_grouped\n\n        if c.name == \"Link\":\n            p = c.pnl.p0.multiply(n.snapshot_weightings.generators, axis=0).sum()\n        elif c.name == \"Line\":\n            continue\n        elif c.name == \"StorageUnit\":\n            p_all = c.pnl.p.multiply(n.snapshot_weightings.generators, axis=0)\n            p_all[p_all &lt; 0.0] = 0.0\n            p = p_all.sum()\n        else:\n            p = c.pnl.p.multiply(n.snapshot_weightings.generators, axis=0).sum()\n\n        # correct sequestration cost\n        if c.name == \"Store\":\n            items = c.df.index[(c.df.carrier == \"co2 stored\") &amp; (c.df.marginal_cost &lt;= -100.0)]\n            c.df.loc[items, \"marginal_cost\"] = -20.0\n\n        marginal_costs = p * c.df.marginal_cost\n\n        marginal_costs_grouped = marginal_costs.groupby(c.df.carrier).sum()\n\n        marginal_costs_grouped = pd.concat([marginal_costs_grouped], keys=[\"marginal\"])\n        marginal_costs_grouped = pd.concat([marginal_costs_grouped], keys=[c.list_name])\n\n        costs = costs.reindex(marginal_costs_grouped.index.union(costs.index))\n\n        costs.loc[marginal_costs_grouped.index, label] = marginal_costs_grouped\n\n    # TODO remove/see if needed, and if yes soft-code\n    # add back in all hydro\n    # costs.loc[(\"storage_units\", \"capital\", \"hydro\"),label] = (0.01)*2e6*n.storage_units.loc[n.storage_units.group==\"hydro\", \"p_nom\"].sum()\n    # costs.loc[(\"storage_units\", \"capital\", \"PHS\"),label] = (0.01)*2e6*n.storage_units.loc[n.storage_units.group==\"PHS\", \"p_nom\"].sum()\n    # costs.loc[(\"generators\", \"capital\", \"ror\"),label] = (0.02)*3e6*n.generators.loc[n.generators.group==\"ror\", \"p_nom\"].sum()\n\n    return costs\n</code></pre>"},{"location":"reference/make_summary/#make_summary.calculate_curtailment","title":"<code>calculate_curtailment(n, label, curtailment)</code>","text":"<p>Calculate curtailed energy by carrier</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>Network</code> <p>the network object</p> required <code>label</code> <code>str</code> <p>the label used by make summaries</p> required <code>curtailment</code> <code>DataFrame</code> <p>the dataframe to fill/update</p> required <p>Returns:     pd.DataFrame: updated curtailment</p> Source code in <code>workflow/scripts/make_summary.py</code> <pre><code>def calculate_curtailment(n: pypsa.Network, label: str, curtailment: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Calculate curtailed energy by carrier\n\n    Args:\n        n (pypsa.Network): the network object\n        label (str): the label used by make summaries\n        curtailment (pd.DataFrame): the dataframe to fill/update\n    Returns:\n        pd.DataFrame: updated curtailment\n    \"\"\"\n    p_avail_by_carr = (\n        n.generators_t.p_max_pu.multiply(n.generators.p_nom_opt)\n        .sum()\n        .groupby(n.generators.carrier)\n        .sum()\n    )\n    used = n.generators_t.p.sum().groupby(n.generators.carrier).sum()\n\n    curtailment[label] = (\n        ((p_avail_by_carr - used).clip(0) / p_avail_by_carr).fillna(0) * 100\n    ).round(3)\n\n    return curtailment\n</code></pre>"},{"location":"reference/make_summary/#make_summary.calculate_energy","title":"<code>calculate_energy(n, label, energy)</code>","text":"<p>Calculate energy production/consumption by carrier from network components.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>Network</code> <p>PyPSA Network object</p> required <code>label</code> <code>str</code> <p>Label for the energy calculation scenario</p> required <code>energy</code> <code>DataFrame</code> <p>DataFrame to store energy results</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: Updated energy DataFrame with calculated values</p> Source code in <code>workflow/scripts/make_summary.py</code> <pre><code>def calculate_energy(n: pypsa.Network, label: str, energy: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Calculate energy production/consumption by carrier from network components.\n\n    Args:\n        n (pypsa.Network): PyPSA Network object\n        label (str): Label for the energy calculation scenario\n        energy (pd.DataFrame): DataFrame to store energy results\n\n    Returns:\n        pd.DataFrame: Updated energy DataFrame with calculated values\n    \"\"\"\n    for c in n.iterate_components(n.one_port_components | n.branch_components):\n        if c.name in n.one_port_components:\n            c_energies = (\n                c.pnl.p.multiply(n.snapshot_weightings.generators, axis=0)\n                .sum()\n                .multiply(c.df.sign)\n                .groupby(c.df.carrier)\n                .sum()\n            )\n        else:\n            c_energies = pd.Series(0.0, c.df.carrier.unique())\n            for port in [col[3:] for col in c.df.columns if col[:3] == \"bus\"]:\n                totals = c.pnl[\"p\" + port].multiply(n.snapshot_weightings.generators, axis=0).sum()\n                # remove values where bus is missing (bug in nomopyomo)\n                no_bus = c.df.index[c.df[\"bus\" + port] == \"\"]\n                totals.loc[no_bus] = float(n.component_attrs[c.name].loc[\"p\" + port, \"default\"])\n                c_energies -= totals.groupby(c.df.carrier).sum()\n\n        c_energies = pd.concat([c_energies], keys=[c.list_name])\n\n        energy = energy.reindex(c_energies.index.union(energy.index))\n\n        energy.loc[c_energies.index, label] = c_energies\n\n    return energy\n</code></pre>"},{"location":"reference/make_summary/#make_summary.calculate_expanded_capacities","title":"<code>calculate_expanded_capacities(n, label, capacities)</code>","text":"<p>Calculate the capacities by carrier</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>Network</code> <p>the network object</p> required <code>label</code> <code>str</code> <p>the label used by make summaries</p> required <code>capacities</code> <code>DataFrame</code> <p>the dataframe to fill</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.Dataframe: updated capacities (bad style)</p> Source code in <code>workflow/scripts/make_summary.py</code> <pre><code>def calculate_expanded_capacities(\n    n: pypsa.Network, label: str, capacities: pd.DataFrame\n) -&gt; pd.DataFrame:\n    \"\"\"Calculate the capacities by carrier\n\n    Args:\n        n (pypsa.Network): the network object\n        label (str): the label used by make summaries\n        capacities (pd.DataFrame): the dataframe to fill\n\n    Returns:\n        pd.Dataframe: updated capacities (bad style)\n    \"\"\"\n    caps = n.statistics.expanded_capacity(\n        groupby=pypsa.statistics.get_carrier_and_bus_carrier, nice_names=False\n    )\n\n    if \"load shedding\" in caps.index.get_level_values(1):\n        caps.drop(\"load shedding\", level=1, inplace=True)\n\n    caps.rename(index={\"AC\": \"Transmission Lines\"}, inplace=True, level=1)\n\n    # track links that feed into AC\n    mask = (n.links.bus1.map(n.buses.carrier) == \"AC\") &amp; (n.links.carrier != \"stations\")\n    to_ac = n.links.loc[mask, \"carrier\"].unique()\n\n    caps_df = caps.reset_index()\n    ac_mask = caps_df[\"carrier\"].isin(to_ac)\n    caps_df.loc[ac_mask, \"end_carrier\"] = \"AC\"\n    caps = caps_df.fillna(\"-\").set_index([\"component\", \"carrier\", \"bus_carrier\", \"end_carrier\"])[0]\n\n    capacities[label] = caps.sort_index(level=0)\n    return capacities\n</code></pre>"},{"location":"reference/make_summary/#make_summary.calculate_market_values","title":"<code>calculate_market_values(n, label, market_values)</code>","text":"<p>Calculate the market value of the generators and links Args:     n (pypsa.Network): the network object     label (str): the label representing the pathway     market_values (pd.DataFrame): the dataframe to write to (not needed, refactor)</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: updated market_values</p> Source code in <code>workflow/scripts/make_summary.py</code> <pre><code>def calculate_market_values(\n    n: pypsa.Network, label: str, market_values: pd.DataFrame\n) -&gt; pd.DataFrame:\n    \"\"\"Calculate the market value of the generators and links\n    Args:\n        n (pypsa.Network): the network object\n        label (str): the label representing the pathway\n        market_values (pd.DataFrame): the dataframe to write to (not needed, refactor)\n\n    Returns:\n        pd.DataFrame: updated market_values\n    \"\"\"\n    # Warning: doesn't include storage units\n\n    carrier = \"AC\"\n\n    buses = n.buses.index[n.buses.carrier == carrier]\n\n    # === First do market value of generators  ===\n    # === First do market value of generators  ===\n\n    generators = n.generators.index[n.buses.loc[n.generators.bus, \"carrier\"] == carrier]\n\n    techs = n.generators.loc[generators, \"carrier\"].value_counts().index\n\n    market_values = market_values.reindex(market_values.index.union(techs))\n\n    for tech in techs:\n        gens = generators[n.generators.loc[generators, \"carrier\"] == tech]\n\n        dispatch = (\n            n.generators_t.p[gens]\n            .groupby(n.generators.loc[gens, \"bus\"], axis=1)\n            .sum()\n            .reindex(columns=buses, fill_value=0.0)\n        )\n\n        revenue = dispatch * n.buses_t.marginal_price[buses]\n\n        market_values.at[tech, label] = revenue.sum().sum() / dispatch.sum().sum()\n\n    # === Now do market value of links  ===\n    # === Now do market value of links  ===\n\n    for i in [\"0\", \"1\"]:\n        carrier_links = n.links[n.links[\"bus\" + i].isin(buses)].index\n\n        techs = n.links.loc[carrier_links, \"carrier\"].value_counts().index\n\n        market_values = market_values.reindex(market_values.index.union(techs))\n\n        for tech in techs:\n            links = carrier_links[n.links.loc[carrier_links, \"carrier\"] == tech]\n\n            dispatch = (\n                n.links_t[\"p\" + i][links]\n                .groupby(n.links.loc[links, \"bus\" + i], axis=1)\n                .sum()\n                .reindex(columns=buses, fill_value=0.0)\n            )\n\n            revenue = dispatch * n.buses_t.marginal_price[buses]\n\n            market_values.at[tech, label] = revenue.sum().sum() / dispatch.sum().sum()\n\n    return market_values\n</code></pre>"},{"location":"reference/make_summary/#make_summary.calculate_metrics","title":"<code>calculate_metrics(n, label, metrics)</code>","text":"<p>LEGACY calculate a set of metrics for lines and co2 Args:     n (pypsa.Network): the network object     label (str): the label to update the table row with     metrics (pd.DataFrame): the dataframe to write to (not needed, refactor)</p> <p>Returns:</p> Type Description <p>pd.DataFrame: updated metrics</p> Source code in <code>workflow/scripts/make_summary.py</code> <pre><code>def calculate_metrics(n: pypsa.Network, label: str, metrics: pd.DataFrame):\n    \"\"\"LEGACY calculate a set of metrics for lines and co2\n    Args:\n        n (pypsa.Network): the network object\n        label (str): the label to update the table row with\n        metrics (pd.DataFrame): the dataframe to write to (not needed, refactor)\n\n    Returns:\n        pd.DataFrame: updated metrics\n    \"\"\"\n\n    metrics_list = [\n        \"line_volume\",\n        \"line_volume_limit\",\n        \"line_volume_AC\",\n        \"line_volume_DC\",\n        \"line_volume_shadow\",\n        \"co2_shadow\",\n        \"co2_budget\",\n    ]\n\n    metrics = metrics.reindex(pd.Index(metrics_list).union(metrics.index))\n\n    metrics.at[\"line_volume_DC\", label] = (n.links.length * n.links.p_nom_opt)[\n        n.links.carrier == \"DC\"\n    ].sum()\n    metrics.at[\"line_volume_AC\", label] = (n.lines.length * n.lines.s_nom_opt).sum()\n    metrics.at[\"line_volume\", label] = metrics.loc[\n        [\"line_volume_AC\", \"line_volume_DC\"], label\n    ].sum()\n\n    if \"lv_limit\" in n.global_constraints.index:\n        metrics.at[\"line_volume_limit\", label] = n.global_constraints.at[\"lv_limit\", \"constant\"]\n        metrics.at[\"line_volume_shadow\", label] = n.global_constraints.at[\"lv_limit\", \"mu\"]\n\n    if \"co2_limit\" in n.global_constraints.index:\n        metrics.at[\"co2_shadow\", label] = n.global_constraints.at[\"co2_limit\", \"mu\"]\n        metrics.at[\"co2_budget\", label] = n.global_constraints.at[\"co2_limit\", \"constant\"]\n    return metrics\n</code></pre>"},{"location":"reference/make_summary/#make_summary.calculate_nodal_capacities","title":"<code>calculate_nodal_capacities(n, label, nodal_capacities)</code>","text":"<p>Calculate the capacities by carrier and node</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>Network</code> <p>the network object</p> required <code>label</code> <code>str</code> <p>the label used by make summaries</p> required <code>nodal_capacities</code> <code>DataFrame</code> <p>the dataframe to fill/update</p> required <p>Returns:     pd.DataFrame: updated nodal_capacities</p> Source code in <code>workflow/scripts/make_summary.py</code> <pre><code>def calculate_nodal_capacities(\n    n: pypsa.Network, label: str, nodal_capacities: pd.DataFrame\n) -&gt; pd.DataFrame:\n    \"\"\"Calculate the capacities by carrier and node\n\n    Args:\n        n (pypsa.Network): the network object\n        label (str): the label used by make summaries\n        nodal_capacities (pd.DataFrame): the dataframe to fill/update\n    Returns:\n        pd.DataFrame: updated nodal_capacities\n    \"\"\"\n    # Beware this also has extraneous locations for country (e.g. biomass) or continent-wide\n    #  (e.g. fossil gas/oil) stuff\n\n    # Filter out reversed links to avoid double-counting transmission capacity\n    # Only include positive links since positive and reversed links have the same capacity\n    positive_links_mask = n.links.index.str.contains(\"positive\")\n\n    # Create a temporary network with only positive links for capacity calculation\n    n_temp = n.copy()\n    reversed_links = n.links.index[~positive_links_mask]\n    n_temp.links = n_temp.links.drop(reversed_links)\n\n    nodal_cap = n_temp.statistics.optimal_capacity(groupby=pypsa.statistics.get_bus_and_carrier)\n    nodal_capacities[label] = nodal_cap.sort_index(level=0)\n    return nodal_capacities\n</code></pre>"},{"location":"reference/make_summary/#make_summary.calculate_nodal_cfs","title":"<code>calculate_nodal_cfs(n, label, nodal_cfs)</code>","text":"<p>Calculate the capacity factors by for each node and genertor Args:     n (pypsa.Network): the network object     label (str): the label used by make summaries     nodal_cfs (pd.DataFrame): the cap fac dataframe to fill/update Returns:     pd.DataFrame: updated nodal_cfs</p> Source code in <code>workflow/scripts/make_summary.py</code> <pre><code>def calculate_nodal_cfs(n: pypsa.Network, label: str, nodal_cfs: pd.DataFrame):\n    \"\"\"Calculate the capacity factors by for each node and genertor\n    Args:\n        n (pypsa.Network): the network object\n        label (str): the label used by make summaries\n        nodal_cfs (pd.DataFrame): the cap fac dataframe to fill/update\n    Returns:\n        pd.DataFrame: updated nodal_cfs\n    \"\"\"\n    # Beware this also has extraneous locations for country (e.g. biomass)\n    # or continent-wide (e.g. fossil gas/oil) stuff\n    for c in n.iterate_components(\n        (n.branch_components ^ {\"Line\", \"Transformer\"})\n        | n.controllable_one_port_components ^ {\"Load\", \"StorageUnit\"}\n    ):\n        capacities_c = c.df.groupby([\"location\", \"carrier\"])[\n            opt_name.get(c.name, \"p\") + \"_nom_opt\"\n        ].sum()\n\n        if c.name == \"Link\":\n            p = c.pnl.p0.abs().mean()\n        elif c.name == \"Generator\":\n            p = c.pnl.p.abs().mean()\n        elif c.name == \"Store\":\n            p = c.pnl.e.abs().mean()\n        else:\n            sys.exit()\n\n        c.df[\"p\"] = p\n        p_c = c.df.groupby([\"location\", \"carrier\"])[\"p\"].sum()\n        cf_c = p_c / capacities_c\n\n        index = pd.MultiIndex.from_tuples([(c.list_name,) + t for t in cf_c.index.to_list()])\n        nodal_cfs = nodal_cfs.reindex(index.union(nodal_cfs.index))\n        nodal_cfs.loc[index, label] = cf_c.values\n\n    return nodal_cfs\n</code></pre>"},{"location":"reference/make_summary/#make_summary.calculate_nodal_costs","title":"<code>calculate_nodal_costs(n, label, nodal_costs)</code>","text":"<p>Calculate the costs by carrier and location Args:     n (pypsa.Network): the network object     label (str): the label used by make summaries     nodal_costs (pd.DataFrame): the dataframe to fill/update Returns:     pd.DataFrame: updated nodal_costs</p> Source code in <code>workflow/scripts/make_summary.py</code> <pre><code>def calculate_nodal_costs(n: pypsa.Network, label: str, nodal_costs: pd.DataFrame):\n    \"\"\"Calculate the costs by carrier and location\n    Args:\n        n (pypsa.Network): the network object\n        label (str): the label used by make summaries\n        nodal_costs (pd.DataFrame): the dataframe to fill/update\n    Returns:\n        pd.DataFrame: updated nodal_costs\n    \"\"\"\n    # Beware this also has extraneous locations for country (e.g. biomass)\n    #  or continent-wide (e.g. fossil gas/oil) stuff\n    for c in n.iterate_components(\n        n.branch_components | n.controllable_one_port_components ^ {\"Load\"}\n    ):\n        c.df[\"capital_costs\"] = c.df.capital_cost * c.df[opt_name.get(c.name, \"p\") + \"_nom_opt\"]\n        capital_costs = c.df.groupby([\"location\", \"carrier\"])[\"capital_costs\"].sum()\n        index = pd.MultiIndex.from_tuples(\n            [(c.list_name, \"capital\") + t for t in capital_costs.index.to_list()]\n        )\n        nodal_costs = nodal_costs.reindex(index.union(nodal_costs.index))\n        nodal_costs.loc[index, label] = capital_costs.values\n\n        if c.name == \"Link\":\n            p = c.pnl.p0.multiply(n.snapshot_weightings.generators, axis=0).sum()\n        elif c.name == \"Line\":\n            continue\n        elif c.name == \"StorageUnit\":\n            p_all = c.pnl.p.multiply(n.snapshot_weightings.generators, axis=0)\n            p_all[p_all &lt; 0.0] = 0.0\n            p = p_all.sum()\n        else:\n            p = c.pnl.p.multiply(n.snapshot_weightings.generators, axis=0).sum()\n\n        # correct sequestration cost\n        if c.name == \"Store\":\n            items = c.df.index[(c.df.carrier == \"co2 stored\") &amp; (c.df.marginal_cost &lt;= -100.0)]\n            c.df.loc[items, \"marginal_cost\"] = -20.0\n\n        c.df[\"marginal_costs\"] = p * c.df.marginal_cost\n        marginal_costs = c.df.groupby([\"location\", \"carrier\"])[\"marginal_costs\"].sum()\n        index = pd.MultiIndex.from_tuples(\n            [(c.list_name, \"marginal\") + t for t in marginal_costs.index.to_list()]\n        )\n        nodal_costs = nodal_costs.reindex(index.union(nodal_costs.index))\n        nodal_costs.loc[index, label] = marginal_costs.values\n\n    return nodal_costs\n</code></pre>"},{"location":"reference/make_summary/#make_summary.calculate_nodal_lcoe","title":"<code>calculate_nodal_lcoe(n, label, nodal_lcoe)</code>","text":"<p>Calculate LCOE by province and technology</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>Network</code> <p>the network object</p> required <code>label</code> <code>str</code> <p>the label used by make summaries</p> required <code>nodal_lcoe</code> <code>DataFrame</code> <p>the dataframe to fill/update</p> required <p>Returns:     pd.DataFrame: updated nodal_lcoe</p> Source code in <code>workflow/scripts/make_summary.py</code> <pre><code>def calculate_nodal_lcoe(n: pypsa.Network, label: str, nodal_lcoe: pd.DataFrame):\n    \"\"\"Calculate LCOE by province and technology\n\n    Args:\n        n (pypsa.Network): the network object\n        label (str): the label used by make summaries\n        nodal_lcoe (pd.DataFrame): the dataframe to fill/update\n    Returns:\n        pd.DataFrame: updated nodal_lcoe\n    \"\"\"\n\n    lcoe_data = calc_lcoe(n, groupby=[\"location\", \"carrier\"])\n\n    lcoe_series = lcoe_data[\"LCOE\"]\n\n    nodal_lcoe[label] = lcoe_series\n\n    return nodal_lcoe\n</code></pre>"},{"location":"reference/make_summary/#make_summary.calculate_peak_dispatch","title":"<code>calculate_peak_dispatch(n, label, supply)</code>","text":"<p>Calculate the MAX dispatch of each component at the buses aggregated by carrier.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>Network</code> <p>the network object</p> required <code>label</code> <code>str</code> <p>the labe representing the pathway</p> required <code>supply</code> <code>DataFrame</code> <p>supply energy balance (empty df)</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: updated supply DF</p> Source code in <code>workflow/scripts/make_summary.py</code> <pre><code>def calculate_peak_dispatch(n: pypsa.Network, label: str, supply: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Calculate the MAX dispatch of each component at the buses aggregated by\n    carrier.\n\n    Args:\n        n (pypsa.Network): the network object\n        label (str): the labe representing the pathway\n        supply (pd.DataFrame): supply energy balance (empty df)\n\n    Returns:\n        pd.DataFrame: updated supply DF\n    \"\"\"\n\n    sup_ = n.statistics.supply(\n        groupby=pypsa.statistics.get_carrier_and_bus_carrier, aggregate_time=\"max\"\n    )\n    supply_reordered = sup_.reorder_levels([2, 0, 1])\n    supply_reordered.sort_index(inplace=True)\n    supply[label] = supply_reordered\n\n    return supply\n</code></pre>"},{"location":"reference/make_summary/#make_summary.calculate_supply_energy","title":"<code>calculate_supply_energy(n, label, supply_energy)</code>","text":"<p>Calculate the total energy supply/consuption of each component at the buses aggregated by carrier.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>Network</code> <p>the network object</p> required <code>label</code> <code>str</code> <p>the labe representing the pathway</p> required <code>supply_energy</code> <code>DataFrame</code> <p>supply energy balance (empty df)</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: updated supply energy balance</p> Source code in <code>workflow/scripts/make_summary.py</code> <pre><code>def calculate_supply_energy(\n    n: pypsa.Network, label: str, supply_energy: pd.DataFrame\n) -&gt; pd.DataFrame:\n    \"\"\"Calculate the total energy supply/consuption of each component at the buses\n    aggregated by carrier.\n\n    Args:\n        n (pypsa.Network): the network object\n        label (str): the labe representing the pathway\n        supply_energy (pd.DataFrame): supply energy balance (empty df)\n\n    Returns:\n        pd.DataFrame: updated supply energy balance\n    \"\"\"\n\n    eb = n.statistics.energy_balance(groupby=pypsa.statistics.get_carrier_and_bus_carrier)\n    # fragile\n    eb_reordered = eb.reorder_levels([2, 0, 1])\n    eb_reordered.sort_index(inplace=True)\n    eb_reordered.rename(index={\"AC\": \"transmission losses\"}, level=2, inplace=True)\n\n    supply_energy[label] = eb_reordered\n\n    return supply_energy\n</code></pre>"},{"location":"reference/make_summary/#make_summary.calculate_t_avgd_prices","title":"<code>calculate_t_avgd_prices(n, label, prices)</code>","text":"<p>Time averaged prices for nodes averaged over carrier (bit silly?)</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>Network</code> <p>the network object</p> required <code>label</code> <code>str</code> <p>the label representing the pathway (not needed, refactor)</p> required <code>prices</code> <code>DataFrame</code> <p>the dataframe to write to (not needed, refactor)</p> required <p>Returns:</p> Type Description <p>pd.DataFrame: updated prices</p> Source code in <code>workflow/scripts/make_summary.py</code> <pre><code>def calculate_t_avgd_prices(n: pypsa.Network, label: str, prices: pd.DataFrame):\n    \"\"\"Time averaged prices for nodes averaged over carrier (bit silly?)\n\n    Args:\n        n (pypsa.Network): the network object\n        label (str): the label representing the pathway (not needed, refactor)\n        prices (pd.DataFrame): the dataframe to write to (not needed, refactor)\n\n    Returns:\n        pd.DataFrame: updated prices\n    \"\"\"\n    prices = prices.reindex(prices.index.union(n.buses.carrier.unique()))\n\n    # WARNING: this is time-averaged, see weighted_prices for load-weighted average\n    prices[label] = n.buses_t.marginal_price.mean().groupby(n.buses.carrier).mean()\n\n    return prices\n</code></pre>"},{"location":"reference/make_summary/#make_summary.calculate_weighted_prices","title":"<code>calculate_weighted_prices(n, label, weighted_prices)</code>","text":"<p>Demand-weighed prices for stores and loads.     For stores if withdrawal is zero, use supply instead.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>Network</code> <p>the network object</p> required <code>label</code> <code>str</code> <p>the label representing the pathway (not needed, refactor)</p> required <code>weighted_prices</code> <code>DataFrame</code> <p>the dataframe to write to (not needed, refactor)</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: updated weighted_prices</p> Source code in <code>workflow/scripts/make_summary.py</code> <pre><code>def calculate_weighted_prices(\n    n: pypsa.Network, label: str, weighted_prices: pd.DataFrame\n) -&gt; pd.DataFrame:\n    \"\"\"Demand-weighed prices for stores and loads.\n        For stores if withdrawal is zero, use supply instead.\n\n    Args:\n        n (pypsa.Network): the network object\n        label (str): the label representing the pathway (not needed, refactor)\n        weighted_prices (pd.DataFrame): the dataframe to write to (not needed, refactor)\n\n    Returns:\n        pd.DataFrame: updated weighted_prices\n    \"\"\"\n    # --- baseline entries ---\n    baseline_entries = pd.Index([\"electricity\", \"heat\", \"H2\", \"CO2 capture\", \"gas\", \"biomass\"])\n\n    # --- loads ---\n    load_rev = -1 * n.statistics.revenue(comps=\"Load\", groupby=pypsa.statistics.get_bus_carrier)\n    load_withdrawal = n.statistics.withdrawal(comps=\"Load\", groupby=pypsa.statistics.get_bus_carrier)\n    prices = (load_rev / load_withdrawal).rename(index={\"AC\": \"electricity\"})\n\n    # --- stores ---\n    w = n.statistics.withdrawal(comps=\"Store\")\n    if not w.empty:\n        w[w == 0] = n.statistics.supply(comps=\"Store\")[w == 0]  # fallback\n    store_rev = n.statistics.revenue(comps=\"Store\")\n\n    if not w.empty and not store_rev.empty:\n        common_idx = w.index.intersection(store_rev.index)\n        wp_stores = store_rev.reindex(common_idx) / w.reindex(common_idx)\n    else:\n        wp_stores = pd.Series(dtype=float)\n\n    # --- combine ---\n    all_prices = pd.concat([prices, wp_stores])\n    all_prices = all_prices[~all_prices.index.duplicated(keep=\"first\")]\n\n    # --- reindex weighted_prices with baseline + dynamic ---\n    all_entries = baseline_entries.union(all_prices.index)  # \u4fdd\u8bc1\u57fa\u51c6\u6709\uff0c\u540c\u65f6\u52a8\u6001\u6269\u5c55\n    weighted_prices = weighted_prices.reindex(all_entries)\n\n    weighted_prices[label] = all_prices\n\n    return weighted_prices\n</code></pre>"},{"location":"reference/make_summary/#make_summary.expand_from_wildcard","title":"<code>expand_from_wildcard(key, config)</code>","text":"<p>Return a list of values for the given key in the config file Args:     key (str): the key to look for in the config file     config (dict): the config file Returns:     list: a list of values for the given key</p> Source code in <code>workflow/scripts/make_summary.py</code> <pre><code>def expand_from_wildcard(key, config) -&gt; list:\n    \"\"\"Return a list of values for the given key in the config file\n    Args:\n        key (str): the key to look for in the config file\n        config (dict): the config file\n    Returns:\n        list: a list of values for the given key\n    \"\"\"\n    w = getattr(wildcards, key)\n    return config[\"scenario\"][key] if w == \"all\" else [w]\n</code></pre>"},{"location":"reference/make_summary/#make_summary.make_summaries","title":"<code>make_summaries(networks_dict, opts=None)</code>","text":"<p>Make summary tables for the given network Args:     networks_dict (dict): a dictionary of (pathway, time):network_path used in the run     opts (dict): options for each summary function Returns:     dict: a dictionary of dataframes with the summary tables</p> Source code in <code>workflow/scripts/make_summary.py</code> <pre><code>def make_summaries(\n    networks_dict: dict[tuple, os.PathLike], opts: dict = None\n) -&gt; dict[str, pd.DataFrame]:\n    \"\"\"Make summary tables for the given network\n    Args:\n        networks_dict (dict): a dictionary of (pathway, time):network_path used in the run\n        opts (dict): options for each summary function\n    Returns:\n        dict: a dictionary of dataframes with the summary tables\n\n    \"\"\"\n    if opts is None:\n        opts = {}\n\n    output_funcs = {\n        \"nodal_costs\": calculate_nodal_costs,\n        \"nodal_capacities\": calculate_nodal_capacities,\n        \"nodal_cfs\": calculate_nodal_cfs,\n        \"nodal_lcoe\": calculate_nodal_lcoe,\n        \"cfs\": calculate_cfs,\n        \"costs\": calculate_costs,\n        \"co2_balance\": calculate_co2_balance,\n        \"capacities\": calculate_capacities,\n        \"capacities_expanded\": calculate_expanded_capacities,\n        \"curtailment_pc\": calculate_curtailment,\n        \"peak_dispatch\": calculate_peak_dispatch,\n        # \"energy\": calculate_energy,\n        \"supply_energy\": calculate_supply_energy,\n        \"time_averaged_prices\": calculate_t_avgd_prices,\n        \"weighted_prices\": calculate_weighted_prices,\n        # \"price_statistics\": calculate_price_statistics,\n        \"market_values\": calculate_market_values,\n        \"metrics\": calculate_metrics,\n    }\n\n    columns = pd.MultiIndex.from_tuples(\n        networks_dict.keys(), names=[\"co2_pathway\", \"planning_horizons\"]\n    )\n    dataframes_dict = {}\n\n    # TO DO: not needed, could be made by the functions\n    for output in output_funcs.keys():\n        dataframes_dict[output] = pd.DataFrame(columns=columns, dtype=float)\n\n    for label, filename in networks_dict.items():\n        logger.info(f\"Make summary for scenario {label}, using {filename}\")\n\n        n = pypsa.Network(filename)\n        assign_carriers(n)\n        assign_locations(n)\n\n        for output, output_fn in output_funcs.items():\n            if output in opts:\n                dataframes_dict[output] = output_fn(\n                    n, label, dataframes_dict[output], **opts[output]\n                )\n            else:\n                dataframes_dict[output] = output_fn(n, label, dataframes_dict[output])\n\n    return dataframes_dict\n</code></pre>"},{"location":"reference/make_summary/#make_summary.to_csv","title":"<code>to_csv(dfs, dir)</code>","text":"<p>Save DataFrames to CSV files in specified directory.</p> <p>Parameters:</p> Name Type Description Default <code>dfs</code> <code>dict</code> <p>Dictionary of DataFrames to save</p> required <code>dir</code> <code>str</code> <p>Output directory path</p> required <p>Returns:</p> Name Type Description <code>None</code> <p>Saves files to disk</p> Source code in <code>workflow/scripts/make_summary.py</code> <pre><code>def to_csv(dfs, dir):\n    \"\"\"Save DataFrames to CSV files in specified directory.\n\n    Args:\n        dfs (dict): Dictionary of DataFrames to save\n        dir (str): Output directory path\n\n    Returns:\n        None: Saves files to disk\n    \"\"\"\n    os.makedirs(dir, exist_ok=True)\n    for key, df in dfs.items():\n        df.to_csv(os.path.join(dir, f\"{key}.csv\"))\n</code></pre>"},{"location":"reference/plot_input_costs/","title":"Plot input costs","text":"<p>Plot input cost data visualization and analysis.</p> <p>This module creates visualizations for technology cost data, including cost trends, comparisons, and other analyses for the PyPSA-China model.</p>"},{"location":"reference/plot_input_costs/#plot_input_costs.apply_conversion","title":"<code>apply_conversion(df, target_unit_installation, target_unit_storage)</code>","text":"<p>Apply unit conversion to each row in the DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input DataFrame containing cost data</p> required <code>target_unit_installation</code> <code>str</code> <p>Target unit for installation costs (e.g. \"eur/kW\")</p> required <code>target_unit_storage</code> <code>str</code> <p>Target unit for storage costs (e.g. \"eur/kWh\")</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: DataFrame with converted units - Preserves original unit in 'original_unit' column - Updates 'unit' column with new unit - Converts all numeric values to target units</p> Source code in <code>workflow/scripts/plot_input_costs.py</code> <pre><code>def apply_conversion(\n    df: pd.DataFrame, target_unit_installation: str, target_unit_storage: str\n) -&gt; pd.DataFrame:\n    \"\"\"Apply unit conversion to each row in the DataFrame.\n\n    Args:\n        df (pd.DataFrame): Input DataFrame containing cost data\n        target_unit_installation (str): Target unit for installation costs (e.g. \"eur/kW\")\n        target_unit_storage (str): Target unit for storage costs (e.g. \"eur/kWh\")\n\n    Returns:\n        pd.DataFrame: DataFrame with converted units\n            - Preserves original unit in 'original_unit' column\n            - Updates 'unit' column with new unit\n            - Converts all numeric values to target units\n    \"\"\"\n    # Ensure essential columns exist\n    for col in [\"technology\", \"reference\", \"unit\"]:\n        if col not in df.columns:\n            df[col] = f\"Default_{col}\"\n\n    # Keep the original unit in a separate column if not present\n    if \"original_unit\" not in df.columns:\n        df[\"original_unit\"] = df[\"unit\"].copy()\n\n    # Convert row by row\n    return df.apply(convert_row_units, axis=1, args=(target_unit_installation, target_unit_storage))\n</code></pre>"},{"location":"reference/plot_input_costs/#plot_input_costs.convert_row_units","title":"<code>convert_row_units(row, target_unit_installation, target_unit_storage)</code>","text":"<p>Convert row's numeric columns to target units.</p> <p>Parameters:</p> Name Type Description Default <code>row</code> <code>Series</code> <p>Input row containing cost data</p> required <code>target_unit_installation</code> <code>str</code> <p>Target unit for installation costs (e.g. \"eur/kW\")</p> required <code>target_unit_storage</code> <code>str</code> <p>Target unit for storage costs (e.g. \"eur/kWh\")</p> required <p>Returns:</p> Type Description <code>Series</code> <p>pd.Series: Row with converted values</p> Source code in <code>workflow/scripts/plot_input_costs.py</code> <pre><code>def convert_row_units(\n    row: pd.Series, target_unit_installation: str, target_unit_storage: str\n) -&gt; pd.Series:\n    \"\"\"Convert row's numeric columns to target units.\n\n    Args:\n        row (pd.Series): Input row containing cost data\n        target_unit_installation (str): Target unit for installation costs (e.g. \"eur/kW\")\n        target_unit_storage (str): Target unit for storage costs (e.g. \"eur/kWh\")\n\n    Returns:\n        pd.Series: Row with converted values\n    \"\"\"\n    if \"unit\" not in row or pd.isna(row[\"unit\"]):\n        return row\n\n    # Parse unit string\n    currency_part, capacity_part = parse_unit_string(row[\"unit\"])\n    if not currency_part or not capacity_part:\n        return row\n\n    # Determine if storage unit and get target unit\n    is_storage = is_storage_unit(capacity_part)\n    target_unit = target_unit_storage if is_storage else target_unit_installation\n    tgt_currency, tgt_capacity = parse_unit_string(target_unit)\n\n    # Get currency conversion factor\n    orig_currency = CURRENCY_ALIASES.get(currency_part.lower(), \"unknown\")\n    currency_factor = EXCHANGE_RATES.get((orig_currency, tgt_currency), 1.0)\n\n    # Get capacity conversion factor\n    norm_capacity = normalize_capacity_unit(capacity_part)\n    norm_target = normalize_capacity_unit(tgt_capacity)\n\n    if norm_capacity in [\"kw\", \"mw\", \"kwh\", \"mwh\"] and norm_target in [\"kw\", \"mw\", \"kwh\", \"mwh\"]:\n        cap_factor = get_capacity_factor(norm_capacity, norm_target)\n        new_capacity_part = norm_target\n    else:\n        cap_factor = 1.0\n        new_capacity_part = capacity_part\n\n    # Calculate total conversion factor\n    factor = currency_factor * cap_factor\n\n    # Convert numeric columns\n    numeric_cols = get_numeric_columns(row)\n    for col in numeric_cols:\n        val = row[col]\n        if pd.notna(val):\n            try:\n                row[col] = float(val) * factor\n            except ValueError:\n                pass\n\n    # Update unit string\n    row[\"unit\"] = f\"{tgt_currency.upper()}/{new_capacity_part}\"\n\n    return row\n</code></pre>"},{"location":"reference/plot_input_costs/#plot_input_costs.detect_file_encoding","title":"<code>detect_file_encoding(file_path)</code>","text":"<p>Detect the encoding of a file.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>Path to the file whose encoding needs to be detected.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The detected encoding of the file.</p> Source code in <code>workflow/scripts/plot_input_costs.py</code> <pre><code>def detect_file_encoding(file_path: str) -&gt; str:\n    \"\"\"Detect the encoding of a file.\n\n    Args:\n        file_path: Path to the file whose encoding needs to be detected.\n\n    Returns:\n        str: The detected encoding of the file.\n    \"\"\"\n    with open(file_path, \"rb\") as f:\n        result = chardet.detect(f.read())\n    encoding = result[\"encoding\"]\n    return encoding\n</code></pre>"},{"location":"reference/plot_input_costs/#plot_input_costs.filter_investment_parameter","title":"<code>filter_investment_parameter(df)</code>","text":"<p>Filter DataFrame to keep only rows where parameter is 'investment'.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input DataFrame containing cost data with a 'parameter' column.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: Filtered DataFrame containing only investment parameter rows. If 'parameter' column does not exist, returns the original DataFrame unchanged.</p> Source code in <code>workflow/scripts/plot_input_costs.py</code> <pre><code>def filter_investment_parameter(df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Filter DataFrame to keep only rows where parameter is 'investment'.\n\n    Args:\n        df (pd.DataFrame): Input DataFrame containing cost data with a 'parameter' column.\n\n    Returns:\n        pd.DataFrame: Filtered DataFrame containing only investment parameter rows.\n            If 'parameter' column does not exist, returns the original DataFrame unchanged.\n    \"\"\"\n    if \"parameter\" not in df.columns:\n        logging.warning(\"Warning: 'parameter' column not found.\")\n        return df\n    # Filter for rows that have parameter == \"investment\"\n    mask = df[\"parameter\"].str.lower() == \"investment\"\n    df_investment = df[mask].copy()\n    return df_investment\n</code></pre>"},{"location":"reference/plot_input_costs/#plot_input_costs.filter_technologies_by_config","title":"<code>filter_technologies_by_config(df, techs_dict)</code>","text":"<p>Filter and categorize technologies based on the provided technology dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input DataFrame containing technology data</p> required <code>techs_dict</code> <code>Dict[str, List[str]]</code> <p>Dictionary containing technology lists with keys: - 'vre_techs': List of variable renewable energy technologies - 'conv_techs': List of conventional technologies - 'store_techs': List of storage technologies</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: Filtered DataFrame with mapped technologies and categories - 'mapped_technology': Standardized technology names - 'category': Technology category (VRE, Conventional, Storage, Solar Thermal)</p> Source code in <code>workflow/scripts/plot_input_costs.py</code> <pre><code>def filter_technologies_by_config(\n    df: pd.DataFrame, techs_dict: dict[str, list[str]]\n) -&gt; pd.DataFrame:\n    \"\"\"Filter and categorize technologies based on the provided technology dictionary.\n\n    Args:\n        df (pd.DataFrame): Input DataFrame containing technology data\n        techs_dict (Dict[str, List[str]]): Dictionary containing technology lists with keys:\n            - 'vre_techs': List of variable renewable energy technologies\n            - 'conv_techs': List of conventional technologies\n            - 'store_techs': List of storage technologies\n\n    Returns:\n        pd.DataFrame: Filtered DataFrame with mapped technologies and categories\n            - 'mapped_technology': Standardized technology names\n            - 'category': Technology category (VRE, Conventional, Storage, Solar Thermal)\n    \"\"\"\n    try:\n        # Validate input DataFrame\n        if df.empty:\n            logging.warning(\"Warning: Input DataFrame is empty\")\n            return pd.DataFrame()\n\n        if \"technology\" not in df.columns:\n            logging.error(\"Error: 'technology' column not found in DataFrame\")\n            return pd.DataFrame()\n\n        # Example aliases\n        alias_map: dict[str, list[str]] = {\n            \"solar thermal\": [\"central solar thermal\", \"decentral solar thermal\"],\n            \"hydroelectricity\": [\"hydro\"],\n            \"heat pump\": [\"central air heat pump\", \"decentral air heat pump\"],\n            \"resistive heater\": [\"central resistive heater\", \"decentral resistive heater\"],\n            \"Sabatier\": [\"methanation\"],\n            \"H2 CHP\": [\"central hydrogen CHP\"],\n            \"OCGT gas\": [\"OCGT\"],\n            \"CHP gas\": [\"central gas CHP\", \"decentral CHP\"],\n            \"gas boiler\": [\"central gas boiler\", \"decentral gas boiler\"],\n            \"coal boiler\": [\"central coal boiler\", \"decentral coal boiler\"],\n            \"coal power plant\": [\"coal\"],\n            \"CHP coal\": [\"central coal CHP\"],\n            \"H2\": [\"H2 pipeline\", \"hydrogen storage tank type 1\"],\n            \"battery\": [\"battery storage\"],\n            \"water tanks\": [\"central water tank storage\", \"decentral water tank storage\"],\n        }\n\n        # Reverse alias map for easier lookup\n        tech_aliases: dict[str, str] = {}\n        for main_tech, aliases in alias_map.items():\n            for alias in aliases:\n                tech_aliases[alias.lower()] = main_tech\n\n        # get all techs\n        vre_techs = techs_dict.get(\"vre_techs\", [])\n        conv_techs = techs_dict.get(\"conv_techs\", [])\n        store_techs = techs_dict.get(\"store_techs\", [])\n        solar_thermal = [\"solar thermal\"]  # special case for solar thermal\n\n        # combine all techs\n        all_techs = vre_techs + conv_techs + store_techs + solar_thermal\n\n        # create tech to category mapping\n        tech_categories: dict[str, str] = {}\n        for tech in vre_techs:\n            if tech != \"solar thermal\":  # exclude solar thermal\n                tech_categories[tech] = \"VRE Technologies\"\n        for tech in conv_techs:\n            tech_categories[tech] = \"Conventional Technologies\"\n        for tech in store_techs:\n            tech_categories[tech] = \"Storage Technologies\"\n        tech_categories[\"solar thermal\"] = \"Solar Thermal\"\n\n        # Create a copy of the DataFrame\n        df_filtered = df.copy()\n\n        # Direct mapping\n        df_filtered[\"mapped_technology\"] = df_filtered[\"technology\"].where(\n            df_filtered[\"technology\"].isin(all_techs)\n        )\n\n        # Alias mapping\n        df_filtered[\"mapped_technology\"].fillna(\n            df_filtered[\"technology\"].str.lower().map(tech_aliases), inplace=True\n        )\n\n        # Log unmapped technologies\n        unmapped_techs = df_filtered[df_filtered[\"mapped_technology\"].isna()][\"technology\"].unique()\n        if len(unmapped_techs) &gt; 0:\n            logging.warning(\n                f\"Warning: The following technologies could not be mapped: {unmapped_techs}\"\n            )\n\n        # Drop rows where no match was found\n        df_filtered = df_filtered.dropna(subset=[\"mapped_technology\"])\n\n        if df_filtered.empty:\n            logging.warning(\"Warning: No matching technologies found!\")\n            return pd.DataFrame()\n\n        # Add category based on mapped technology\n        df_filtered[\"category\"] = df_filtered[\"mapped_technology\"].map(tech_categories)\n\n        return df_filtered\n\n    except KeyError as e:\n        logging.error(f\"Error: Missing key in technology dictionary - {str(e)}\")\n        return pd.DataFrame()\n    except AttributeError as e:\n        logging.error(f\"Error: Invalid DataFrame structure - {str(e)}\")\n        return pd.DataFrame()\n    except Exception as e:\n        logging.error(f\"Unexpected error in technology filtering: {str(e)}\")\n        return pd.DataFrame()\n</code></pre>"},{"location":"reference/plot_input_costs/#plot_input_costs.get_capacity_factor","title":"<code>get_capacity_factor(orig_cap, target_cap)</code>","text":"<p>Calculate the capacity conversion factor.</p> <p>Parameters:</p> Name Type Description Default <code>orig_cap</code> <code>str</code> <p>Original capacity unit</p> required <code>target_cap</code> <code>str</code> <p>Target capacity unit</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>Conversion factor between capacity units</p> Source code in <code>workflow/scripts/plot_input_costs.py</code> <pre><code>def get_capacity_factor(orig_cap: str, target_cap: str) -&gt; float:\n    \"\"\"Calculate the capacity conversion factor.\n\n    Args:\n        orig_cap (str): Original capacity unit\n        target_cap (str): Target capacity unit\n\n    Returns:\n        float: Conversion factor between capacity units\n    \"\"\"\n    capacity_factors: dict[tuple[str, str], float] = {\n        (\"kw\", \"kw\"): 1.0,\n        (\"mw\", \"kw\"): 1 / 1000.0,\n        (\"kwh\", \"kwh\"): 1.0,\n        (\"mwh\", \"kwh\"): 1 / 1000.0,\n        (\"kw\", \"mw\"): 1000.0,\n        (\"mw\", \"mw\"): 1.0,\n        (\"kwh\", \"mwh\"): 1000.0,\n        (\"mwh\", \"mwh\"): 1.0,\n    }\n    return capacity_factors.get((orig_cap.lower(), target_cap.lower()), 1.0)\n</code></pre>"},{"location":"reference/plot_input_costs/#plot_input_costs.get_numeric_columns","title":"<code>get_numeric_columns(row)</code>","text":"<p>Get list of numeric columns in the row.</p> <p>Parameters:</p> Name Type Description Default <code>row</code> <code>Series</code> <p>Input row</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>List[str]: List of column names containing numeric values - Year columns (e.g., \"2020\", \"2025\") - Cost columns (e.g., \"cost_2020\", \"cost_2025\")</p> Source code in <code>workflow/scripts/plot_input_costs.py</code> <pre><code>def get_numeric_columns(row: pd.Series) -&gt; list[str]:\n    \"\"\"Get list of numeric columns in the row.\n\n    Args:\n        row (pd.Series): Input row\n\n    Returns:\n        List[str]: List of column names containing numeric values\n            - Year columns (e.g., \"2020\", \"2025\")\n            - Cost columns (e.g., \"cost_2020\", \"cost_2025\")\n    \"\"\"\n    numeric_cols = []\n\n    # Check for year columns\n    for col in row.index:\n        if isinstance(col, str) and col.isdigit():\n            numeric_cols.append(col)\n\n    # Check for cost columns\n    if not numeric_cols:\n        for col in row.index:\n            if isinstance(col, str) and \"cost_\" in col and col.split(\"_\")[-1].isdigit():\n                numeric_cols.append(col)\n\n    # Check for specific years\n    possible_years = [\"2020\", \"2025\", \"2030\", \"2035\", \"2040\", \"2045\", \"2050\", \"2055\", \"2060\"]\n    for year in possible_years:\n        if year in row.index and year not in numeric_cols:\n            numeric_cols.append(year)\n\n    return numeric_cols\n</code></pre>"},{"location":"reference/plot_input_costs/#plot_input_costs.is_storage_unit","title":"<code>is_storage_unit(capacity_part)</code>","text":"<p>Check if the capacity part indicates a storage unit.</p> <p>Parameters:</p> Name Type Description Default <code>capacity_part</code> <code>str</code> <p>Capacity part of the unit string</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the unit is for storage (kWh or MWh)</p> Source code in <code>workflow/scripts/plot_input_costs.py</code> <pre><code>def is_storage_unit(capacity_part: str) -&gt; bool:\n    \"\"\"Check if the capacity part indicates a storage unit.\n\n    Args:\n        capacity_part (str): Capacity part of the unit string\n\n    Returns:\n        bool: True if the unit is for storage (kWh or MWh)\n    \"\"\"\n    return \"kwh\" in capacity_part.lower() or \"mwh\" in capacity_part.lower()\n</code></pre>"},{"location":"reference/plot_input_costs/#plot_input_costs.load_and_clean_data","title":"<code>load_and_clean_data(file_path)</code>","text":"<p>Load and clean data from a CSV file.</p> <p>load a CSV file, detects its encoding, replaces '-' with NaN, and drops the 'link' column if it exists.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>Path to the CSV file to be loaded.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: Cleaned DataFrame with standardized data.</p> Source code in <code>workflow/scripts/plot_input_costs.py</code> <pre><code>def load_and_clean_data(file_path: str) -&gt; pd.DataFrame:\n    \"\"\"Load and clean data from a CSV file.\n\n    load a CSV file, detects its encoding, replaces '-' with NaN,\n    and drops the 'link' column if it exists.\n\n    Args:\n        file_path: Path to the CSV file to be loaded.\n\n    Returns:\n        pd.DataFrame: Cleaned DataFrame with standardized data.\n    \"\"\"\n    encoding = detect_file_encoding(file_path)\n    try:\n        df = pd.read_csv(file_path, encoding=encoding)\n        df.replace(\"-\", np.nan, inplace=True)\n        if \"link\" in df.columns:\n            df.drop(columns=[\"link\"], inplace=True)\n        return df\n    except Exception as e:\n        logging.error(f\"Error loading file {file_path}: {e}\")\n        return pd.DataFrame()\n</code></pre>"},{"location":"reference/plot_input_costs/#plot_input_costs.load_reference_data","title":"<code>load_reference_data(file_path)</code>","text":"<p>Load and process reference data for technology cost comparison.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>Path to the reference data CSV file.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: Processed reference data with standardized units.</p> Source code in <code>workflow/scripts/plot_input_costs.py</code> <pre><code>def load_reference_data(file_path: str) -&gt; pd.DataFrame:\n    \"\"\"Load and process reference data for technology cost comparison.\n\n    Args:\n        file_path: Path to the reference data CSV file.\n\n    Returns:\n        pd.DataFrame: Processed reference data with standardized units.\n    \"\"\"\n    try:\n        ref_df = load_and_clean_data(file_path)\n        ref_df = ref_df[ref_df[\"reference\"] != \"PyPSA-China\"]\n\n        if \"parameter\" in ref_df.columns:\n            ref_df = filter_investment_parameter(ref_df)\n        ref_df = apply_conversion(ref_df, \"eur/kW\", \"eur/kWh\")\n        return ref_df\n    except Exception as e:\n        logging.error(f\"Error loading reference data: {e}\")\n        return pd.DataFrame()\n</code></pre>"},{"location":"reference/plot_input_costs/#plot_input_costs.normalize_capacity_unit","title":"<code>normalize_capacity_unit(cap)</code>","text":"<p>Normalize capacity unit string.</p> <p>Parameters:</p> Name Type Description Default <code>cap</code> <code>str</code> <p>Capacity unit string</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Normalized capacity unit - Converts to lowercase - Removes spaces - Standardizes square notation</p> Source code in <code>workflow/scripts/plot_input_costs.py</code> <pre><code>def normalize_capacity_unit(cap: str) -&gt; str:\n    \"\"\"Normalize capacity unit string.\n\n    Args:\n        cap (str): Capacity unit string\n\n    Returns:\n        str: Normalized capacity unit\n            - Converts to lowercase\n            - Removes spaces\n            - Standardizes square notation\n    \"\"\"\n    if not cap:\n        return \"\"\n    cap = cap.lower()\n    cap = cap.replace(\" \", \"\")\n    cap = cap.replace(\"\u00b2\", \"2\")\n    return cap\n</code></pre>"},{"location":"reference/plot_input_costs/#plot_input_costs.parse_unit_string","title":"<code>parse_unit_string(unit_str)</code>","text":"<p>Parse a unit string into currency and capacity parts.</p> <p>Parameters:</p> Name Type Description Default <code>unit_str</code> <code>str</code> <p>Unit string in format \"currency/capacity\"</p> required <p>Returns:</p> Type Description <code>tuple[str | None, str | None]</code> <p>Tuple[str | None, str | None]: - First element: currency part (e.g., \"eur\", \"usd\", \"cny\") - Second element: capacity part (e.g., \"kw\", \"kwh\") Returns (None, None) if invalid format</p> Source code in <code>workflow/scripts/plot_input_costs.py</code> <pre><code>def parse_unit_string(unit_str: str) -&gt; tuple[str | None, str | None]:\n    \"\"\"Parse a unit string into currency and capacity parts.\n\n    Args:\n        unit_str (str): Unit string in format \"currency/capacity\"\n\n    Returns:\n        Tuple[str | None, str | None]:\n            - First element: currency part (e.g., \"eur\", \"usd\", \"cny\")\n            - Second element: capacity part (e.g., \"kw\", \"kwh\")\n            Returns (None, None) if invalid format\n    \"\"\"\n    if pd.isna(unit_str) or \"/\" not in unit_str:\n        return None, None\n    currency_part, capacity_part = unit_str.split(\"/\", 1)\n    return currency_part.strip().lower(), capacity_part.strip().lower()\n</code></pre>"},{"location":"reference/plot_input_costs/#plot_input_costs.plot_technologies_by_category","title":"<code>plot_technologies_by_category(costs_df, ref_df=None, tech_colors=None, font_size=14, plot_reference=True)</code>","text":"<p>Plot technology cost trends with literature comparison.</p> <p>Parameters:</p> Name Type Description Default <code>costs_df</code> <code>DataFrame</code> <p>DataFrame containing the main cost data</p> required <code>ref_df</code> <code>DataFrame | None</code> <p>DataFrame containing reference data for comparison</p> <code>None</code> <code>tech_colors</code> <code>Dict[str, str] | None</code> <p>Dictionary mapping technology names to colors</p> <code>None</code> <code>font_size</code> <code>int</code> <p>Font size for plot elements</p> <code>14</code> <code>plot_reference</code> <code>bool</code> <p>Whether to plot reference data</p> <code>True</code> <p>Returns:</p> Type Description <code>Figure</code> <p>plt.Figure: The generated plot figure</p> Source code in <code>workflow/scripts/plot_input_costs.py</code> <pre><code>def plot_technologies_by_category(\n    costs_df: pd.DataFrame,\n    ref_df: pd.DataFrame | None = None,\n    tech_colors: dict[str, str] | None = None,\n    font_size: int = 14,\n    plot_reference: bool = True,\n) -&gt; plt.Figure:\n    \"\"\"Plot technology cost trends with literature comparison.\n\n    Args:\n        costs_df (pd.DataFrame): DataFrame containing the main cost data\n        ref_df (pd.DataFrame | None): DataFrame containing reference data for comparison\n        tech_colors (Dict[str, str] | None): Dictionary mapping technology names to colors\n        font_size (int): Font size for plot elements\n        plot_reference (bool): Whether to plot reference data\n\n    Returns:\n        plt.Figure: The generated plot figure\n    \"\"\"\n    if costs_df.empty:\n        logging.error(\"Error: The costs DataFrame is empty; cannot plot.\")\n        return None\n\n    if tech_colors is None:\n        tech_colors = {}\n\n    # Get year columns\n    year_cols = [col for col in costs_df.columns if col.isdigit()]\n    if not year_cols:\n        year_cols = [\n            col.split(\"_\")[-1]\n            for col in costs_df.columns\n            if \"cost_\" in col and col.split(\"_\")[-1].isdigit()\n        ]\n    if not year_cols:\n        possible_years = [\"2020\", \"2025\", \"2030\", \"2035\", \"2040\", \"2045\", \"2050\", \"2055\", \"2060\"]\n        year_cols = [year for year in possible_years if year in costs_df.columns]\n    if not year_cols:\n        logging.error(\"Error: Unable to identify year columns in costs data.\")\n        return None\n    year_cols = sorted(year_cols)\n\n    ref_year_cols = [col for col in ref_df.columns if col.isdigit()] if ref_df is not None else []\n    if plot_reference and ref_df is not None and not ref_year_cols:\n        logging.error(\"Error: Unable to identify year columns in reference data.\")\n        return None\n    ref_year_cols = sorted(ref_year_cols)\n\n    # Create subplots\n    technologies = costs_df[\"technology\"].unique()\n    num_techs = len(technologies)\n    num_cols = 6\n    num_rows = (num_techs + num_cols - 1) // num_cols\n\n    fig, axs = uplt.subplots(\n        nrows=num_rows, ncols=num_cols, figwidth=6 * num_cols, sharex=True, sharey=False\n    )\n    if not isinstance(axs, np.ndarray):\n        axs = np.array([axs])\n    axs = axs.flatten()\n\n    dash_styles = [\"--\", \"-.\", \":\", (0, (3, 1, 1, 1))]\n\n    # Plot each technology\n    for i, tech in enumerate(technologies):\n        if i &gt;= len(axs):\n            logging.warning(f\"Warning: Exceeded subplot limit, skipping {tech}\")\n            continue\n\n        ax = axs[i]\n        tech_df = costs_df[costs_df[\"technology\"] == tech]\n        ref_tech_df = ref_df[ref_df[\"technology\"] == tech] if ref_df is not None else None\n        color = tech_colors.get(tech, \"#999999\")\n\n        # Plot main data\n        tech_years, tech_values = [], []\n        for year in year_cols:\n            values = pd.to_numeric(tech_df[year], errors=\"coerce\").dropna().values\n            if values.size &gt; 0:\n                tech_years.append(int(year))\n                tech_values.append(np.median(values))\n\n        legend_handles = []\n        legend_labels = []\n        if tech_years:\n            (line,) = ax.plot(\n                tech_years, tech_values, linewidth=2.5, color=color, linestyle=\"-\", label=tech\n            )\n            legend_handles.append(line)\n            legend_labels.append(tech)\n\n        # Plot reference data\n        if plot_reference and ref_tech_df is not None and ref_tech_df.shape[0] &gt; 0:\n            for j, (ref_name, ref_group) in enumerate(ref_tech_df.groupby(\"reference\")):\n                ref_years, ref_values = [], []\n                for year in ref_year_cols:\n                    vals = pd.to_numeric(ref_group[year], errors=\"coerce\").dropna().values\n                    if vals.size &gt; 0:\n                        ref_years.append(int(year))\n                        ref_values.append(np.median(vals))\n\n                if ref_years:\n                    dash_style = dash_styles[j % len(dash_styles)]\n                    (ref_line,) = ax.plot(\n                        ref_years,\n                        ref_values,\n                        linewidth=2,\n                        color=color,\n                        linestyle=dash_style,\n                        label=ref_name,\n                    )\n                    legend_handles.append(ref_line)\n                    legend_labels.append(ref_name)\n\n        # Set labels and formatting\n        if \"unit\" in tech_df.columns and not tech_df[\"unit\"].isna().all():\n            unit = tech_df[\"unit\"].iloc[0]\n            unit_parts = unit.split(\"/\")\n            if len(unit_parts) == 2:\n                currency, capacity = unit_parts\n                if \"eur\" in currency.lower():\n                    ax.set_ylabel(f\"EUR/{capacity.upper()}\", fontsize=font_size)\n                else:\n                    ax.set_ylabel(f\"{unit}\", fontsize=font_size)\n            else:\n                ax.set_ylabel(f\"{unit}\", fontsize=font_size)\n        else:\n            ax.set_ylabel(\"Cost (EUR)\", fontsize=font_size)\n\n        ax2 = ax.twinx()\n        if \"unit\" in tech_df.columns and not tech_df[\"unit\"].isna().all():\n            unit = tech_df[\"unit\"].iloc[0]\n            unit_parts = unit.split(\"/\")\n            if len(unit_parts) == 2:\n                _, capacity = unit_parts\n                ax2.set_ylabel(f\"CNY/{capacity.upper()}\", fontsize=font_size)\n            else:\n                ax2.set_ylabel(\"Cost (CNY)\", fontsize=font_size)\n        else:\n            ax2.set_ylabel(\"Cost (CNY)\", fontsize=font_size)\n\n        y_min, y_max = ax.get_ylim()\n        ax2.set_ylim(y_min * 7.8, y_max * 7.8)\n\n        ax.set_title(f\"{tech}\", fontsize=font_size)\n        ax.grid(True, linestyle=\"--\", alpha=0.7)\n        ax.tick_params(axis=\"y\", labelsize=font_size)\n        ax2.tick_params(axis=\"y\", labelsize=font_size)\n        ax.legend(legend_handles, legend_labels, loc=\"upper right\", fontsize=font_size - 2, ncol=1)\n\n    # Hide unused subplots\n    total_plots = num_rows * num_cols\n    for i in range(num_techs, total_plots):\n        if i &lt; len(axs):\n            axs[i].set_visible(False)\n\n    axs[0].figure.format(\n        suptitle=\"Technology Cost Trends with Literature Comparison\",\n        abc=True,\n        abcloc=\"ul\",\n        xlabel=\"Year\",\n        fontsize=font_size,\n    )\n\n    return fig\n</code></pre>"},{"location":"reference/plot_inputs_visualisation/","title":"Plot inputs visualisation","text":"<p>Plot input data visualizations for model validation and analysis.</p> <p>This module creates geographical and statistical visualizations of input data including renewable resources, demand patterns, and infrastructure data.</p>"},{"location":"reference/plot_inputs_visualisation/#plot_inputs_visualisation.plot_average_distances","title":"<code>plot_average_distances(distances, ax=None)</code>","text":"<p>Plot the average distances to the node (region com/repr point) for each vre class Args:     distances (xr.DataArray): the average distances for each class to the node     ax (plt.Axes, optional): the axes to plot on. Defaults to None.</p> <p>Returns:</p> Type Description <code>tuple[Figure, Axes]</code> <p>tuple[plt.Figure, plt.Axes]: the figure and axes</p> Source code in <code>workflow/scripts/plot_inputs_visualisation.py</code> <pre><code>def plot_average_distances(\n    distances: xr.DataArray, ax: plt.Axes = None\n) -&gt; tuple[plt.Figure, plt.Axes]:\n    \"\"\"Plot the average distances to the node (region com/repr point) for each vre class\n    Args:\n        distances (xr.DataArray): the average distances for each class to the node\n        ax (plt.Axes, optional): the axes to plot on. Defaults to None.\n\n    Returns:\n        tuple[plt.Figure, plt.Axes]: the figure and axes\n    \"\"\"\n    if not ax:\n        fig, ax = plt.subplots(figsize=(12, 6))\n    else:\n        fig = ax.get_figure()\n\n    distances.to_series().plot.hist(\n        bins=30,\n        figsize=(10, 6),\n        title=\"Frequency Distribution of Average Distance\",\n        xlabel=\"Average Distance (km)\",\n        ylabel=\"Frequency\",\n        # color=\"skyblue\",\n        edgecolor=\"black\",\n    )\n    fig.tight_layout()\n    return fig, ax\n</code></pre>"},{"location":"reference/plot_inputs_visualisation/#plot_inputs_visualisation.plot_resource_class_bins","title":"<code>plot_resource_class_bins(resource_classes, regions, technology, ax=None)</code>","text":"<p>Map of VRE grades (by grade/ bin number) for each node Args:     resource_classes (gpd.GeoDataFrame): the resource classes     regions (gpd.GeoDataFrame): the regions/node regions     technology (str): the technology name     ax (plt.Axes, optional): the axes to plot on. Defaults to None.</p> <p>Returns:</p> Type Description <code>tuple[Figure, Axes]</code> <p>tuple[plt.Figure, plt.Axes]: the figure and axes</p> Source code in <code>workflow/scripts/plot_inputs_visualisation.py</code> <pre><code>def plot_resource_class_bins(\n    resource_classes: gpd.GeoDataFrame,\n    regions: gpd.GeoDataFrame,\n    technology: str,\n    ax: plt.Axes = None,\n) -&gt; tuple[plt.Figure, plt.Axes]:\n    \"\"\"Map of VRE grades (by grade/ bin number) for each node\n    Args:\n        resource_classes (gpd.GeoDataFrame): the resource classes\n        regions (gpd.GeoDataFrame): the regions/node regions\n        technology (str): the technology name\n        ax (plt.Axes, optional): the axes to plot on. Defaults to None.\n\n    Returns:\n        tuple[plt.Figure, plt.Axes]: the figure and axes\n    \"\"\"\n\n    if not ax:\n        fig, ax = plt.subplots(figsize=(12, 6))\n    else:\n        fig = ax.get_figure()\n\n    nbins = resource_classes.bin.nunique()\n\n    cmap = plt.cm.get_cmap(\"magma_r\", nbins)  # Inverted discrete colormap with `nbins` colors\n    cmap.set_bad(color=\"lightgrey\")  # Set color for missing values\n    norm = mcolors.BoundaryNorm(boundaries=range(nbins + 1), ncolors=nbins)\n\n    # Plot the class regions\n    resource_classes.reset_index().plot(\"bin\", cmap=cmap, legend=True, ax=ax, norm=norm)\n    ax.set_title(f\"Resource Classes for {technology.capitalize()} simple\")\n\n    # Add administrative region borders\n    regions.boundary.plot(ax=ax, color=\"black\", linewidth=0.5, linestyle=\"--\")\n\n    # coords.plot(ax=ax, color=\"black\", markersize=1)\n    fig.tight_layout()\n\n    return fig, ax\n</code></pre>"},{"location":"reference/plot_inputs_visualisation/#plot_inputs_visualisation.plot_resource_class_cfs","title":"<code>plot_resource_class_cfs(resource_classes, regions, technology, ax=None)</code>","text":"<p>Map of VRE capacity factors for each node and vre grade Args:     resource_classes (gpd.GeoDataFrame): the resource classes     regions (gpd.GeoDataFrame): the regions/node regions     technology (str): the technology name     ax (plt.Axes, optional): the axes to plot on. Defaults to None.</p> <p>Returns:</p> Type Description <p>tuple[plt.Figure, plt.Axes]: the figure and axes</p> Source code in <code>workflow/scripts/plot_inputs_visualisation.py</code> <pre><code>def plot_resource_class_cfs(\n    resource_classes: gpd.GeoDataFrame,\n    regions: gpd.GeoDataFrame,\n    technology: str,\n    ax: plt.Axes = None,\n):\n    \"\"\"Map of VRE capacity factors for each node and vre grade\n    Args:\n        resource_classes (gpd.GeoDataFrame): the resource classes\n        regions (gpd.GeoDataFrame): the regions/node regions\n        technology (str): the technology name\n        ax (plt.Axes, optional): the axes to plot on. Defaults to None.\n\n    Returns:\n        tuple[plt.Figure, plt.Axes]: the figure and axes\n    \"\"\"\n\n    if not ax:\n        fig, ax = plt.subplots(figsize=(12, 6))\n    else:\n        fig = ax.get_figure()\n\n    # Plot the class regions\n    resource_classes.plot(\"cf\", cmap=\"magma_r\", legend=True, ax=ax)\n    ax.set_title(f\"Resource Classes for {technology.capitalize()} simple\")\n\n    # Add administrative region borders\n    regions.boundary.plot(ax=ax, color=\"black\", linewidth=0.5, linestyle=\"--\")\n\n    # coords.plot(ax=ax, color=\"black\", markersize=1)\n    fig.tight_layout()\n\n    return fig, ax\n</code></pre>"},{"location":"reference/plot_network/","title":"Plot network","text":"<p>Plot network maps and energy system visualizations.</p> <p>This module creates geographical network maps showing generation capacities, transmission lines, energy flows, and other network characteristics for the PyPSA-China energy system model.</p>"},{"location":"reference/plot_network/#plot_network.add_cost_pannel","title":"<code>add_cost_pannel(df, fig, preferred_order, tech_colors, plot_additions, ax_loc=[-0.09, 0.28, 0.09, 0.45], **kwargs)</code>","text":"<p>Add a cost pannel to the figure</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>the cost data to plot</p> required <code>fig</code> <code>Figure</code> <p>the figure object to which the cost pannel will be added</p> required <code>preferred_order</code> <code>Index</code> <p>index, the order in whiich to plot</p> required <code>tech_colors</code> <code>dict</code> <p>the tech colors</p> required <code>plot_additions</code> <code>bool</code> <p>plot the additions</p> required <code>ax_loc</code> <code>list</code> <p>the location of the cost pannel. Defaults to [-0.09, 0.28, 0.09, 0.45].</p> <code>[-0.09, 0.28, 0.09, 0.45]</code> Source code in <code>workflow/scripts/plot_network.py</code> <pre><code>def add_cost_pannel(\n    df: pd.DataFrame,\n    fig: plt.Figure,\n    preferred_order: pd.Index,\n    tech_colors: dict,\n    plot_additions: bool,\n    ax_loc=[-0.09, 0.28, 0.09, 0.45],\n    **kwargs: dict,\n) -&gt; None:\n    \"\"\"Add a cost pannel to the figure\n\n    Args:\n        df (pd.DataFrame): the cost data to plot\n        fig (plt.Figure): the figure object to which the cost pannel will be added\n        preferred_order (pd.Index): index, the order in whiich to plot\n        tech_colors (dict): the tech colors\n        plot_additions (bool): plot the additions\n        ax_loc (list, optional): the location of the cost pannel.\n            Defaults to [-0.09, 0.28, 0.09, 0.45].\n    \"\"\"\n    ax3 = fig.add_axes(ax_loc)\n    reordered = preferred_order.intersection(df.index).append(df.index.difference(preferred_order))\n    colors = {k.lower(): v for k, v in tech_colors.items()}\n\n    # Create color list with default color for missing carriers\n    color_list = []\n    for k in reordered:\n        if k.lower() in colors:\n            color_list.append(colors[k.lower()])\n        else:\n            color_list.append(\"lightgrey\")  # Default color for missing carriers\n\n    df.loc[reordered, df.columns].T.plot(\n        kind=\"bar\",\n        ax=ax3,\n        stacked=True,\n        color=color_list,\n        **kwargs,\n    )\n    ax3.legend().remove()\n    ax3.set_ylabel(\"annualized system cost bEUR/a\")\n    ax3.set_xticklabels(ax3.get_xticklabels(), rotation=\"horizontal\")\n    ax3.grid(axis=\"y\")\n    ax3.set_ylim([0, df.sum().max() * 1.1])\n    if plot_additions:\n        # add label\n        percent = np.round((df.sum()[\"added\"] / df.sum()[\"total\"]) * 100)\n        ax3.text(0.85, (df.sum()[\"added\"] + 15), str(percent) + \"%\", color=\"black\")\n\n    fig.tight_layout()\n    return ax3\n</code></pre>"},{"location":"reference/plot_network/#plot_network.add_energy_pannel","title":"<code>add_energy_pannel(df, fig, preferred_order, colors, ax_loc=[-0.09, 0.28, 0.09, 0.45])</code>","text":"<p>Add a cost pannel to the figure</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>the statistics supply output by carrier (from plot_energy map)</p> required <code>fig</code> <code>Figure</code> <p>the figure object to which the cost pannel will be added</p> required <code>preferred_order</code> <code>Index</code> <p>index, the order in whiich to plot</p> required <code>colors</code> <code>Series</code> <p>the colors for the techs, with the correct index and no extra techs</p> required <code>ax_loc</code> <code>list</code> <p>the pannel location. Defaults to [-0.09, 0.28, 0.09, 0.45].</p> <code>[-0.09, 0.28, 0.09, 0.45]</code> Source code in <code>workflow/scripts/plot_network.py</code> <pre><code>def add_energy_pannel(\n    df: pd.DataFrame,\n    fig: plt.Figure,\n    preferred_order: pd.Index,\n    colors: pd.Series,\n    ax_loc=[-0.09, 0.28, 0.09, 0.45],\n) -&gt; None:\n    \"\"\"Add a cost pannel to the figure\n\n    Args:\n        df (pd.DataFrame): the statistics supply output by carrier (from plot_energy map)\n        fig (plt.Figure): the figure object to which the cost pannel will be added\n        preferred_order (pd.Index): index, the order in whiich to plot\n        colors (pd.Series): the colors for the techs, with the correct index and no extra techs\n        ax_loc (list, optional): the pannel location. Defaults to [-0.09, 0.28, 0.09, 0.45].\n    \"\"\"\n    ax3 = fig.add_axes(ax_loc)\n    reordered = preferred_order.intersection(df.index).append(df.index.difference(preferred_order))\n    df = df / PLOT_SUPPLY_UNITS\n    # only works if colors has correct index\n    df.loc[reordered, df.columns].T.plot(\n        kind=\"bar\",\n        ax=ax3,\n        stacked=True,\n        color=colors[reordered],\n    )\n\n    ax3.legend().remove()\n    ax3.set_ylabel(\"Electricity supply [TWh]\")\n    ax3.set_xticklabels(ax3.get_xticklabels(), rotation=\"horizontal\")\n    ax3.grid(axis=\"y\")\n    ax3.set_ylim([0, df.sum().max() * 1.1])\n\n    fig.tight_layout()\n</code></pre>"},{"location":"reference/plot_network/#plot_network.plot_cost_map","title":"<code>plot_cost_map(network, opts, base_year=2020, plot_additions=True, capex_only=False, cost_pannel=True, save_path=None)</code>","text":"<p>Plot the cost of each node on a map as well as the line capacities</p> <p>Parameters:</p> Name Type Description Default <code>network</code> <code>Network</code> <p>the network object</p> required <code>opts</code> <code>dict</code> <p>the plotting config (snakemake.config[\"plotting\"])</p> required <code>base_year</code> <code>int</code> <p>the base year (for cost delta). Defaults to 2020.</p> <code>2020</code> <code>capex_only</code> <code>bool</code> <p>do not plot VOM (FOM is in CAPEX). Defaults to False.</p> <code>False</code> <code>plot_additions</code> <code>bool</code> <p>plot a map of investments (p_nom_opt vs p_nom).   Defaults to True.</p> <code>True</code> <code>cost_pannel</code> <code>bool</code> <p>add a bar graph with costs. Defaults to True.</p> <code>True</code> <code>save_path</code> <code>PathLike</code> <p>save figure to path (or not if None). Defaults to None.</p> <code>None</code> <p>raises:     ValueError: if plot_additions and not capex_only</p> Source code in <code>workflow/scripts/plot_network.py</code> <pre><code>def plot_cost_map(\n    network: pypsa.Network,\n    opts: dict,\n    base_year=2020,\n    plot_additions=True,\n    capex_only=False,\n    cost_pannel=True,\n    save_path: os.PathLike = None,\n):\n    \"\"\"Plot the cost of each node on a map as well as the line capacities\n\n    Args:\n        network (pypsa.Network): the network object\n        opts (dict): the plotting config (snakemake.config[\"plotting\"])\n        base_year (int, optional): the base year (for cost delta). Defaults to 2020.\n        capex_only (bool, optional): do not plot VOM (FOM is in CAPEX). Defaults to False.\n        plot_additions (bool, optional): plot a map of investments (p_nom_opt vs p_nom).\n              Defaults to True.\n        cost_pannel (bool, optional): add a bar graph with costs. Defaults to True.\n        save_path (os.PathLike, optional): save figure to path (or not if None). Defaults to None.\n    raises:\n        ValueError: if plot_additions and not capex_only\n    \"\"\"\n\n    if plot_additions and not capex_only:\n        raise ValueError(\"Cannot plot additions without capex only\")\n\n    tech_colors = make_nice_tech_colors(opts[\"tech_colors\"], opts[\"nice_names\"])\n\n    # TODO scale edges by cost from capex summary\n    def calc_link_plot_width(row, carrier=\"AC\", additions=False):\n        if row.length == 0 or row.carrier != carrier or not row.plottable:\n            return 0\n        elif additions:\n            return row.p_nom\n        else:\n            return row.p_nom_opt\n\n    # ============ === Stats by bus ===\n    # calc costs &amp; sum over component types to keep bus &amp; carrier (remove no loc)\n    costs = network.statistics.capex(groupby=[\"location\", \"carrier\"])\n    costs = costs.groupby(level=[1, 2]).sum()\n    if \"\" in costs.index:\n        costs.drop(\"\", inplace=True)\n    # we miss some buses by grouping epr location, fill w 0s\n    bus_idx = pd.MultiIndex.from_product([network.buses.index, [\"AC\"]])\n    costs = costs.reindex(bus_idx.union(costs.index), fill_value=0)\n    # add marginal (excluding quasi fixed) to costs if desired\n    if not capex_only:\n        opex = network.statistics.opex(groupby=[\"location\", \"carrier\"])\n        opex = opex.groupby(level=[1, 2]).sum()\n        cost_pies = costs + opex.reindex(costs.index, fill_value=0)\n\n    # === make map components: pies and edges\n    cost_pies = costs.fillna(0)\n    cost_pies.index.names = [\"bus\", \"carrier\"]\n    carriers = cost_pies.index.get_level_values(1).unique()\n    # map edges\n    link_plot_w = network.links.apply(lambda row: calc_link_plot_width(row, carrier=\"AC\"), axis=1)\n    edges = pd.concat([network.lines.s_nom_opt, link_plot_w]).groupby(level=0).sum()\n    line_lower_threshold = opts.get(\"min_edge_capacity\", 0)\n    edge_widths = edges.clip(line_lower_threshold, edges.max()).replace(line_lower_threshold, 0)\n\n    # === Additions ===\n    # for pathways sometimes interested in additions from last time step\n    if plot_additions:\n        installed = (\n            network.statistics.installed_capex(groupby=[\"location\", \"carrier\"])\n            .groupby(level=[1, 2])\n            .sum()\n        )\n        costs_additional = costs - installed.reindex(costs.index, fill_value=0)\n        cost_pies_additional = costs_additional.fillna(0)\n        cost_pies_additional.index.names = [\"bus\", \"carrier\"]\n\n        link_additions = network.links.apply(\n            lambda row: calc_link_plot_width(row, carrier=\"AC\", additions=True), axis=1\n        )\n        added_links = link_plot_w - link_additions.reindex(link_plot_w.index, fill_value=0)\n        added_lines = network.lines.s_nom_opt - network.lines.s_nom.reindex(\n            network.lines.index, fill_value=0\n        )\n        edge_widths_added = pd.concat([added_links, added_lines]).groupby(level=0).sum()\n\n        # add to carrier types\n        carriers = carriers.union(cost_pies_additional.index.get_level_values(1).unique())\n\n    preferred_order = pd.Index(opts[\"preferred_order\"])\n    carriers = carriers.tolist()\n\n    # Make figure with right number of pannels\n    if plot_additions:\n        fig, (ax1, ax2) = plt.subplots(1, 2, subplot_kw={\"projection\": ccrs.PlateCarree()})\n        fig.set_size_inches(opts[\"cost_map\"][\"figsize_w_additions\"])\n    else:\n        fig, ax1 = plt.subplots(subplot_kw={\"projection\": ccrs.PlateCarree()})\n        fig.set_size_inches(opts[\"cost_map\"][\"figsize\"])\n\n    # Add the total costs\n    bus_size_factor = opts[\"cost_map\"][\"bus_size_factor\"]\n    linewidth_factor = opts[\"cost_map\"][\"linewidth_factor\"]\n\n    plot_map(\n        network,\n        tech_colors=tech_colors,\n        edge_widths=edge_widths / linewidth_factor,\n        bus_colors=tech_colors,\n        bus_sizes=cost_pies / bus_size_factor,\n        edge_colors=opts[\"cost_map\"][\"edge_color\"],\n        ax=ax1,\n        add_legend=not plot_additions,\n        bus_ref_title=f\"System costs{' (CAPEX)' if capex_only else ''}\",\n        **opts[\"cost_map\"],\n    )\n\n    # TODO check edges is working\n    # Add the added pathway costs\n    if plot_additions:\n        ax2 = plot_map(\n            network,\n            tech_colors=tech_colors,\n            edge_widths=edge_widths_added / linewidth_factor,\n            bus_colors=tech_colors,\n            bus_sizes=cost_pies_additional / bus_size_factor,\n            edge_colors=\"rosybrown\",\n            ax=ax2,\n            bus_ref_title=f\"Added costs{' (CAPEX)' if capex_only else ''}\",\n            add_legend=True,\n            **opts[\"cost_map\"],\n        )\n\n    # Add the optional cost pannel\n    if cost_pannel:\n        df = pd.DataFrame(columns=[\"total\"])\n        df[\"total\"] = network.statistics.capex(nice_names=False).groupby(level=1).sum()\n        if not capex_only:\n            df[\"opex\"] = network.statistics.opex(nice_names=False).groupby(level=1).sum()\n            df.rename(columns={\"total\": \"capex\"}, inplace=True)\n        elif plot_additions:\n            df[\"added\"] = (\n                df[\"total\"]\n                - network.statistics.installed_capex(nice_names=False).groupby(level=1).sum()\n            )\n\n        df.fillna(0, inplace=True)\n        df = df / PLOT_COST_UNITS\n        # TODO decide discount\n        # df = df / (1 + discount_rate) ** (int(planning_horizon) - base_year)\n        ax3 = add_cost_pannel(\n            df,\n            fig,\n            preferred_order,\n            tech_colors,\n            plot_additions,\n            ax_loc=[-0.09, 0.28, 0.09, 0.45],\n        )\n        # Set x-label angle to 45 degrees for better readability\n        for label in ax3.get_xticklabels():\n            label.set_rotation(45)\n\n    fig.set_size_inches(opts[\"cost_map\"][f\"figsize{'_w_additions' if plot_additions else ''}\"])\n    fig.tight_layout()\n    if save_path:\n        fig.savefig(save_path, transparent=opts[\"transparent\"], bbox_inches=\"tight\")\n</code></pre>"},{"location":"reference/plot_network/#plot_network.plot_energy_map","title":"<code>plot_energy_map(network, opts, energy_pannel=True, save_path=None, carrier='AC', plot_ac_imports=False, exclude_batteries=True, components=['Generator', 'Link'])</code>","text":"<p>A map plot of energy, either AC or heat</p> <p>Parameters:</p> Name Type Description Default <code>network</code> <code>Network</code> <p>the pyPSA network object</p> required <code>opts</code> <code>dict</code> <p>the plotting options (snakemake.config[\"plotting\"])</p> required <code>energy_pannel</code> <code>bool</code> <p>add an anergy pie to the left. Defaults to True.</p> <code>True</code> <code>save_path</code> <code>PathLike</code> <p>Fig outp path. Defaults to None (no save).</p> <code>None</code> <code>carrier</code> <code>str</code> <p>the energy carrier. Defaults to \"AC\".</p> <code>'AC'</code> <code>plot_ac_imports</code> <code>bool</code> <p>plot electricity imports. Defaults to False.</p> <code>False</code> <code>exclude_batteries</code> <code>bool</code> <p>exclude battery dischargers from the supply pie.</p> <code>True</code> <code>components</code> <code>list</code> <p>the components to plot. Defaults to [\"Generator\", \"Link\"].</p> <code>['Generator', 'Link']</code> <p>raises:     ValueError: if carrier is not AC or heat</p> Source code in <code>workflow/scripts/plot_network.py</code> <pre><code>def plot_energy_map(\n    network: pypsa.Network,\n    opts: dict,\n    energy_pannel=True,\n    save_path: os.PathLike = None,\n    carrier=\"AC\",\n    plot_ac_imports=False,\n    exclude_batteries=True,\n    components=[\"Generator\", \"Link\"],\n):\n    \"\"\"A map plot of energy, either AC or heat\n\n    Args:\n        network (pypsa.Network): the pyPSA network object\n        opts (dict): the plotting options (snakemake.config[\"plotting\"])\n        energy_pannel (bool, optional): add an anergy pie to the left. Defaults to True.\n        save_path (os.PathLike, optional): Fig outp path. Defaults to None (no save).\n        carrier (str, optional): the energy carrier. Defaults to \"AC\".\n        plot_ac_imports (bool, optional): plot electricity imports. Defaults to False.\n        exclude_batteries (bool, optional): exclude battery dischargers from the supply pie.\n        components (list, optional): the components to plot. Defaults to [\"Generator\", \"Link\"].\n    raises:\n        ValueError: if carrier is not AC or heat\n    \"\"\"\n    if carrier not in [\"AC\", \"heat\"]:\n        raise ValueError(\"Carrier must be either 'AC' or 'heat'\")\n\n    # make the statistics. Buses not assigned to a region will be included\n    # if they are linked to a region (e.g. turbine link w carrier = hydroelectricity)\n    energy_supply = network.statistics.supply(\n        groupby=get_location_and_carrier,\n        bus_carrier=carrier,\n        comps=components,\n    )\n    # get rid of components\n    supply_pies = energy_supply.groupby(level=[1, 2]).sum()\n\n    # TODO fix  this for heat\n    # # calc costs &amp; sum over component types to keep bus &amp; carrier (remove no loc)\n    # energy_supply = network.statistics.capex(groupby=[\"location\", \"carrier\"])\n    # energy_supply = energy_supply.groupby(level=[1, 2]).sum().drop(\"\")\n    # # we miss some buses by grouping epr location, fill w 0s\n    # bus_idx = pd.MultiIndex.from_product([network.buses.index, [\"AC\"]])\n    # supply_pies = energy_supply.reindex(bus_idx.union(energy_supply.index), fill_value=0)\n\n    # remove imports from supply pies\n    if carrier == \"AC\" and not plot_ac_imports:\n        supply_pies = supply_pies.loc[supply_pies.index.get_level_values(1) != \"AC\"]\n\n    # TODO aggregate costs below threshold into \"other\" -&gt; requires messing with network\n    # network.add(\"Carrier\", \"Other\")\n\n    # get all carrier types\n    carriers_list = supply_pies.index.get_level_values(1).unique()\n    carriers_list = carriers_list.tolist()\n\n    # TODO make line handling nicer\n    line_lower_threshold = opts.get(\"min_edge_capacity\", 500)\n    # Make figur\n    fig, ax = plt.subplots(subplot_kw={\"projection\": ccrs.PlateCarree()})\n    fig.set_size_inches(opts[\"energy_map\"][\"figsize\"])\n    # get colors\n    bus_colors = network.carriers.loc[network.carriers.nice_name.isin(carriers_list), \"color\"]\n    bus_colors.rename(opts[\"nice_names\"], inplace=True)\n\n    preferred_order = pd.Index(opts[\"preferred_order\"])\n    reordered = preferred_order.intersection(bus_colors.index).append(\n        bus_colors.index.difference(preferred_order)\n    )\n    # TODO there'sa  problem with network colors when using heat, pies aren't grouped by location\n    colors = network.carriers.color.copy()\n    colors.index = colors.index.map(opts[\"nice_names\"])\n    tech_colors = make_nice_tech_colors(opts[\"tech_colors\"], opts[\"nice_names\"])\n\n    # make sure plot isnt overpopulated\n    def calc_link_plot_width(row, carrier=\"AC\"):\n        if row.length == 0 or row.carrier != carrier or not row.plottable:\n            return 0\n        else:\n            return row.p_nom_opt\n\n    edge_carrier = \"H2 pipeline\" if carrier == \"heat\" else \"AC\"\n    link_plot_w = network.links.apply(lambda row: calc_link_plot_width(row, edge_carrier), axis=1)\n    edges = pd.concat([network.lines.s_nom_opt, link_plot_w])\n    edge_widths = edges.clip(line_lower_threshold, edges.max()).replace(line_lower_threshold, 0)\n\n    opts_plot = opts[\"energy_map\"].copy()\n    if carrier == \"heat\":\n        opts_plot[\"ref_bus_sizes\"] = opts_plot[\"ref_bus_sizes_heat\"]\n        opts_plot[\"ref_edge_sizes\"] = opts_plot[\"ref_edge_sizes_heat\"]\n        opts_plot[\"linewidth_factor\"] = opts_plot[\"linewidth_factor_heat\"]\n        opts_plot[\"bus_size_factor\"] = opts_plot[\"bus_size_factor_heat\"]\n    # exclude battery dischargers from bus sizes\n    if exclude_batteries:\n        bus_sizes = (\n            supply_pies.loc[~supply_pies.index.get_level_values(1).str.contains(\"battery\")]\n            / opts_plot[\"bus_size_factor\"]\n        )\n    else:\n        bus_sizes = supply_pies / opts_plot[\"bus_size_factor\"]\n    ax = plot_map(\n        network,\n        tech_colors=tech_colors,  # colors.to_dict(),\n        edge_widths=edge_widths / opts_plot[\"linewidth_factor\"],\n        bus_colors=bus_colors.loc[reordered],\n        bus_sizes=bus_sizes,\n        edge_colors=opts_plot[\"edge_color\"],\n        ax=ax,\n        edge_unit_conv=PLOT_CAP_UNITS,\n        bus_unit_conv=PLOT_SUPPLY_UNITS,\n        add_legend=True,\n        **opts_plot,\n    )\n    # # Add the optional cost pannel\n    if energy_pannel:\n        df = supply_pies.groupby(level=1).sum().to_frame()\n        df = df.fillna(0)\n        df.rename(columns={0: \"\"}, inplace=True)\n        add_energy_pannel(df, fig, preferred_order, bus_colors, ax_loc=[-0.09, 0.28, 0.09, 0.45])\n\n    handles, labels = ax.get_legend_handles_labels()\n    ax.legend(handles, labels, ncol=1, bbox_to_anchor=[1, 1], loc=\"upper left\")\n    fig.tight_layout()\n\n    if save_path:\n        fig.savefig(save_path, transparent=opts[\"transparent\"], bbox_inches=\"tight\")\n</code></pre>"},{"location":"reference/plot_network/#plot_network.plot_map","title":"<code>plot_map(network, tech_colors, edge_widths, bus_colors, bus_sizes, edge_colors='black', add_ref_edge_sizes=True, add_ref_bus_sizes=True, add_legend=True, bus_unit_conv=PLOT_COST_UNITS, edge_unit_conv=PLOT_CAP_UNITS, ax=None, **kwargs)</code>","text":"<p>Plot the network on a map</p> <p>Parameters:</p> Name Type Description Default <code>network</code> <code>Network</code> <p>the pypsa network (filtered to contain only relevant buses &amp; links)</p> required <code>tech_colors</code> <code>dict</code> <p>config mapping</p> required <code>edge_colors</code> <code>Series | str</code> <p>the series of edge colors</p> <code>'black'</code> <code>edge_widths</code> <code>Series</code> <p>the edge widths</p> required <code>bus_colors</code> <code>Series</code> <p>the series of bus colors</p> required <code>bus_sizes</code> <code>Series</code> <p>the series of bus sizes</p> required <code>add_ref_edge_sizes</code> <code>bool</code> <p>add reference line sizes in legend (requires edge_colors=True). Defaults to True.</p> <code>True</code> <code>add_ref_bus_sizes</code> <code>bool</code> <p>add reference bus sizes in legend. Defaults to True.</p> <code>True</code> <code>ax</code> <code>Axes</code> <p>the plotting ax. Defaults to None (new figure).</p> <code>None</code> Source code in <code>workflow/scripts/plot_network.py</code> <pre><code>def plot_map(\n    network: pypsa.Network,\n    tech_colors: dict,\n    edge_widths: pd.Series,\n    bus_colors: dict | pd.Series,\n    bus_sizes: pd.Series,\n    edge_colors: pd.Series | str = \"black\",\n    add_ref_edge_sizes=True,\n    add_ref_bus_sizes=True,\n    add_legend=True,\n    bus_unit_conv=PLOT_COST_UNITS,\n    edge_unit_conv=PLOT_CAP_UNITS,\n    ax=None,\n    **kwargs,\n) -&gt; plt.Axes:\n    \"\"\"Plot the network on a map\n\n    Args:\n        network (pypsa.Network): the pypsa network (filtered to contain only relevant buses &amp; links)\n        tech_colors (dict): config mapping\n        edge_colors (pd.Series|str): the series of edge colors\n        edge_widths (pd.Series): the edge widths\n        bus_colors (pd.Series): the series of bus colors\n        bus_sizes (pd.Series): the series of bus sizes\n        add_ref_edge_sizes (bool, optional): add reference line sizes in legend\n            (requires edge_colors=True). Defaults to True.\n        add_ref_bus_sizes (bool, optional): add reference bus sizes in legend.\n            Defaults to True.\n        ax (plt.Axes, optional): the plotting ax. Defaults to None (new figure).\n    \"\"\"\n\n    if not ax:\n        fig, ax = plt.subplots()\n    else:\n        fig = ax.get_figure()\n\n    bus_colors = pd.Series(bus_colors)\n    if bus_sizes.index.nlevels &gt; 1:\n        missing = bus_sizes.index.get_level_values(1).difference(bus_colors.index)\n    else:\n        missing = bus_sizes.index.difference(bus_colors.index)\n    if not missing.empty:\n        raise ValueError(f\"Missing colors for bus carriers: {missing.tolist()}\")\n\n    network.plot(\n        bus_sizes=bus_sizes,\n        bus_colors=bus_colors,\n        line_colors=edge_colors,\n        link_colors=edge_colors,\n        line_widths=edge_widths,\n        link_widths=edge_widths,\n        ax=ax,\n        color_geomap=True,\n        boundaries=kwargs.get(\"boundaries\", None),\n    )\n\n    ax.add_feature(cfeature.BORDERS, linewidth=0.5, edgecolor=\"gray\")\n    states_provinces = cfeature.NaturalEarthFeature(\n        category=\"cultural\",\n        name=\"admin_1_states_provinces_lines\",\n        scale=\"50m\",\n        facecolor=\"none\",\n    )\n    # Add our states feature.\n    ax.add_feature(states_provinces, edgecolor=\"lightgray\", alpha=0.7)\n\n    if add_legend:\n        carriers = bus_sizes.index.get_level_values(1).unique()\n        # Ensure all carriers have colors, use default color for missing ones\n        colors = []\n        for carrier in carriers:\n            if carrier in tech_colors:\n                colors.append(tech_colors[carrier])\n            else:\n                colors.append(\"lightgrey\")  # Default color for missing carriers\n\n        if isinstance(edge_colors, str):\n            colors += [edge_colors]\n            labels = carriers.to_list() + [\"HVDC or HVAC link\"]\n        else:\n            colors += edge_colors.values.to_list()\n            labels = carriers.to_list() + edge_colors.index.to_list()\n        leg_opt = {\"bbox_to_anchor\": (1.42, 1.04), \"frameon\": False}\n        add_legend_patches(ax, colors, labels, legend_kw=leg_opt)\n\n    if add_ref_edge_sizes &amp; isinstance(edge_colors, str):\n        ref_unit = kwargs.get(\"ref_edge_unit\", \"GW\")\n        size_factor = float(kwargs.get(\"linewidth_factor\", 1e5))\n        ref_sizes = kwargs.get(\"ref_edge_sizes\", [1e5, 5e5])\n\n        labels = [f\"{float(s) / edge_unit_conv} {ref_unit}\" for s in ref_sizes]\n        ref_sizes = list(map(lambda x: float(x) / size_factor, ref_sizes))\n        legend_kw = dict(\n            loc=\"upper left\",\n            bbox_to_anchor=(0.26, 1.0),\n            frameon=False,\n            labelspacing=0.8,\n            handletextpad=2,\n            title=kwargs.get(\"edge_ref_title\", \"Grid cap.\"),\n        )\n        add_legend_lines(\n            ax, ref_sizes, labels, patch_kw=dict(color=edge_colors), legend_kw=legend_kw\n        )\n\n    # add reference bus sizes ferom the units\n    if add_ref_bus_sizes:\n        ref_unit = kwargs.get(\"ref_bus_unit\", \"bEUR/a\")\n        size_factor = float(kwargs.get(\"bus_size_factor\", 1e10))\n        ref_sizes = kwargs.get(\"ref_bus_sizes\", [2e10, 1e10, 5e10])\n        labels = [f\"{float(s) / bus_unit_conv:.0f} {ref_unit}\" for s in ref_sizes]\n        ref_sizes = list(map(lambda x: float(x) / size_factor, ref_sizes))\n\n        legend_kw = {\n            \"loc\": \"upper left\",\n            \"bbox_to_anchor\": (0.0, 1.0),\n            \"labelspacing\": 0.8,\n            \"frameon\": False,\n            \"handletextpad\": 0,\n            \"title\": kwargs.get(\"bus_ref_title\", \"UNDEFINED TITLE\"),\n        }\n\n        add_legend_circles(\n            ax,\n            ref_sizes,\n            labels,\n            srid=network.srid,\n            patch_kw=dict(facecolor=\"lightgrey\"),\n            legend_kw=legend_kw,\n        )\n\n    fig.tight_layout()\n\n    return ax\n</code></pre>"},{"location":"reference/plot_network/#plot_network.plot_nodal_prices","title":"<code>plot_nodal_prices(network, opts, carrier='AC', save_path=None)</code>","text":"<p>A map plot of energy, either AC or heat</p> <p>Parameters:</p> Name Type Description Default <code>network</code> <code>Network</code> <p>the pyPSA network object</p> required <code>opts</code> <code>dict</code> <p>the plotting options (snakemake.config[\"plotting\"])</p> required <code>save_path</code> <code>PathLike</code> <p>Fig outp path. Defaults to None (no save).</p> <code>None</code> <code>carrier</code> <code>str</code> <p>the energy carrier. Defaults to \"AC\".</p> <code>'AC'</code> <p>raises:     ValueError: if carrier is not AC or heat</p> Source code in <code>workflow/scripts/plot_network.py</code> <pre><code>def plot_nodal_prices(\n    network: pypsa.Network,\n    opts: dict,\n    carrier=\"AC\",\n    save_path: os.PathLike = None,\n):\n    \"\"\"A map plot of energy, either AC or heat\n\n    Args:\n        network (pypsa.Network): the pyPSA network object\n        opts (dict): the plotting options (snakemake.config[\"plotting\"])\n        save_path (os.PathLike, optional): Fig outp path. Defaults to None (no save).\n        carrier (str, optional): the energy carrier. Defaults to \"AC\".\n    raises:\n        ValueError: if carrier is not AC or heat\n    \"\"\"\n    if carrier not in [\"AC\", \"heat\"]:\n        raise ValueError(\"Carrier must be either 'AC' or 'heat'\")\n\n    # demand weighed prices per node\n    nodal_prices = (\n        network.statistics.revenue(\n            groupby=pypsa.statistics.get_bus_and_carrier_and_bus_carrier,\n            comps=\"Load\",\n            bus_carrier=carrier,\n        )\n        / network.statistics.withdrawal(\n            comps=\"Load\",\n            groupby=pypsa.statistics.get_bus_and_carrier_and_bus_carrier,\n            bus_carrier=carrier,\n        )\n        * -1\n    )\n    # drop the carrier and bus_carrier, map to colors\n    nodal_prices = nodal_prices.droplevel(1).droplevel(1)\n    norm = plt.Normalize(vmin=nodal_prices.min(), vmax=nodal_prices.max())\n    cmap = plt.get_cmap(\"plasma\")\n    bus_colors = nodal_prices.map(lambda x: cmap(norm(x)))\n\n    energy_consum = network.statistics.withdrawal(\n        groupby=pypsa.statistics.get_bus_and_carrier,\n        bus_carrier=carrier,\n        comps=[\"Load\"],\n    )\n    consum_pies = energy_consum.groupby(level=1).sum()\n\n    # Make figure\n    fig, ax = plt.subplots(subplot_kw={\"projection\": ccrs.PlateCarree()})\n    fig.set_size_inches(opts[\"price_map\"][\"figsize\"])\n    # get colors\n\n    # TODO make line handling nicer\n    # make sure plot isnt overpopulated\n    def calc_plot_width(row, carrier=\"AC\"):\n        if row.length == 0:\n            return 0\n        elif row.carrier != carrier:\n            return 0\n        else:\n            return row.p_nom_opt\n\n    line_lower_threshold = opts.get(\"min_edge_capacity\", 500)\n    edge_carrier = \"H2\" if carrier == \"heat\" else \"AC\"\n    link_plot_w = network.links.apply(lambda row: calc_plot_width(row, edge_carrier), axis=1)\n    edges = pd.concat([network.lines.s_nom_opt, link_plot_w])\n    edge_widths = edges.clip(line_lower_threshold, edges.max()).replace(line_lower_threshold, 0)\n\n    bus_size_factor = opts[\"price_map\"][\"bus_size_factor\"]\n    linewidth_factor = opts[\"price_map\"][f\"linewidth_factor{'_heat' if carrier == 'heat' else ''}\"]\n    plot_map(\n        network,\n        tech_colors=None,\n        edge_widths=edge_widths / linewidth_factor,\n        bus_colors=bus_colors,\n        bus_sizes=consum_pies / bus_size_factor,\n        edge_colors=opts[\"price_map\"][\"edge_color\"],\n        ax=ax,\n        edge_unit_conv=PLOT_CAP_UNITS,\n        bus_unit_conv=PLOT_SUPPLY_UNITS,\n        add_legend=False,\n        **opts[\"price_map\"],\n    )\n\n    # Add colorbar based on bus_colors\n    # fig.tight_layout()\n    fig.subplots_adjust(right=0.85)\n    cax = fig.add_axes([0.87, ax.get_position().y0, 0.02, ax.get_position().height])\n    sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n    sm.set_array([])\n    cbar = plt.colorbar(sm, cax=cax, orientation=\"vertical\")\n    cbar.set_label(f\"Nodal Prices ${CURRENCY}/MWh\")\n\n    if save_path:\n        fig.savefig(save_path, transparent=opts[\"transparent\"], bbox_inches=\"tight\")\n</code></pre>"},{"location":"reference/plot_network_heat/","title":"Plot network heat","text":"<p>Legacy functions that are not currently included in the workflow</p>"},{"location":"reference/plot_network_heat/#plot_network_heat.make_handler_map_to_scale_circles_as_in","title":"<code>make_handler_map_to_scale_circles_as_in(ax, dont_resize_actively=False)</code>","text":"<p>Create a handler map for scaling circles in legend to match plot dimensions.</p> <p>Parameters:</p> Name Type Description Default <code>ax</code> <p>The matplotlib axes object</p> required <code>dont_resize_actively</code> <code>bool</code> <p>If True, disable active resizing. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>Handler map for Circle patches</p> Source code in <code>workflow/scripts/plot_network_heat.py</code> <pre><code>def make_handler_map_to_scale_circles_as_in(ax, dont_resize_actively=False):\n    \"\"\"Create a handler map for scaling circles in legend to match plot dimensions.\n\n    Args:\n        ax: The matplotlib axes object\n        dont_resize_actively (bool, optional): If True, disable active resizing. Defaults to False.\n\n    Returns:\n        dict: Handler map for Circle patches\n    \"\"\"\n    fig = ax.get_figure()\n\n    def axes2pt():\n        return np.diff(ax.transData.transform([(0, 0), (1, 1)]), axis=0)[0] * (72.0 / fig.dpi)\n\n    ellipses = []\n    if not dont_resize_actively:\n\n        def update_width_height(event):\n            dist = axes2pt()\n            for e, radius in ellipses:\n                e.width, e.height = 2.0 * radius * dist\n\n        fig.canvas.mpl_connect(\"resize_event\", update_width_height)\n        ax.callbacks.connect(\"xlim_changed\", update_width_height)\n        ax.callbacks.connect(\"ylim_changed\", update_width_height)\n\n    def legend_circle_handler(legend, orig_handle, xdescent, ydescent, width, height, fontsize):\n        w, h = 2.0 * orig_handle.get_radius() * axes2pt()\n        e = Ellipse(\n            xy=(0.5 * width - 0.5 * xdescent, 0.5 * height - 0.5 * ydescent),\n            width=w,\n            height=w,\n        )\n        ellipses.append((e, orig_handle.get_radius()))\n        return e\n\n    return {Circle: HandlerPatch(patch_func=legend_circle_handler)}\n</code></pre>"},{"location":"reference/plot_network_heat/#plot_network_heat.make_legend_circles_for","title":"<code>make_legend_circles_for(sizes, scale=1.0, **kw)</code>","text":"<p>Create circles for legend with specified sizes.</p> <p>Parameters:</p> Name Type Description Default <code>sizes</code> <p>List of sizes for the circles</p> required <code>scale</code> <code>float</code> <p>Scale factor for circle sizes. Defaults to 1.0.</p> <code>1.0</code> <code>**kw</code> <p>Additional keyword arguments passed to Circle constructor</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>list</code> <p>List of Circle objects for legend</p> Source code in <code>workflow/scripts/plot_network_heat.py</code> <pre><code>def make_legend_circles_for(sizes, scale=1.0, **kw):\n    \"\"\"Create circles for legend with specified sizes.\n\n    Args:\n        sizes: List of sizes for the circles\n        scale (float, optional): Scale factor for circle sizes. Defaults to 1.0.\n        **kw: Additional keyword arguments passed to Circle constructor\n\n    Returns:\n        list: List of Circle objects for legend\n    \"\"\"\n    return [Circle((0, 0), radius=(s / scale) ** 0.5, **kw) for s in sizes]\n</code></pre>"},{"location":"reference/plot_network_heat/#plot_network_heat.plot_opt_map","title":"<code>plot_opt_map(n, plot_config, ax=None, attribute='p_nom')</code>","text":"<p>Plot an optimized network map showing generation capacities and transmission lines.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <p>PyPSA Network object</p> required <code>plot_config</code> <code>dict</code> <p>Configuration dictionary with plotting parameters</p> required <code>ax</code> <p>Matplotlib axes object. If None, creates new figure. Defaults to None.</p> <code>None</code> <code>attribute</code> <code>str</code> <p>Network attribute to plot. Defaults to \"p_nom\".</p> <code>'p_nom'</code> <p>Returns:</p> Type Description <p>matplotlib.axes.Axes: The axes object with the plot</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If attribute plotting is not implemented</p> Source code in <code>workflow/scripts/plot_network_heat.py</code> <pre><code>def plot_opt_map(n, plot_config, ax=None, attribute=\"p_nom\"):\n    \"\"\"Plot an optimized network map showing generation capacities and transmission lines.\n\n    Args:\n        n: PyPSA Network object\n        plot_config (dict): Configuration dictionary with plotting parameters\n        ax: Matplotlib axes object. If None, creates new figure. Defaults to None.\n        attribute (str, optional): Network attribute to plot. Defaults to \"p_nom\".\n\n    Returns:\n        matplotlib.axes.Axes: The axes object with the plot\n\n    Raises:\n        ValueError: If attribute plotting is not implemented\n    \"\"\"\n    if ax is None:\n        fig, ax = plt.subplots(figsize=(10, 10), subplot_kw={\"projection\": ccrs.PlateCarree()})\n\n    # colors\n    line_colors = {\n        \"cur\": \"purple\",\n        \"exp\": mpl.colors.rgb2hex(to_rgba(\"red\", 0.7), True),\n    }\n    tech_colors = plot_config[\"tech_colors\"]\n\n    if attribute == \"p_nom\":\n        # size by total p_nom (installed cap)\n        bus_sizes = pd.concat(\n            (\n                n.generators.query('carrier != \"solar thermal\"' and 'carrier != \"hydro_inflow\"')\n                .groupby([\"bus\", \"carrier\"])\n                .p_nom_opt.sum(),\n                n.links.query('carrier == [\"gas-AC\",\"coal-AC\",\"stations-AC\"]')\n                .groupby([\"bus1\", \"carrier\"])\n                .p_nom_opt.sum(),\n            )\n        )\n        bus_sizes.index.names = [\"bus\", \"carrier\"]\n        bus_sizes = bus_sizes.groupby([\"bus\", \"carrier\"]).sum()\n        line_widths_exp = n.lines.s_nom_opt\n        link_widths_exp = (\n            pd.concat(\n                [\n                    n.links.query('carrier == [\"AC-AC\"]').p_nom_opt,\n                    n.links.query('carrier != [\"AC-AC\"]').p_min_pu,\n                ]\n            )\n            - n.links.p_nom_min\n        )\n    else:\n        raise f\"plotting of {attribute} has not been implemented yet\"\n\n    # FORMAT\n    linewidth_factor = plot_config[\"map\"][\"linewidth_factor\"]\n    bus_size_factor = plot_config[\"map\"][\"bus_size_factor\"]\n\n    # PLOT\n    n.plot(\n        line_widths=line_widths_exp / linewidth_factor,\n        link_widths=link_widths_exp / linewidth_factor,\n        line_colors=line_colors[\"exp\"],\n        link_colors=line_colors[\"exp\"],\n        bus_sizes=bus_sizes / bus_size_factor / 1000,\n        bus_colors=tech_colors,\n        boundaries=map_boundaries,\n        # color_geomap=True,\n        geomap=True,\n        ax=ax,\n    )\n    # n.plot(\n    #     line_widths=line_widths_cur / linewidth_factor,\n    #     link_widths=link_widths_cur / linewidth_factor,\n    #     line_colors=line_colors_with_alpha,\n    #     link_colors=link_colors_with_alpha,\n    #     bus_sizes=0,\n    #     boundaries=map_boundaries,\n    #     color_geomap=True,\n    #     geomap=False,\n    #     ax=ax,\n    # )\n    ax.set_aspect(\"equal\")\n    ax.axis(\"off\")\n\n    # Rasterize basemap\n    # TODO : Check if this also works with cartopy\n    for c in ax.collections[:2]:\n        c.set_rasterized(True)\n\n    # LEGEND\n    handles = []\n    labels = []\n\n    for s in (50, 10):\n        handles.append(\n            plt.Line2D([0], [0], color=line_colors[\"exp\"], linewidth=s * 1e3 / linewidth_factor)\n        )\n        labels.append(f\"{s} GW\")\n    l1_1 = ax.legend(\n        handles,\n        labels,\n        loc=\"upper left\",\n        bbox_to_anchor=(0.24, 1.01),\n        frameon=False,\n        labelspacing=0.8,\n        handletextpad=1.5,\n        title=\"Transmission Exist./Exp.             \",\n    )\n    ax.add_artist(l1_1)\n\n    handles = []\n    labels = []\n    for s in (50, 10):\n        handles.append(\n            plt.Line2D([0], [0], color=line_colors[\"cur\"], linewidth=s * 1e3 / linewidth_factor)\n        )\n        labels.append(\"/\")\n    l1_2 = ax.legend(\n        handles,\n        labels,\n        loc=\"upper left\",\n        bbox_to_anchor=(0.26, 1.01),\n        frameon=False,\n        labelspacing=0.8,\n        handletextpad=0.5,\n        title=\" \",\n    )\n    ax.add_artist(l1_2)\n\n    handles = make_legend_circles_for([10e4, 5e4, 1e4], scale=bus_size_factor, facecolor=\"w\")\n    labels = [f\"{s} GW\" for s in (100, 50, 10)]\n    l2 = ax.legend(\n        handles,\n        labels,\n        loc=\"upper left\",\n        bbox_to_anchor=(0.01, 1.01),\n        frameon=False,\n        labelspacing=1.0,\n        title=\"Generation\",\n        handler_map=make_handler_map_to_scale_circles_as_in(ax),\n    )\n    ax.add_artist(l2)\n\n    techs = (bus_sizes.index.levels[1]).intersection(\n        pd.Index(\n            plot_config[\"vre_techs\"] + plot_config[\"conv_techs\"] + plot_config[\"storage_techs\"]\n        )\n    )\n    handles = []\n    labels = []\n    for t in techs:\n        handles.append(\n            plt.Line2D([0], [0], color=tech_colors[t], marker=\"o\", markersize=8, linewidth=0)\n        )\n        labels.append(plot_config[\"nice_names\"].get(t, t))\n    ax.legend(\n        handles,\n        labels,\n        loc=\"upper center\",\n        bbox_to_anchor=(0.5, -0.0),  # bbox_to_anchor=(0.72, -0.05),\n        handletextpad=0.0,\n        columnspacing=0.5,\n        ncol=4,\n        title=\"Technology\",\n    )\n\n    return ax\n</code></pre>"},{"location":"reference/plot_network_heat/#plot_network_heat.plot_total_cost_bar","title":"<code>plot_total_cost_bar(n, plot_config, ax=None)</code>","text":"<p>Plot a stacked bar chart showing total system costs by technology.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>Network</code> <p>PyPSA Network object</p> required <code>plot_config</code> <code>dict</code> <p>Configuration dictionary with plotting parameters including cost thresholds</p> required <code>ax</code> <p>Matplotlib axes object. If None, uses current axes. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>None</code> <p>Modifies the axes in place</p> Source code in <code>workflow/scripts/plot_network_heat.py</code> <pre><code>def plot_total_cost_bar(n: Network, plot_config: dict, ax=None):\n    \"\"\"Plot a stacked bar chart showing total system costs by technology.\n\n    Args:\n        n (Network): PyPSA Network object\n        plot_config (dict): Configuration dictionary with plotting parameters including cost thresholds\n        ax: Matplotlib axes object. If None, uses current axes. Defaults to None.\n\n    Returns:\n        None: Modifies the axes in place\n    \"\"\"\n    if ax is None:\n        ax = plt.gca()\n\n    total_load = (n.snapshot_weightings.generators * n.loads_t.p.sum(axis=1)).sum()\n    tech_colors = plot_config[\"tech_colors\"]\n\n    def split_costs(n: Network):\n        costs = aggregate_costs(n).reset_index(level=0, drop=True)\n        costs.index.rename([\"cost\", \"carrier\"], inplace=True)\n        costs = costs.groupby([\"cost\", \"carrier\"]).sum()\n        costs_ex = aggregate_costs(n, existing_only=True).reset_index(level=0, drop=True)\n        costs_ex.index.rename([\"cost\", \"carrier\"], inplace=True)\n        costs_ex = costs_ex.groupby([\"cost\", \"carrier\"]).sum()\n        return (\n            costs[\"capital\"].add(costs[\"marginal\"], fill_value=0.0),\n            costs_ex[\"capital\"],\n            costs[\"capital\"] - costs_ex[\"capital\"],\n            costs[\"marginal\"],\n        )\n\n    costs, costs_cap_ex, costs_cap_new, costs_marg = split_costs(n)\n\n    costs_graph = pd.DataFrame(dict(a=costs[costs &gt; plot_config[\"costs_threshold\"]])).dropna()\n    # TODO aggregate rest into othrer\n    bottom = np.array([0.0, 0.0])\n    texts = []\n\n    # fix_colors\n    costs = costs.to_frame()\n    costs[\"color\"] = costs.index.map(tech_colors)\n    costs[\"nice_name\"] = costs.index.map(config[\"plotting\"][\"nice_names\"])\n    costs.loc[costs.color.isna(), \"color\"] = (\n        costs[costs.color.isna()].nice_name.str.lower().map(tech_colors).fillna(\"pink\")\n    )\n\n    for i, ind in enumerate(costs_graph.index):\n        data = np.asarray(costs_graph.loc[ind]) / total_load\n        ax.bar([0.5], data, bottom=bottom, color=costs.color, width=0.7, zorder=-1)\n        bottom_sub = bottom\n        bottom = bottom + data\n\n        if ind in plot_config[\"conv_techs\"] + [\"AC line\"]:\n            for c in [costs_cap_ex, costs_marg]:\n                if ind in c:\n                    data_sub = np.asarray([c.loc[ind]]) / total_load\n                    ax.bar(\n                        [0.5],\n                        data_sub,\n                        linewidth=0,\n                        bottom=bottom_sub,\n                        color=costs.color,\n                        width=0.7,\n                        zorder=-1,\n                        alpha=0.8,\n                    )\n                    bottom_sub += data_sub\n\n        if abs(data[-1]) &lt; 5:\n            continue\n\n        text = ax.text(1.1, (bottom - 0.5 * data)[-1] - 3, plot_config[\"nice_names\"].get(ind, ind))\n        texts.append(text)\n\n    ax.set_ylabel(\"Average system cost [Eur/MWh]\")\n    ax.set_ylim([0, plot_config.get(\"costs_avg\", 80)])\n    ax.set_xlim([0, 1])\n    ax.set_xticklabels([])\n    ax.grid(True, axis=\"y\", color=\"k\", linestyle=\"dotted\")\n</code></pre>"},{"location":"reference/plot_network_heat/#plot_network_heat.plot_total_energy_pie","title":"<code>plot_total_energy_pie(n, plot_config, ax=None)</code>","text":"<p>Plot a pie chart showing total energy production by technology.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <p>PyPSA Network object</p> required <code>plot_config</code> <code>dict</code> <p>Configuration dictionary with plotting parameters</p> required <code>ax</code> <p>Matplotlib axes object. If None, creates new figure. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>None</code> <p>Modifies the axes in place</p> Source code in <code>workflow/scripts/plot_network_heat.py</code> <pre><code>def plot_total_energy_pie(n, plot_config, ax=None):\n    \"\"\"Plot a pie chart showing total energy production by technology.\n\n    Args:\n        n: PyPSA Network object\n        plot_config (dict): Configuration dictionary with plotting parameters\n        ax: Matplotlib axes object. If None, creates new figure. Defaults to None.\n\n    Returns:\n        None: Modifies the axes in place\n    \"\"\"\n    if ax is None:\n        fig, ax = plt.subplots(figsize=(5, 5))\n\n    ax.set_title(\"Energy per technology\", fontdict=dict(fontsize=\"medium\"))\n\n    e_primary = aggregate_p(n).drop(\"load\", errors=\"ignore\").loc[lambda s: s &gt; 1]\n\n    e_primary = e_primary.groupby(\"carrier\").sum()\n\n    patches, texts, autotexts = ax.pie(\n        e_primary,\n        startangle=90,\n        labels=e_primary.rename(plot_config[\"nice_names\"][\"energy\"]).index,\n        autopct=\"%.0f%%\",\n        shadow=False,\n        colors=n.carriers.color.loc[e_primary.index],\n    )\n    for t1, t2, i in zip(texts, autotexts, e_primary.index):\n        if e_primary.at[i] &lt; 0.04 * e_primary.sum():\n            t1.remove()\n            t2.remove()\n</code></pre>"},{"location":"reference/plot_statistics/","title":"Plot statistics","text":"<p>Plot statistical analysis and summary charts for energy system results.</p> <p>This module creates statistical plots including capacity factors, cost breakdowns, energy balances, and other key performance indicators for the PyPSA-China model. Adapted from PyPSA-Eur by PyPSA-China authors.</p>"},{"location":"reference/plot_statistics/#plot_statistics.add_second_xaxis","title":"<code>add_second_xaxis(data, ax, label, **kwargs)</code>","text":"<p>Add a secondary X-axis to the plot.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Series</code> <p>The data to plot. Its values will be plotted on the secondary X-axis.</p> required <code>ax</code> <code>Axes</code> <p>The main matplotlib Axes object.</p> required <code>label</code> <code>str</code> <p>The label for the secondary X-axis.</p> required <code>**kwargs</code> <p>Optional keyword arguments for plot styling.</p> <code>{}</code> Source code in <code>workflow/scripts/plot_statistics.py</code> <pre><code>def add_second_xaxis(data: pd.Series, ax, label, **kwargs):\n    \"\"\"\n    Add a secondary X-axis to the plot.\n\n    Args:\n        data (pd.Series): The data to plot. Its values will be plotted on the secondary X-axis.\n        ax (matplotlib.axes.Axes): The main matplotlib Axes object.\n        label (str): The label for the secondary X-axis.\n        **kwargs: Optional keyword arguments for plot styling.\n    \"\"\"\n    defaults = {\"color\": \"red\", \"text_offset\": 0.5, \"markersize\": 8, \"fontsize\": 9}\n    kwargs.update(defaults)\n\n    ax2 = ax.twiny()\n    # y_pos creates a sequence of integers (e.g., [0, 1, 2, 3]) to serve\n    # as distinct vertical positions for each data point on the shared Y-axis.\n    # This is necessary because data.values are plotted horizontally on the\n    # secondary X-axis (ax2), requiring vertical separation for clarity.\n    y_pos = range(len(data))\n\n    ax2.plot(\n        data.values,\n        y_pos,\n        marker=\"o\",\n        linestyle=\"\",\n        color=kwargs[\"color\"],\n        markersize=kwargs[\"markersize\"],\n        label=\"Generation Share (%)\",\n    )\n\n    for i, val in enumerate(data.values):\n        ax2.text(\n            val + kwargs[\"text_offset\"],\n            i,\n            f\"{val:.1f}%\",\n            color=kwargs[\"color\"],\n            va=\"center\",\n            ha=\"left\",\n            fontsize=kwargs[\"fontsize\"],\n        )\n\n    ax2.set_xlim(left=0)\n    ax2.set_xlabel(label)\n    ax2.grid(False)\n    ax2.tick_params(axis=\"x\", labelsize=kwargs[\"fontsize\"])  # Remove color setting for ticks\n\n    return ax2\n</code></pre>"},{"location":"reference/plot_statistics/#plot_statistics.filter_small_caps","title":"<code>filter_small_caps(n, threshold=100)</code>","text":"<p>Drop small capacities for plotting (eliminate numerical zeroes) -&gt; this would be more robust based on the objective cost tolerance</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>Network</code> <p>the pypsa network to remove small comps from</p> required <code>threshold</code> <code>int</code> <p>the removal threshold. Defaults to 100.</p> <code>100</code> Source code in <code>workflow/scripts/plot_statistics.py</code> <pre><code>def filter_small_caps(n: pypsa.Network, threshold=100):\n    \"\"\"Drop small capacities for plotting (eliminate numerical zeroes)\n    -&gt; this would be more robust based on the objective cost tolerance\n\n    Args:\n        n (pypsa.Network): the pypsa network to remove small comps from\n        threshold (int, optional): the removal threshold. Defaults to 100.\n    \"\"\"\n    for c in [\"links\", \"generators\", \"stores\", \"storage_units\"]:\n        attr = \"e_nom_opt\" if c == \"stores\" else \"p_nom_opt\"\n        comp = getattr(n, c)\n        mask = comp[attr] &gt; threshold\n        comp = comp.loc[mask]\n        setattr(n, c, comp)\n</code></pre>"},{"location":"reference/plot_statistics/#plot_statistics.fix_load_carriers","title":"<code>fix_load_carriers(n, config)</code>","text":"<p>Set unspecified load carriers to load</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>Network</code> <p>The PyPSA network instance.</p> required <code>config</code> <code>dict</code> <p>the plotting config</p> required Source code in <code>workflow/scripts/plot_statistics.py</code> <pre><code>def fix_load_carriers(n: pypsa.Network, config: dict):\n    \"\"\"Set unspecified load carriers to load\n\n    Args:\n        n (pypsa.Network): The PyPSA network instance.\n        config (dict): the plotting config\n    \"\"\"\n    mask = n.loads.query(\"carrier==''\").index\n    n.loads.loc[mask, \"carrier\"] = \"Load\"\n    n.carriers.loc[\"Load\", [\"nice_name\", \"color\"]] = (\n        \"Load\",\n        config[\"tech_colors\"][\"electric load\"],\n    )\n</code></pre>"},{"location":"reference/plot_statistics/#plot_statistics.format_axis_label","title":"<code>format_axis_label(name, unit)</code>","text":"<p>Format axis label by cleaning name and conditionally adding unit.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name/description for the axis.</p> required <code>unit</code> <code>str</code> <p>The unit of measurement (may be empty/None).</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Formatted label with underscores replaced by spaces, capitalized, and unit only included if non-empty.</p> Source code in <code>workflow/scripts/plot_statistics.py</code> <pre><code>def format_axis_label(name: str, unit: str) -&gt; str:\n    \"\"\"Format axis label by cleaning name and conditionally adding unit.\n\n    Args:\n        name (str): The name/description for the axis.\n        unit (str): The unit of measurement (may be empty/None).\n\n    Returns:\n        str: Formatted label with underscores replaced by spaces,\n            capitalized, and unit only included if non-empty.\n    \"\"\"\n    clean_name = name.replace(\"_\", \" \").capitalize()\n    if unit and unit.strip():\n        return f\"{clean_name} [{unit}]\"\n    return clean_name\n</code></pre>"},{"location":"reference/plot_statistics/#plot_statistics.plot_capacity_factor","title":"<code>plot_capacity_factor(cf_filtered, theo_cf_filtered, ax, colors, **kwargs)</code>","text":"<p>Plot actual and theoretical capacity factors for each technology.</p> <p>Parameters:</p> Name Type Description Default <code>cf_filtered</code> <code>Series</code> <p>Actual capacity factors indexed by technology.</p> required <code>theo_cf_filtered</code> <code>Series</code> <p>Theoretical capacity factors indexed by technology.</p> required <code>ax</code> <code>Axes</code> <p>The axis to plot on.</p> required <code>colors</code> <code>dict</code> <p>Color mapping for technologies.</p> required <p>Returns:</p> Type Description <p>matplotlib.axes.Axes: The axis with the plot.</p> Source code in <code>workflow/scripts/plot_statistics.py</code> <pre><code>def plot_capacity_factor(\n    cf_filtered: pd.Series, theo_cf_filtered: pd.Series, ax: axes.Axes, colors: dict, **kwargs\n):\n    \"\"\"\n    Plot actual and theoretical capacity factors for each technology.\n\n    Args:\n        cf_filtered (pd.Series): Actual capacity factors indexed by technology.\n        theo_cf_filtered (pd.Series): Theoretical capacity factors indexed by technology.\n        ax (matplotlib.axes.Axes): The axis to plot on.\n        colors (dict): Color mapping for technologies.\n\n    Returns:\n        matplotlib.axes.Axes: The axis with the plot.\n    \"\"\"\n    x_pos = range(len(cf_filtered))\n    width = 0.35\n\n    ax.barh(\n        [i - width / 2 for i in x_pos],\n        cf_filtered.values,\n        width,\n        color=[colors.get(tech, \"lightgrey\") for tech in cf_filtered.index],\n        alpha=0.8,\n        label=\"Actual CF\",\n    )\n    ax.barh(\n        [i + width / 2 for i in x_pos],\n        theo_cf_filtered.values,\n        width,\n        color=[colors.get(tech, \"lightgrey\") for tech in theo_cf_filtered.index],\n        alpha=0.4,\n        label=\"Theoretical CF\",\n    )\n\n    for i, (tech, cf_val) in enumerate(cf_filtered.items()):\n        ax.text(\n            cf_val + 0.01,\n            i - width / 2,\n            f\"{cf_val:.2f}\",\n            va=\"center\",\n            ha=\"left\",\n            fontsize=8,\n            bbox=dict(boxstyle=\"round,pad=0.2\", facecolor=\"white\", alpha=0.8),\n        )\n        theo_val = theo_cf_filtered.get(tech, 0)\n        ax.text(\n            theo_val + 0.01,\n            i + width / 2,\n            f\"{theo_val:.2f}\",\n            va=\"center\",\n            ha=\"left\",\n            fontsize=8,\n            bbox=dict(boxstyle=\"round,pad=0.2\", facecolor=\"white\", alpha=0.5),\n        )\n\n    ax.set_yticks(list(x_pos))\n    ax.set_yticklabels([label.capitalize() for label in cf_filtered.index])\n    ax.set_xlabel(\"Capacity Factor\")\n    ax.set_xlim(0, max(cf_filtered.max(), theo_cf_filtered.max()) * 1.1)\n    ax.grid(False)\n    ax.legend()\n\n    return ax\n</code></pre>"},{"location":"reference/plot_statistics/#plot_statistics.plot_province_peakload_capacity","title":"<code>plot_province_peakload_capacity(df_plot, bar_cols, color_list, outp_dir)</code>","text":"<p>Plot province peak load vs installed capacity by technology.</p> <p>Parameters:</p> Name Type Description Default <code>df_plot</code> <p>DataFrame with provinces as index, columns as technologies and 'Peak Load'.</p> required <code>bar_cols</code> <p>List of technology columns to plot as bars.</p> required <code>color_list</code> <p>List of colors for each technology.</p> required <code>outp_dir</code> <p>Output directory for saving the figure.</p> required Source code in <code>workflow/scripts/plot_statistics.py</code> <pre><code>def plot_province_peakload_capacity(df_plot, bar_cols, color_list, outp_dir):\n    \"\"\"\n    Plot province peak load vs installed capacity by technology.\n\n    Args:\n        df_plot: DataFrame with provinces as index, columns as technologies and 'Peak Load'.\n        bar_cols: List of technology columns to plot as bars.\n        color_list: List of colors for each technology.\n        outp_dir: Output directory for saving the figure.\n    \"\"\"\n    fig, ax = plt.subplots(figsize=(14, 8))\n\n    df_plot[bar_cols].plot(kind=\"barh\", stacked=True, ax=ax, color=color_list, alpha=0.8)\n    # Plot peak load as red vertical line\n    for i, prov in enumerate(df_plot.index):\n        ax.plot(\n            df_plot.loc[prov, \"Peak Load\"],\n            i,\n            \"r|\",\n            markersize=18,\n            label=\"Peak Load\" if i == 0 else \"\",\n        )\n    ax.set_xlabel(\"Capacity [GW]\")\n    ax.set_ylabel(\"Province\")\n    ax.set_title(\"Peak Load vs Installed Capacity by Province\")\n    ax.grid(False)\n    # Only keep one Peak Load legend\n    handles, labels = ax.get_legend_handles_labels()\n    seen = set()\n    new_handles, new_labels = [], []\n    for h, l in zip(handles, labels):\n        if l not in seen:\n            new_handles.append(h)\n            new_labels.append(l)\n            seen.add(l)\n    ax.legend(new_handles, new_labels, loc=\"best\")\n    fig.tight_layout()\n    fig.savefig(os.path.join(outp_dir, \"province_peakload_capacity.png\"))\n</code></pre>"},{"location":"reference/plot_statistics/#plot_statistics.plot_static_per_carrier","title":"<code>plot_static_per_carrier(ds, ax, colors, drop_zero_vals=True, add_labels=True, autofigsize=True)</code>","text":"<p>Generic function to plot different statics</p> <p>Parameters:</p> Name Type Description Default <code>ds</code> <code>Series</code> <p>the data to plot</p> required <code>ax</code> <code>Axes</code> <p>plotting axes</p> required <code>colors</code> <code>Series</code> <p>colors for the carriers</p> required <code>drop_zero_vals</code> <code>bool</code> <p>Drop zeroes from data. Defaults to True.</p> <code>True</code> <code>add_labels</code> <code>bool</code> <p>Add value labels on bars. Defaults to True.</p> <code>True</code> <code>autofigsize</code> <code>bool</code> <p>Automatically size figure based on number of bars. Defaults to True.</p> <code>True</code> Source code in <code>workflow/scripts/plot_statistics.py</code> <pre><code>def plot_static_per_carrier(\n    ds: pd.Series,\n    ax: axes.Axes,\n    colors: pd.Series,\n    drop_zero_vals=True,\n    add_labels=True,\n    autofigsize=True,\n):\n    \"\"\"Generic function to plot different statics\n\n    Args:\n        ds (pd.Series): the data to plot\n        ax (matplotlib.axes.Axes): plotting axes\n        colors (pd.Series): colors for the carriers\n        drop_zero_vals (bool, optional): Drop zeroes from data. Defaults to True.\n        add_labels (bool, optional): Add value labels on bars. Defaults to True.\n        autofigsize (bool, optional): Automatically size figure based on number\n            of bars. Defaults to True.\n    \"\"\"\n    if drop_zero_vals:\n        ds = ds[ds != 0]\n    ds = ds.dropna()\n\n    n_bars = len(ds)\n\n    # Determine figure size\n    if autofigsize:\n        bar_height = 0.4  # Height per bar in inches\n        fig_height = max(4, min(n_bars * bar_height, 16))  # Between 4 and 16 inches\n        figsize = (8, fig_height)\n    else:\n        figsize = None  # Use matplotlib default\n\n    # Create or get figure\n    if not ax:\n        fig, ax = plt.subplots(figsize=figsize)\n    else:\n        fig = ax.get_figure()\n        if autofigsize:\n            fig.set_size_inches(8, fig_height)\n\n    c = colors[ds.index.get_level_values(\"carrier\")]\n    ds = ds.pipe(rename_index)\n    label = format_axis_label(ds.attrs[\"name\"], ds.attrs[\"unit\"])\n    ds.plot.barh(color=c.values, xlabel=label, ax=ax, alpha=0.9)\n\n    # Remove y-axis label\n    ax.set_ylabel(\"\")\n\n    # Adjust spacing between bars\n    ax.margins(y=0.01)\n\n    if add_labels:\n        ymax = ax.get_xlim()[1] * 1.05\n        for i, (index, value) in enumerate(ds.items()):\n            align = \"left\"\n            txt = f\"{value:.2f}\" if value &lt;= 100 else f\"{value:.1e}\"\n            ax.text(ymax, i, txt, va=\"center\", ha=align, fontsize=8)\n        # # Add outer y-ticks at the right y-axis frame\n        # ax.tick_params(axis=\"y\", direction=\"out\", right=True, left=False)\n    ax.grid(axis=\"y\")\n    fig.tight_layout()\n\n    return fig\n</code></pre>"},{"location":"reference/plot_statistics/#plot_statistics.prepare_capacity_factor_data","title":"<code>prepare_capacity_factor_data(n, carrier)</code>","text":"<p>Prepare Series for actual and theoretical capacity factors per technology.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>Network</code> <p>The PyPSA network instance.</p> required <code>carrier</code> <code>str</code> <p>The carrier for which to prepare the data.</p> required <p>Returns:</p> Name Type Description <code>cf_filtered</code> <p>Series of actual capacity factors (index: nice_name)</p> <code>theo_cf_filtered</code> <p>Series of theoretical capacity factors (index: nice_name)</p> Source code in <code>workflow/scripts/plot_statistics.py</code> <pre><code>def prepare_capacity_factor_data(n: pypsa.Network, carrier: str):\n    \"\"\"\n    Prepare Series for actual and theoretical capacity factors per technology.\n\n    Args:\n        n (pypsa.Network): The PyPSA network instance.\n        carrier (str): The carrier for which to prepare the data.\n\n    Returns:\n        cf_filtered: Series of actual capacity factors (index: nice_name)\n        theo_cf_filtered: Series of theoretical capacity factors (index: nice_name)\n    \"\"\"\n    cf_data = n.statistics.capacity_factor(groupby=[\"carrier\"]).dropna()\n    if (\"Link\", \"battery\") in cf_data.index:\n        cf_data.loc[(\"Link\", \"battery charger\")] = cf_data.loc[(\"Link\", \"battery\")]\n        cf_data.drop(index=(\"Link\", \"battery\"), inplace=True)\n    cf_data = cf_data.groupby(level=1).mean()\n\n    # Theoretical capacity factor\n    gen = n.generators.copy()\n    p_max_pu = n.generators_t.p_max_pu\n    gen[\"p_nom_used\"] = gen[\"p_nom_opt\"].fillna(gen[\"p_nom\"])\n    weighted_energy_per_gen = (p_max_pu * gen[\"p_nom_used\"]).sum()\n    gen[\"weighted_energy\"] = weighted_energy_per_gen\n\n    gen[\"nice_name\"] = gen[\"carrier\"].map(\n        lambda x: n.carriers.loc[x, \"nice_name\"] if x in n.carriers.index else x\n    )\n    grouped_energy = gen.groupby(\"nice_name\")[\"weighted_energy\"].sum()\n    grouped_capacity = gen.groupby(\"nice_name\")[\"p_nom_used\"].sum()\n    theoretical_cf_weighted = grouped_energy / grouped_capacity / len(n.snapshots)\n\n    # Only keep technologies present in both actual and theoretical CF\n    common_techs = cf_data.index.intersection(theoretical_cf_weighted.index)\n    cf_filtered = cf_data.loc[common_techs]\n    theo_cf_filtered = theoretical_cf_weighted.loc[cf_filtered.index]\n    # Todo: use config nondispatchable_techs\n    non_zero_mask = (cf_filtered != 0) &amp; (theo_cf_filtered != 0)\n    cf_filtered = cf_filtered[non_zero_mask]\n    theo_cf_filtered = theo_cf_filtered[non_zero_mask]\n    cf_filtered = cf_filtered.sort_values(ascending=True)\n    theo_cf_filtered = theo_cf_filtered.loc[cf_filtered.index]\n\n    return cf_filtered, theo_cf_filtered\n</code></pre>"},{"location":"reference/plot_statistics/#plot_statistics.prepare_province_peakload_capacity_data","title":"<code>prepare_province_peakload_capacity_data(n, attached_carriers=None)</code>","text":"<p>Prepare DataFrame for province peak load and installed capacity by technology.</p> <p>Returns:</p> Name Type Description <code>df_plot</code> <p>DataFrame with provinces as index, columns as technologies and 'Peak Load'.</p> <code>bar_cols</code> <p>List of technology columns to plot as bars.</p> <code>color_list</code> <p>List of colors for each technology.</p> Source code in <code>workflow/scripts/plot_statistics.py</code> <pre><code>def prepare_province_peakload_capacity_data(n, attached_carriers=None):\n    \"\"\"\n    Prepare DataFrame for province peak load and installed capacity by technology.\n\n    Returns:\n        df_plot: DataFrame with provinces as index, columns as technologies and 'Peak Load'.\n        bar_cols: List of technology columns to plot as bars.\n        color_list: List of colors for each technology.\n    \"\"\"\n    # Calculate peak load per province\n    load = n.loads.copy()\n    load[\"province\"] = load[\"bus\"].map(n.buses[\"location\"])\n    peak_load = n.loads_t.p_set.groupby(load[\"province\"], axis=1).sum().max()\n    peak_load = peak_load / PLOT_CAP_UNITS  # ensure peak load is in GW\n\n    # Calculate installed capacity per province and technology using optimal_capacity\n    ds = n.statistics.optimal_capacity(groupby=[\"location\", \"carrier\"]).dropna()\n    valid_components = [\"Generator\", \"StorageUnit\", \"Link\"]\n    ds = ds.loc[ds.index.get_level_values(0).isin(valid_components)]\n    if (\"Link\", \"battery\") in ds.index:\n        ds.loc[(\"Link\", \"battery charger\")] = ds.loc[(\"Link\", \"battery\")]\n        ds = ds.drop(index=(\"Link\", \"battery\"))\n    if \"stations\" in ds.index.get_level_values(2):\n        ds = ds.drop(\"stations\", level=2)\n    if \"load shedding\" in ds.index.get_level_values(2):\n        ds = ds.drop(\"load shedding\", level=2)\n    ds = ds.groupby(level=[1, 2]).sum()\n    ds.index = pd.MultiIndex.from_tuples(\n        [\n            (prov, n.carriers.loc[carrier, \"nice_name\"] if carrier in n.carriers.index else carrier)\n            for prov, carrier in ds.index\n        ],\n        names=[\"province\", \"nice_name\"],\n    )\n    cap_by_prov_tech = ds.unstack(level=-1).fillna(0)\n    cap_by_prov_tech = cap_by_prov_tech.abs() / PLOT_CAP_UNITS\n\n    if \"Battery Discharger\" in cap_by_prov_tech.columns:\n        cap_by_prov_tech = cap_by_prov_tech.drop(columns=\"Battery Discharger\")\n    if \"AC\" in cap_by_prov_tech.columns:\n        cap_by_prov_tech = cap_by_prov_tech.drop(columns=\"AC\")\n    # Only keep columns in attached_carriers if provided\n    if attached_carriers is not None:\n        # Ensure nice_name mapping for attached_carriers\n        attached_nice_names = [\n            n.carriers.loc[c, \"nice_name\"] if c in n.carriers.index else c\n            for c in attached_carriers\n        ]\n        cap_by_prov_tech = cap_by_prov_tech[\n            [c for c in cap_by_prov_tech.columns if c in attached_nice_names]\n        ]\n\n    # Merge peak load and capacity\n    df_plot = cap_by_prov_tech.copy()\n    df_plot[\"Peak Load\"] = peak_load\n\n    # Bar columns: exclude Peak Load, only keep nonzero\n    bar_cols = [c for c in df_plot.columns if c != \"Peak Load\"]\n    bar_cols = [c for c in bar_cols if df_plot[c].sum() &gt; 0]\n    color_list = [\n        n.carriers.set_index(\"nice_name\").color.get(tech, \"lightgrey\") for tech in bar_cols\n    ]\n    return df_plot, bar_cols, color_list\n</code></pre>"},{"location":"reference/plot_statistics/#plot_statistics.set_link_output_capacities","title":"<code>set_link_output_capacities(n, carriers)</code>","text":"<p>Set link capacity to output and not input. PyPSA uses input link capacities but typically want to report output capacities (e.g MWel)</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>Network</code> <p>The PyPSA network instance.</p> required <code>carriers</code> <code>list</code> <p>List of carrier names to adjust.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: the original link capacities.</p> Source code in <code>workflow/scripts/plot_statistics.py</code> <pre><code>def set_link_output_capacities(n: pypsa.Network, carriers: list) -&gt; pd.DataFrame:\n    \"\"\"Set link capacity to output and not input.\n    PyPSA uses input link capacities but typically want to report output\n    capacities (e.g MWel)\n\n    Args:\n        n (pypsa.Network): The PyPSA network instance.\n        carriers (list): List of carrier names to adjust.\n\n    Returns:\n        pd.DataFrame: the original link capacities.\n    \"\"\"\n    # Temporarily save original link capacities\n    original_p_nom_opt = n.links.p_nom_opt.copy()\n\n    # For links where bus1 is AC, multiply capacity by efficiency\n    # coefficient to get AC side capacity\n    ac_links = n.links[n.links.bus1.map(n.buses.carrier).isin(carriers)].index\n    n.links.loc[ac_links, \"p_nom_opt\"] *= n.links.loc[ac_links, \"efficiency\"]\n    n.links.loc[ac_links, \"p_nom\"] *= n.links.loc[ac_links, \"efficiency\"]\n\n    # ignore lossy link dummies\n    pseudo_links = n.links.query(\"Link.str.contains('reversed') &amp; capital_cost ==0 \").index\n    n.links.loc[pseudo_links, \"p_nom_opt\"] = 0\n\n    return original_p_nom_opt\n</code></pre>"},{"location":"reference/plot_summary_all/","title":"Plot summary all","text":"<p>Plots energy and cost summaries for solved networks. This script collects functions that plot across planning horizons.</p>"},{"location":"reference/plot_summary_all/#plot_summary_all.plot_capacity_factors","title":"<code>plot_capacity_factors(file_list, config, techs, fig_name=None, ax=None)</code>","text":"<p>Plot evolution of capacity factors for the given technologies</p> <p>Parameters:</p> Name Type Description Default <code>file_list</code> <code>list</code> <p>the input csvs from make_summary</p> required <code>config</code> <code>dict</code> <p>the configuration for plotting (snakemake.config[\"plotting\"])</p> required <code>techs</code> <code>list</code> <p>the technologies to plot</p> required <code>fig_name</code> <code>PathLike</code> <p>the figure name. Defaults to None.</p> <code>None</code> <code>ax</code> <code>Axes</code> <p>the axes to plot on. Defaults to None.</p> <code>None</code> Source code in <code>workflow/scripts/plot_summary_all.py</code> <pre><code>def plot_capacity_factors(\n    file_list: list, config: dict, techs: list, fig_name=None, ax: object = None\n):\n    \"\"\"Plot evolution of capacity factors for the given technologies\n\n    Args:\n        file_list (list): the input csvs from make_summary\n        config (dict): the configuration for plotting (snakemake.config[\"plotting\"])\n        techs (list): the technologies to plot\n        fig_name (os.PathLike, optional): the figure name. Defaults to None.\n        ax (matplotlib.axes.Axes, optional): the axes to plot on. Defaults to None.\n    \"\"\"\n\n    capfacs_df = pd.DataFrame()\n    for results_file in file_list:\n        df_year = pd.read_csv(results_file, index_col=list(range(2)), header=[1]).T\n        capfacs_df = pd.concat([df_year, capfacs_df])\n\n    if (\"links\", \"battery\") in capfacs_df.columns:\n        capfacs_df.loc[:, (\"links\", \"battery charger\")] = capfacs_df.loc[:, (\"links\", \"battery\")]\n        capfacs_df.drop(columns=(\"links\", \"battery\"), inplace=True)\n\n    capfacs_df = capfacs_df.droplevel(0, axis=1).fillna(0)\n    capfacs_df.sort_index(axis=0, inplace=True)\n\n    invalid = [t for t in techs if t not in capfacs_df.columns]\n    logger.warning(f\"Technologies {invalid} not found in capacity factors data. Skipping them.\")\n    valid_techs = [t for t in techs if t in capfacs_df.columns]\n    capfacs_df = capfacs_df[valid_techs]\n\n    if not ax:\n        fig, ax = plt.subplots()\n    else:\n        fig = ax.get_figure()\n    fig.set_size_inches((12, 8))\n\n    colors = pd.Series(config[\"tech_colors\"], index=capfacs_df.columns)\n    # missing color may have had nice name, else NAN default\n    nice_name_colors = pd.Series(\n        config[\"tech_colors\"], index=capfacs_df.columns.map(config[\"nice_names\"])\n    ).dropna()\n    colors = colors.fillna(nice_name_colors).fillna(NAN_COLOR)\n\n    capfacs_df.plot(\n        ax=ax,\n        kind=\"line\",\n        color=colors,\n        linewidth=3,\n        marker=\"o\",\n    )\n    ax.set_ylim([0, capfacs_df.max().max() * 1.1])\n    ax.set_ylabel(\"capacity factor\")\n    ax.set_xlabel(\"\")\n    ax.grid(axis=\"y\")\n\n    handles, labels = ax.get_legend_handles_labels()\n    handles.reverse()\n    labels.reverse()\n    ax.legend(handles, labels, ncol=1, bbox_to_anchor=[1, 1], loc=\"upper left\")\n    fig.tight_layout()\n\n    if fig_name is not None:\n        fig.savefig(fig_name, transparent=False)\n</code></pre>"},{"location":"reference/plot_summary_all/#plot_summary_all.plot_co2_prices","title":"<code>plot_co2_prices(co2_prices, config, fig_name=None)</code>","text":"<p>Plot the CO2 prices Args:     co2_prices (dict): the CO2 prices per year (from the config)     config (dict): the plotting configuration     fig_name (os.PathLike, optional): the figure name. Defaults to None.</p> Source code in <code>workflow/scripts/plot_summary_all.py</code> <pre><code>def plot_co2_prices(co2_prices: dict, config: dict, fig_name=None):\n    \"\"\"Plot the CO2 prices\n    Args:\n        co2_prices (dict): the CO2 prices per year (from the config)\n        config (dict): the plotting configuration\n        fig_name (os.PathLike, optional): the figure name. Defaults to None.\n    \"\"\"\n    fig, ax = plt.subplots()\n    fig.set_size_inches((12, 8))\n\n    ax.plot(\n        co2_prices.keys(),\n        np.abs(list(co2_prices.values())),\n        marker=\"o\",\n        color=\"black\",\n        lw=2,\n    )\n    ax.set_ylabel(\"CO2 price\")\n    ax.set_xlabel(\"Year\")\n    ax.plot(co2_prices.keys(), co2_prices.values(), marker=\"o\", color=\"black\", lw=2)\n\n    fig.tight_layout()\n    if fig_name is not None:\n        fig.savefig(fig_name, transparent=config[\"transparent\"])\n</code></pre>"},{"location":"reference/plot_summary_all/#plot_summary_all.plot_co2_shadow_price","title":"<code>plot_co2_shadow_price(file_list, config, fig_name=None)</code>","text":"<p>Plot the co2 price</p> <p>Parameters:</p> Name Type Description Default <code>file_list</code> <code>list</code> <p>the input csvs from make_summaries</p> required <code>config</code> <code>dict</code> <p>the snakemake configuration</p> required <code>fig_name</code> <code>PathLike</code> <p>the figure name. Defaults to None.</p> <code>None</code> Source code in <code>workflow/scripts/plot_summary_all.py</code> <pre><code>def plot_co2_shadow_price(file_list: list, config: dict, fig_name=None):\n    \"\"\"Plot the co2 price\n\n    Args:\n        file_list (list): the input csvs from make_summaries\n        config (dict): the snakemake configuration\n        fig_name (os.PathLike, optional): the figure name. Defaults to None.\n    \"\"\"\n    co2_prices = {}\n    co2_budget = {}\n    for i, results_file in enumerate(file_list):\n        df_metrics = pd.read_csv(results_file, index_col=list(range(1)), header=[1])\n        co2_prices.update(dict(df_metrics.loc[\"co2_shadow\"]))\n        co2_budget.update(dict(df_metrics.loc[\"co2_budget\"]))\n\n    fig, ax = plt.subplots()\n    fig.set_size_inches((12, 8))\n\n    ax.plot(\n        co2_prices.keys(),\n        np.abs(list(co2_prices.values())),\n        marker=\"o\",\n        color=\"black\",\n        lw=2,\n    )\n    ax.set_ylabel(\"CO2 Shadow price\")\n    ax.set_xlabel(\"Year\")\n\n    ax2 = ax.twinx()\n    ax2.plot(\n        co2_budget.keys(),\n        [v / PLOT_CO2_UNITS for v in co2_budget.values()],\n        marker=\"D\",\n        color=\"blue\",\n        lw=2,\n    )\n    ax2.set_ylabel(f\"CO2 Budget [{PLOT_CO2_LABEL}]\", color=\"blue\")\n    ax2.tick_params(axis=\"y\", colors=\"blue\")\n\n    fig.tight_layout()\n\n    if fig_name is not None:\n        fig.savefig(fig_name, transparent=config[\"transparent\"])\n</code></pre>"},{"location":"reference/plot_summary_all/#plot_summary_all.plot_electricty_heat_balance","title":"<code>plot_electricty_heat_balance(file_list, config, fig_dir=None, plot_heat=True)</code>","text":"<p>Plot the energy production and consumption</p> <p>Parameters:</p> Name Type Description Default <code>file_list</code> <code>list</code> <p>the input csvs  from make_dirs([year/supply_energy.csv])</p> required <code>config</code> <code>dict</code> <p>the configuration for plotting (snamkemake.config[\"plotting\"])</p> required <code>fig_dir</code> <code>PathLike</code> <p>the figure name. Defaults to None.</p> <code>None</code> <code>plot_heat</code> <code>bool</code> <p>plot heat balances. Defaults to True.</p> <code>True</code> Source code in <code>workflow/scripts/plot_summary_all.py</code> <pre><code>def plot_electricty_heat_balance(\n    file_list: list[os.PathLike], config: dict, fig_dir=None, plot_heat=True\n):\n    \"\"\"Plot the energy production and consumption\n\n    Args:\n        file_list (list): the input csvs  from make_dirs([year/supply_energy.csv])\n        config (dict): the configuration for plotting (snamkemake.config[\"plotting\"])\n        fig_dir (os.PathLike, optional): the figure name. Defaults to None.\n        plot_heat (bool, optional): plot heat balances. Defaults to True.\n    \"\"\"\n    elec_df = pd.DataFrame()\n    heat_df = pd.DataFrame()\n\n    for results_file in file_list:\n        balance_df = pd.read_csv(results_file, index_col=list(range(2)), header=[1])\n        elec = balance_df.loc[\"AC\"].copy()\n        elec.set_index(elec.columns[0], inplace=True)\n        elec.rename(index={\"-\": \"electric load\"}, inplace=True)\n        elec.index.rename(\"carrier\", inplace=True)\n        # this groups subgroups of the same carrier. For example, baseyar hydro = link from dams\n        # but new hydro is generator from province\n        elec = elec.groupby(elec.index).sum()\n        to_drop = elec.index[\n            elec.max(axis=1).abs() &lt; config[\"energy_threshold\"] / PLOT_SUPPLY_UNITS\n        ]\n        elec.loc[\"Other\"] = elec.loc[to_drop].sum(axis=0)\n        elec.drop(to_drop, inplace=True)\n        elec_df = pd.concat([elec, elec_df], axis=1)\n\n        if plot_heat:\n            heat = balance_df.loc[\"heat\"].copy()\n            heat.set_index(heat.columns[0], inplace=True)\n            heat.rename(index={\"-\": \"heat load\"}, inplace=True)\n            heat.index.rename(\"carrier\", inplace=True)\n            heat = heat.groupby(heat.index).sum()\n            to_drop = heat.index[\n                heat.max(axis=1).abs() &lt; config[\"energy_threshold\"] / PLOT_SUPPLY_UNITS\n            ]\n            heat.loc[\"Other\"] = heat.loc[to_drop].sum(axis=0)\n            heat.drop(to_drop, inplace=True)\n            heat_df = pd.concat([heat, heat_df], axis=1)\n        else:\n            heat_df = pd.DataFrame()\n\n    elec_df.fillna(0, inplace=True)\n    elec_df.sort_index(axis=1, inplace=True, ascending=True)\n    elec_df = elec_df / PLOT_SUPPLY_UNITS\n\n    heat_df.fillna(0, inplace=True)\n    heat_df.sort_index(axis=1, inplace=True, ascending=True)\n    heat_df = heat_df / PLOT_SUPPLY_UNITS\n\n    # # split into consumption and generation\n    el_gen = elec_df.where(elec_df &gt;= 0).dropna(axis=0, how=\"all\").fillna(0)\n    el_con = elec_df.where(elec_df &lt; 0).dropna(axis=0, how=\"all\").fillna(0)\n    heat_gen = heat_df.where(heat_df &gt; 0).dropna(axis=0, how=\"all\").fillna(0)\n    heat_con = heat_df.where(heat_df &lt; 0).dropna(axis=0, how=\"all\").fillna(0)\n\n    # group identical values\n    el_con = el_con.groupby(el_con.index).sum()\n    el_gen = el_gen.groupby(el_gen.index).sum()\n    heat_con = heat_con.groupby(heat_con.index).sum()\n    heat_gen = heat_gen.groupby(heat_gen.index).sum()\n\n    logger.info(f\"Total energy of {round(elec_df.sum()[0])} TWh/a\")\n\n    # ===========        electricity =================\n    fig, ax = plt.subplots()\n    fig.set_size_inches((12, 8))\n\n    preferred_order = pd.Index(config[\"preferred_order\"])\n    for df in [el_gen, el_con]:\n        new_index = preferred_order.intersection(df.index).append(\n            df.index.difference(preferred_order)\n        )\n        logger.info(\n            f\"Missing technologies in preferred order: {df.index.difference(preferred_order)}\"\n        )\n\n        colors = pd.DataFrame(\n            new_index.map(config[\"tech_colors\"]), index=new_index, columns=[\"color\"]\n        )\n        colors.fillna(NAN_COLOR, inplace=True)\n        df.loc[new_index].T.plot(\n            kind=\"bar\",\n            ax=ax,\n            stacked=True,\n            color=colors[\"color\"],\n        )\n\n        handles, labels = ax.get_legend_handles_labels()\n        handles.reverse()\n        labels.reverse()\n\n    ax.set_ylim([el_con.sum(axis=0).min() * 1.1, el_gen.sum(axis=0).max() * 1.1])\n    ax.set_ylabel(\"Energy [TWh/a]\")\n    ax.set_xlabel(\"\")\n    ax.grid(axis=\"y\")\n    ax.legend(\n        handles,\n        [l.title() for l in labels],\n        ncol=1,\n        bbox_to_anchor=[1, 1],\n        loc=\"upper left\",\n    )\n\n    if config.get(\"add_bar_labels\", False):\n        label_stacked_bars(ax, len(el_gen.columns))\n\n    fig.tight_layout()\n\n    if fig_dir is not None:\n        fig.savefig(os.path.join(fig_dir, \"elec_balance.png\"), transparent=config[\"transparent\"])\n\n    # =================     heat     =================\n    fig, ax = plt.subplots()\n    fig.set_size_inches((12, 8))\n\n    for df in [heat_gen, heat_con]:\n        if not plot_heat:\n            break\n\n        preferred_order = pd.Index(config[\"preferred_order\"])\n        new_index = preferred_order.intersection(df.index).append(\n            df.index.difference(preferred_order)\n        )\n        colors = pd.DataFrame(\n            new_index.map(config[\"tech_colors\"]), index=new_index, columns=[\"color\"]\n        )\n        colors.fillna(NAN_COLOR, inplace=True)\n        df.loc[new_index].T.plot(\n            kind=\"bar\",\n            ax=ax,\n            stacked=True,\n            color=colors[\"color\"],\n        )\n\n        handles, labels = ax.get_legend_handles_labels()\n        handles.reverse()\n        labels.reverse()\n\n    if plot_heat:\n        ax.set_ylim([heat_con.sum(axis=0).min() * 1.1, heat_gen.sum(axis=0).max() * 1.1])\n        ax.set_ylabel(\"Energy [TWh/a]\")\n        ax.set_xlabel(\"\")\n        ax.grid(axis=\"y\")\n        ax.legend(handles, labels, ncol=1, bbox_to_anchor=[1, 1], loc=\"upper left\")\n        fig.tight_layout()\n\n        if fig_dir is not None:\n            fig.savefig(\n                os.path.join(fig_dir, \"heat_balance.png\"),\n                transparent=config[\"transparent\"],\n            )\n</code></pre>"},{"location":"reference/plot_summary_all/#plot_summary_all.plot_energy","title":"<code>plot_energy(file_list, config, fig_name=None)</code>","text":"<p>Plot the energy production and consumption</p> <p>Parameters:</p> Name Type Description Default <code>file_list</code> <code>list</code> <p>the input csvs</p> required <code>config</code> <code>dict</code> <p>the configuration for plotting (snamkemake.config[\"plotting\"])</p> required <code>fig_name</code> <code>PathLike</code> <p>the figure name. Defaults to None.</p> <code>None</code> Source code in <code>workflow/scripts/plot_summary_all.py</code> <pre><code>def plot_energy(file_list: list, config: dict, fig_name=None):\n    \"\"\"Plot the energy production and consumption\n\n    Args:\n        file_list (list): the input csvs\n        config (dict): the configuration for plotting (snamkemake.config[\"plotting\"])\n        fig_name (os.PathLike, optional): the figure name. Defaults to None.\n    \"\"\"\n    energy_df = pd.DataFrame()\n    for results_file in file_list:\n        en_df = pd.read_csv(results_file, index_col=list(range(2)), header=[1])\n        df_ = en_df.groupby(en_df.index.get_level_values(1)).sum()\n        # do this here so aggregate costs of small items only for that year\n        # convert MWh to TWh\n        df_ = df_ / PLOT_SUPPLY_UNITS\n        df_ = df_.groupby(df_.index.map(rename_techs)).sum()\n        to_drop = df_.index[df_.max(axis=1) &lt; config[\"energy_threshold\"] / PLOT_SUPPLY_UNITS]\n        df_.loc[\"Other\"] = df_.loc[to_drop].sum(axis=0)\n        df_ = df_.drop(to_drop)\n\n        energy_df = pd.concat([df_, energy_df], axis=1)\n    energy_df.fillna(0, inplace=True)\n    energy_df.sort_index(axis=1, inplace=True)\n\n    logger.info(f\"Total energy of {round(energy_df.sum()[0])} {PLOT_SUPPLY_LABEL}/a\")\n    preferred_order = pd.Index(config[\"preferred_order\"])\n    new_index = preferred_order.intersection(energy_df.index).append(\n        energy_df.index.difference(preferred_order)\n    )\n    new_columns = energy_df.columns.sort_values()\n\n    fig, ax = plt.subplots()\n    fig.set_size_inches((12, 8))\n\n    logger.debug(energy_df.loc[new_index, new_columns])\n\n    energy_df.loc[new_index, new_columns].T.plot(\n        kind=\"bar\",\n        ax=ax,\n        stacked=True,\n        color=[config[\"tech_colors\"][i] for i in new_index],\n    )\n\n    handles, labels = ax.get_legend_handles_labels()\n\n    handles.reverse()\n    labels.reverse()\n\n    ax.set_ylim([0, energy_df.sum(axis=0).max() * 1.1])\n    ax.set_ylabel(f\"Energy [{PLOT_SUPPLY_LABEL}/a]\")\n    ax.set_xlabel(\"\")\n    ax.grid(axis=\"y\")\n    ax.legend(handles, labels, ncol=1, bbox_to_anchor=[1, 1], loc=\"upper left\")\n    fig.tight_layout()\n\n    if fig_name is not None:\n        fig.savefig(fig_name, transparent=config[\"transparent\"])\n</code></pre>"},{"location":"reference/plot_summary_all/#plot_summary_all.plot_expanded_capacities","title":"<code>plot_expanded_capacities(file_list, config, plot_heat=False, plot_h2=True, fig_name=None)</code>","text":"<p>Plot the expanded capacities</p> <p>Parameters:</p> Name Type Description Default <code>file_list</code> <code>list</code> <p>the input csvs from make_summary</p> required <code>config</code> <code>dict</code> <p>the configuration for plotting (snakemake.config[\"plotting\"])</p> required <code>fig_name</code> <code>PathLike</code> <p>the figure name. Defaults to None.</p> <code>None</code> <code>plot_heat</code> <code>bool</code> <p>plot heat capacities. Defaults to True.</p> <code>False</code> <code>plot_h2</code> <code>bool</code> <p>plot hydrogen capacities. Defaults to True.</p> <code>True</code> Source code in <code>workflow/scripts/plot_summary_all.py</code> <pre><code>def plot_expanded_capacities(\n    file_list: list, config: dict, plot_heat=False, plot_h2=True, fig_name=None\n):\n    \"\"\"Plot the expanded capacities\n\n    Args:\n        file_list (list): the input csvs from make_summary\n        config (dict): the configuration for plotting (snakemake.config[\"plotting\"])\n        fig_name (os.PathLike, optional): the figure name. Defaults to None.\n        plot_heat (bool, optional): plot heat capacities. Defaults to True.\n        plot_h2 (bool, optional): plot hydrogen capacities. Defaults to True.\n    \"\"\"\n\n    fig, axes = plot_pathway_capacities(file_list, config, plot_heat, plot_h2, fig_name=None)\n    for i, ax in enumerate(axes.flat):\n        ylabel = ax.get_ylabel()\n        if \"Installed\" in ylabel:\n            ax.set_ylabel(ylabel.replace(\"Installed\", \"Additional\"))\n\n    if fig_name is not None:\n        fig.savefig(fig_name, transparent=config[\"transparent\"])\n</code></pre>"},{"location":"reference/plot_summary_all/#plot_summary_all.plot_investments","title":"<code>plot_investments(file_list, config, fig_name=None, ax=None)</code>","text":"<p>Plot investment analysis (placeholder function).</p> <p>Parameters:</p> Name Type Description Default <code>file_list</code> <code>list</code> <p>List of input files for investment analysis</p> required <code>config</code> <code>dict</code> <p>Configuration dictionary with plotting parameters</p> required <code>fig_name</code> <p>Output figure name. Defaults to None.</p> <code>None</code> <code>ax</code> <code>object</code> <p>Matplotlib axes object. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>None</code> <p>Currently a placeholder function</p> Source code in <code>workflow/scripts/plot_summary_all.py</code> <pre><code>def plot_investments(file_list: list, config: dict, fig_name=None, ax: object = None):\n    \"\"\"Plot investment analysis (placeholder function).\n\n    Args:\n        file_list (list): List of input files for investment analysis\n        config (dict): Configuration dictionary with plotting parameters\n        fig_name: Output figure name. Defaults to None.\n        ax (object, optional): Matplotlib axes object. Defaults to None.\n\n    Returns:\n        None: Currently a placeholder function\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/plot_summary_all/#plot_summary_all.plot_pathway_capacities","title":"<code>plot_pathway_capacities(file_list, config, plot_heat=True, plot_h2=True, fig_name=None)</code>","text":"<p>Plot the capacities</p> <p>Parameters:</p> Name Type Description Default <code>file_list</code> <code>list</code> <p>the input csvs from make_summary</p> required <code>config</code> <code>dict</code> <p>the configuration for plotting (snakemake.config[\"plotting\"])</p> required <code>fig_name</code> <code>PathLike</code> <p>the figure name. Defaults to None.</p> <code>None</code> <code>plot_heat</code> <code>bool</code> <p>plot heat capacities. Defaults to True.</p> <code>True</code> <code>plot_h2</code> <code>bool</code> <p>plot hydrogen capacities. Defaults to True.</p> <code>True</code> Source code in <code>workflow/scripts/plot_summary_all.py</code> <pre><code>def plot_pathway_capacities(\n    file_list: list, config: dict, plot_heat=True, plot_h2=True, fig_name=None\n):\n    \"\"\"Plot the capacities\n\n    Args:\n        file_list (list): the input csvs from make_summary\n        config (dict): the configuration for plotting (snakemake.config[\"plotting\"])\n        fig_name (os.PathLike, optional): the figure name. Defaults to None.\n        plot_heat (bool, optional): plot heat capacities. Defaults to True.\n        plot_h2 (bool, optional): plot hydrogen capacities. Defaults to True.\n    \"\"\"\n\n    caps_heat, caps_h2, caps_ac, caps_stores = (\n        pd.DataFrame(),\n        pd.DataFrame(),\n        pd.DataFrame(),\n        pd.DataFrame(),\n    )\n    # loop over each year result\n    for results_file in file_list:\n        cap_df = pd.read_csv(results_file, index_col=list(range(4)), header=[1, 2])\n        # format table\n        cap_df.index.names = [\"component\", \"carrier\", \"bus_carrier\", \"end_carrier\"]\n        year = cap_df.columns.get_level_values(0)[0]\n        cap_df = cap_df.droplevel(0, axis=1).rename(columns={\"Unnamed: 4_level_1\": year})\n        cap_df /= PLOT_CAP_UNITS\n        if \"Load Shedding\" in cap_df.index.get_level_values(\"carrier\"):\n            cap_df.drop(\"Load Shedding\", level=\"carrier\", inplace=True)\n\n        # get stores relevant for reporting according to config, use later\n        stores = (\n            cap_df[\n                (cap_df.index.get_level_values(0) == \"Store\")\n                &amp; (cap_df.index.get_level_values(1).isin(config[\"capacity_tracking\"][\"stores\"]))\n            ]\n            .groupby(level=1)\n            .sum()\n        )\n\n        # drop stores from cap df\n        cap_df.drop(cap_df[cap_df.index.get_level_values(0) == \"Store\"].index, inplace=True)\n        # drop charger/dischargers for stores\n        cap_df.drop(\n            cap_df[\n                (cap_df.index.get_level_values(0) == \"Link\")\n                &amp; (cap_df.index.get_level_values(1).isin(config[\"capacity_tracking\"][\"drop_links\"]))\n            ].index,\n            inplace=True,\n        )\n\n        # select AC (important for links) and group\n        cap_ac = cap_df.reset_index().query(\n            \"bus_carrier == 'AC' | carrier =='AC' | end_carrier =='AC'\"\n        )\n        cap_ac = cap_ac.groupby(\"carrier\").sum()[year]\n\n        cap_h2 = pd.DataFrame()\n        if plot_h2:\n            cap_h2 = cap_df.reset_index().query(\n                \"bus_carrier == 'H2' | carrier =='H2' | end_carrier =='H2'\"\n            )\n            cap_h2 = cap_h2.groupby(\"carrier\").sum()[year]\n            if caps_h2.empty:\n                caps_h2 = cap_h2\n            else:\n                caps_h2 = pd.concat([caps_h2, cap_h2], axis=1).fillna(0)\n        if plot_heat:\n            # TODO issue for CHP in case of several end buses. Bus2 will not be caught\n            cap_heat = cap_df.reset_index().query(\n                \"bus_carrier == 'heat' | carrier =='heat' | end_carrier =='heat'\"\n            )\n            cap_heat = cap_heat.groupby(\"carrier\").sum()[year]\n            if caps_heat.empty:\n                caps_heat = cap_heat\n            else:\n                caps_heat = pd.concat([caps_heat, cap_h2], axis=1).fillna(0)\n\n        caps_stores = pd.concat([stores, caps_stores], axis=1).fillna(0)\n        if caps_ac.empty:\n            caps_ac = cap_ac\n        else:\n            caps_ac = pd.concat([cap_ac, caps_ac], axis=1).fillna(0)\n\n    fig, axes = plt.subplots(2, 2)\n    fig.set_size_inches((14, 15))\n\n    for i, capacity_df in enumerate([caps_ac, caps_heat, caps_stores, caps_h2]):\n        if capacity_df.empty:\n            continue\n        if isinstance(capacity_df, pd.Series):\n            capacity_df = capacity_df.to_frame()\n        k, j = divmod(i, 2)\n        ax = axes[k, j]\n        preferred_order = pd.Index(config[\"preferred_order\"])\n        new_index = preferred_order.intersection(capacity_df.index).append(\n            capacity_df.index.difference(preferred_order)\n        )\n        new_columns = capacity_df.columns.sort_values()\n\n        logger.debug(capacity_df.loc[new_index, new_columns])\n\n        capacity_df.loc[new_index, new_columns].T.plot(\n            kind=\"bar\",\n            ax=ax,\n            stacked=True,\n            color=[config[\"tech_colors\"][i] for i in new_index],\n        )\n\n        handles, labels = ax.get_legend_handles_labels()\n\n        handles.reverse()\n        labels.reverse()\n\n        if capacity_df.index.difference(caps_stores.index).empty:\n            ax.set_ylabel(f\"Installed Storage Capacity [{PLOT_CAP_LABEL}h]\")\n        else:\n            ax.set_ylabel(f\"Installed Capacity [{PLOT_CAP_LABEL}]\")\n        ax.set_ylim([0, capacity_df.sum(axis=0).max() * 1.1])\n        ax.set_xlabel(\"\")\n        ax.grid(axis=\"y\")\n        # ax.yaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: f\"{x:.1e}\"))\n        ax.legend(handles, labels, ncol=2, bbox_to_anchor=(0.5, -0.15), loc=\"upper center\")\n    fig.tight_layout()\n    fig.subplots_adjust(wspace=0.42)\n\n    if fig_name is not None:\n        fig.savefig(fig_name, transparent=config[\"transparent\"])\n\n    return fig, axes\n</code></pre>"},{"location":"reference/plot_summary_all/#plot_summary_all.plot_pathway_co2","title":"<code>plot_pathway_co2(file_list, config, fig_name=None)</code>","text":"<p>Plot the CO2 pathway balance and totals</p> <p>Parameters:</p> Name Type Description Default <code>file_list</code> <code>list</code> <p>the input csvs</p> required <code>config</code> <code>dict</code> <p>the plotting configuration</p> required <code>fig_name</code> <code>_type_</code> <p>description. Defaults to None.</p> <code>None</code> Source code in <code>workflow/scripts/plot_summary_all.py</code> <pre><code>def plot_pathway_co2(file_list: list, config: dict, fig_name=None):\n    \"\"\"Plot the CO2 pathway balance and totals\n\n    Args:\n        file_list (list): the input csvs\n        config (dict): the plotting configuration\n        fig_name (_type_, optional): _description_. Defaults to None.\n    \"\"\"\n\n    co2_balance_df = pd.DataFrame()\n    for results_file in file_list:\n        df_year = pd.read_csv(results_file, index_col=list(range(1)), header=[1]).T\n        co2_balance_df = pd.concat([df_year, co2_balance_df])\n\n    co2_balance_df.sort_index(axis=0, inplace=True)\n\n    fig, ax = plt.subplots()\n    bar_width = 0.6\n    colors = co2_balance_df.T.index.map(config[\"tech_colors\"]).values\n    co2_balance_df = co2_balance_df / PLOT_CO2_UNITS\n    co2_balance_df.plot(\n        kind=\"bar\",\n        stacked=True,\n        width=bar_width,\n        color=pd.Series(colors).fillna(NAN_COLOR),\n        ax=ax,\n    )\n    bar_centers = np.unique([patch.get_x() + bar_width / 2 for patch in ax.patches])\n    ax.plot(\n        bar_centers,\n        co2_balance_df.sum(axis=1).values,\n        color=\"black\",\n        marker=\"D\",\n        markersize=10,\n        lw=3,\n        label=\"Total\",\n    )\n    ax.set_ylabel(PLOT_CO2_LABEL)\n\n    handles, labels = ax.get_legend_handles_labels()\n    ax.legend(handles, labels, ncol=1, bbox_to_anchor=[1, 1], loc=\"upper left\")\n    ax.set_ylim([co2_balance_df.min().min() * 1.1, co2_balance_df.sum(axis=1).max() * 1.1])\n    fig.tight_layout()\n    if fig_name is not None:\n        fig.savefig(fig_name, transparent=config[\"transparent\"])\n</code></pre>"},{"location":"reference/plot_summary_all/#plot_summary_all.plot_pathway_costs","title":"<code>plot_pathway_costs(file_list, config, social_discount_rate=0.0, fig_name=None)</code>","text":"<p>Plot the costs</p> <p>Parameters:</p> Name Type Description Default <code>file_list</code> <code>list</code> <p>the input csvs from make_summary</p> required <code>config</code> <code>dict</code> <p>the configuration for plotting (snakemake.config[\"plotting\"])</p> required <code>social_discount_rate</code> <code>float</code> <p>the social discount rate (0.02). Defaults to 0.0.</p> <code>0.0</code> <code>fig_name</code> <code>PathLike</code> <p>the figure name. Defaults to None.</p> <code>None</code> Source code in <code>workflow/scripts/plot_summary_all.py</code> <pre><code>def plot_pathway_costs(\n    file_list: list,\n    config: dict,\n    social_discount_rate=0.0,\n    fig_name: os.PathLike = None,\n):\n    \"\"\"Plot the costs\n\n    Args:\n        file_list (list): the input csvs from make_summary\n        config (dict): the configuration for plotting (snakemake.config[\"plotting\"])\n        social_discount_rate (float, optional): the social discount rate (0.02). Defaults to 0.0.\n        fig_name (os.PathLike, optional): the figure name. Defaults to None.\n    \"\"\"\n    # all years in one df\n    df = pd.DataFrame()\n    for results_file in file_list:\n        cost_df = pd.read_csv(results_file, index_col=list(range(3)), header=[1])\n        df_ = cost_df.groupby(cost_df.index.get_level_values(2)).sum()\n        # do this here so aggregate costs of small items only for that year\n        df_ = df_ * COST_UNIT / PLOT_COST_UNITS\n        df_ = df_.groupby(df_.index.map(rename_techs)).sum()\n        to_drop = df_.index[df_.max(axis=1) &lt; config[\"costs_threshold\"] / PLOT_COST_UNITS]\n        df_.loc[\"Other\"] = df_.loc[to_drop].sum(axis=0)\n        df_ = df_.drop(to_drop)\n        df = pd.concat([df_, df], axis=1)\n\n    df.fillna(0, inplace=True)\n    df.rename(columns={int(y): y for y in df.columns}, inplace=True)\n    df.sort_index(axis=1, inplace=True, ascending=True)\n\n    # apply social discount rate\n    if social_discount_rate &gt; 0:\n        base_year = min([int(y) for y in df.columns])\n        df = df.apply(\n            lambda x: x / (1 + social_discount_rate) ** (int(x.name) - base_year),\n            axis=0,\n        )\n    elif social_discount_rate &lt; 0:\n        raise ValueError(\"Social discount rate must be positive\")\n\n    preferred_order = pd.Index(config[\"preferred_order\"])\n    new_index = preferred_order.intersection(df.index).append(df.index.difference(preferred_order))\n    logger.info(f\"Missing technologies in preferred order: {df.index.difference(preferred_order)}\")\n    fig, ax = plt.subplots()\n    fig.set_size_inches((12, 8))\n\n    df.loc[new_index].T.plot(\n        kind=\"bar\",\n        ax=ax,\n        stacked=True,\n        color=[config[\"tech_colors\"][i] for i in new_index],\n    )\n\n    handles, labels = ax.get_legend_handles_labels()\n\n    ax.set_ylim([0, df.sum(axis=0).max() * 1.1])\n    ax.set_ylabel(\"System Cost [EUR billion per year]\")\n    ax.set_xlabel(\"\")\n    ax.grid(axis=\"y\")\n    # TODO fix this - doesnt work with non-constant interval\n    ax.annotate(\n        f\"Total cost in bn Eur: {df.sum().sum() * 5:.2f}\",\n        xy=(0.75, 0.9),\n        color=\"darkgray\",\n        xycoords=\"axes fraction\",\n        ha=\"right\",\n        va=\"top\",\n    )\n\n    ax.legend(\n        handles,\n        [l.title() for l in labels],\n        ncol=1,\n        bbox_to_anchor=[1, 1],\n        loc=\"upper left\",\n    )\n\n    fig.tight_layout()\n\n    if fig_name is not None:\n        fig.savefig(fig_name, transparent=config[\"transparent\"])\n</code></pre>"},{"location":"reference/plot_summary_all/#plot_summary_all.plot_prices","title":"<code>plot_prices(file_list, config, fig_name=None, absolute=False, ax=None, unit='\u20ac/MWh', **kwargs)</code>","text":"<p>Plot the prices</p> <p>Parameters:</p> Name Type Description Default <code>file_list</code> <code>list</code> <p>the input csvs from make_summary</p> required <code>config</code> <code>dict</code> <p>the configuration for plotting (snakemake.config[\"plotting\"])</p> required <code>fig_name</code> <code>PathLike</code> <p>the figure name. Defaults to None.</p> <code>None</code> <code>absolute</code> <code>bool</code> <p>plot absolute prices. Defaults to False.</p> <code>False</code> <code>ax</code> <code>Axes</code> <p>the axes to plot on. Defaults to None.</p> <code>None</code> <code>unit</code> <code>str</code> <p>the unit of the prices. Defaults to \"\u20ac/MWh\".</p> <code>'\u20ac/MWh'</code> Source code in <code>workflow/scripts/plot_summary_all.py</code> <pre><code>def plot_prices(\n    file_list: list,\n    config: dict,\n    fig_name=None,\n    absolute=False,\n    ax: object = None,\n    unit=\"\u20ac/MWh\",\n    **kwargs,\n):\n    \"\"\"Plot the prices\n\n    Args:\n        file_list (list): the input csvs from make_summary\n        config (dict): the configuration for plotting (snakemake.config[\"plotting\"])\n        fig_name (os.PathLike, optional): the figure name. Defaults to None.\n        absolute (bool, optional): plot absolute prices. Defaults to False.\n        ax (matplotlib.axes.Axes, optional): the axes to plot on. Defaults to None.\n        unit (str, optional): the unit of the prices. Defaults to \"\u20ac/MWh\".\n    \"\"\"\n    prices_df = pd.DataFrame()\n    for results_file in file_list:\n        df_year = pd.read_csv(results_file, index_col=list(range(1)), header=[1]).T\n\n        prices_df = pd.concat([df_year, prices_df])\n    prices_df.sort_index(axis=0, inplace=True)\n    if not ax:\n        fig, ax = plt.subplots()\n    else:\n        fig = ax.get_figure()\n    fig.set_size_inches((12, 8))\n\n    colors = config[\"tech_colors\"]\n\n    if absolute:\n        prices_df = prices_df.abs()\n\n    defaults = {\"lw\": 3, \"marker\": \"o\", \"markersize\": 5, \"alpha\": 0.8}\n    if \"linewidth\" in kwargs:\n        kwargs[\"lw\"] = kwargs.pop(\"linewidth\")\n    defaults.update(kwargs)\n    prices_df.plot(\n        ax=ax,\n        kind=\"line\",\n        color=[colors[k] if k in colors else \"k\" for k in prices_df.columns],\n        **defaults,\n    )\n    min_ = prices_df.min().min()\n    if np.sign(min_) &lt; 0:\n        min_ *= 1.1\n    else:\n        min_ *= 0.9\n    ax.set_ylim([min_, prices_df.max().max() * 1.1])\n    ax.set_ylabel(f\"Prices [{unit}]\")\n    ax.set_xlabel(\"\")\n    ax.grid(axis=\"y\")\n\n    handles, labels = ax.get_legend_handles_labels()\n\n    handles.reverse()\n    labels.reverse()\n    ax.legend(handles, labels, ncol=1, bbox_to_anchor=[1, 1], loc=\"upper left\")\n    fig.tight_layout()\n\n    if fig_name is not None:\n        fig.savefig(fig_name, transparent=False)\n</code></pre>"},{"location":"reference/plot_summary_all/#plot_summary_all.rename_techs","title":"<code>rename_techs(label)</code>","text":"<p>Rename techs into grouped categories</p> <p>Parameters:</p> Name Type Description Default <code>label</code> <code>Index | iterable</code> <p>the index techs to rename</p> required <p>Returns:     pd.Index | iterable: the renamed index / iterable</p> Source code in <code>workflow/scripts/plot_summary_all.py</code> <pre><code>def rename_techs(label: pd.Index) -&gt; pd.Index:\n    \"\"\"Rename techs into grouped categories\n\n    Args:\n        label (pd.Index | iterable): the index techs to rename\n    Returns:\n        pd.Index | iterable: the renamed index / iterable\n    \"\"\"\n    prefix_to_remove = [\n        \"central \",\n        \"decentral \",\n    ]\n\n    rename_if_contains_dict = {\n        \"water tanks\": \"hot water storage\",\n        \"H2\": \"H2\",\n        \"coal cc\": \"CC\",\n    }\n    rename_if_contains = [\"gas\", \"coal\"]\n    rename = {\n        \"solar\": \"solar PV\",\n        \"Sabatier\": \"methanation\",\n        \"offwind\": \"offshore wind\",\n        \"onwind\": \"onshore wind\",\n        \"ror\": \"hydroelectricity\",\n        \"hydro\": \"hydroelectricity\",\n        \"PHS\": \"pumped hydro storage\",\n        \"hydro_inflow\": \"hydroelectricity\",\n        \"stations\": \"hydroelectricity\",\n        \"AC\": \"transmission lines\",\n        \"CO2 capture\": \"biomass carbon capture\",\n        \"CC\": \"coal carbon capture\",\n        \"battery\": \"battery\",\n    }\n\n    for ptr in prefix_to_remove:\n        if label[: len(ptr)] == ptr:\n            label = label[len(ptr) :]\n\n    for old, new in rename_if_contains_dict.items():\n        if old in label:\n            label = new\n\n    for rif in rename_if_contains:\n        if rif in label:\n            label = rif\n\n    for old, new in rename.items():\n        if old == label:\n            label = new\n    return label\n</code></pre>"},{"location":"reference/plot_summary_all/#plot_summary_all.write_data","title":"<code>write_data(data_paths, outp_dir)</code>","text":"<p>Write some selected data</p> <p>Parameters:</p> Name Type Description Default <code>data_paths</code> <code>dict</code> <p>the paths to the summary data (different per year and type)</p> required <code>outp_dir</code> <code>PathLike</code> <p>target file (summary dir)</p> required Source code in <code>workflow/scripts/plot_summary_all.py</code> <pre><code>def write_data(data_paths: dict, outp_dir: os.PathLike):\n    \"\"\"Write some selected data\n\n    Args:\n        data_paths (dict): the paths to the summary data (different per year and type)\n        outp_dir (os.PathLike): target file (summary dir)\n    \"\"\"\n    # make a summary of the co2 prices\n    co2_prices = {}\n    co2_budget = {}\n    for i, results_file in enumerate(data_paths[\"co2_price\"]):\n        df_metrics = pd.read_csv(results_file, index_col=list(range(1)), header=[1])\n        co2_prices.update(dict(df_metrics.loc[\"co2_shadow\"]))\n        co2_budget.update(dict(df_metrics.loc[\"co2_budget\"]))\n    years = list(co2_budget.keys())\n    co2_df = pd.DataFrame(\n        {\n            \"Year\": years,\n            \"CO2 Budget\": [co2_budget[year] for year in years],\n            \"CO2 Shadow Price\": [co2_prices[year] * -1 for year in years],\n        }\n    )\n    outp_p = os.path.join(outp_dir, \"co2_prices.csv\")\n    co2_df.to_csv(outp_p, index=False)\n\n    df = pd.DataFrame()\n    for results_file in data_paths[\"costs\"]:\n        cost_df = pd.read_csv(results_file, index_col=list(range(3)), header=[1])\n        df_ = cost_df.groupby(level=[1, 2]).sum()\n        df_ = df_ * COST_UNIT / PLOT_COST_UNITS\n        df = pd.concat([df_, df], axis=1)\n    df.to_csv(os.path.join(outp_dir, \"pathway_costs_not_discounted.csv\"))\n\n    prices_df = pd.DataFrame()\n    for results_file in data_paths[\"weighted_prices\"]:\n        df_year = pd.read_csv(results_file, index_col=list(range(1)), header=[1]).T\n\n        prices_df = pd.concat([df_year, prices_df])\n    prices_df.to_csv(os.path.join(outp_dir, \"weighted_prices.csv\"))\n</code></pre>"},{"location":"reference/plot_time_series/","title":"Plot time series","text":"<p>Plot time series data for energy system analysis.</p> <p>This module creates time series plots including load profiles, generation patterns, storage operations, and other temporal data visualizations for the PyPSA-China model.</p>"},{"location":"reference/plot_time_series/#plot_time_series.plot_energy_balance","title":"<code>plot_energy_balance(n, plot_config, bus_carrier='AC', start_date='2060-03-31 21:00', end_date='2060-04-06 12:00:00', aggregate_fossil=False, add_load_line=True, add_reserves=False, ax=None)</code>","text":"<p>Plot the electricity balance of the network for the given time range</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>Network</code> <p>the network</p> required <code>plot_config</code> <code>dict</code> <p>the plotting config (snakemake.config[\"plotting\"])</p> required <code>bus_carrier</code> <code>str</code> <p>the carrier for the energy_balance op. Defaults to \"AC\".</p> <code>'AC'</code> <code>start_date</code> <code>str</code> <p>the range to plot. Defaults to \"2060-03-31 21:00\".</p> <code>'2060-03-31 21:00'</code> <code>end_date</code> <code>str</code> <p>the range to plot. Defaults to \"2060-04-06 12:00:00\".</p> <code>'2060-04-06 12:00:00'</code> <code>aggregate_fossil</code> <code>bool</code> <p>whether to aggregate fossil fuels. Defaults to False.</p> <code>False</code> <code>add_load_line</code> <code>bool</code> <p>add a dashed line for the load. Defaults to True.</p> <code>True</code> Source code in <code>workflow/scripts/plot_time_series.py</code> <pre><code>def plot_energy_balance(\n    n: pypsa.Network,\n    plot_config: dict,\n    bus_carrier=\"AC\",\n    start_date=\"2060-03-31 21:00\",\n    end_date=\"2060-04-06 12:00:00\",\n    aggregate_fossil=False,\n    add_load_line=True,\n    add_reserves=False,\n    ax: plt.Axes = None,\n):\n    \"\"\"Plot the electricity balance of the network for the given time range\n\n    Args:\n        n (pypsa.Network): the network\n        plot_config (dict): the plotting config (snakemake.config[\"plotting\"])\n        bus_carrier (str, optional): the carrier for the energy_balance op. Defaults to \"AC\".\n        start_date (str, optional): the range to plot. Defaults to \"2060-03-31 21:00\".\n        end_date (str, optional): the range to plot. Defaults to \"2060-04-06 12:00:00\".\n        aggregate_fossil (bool, optional): whether to aggregate fossil fuels. Defaults to False.\n        add_load_line (bool, optional): add a dashed line for the load. Defaults to True.\n    \"\"\"\n    if not ax:\n        fig, ax = plt.subplots(figsize=(16, 8))\n    else:\n        fig = ax.get_figure()\n\n    p = (\n        n.statistics.energy_balance(aggregate_time=False, bus_carrier=bus_carrier)\n        .dropna(how=\"all\")\n        .groupby(\"carrier\")\n        .sum()\n        .div(PLOT_SUPPLY_UNITS)\n        # .drop(\"-\")\n        .T\n    )\n\n    p = p.loc[start_date:end_date]\n    p.rename(columns={\"-\": \"Load\", \"AC\": \"transmission losses\"}, inplace=True)\n\n    # aggreg fossil\n    if aggregate_fossil:\n        coal = p.filter(regex=\"[C|c]oal\")\n        p.drop(columns=coal.columns, inplace=True)\n        p[\"Coal\"] = coal.sum(axis=1)\n        gas = p.filter(regex=\"[G|g]as\")\n        p.drop(columns=gas.columns, inplace=True)\n        p[\"Gas\"] = gas.sum(axis=1)\n\n    extra_c = {\n        \"Load\": plot_config[\"tech_colors\"][\"electric load\"],\n        \"transmission losses\": plot_config[\"tech_colors\"][\"transmission losses\"],\n    }\n    nice_tech_colors = make_nice_tech_colors(plot_config[\"tech_colors\"], plot_config[\"nice_names\"])\n    color_series = get_stat_colors(n, nice_tech_colors, extra_colors=extra_c)\n    # colors &amp; names part 1\n    p.rename(plot_config[\"nice_names\"], inplace=True)\n    p.rename(columns={k: k.title() for k in p.columns}, inplace=True)\n    p.rename(columns={\"Heat\": \"Heat Load\"}, inplace=True)\n    color_series.index = color_series.index.str.strip()\n    # split into supply and wothdrawal\n    supply = p.where(p &gt; 0).dropna(axis=1, how=\"all\")\n    charge = p.where(p &lt; 0).dropna(how=\"all\", axis=1)\n\n    # fix names and order\n\n    charge.rename(columns={\"Battery Storage\": \"Battery\"}, inplace=True)\n    supply.rename(columns={\"Battery Discharger\": \"Battery\"}, inplace=True)\n    color_series = color_series[charge.columns.union(supply.columns)]\n    color_series.rename(\n        {\"Battery Discharger\": \"Battery\", \"Battery Storage\": \"Battery\"},\n        inplace=True,\n    )\n    # Deduplicate color_series\n    color_series = color_series[~color_series.index.duplicated(keep=\"first\")]\n\n    preferred_order = plot_config[\"preferred_order\"]\n    plot_order = (\n        supply.columns.intersection(preferred_order).to_list()\n        + supply.columns.difference(preferred_order).to_list()\n    )\n\n    plot_order_charge = [name for name in preferred_order if name in charge.columns] + [\n        name for name in charge.columns if name not in preferred_order\n    ]\n\n    supply = supply.reindex(columns=plot_order)\n    charge = charge.reindex(columns=plot_order_charge)\n    if not charge.empty:\n        charge.plot.area(ax=ax, linewidth=0, color=color_series.loc[charge.columns])\n\n    supply.plot.area(\n        ax=ax,\n        linewidth=0,\n        color=color_series.loc[supply.columns].values,\n    )\n    if add_load_line:\n        # charge.rename(columns={\"Heat Load\": \"Load\"}, inplace=True)\n        charge[\"load_pos\"] = charge[\"Load\"] * -1\n        charge[\"load_pos\"].plot(linewidth=2, color=\"black\", label=\"Load\", ax=ax, linestyle=\"--\")\n        charge.drop(columns=\"load_pos\", inplace=True)\n\n    ax.legend(ncol=1, loc=\"center left\", bbox_to_anchor=(1, 0.5), frameon=False, fontsize=16)\n    ax.set_ylabel(PLOT_SUPPLY_LABEL)\n    ax.set_ylim(charge.sum(axis=1).min() * 1.07, supply.sum(axis=1).max() * 1.07)\n    ax.grid(axis=\"y\")\n    ax.set_xlim(supply.index.min(), supply.index.max())\n\n    fig.tight_layout()\n\n    return ax\n</code></pre>"},{"location":"reference/plot_time_series/#plot_time_series.plot_load_duration_curve","title":"<code>plot_load_duration_curve(network, carrier='AC', ax=None)</code>","text":"<p>Plot the load duration curve for the given carrier</p> <p>Parameters:</p> Name Type Description Default <code>network</code> <code>Network</code> <p>the pypasa network object</p> required <code>carrier</code> <code>str</code> <p>the load carrier, defaults to AC</p> <code>'AC'</code> <code>ax</code> <code>Axes</code> <p>figure axes, if none fig will be created. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Axes</code> <p>plt.Axes: the plotting axes</p> Source code in <code>workflow/scripts/plot_time_series.py</code> <pre><code>def plot_load_duration_curve(\n    network: pypsa.Network, carrier: str = \"AC\", ax: plt.Axes = None\n) -&gt; plt.Axes:\n    \"\"\"Plot the load duration curve for the given carrier\n\n    Args:\n        network (pypsa.Network): the pypasa network object\n        carrier (str, optional): the load carrier, defaults to AC\n        ax (plt.Axes, optional): figure axes, if none fig will be created. Defaults to None.\n\n    Returns:\n        plt.Axes: the plotting axes\n    \"\"\"\n\n    if not ax:\n        fig, ax = plt.subplots(figsize=(16, 8))\n    else:\n        fig = ax.get_figure()\n\n    load = network.statistics.withdrawal(\n        groupby=get_location_and_carrier,\n        aggregate_time=False,\n        bus_carrier=carrier,\n        comps=\"Load\",\n    ).sum()\n    load_curve = load.sort_values(ascending=False) / PLOT_CAP_UNITS\n    load_curve.reset_index(drop=True).plot(ax=ax, lw=3)\n    ax.set_ylabel(f\"Load [{PLOT_CAP_LABEL}]\")\n    ax.set_xlabel(\"Hours\")\n\n    fig.tight_layout()\n    return ax\n</code></pre>"},{"location":"reference/plot_time_series/#plot_time_series.plot_price_duration_by_node","title":"<code>plot_price_duration_by_node(network, carrier='AC', logy=True, y_lower=0.001, fig_shape=(8, 4))</code>","text":"<p>Plot the price duration curve for the given carrier by node Args:     network (pypsa.Network): the pypsa network object     carrier (str, optional): the load carrier, defaults to AC (bus suffix)     logy (bool, optional): use log scale for y axis, defaults to True     y_lower (float, optional): lower limit for y axis, defaults to 1e-3     fig_shape (tuple, optional): shape of the figure, defaults to (8, 4)</p> <p>Returns:</p> Type Description <code>Axes</code> <p>plt.Axes: the plotting axes</p> <p>Raises:     ValueError: if the figure shape is too small for the number of regions</p> Source code in <code>workflow/scripts/plot_time_series.py</code> <pre><code>def plot_price_duration_by_node(\n    network: pypsa.Network,\n    carrier: str = \"AC\",\n    logy=True,\n    y_lower=1e-3,\n    fig_shape=(8, 4),\n) -&gt; plt.Axes:\n    \"\"\"Plot the price duration curve for the given carrier by node\n    Args:\n        network (pypsa.Network): the pypsa network object\n        carrier (str, optional): the load carrier, defaults to AC (bus suffix)\n        logy (bool, optional): use log scale for y axis, defaults to True\n        y_lower (float, optional): lower limit for y axis, defaults to 1e-3\n        fig_shape (tuple, optional): shape of the figure, defaults to (8, 4)\n\n    Returns:\n        plt.Axes: the plotting axes\n    Raises:\n        ValueError: if the figure shape is too small for the number of regions\n    \"\"\"\n\n    if carrier == \"AC\":\n        suffix = \"\"\n    else:\n        suffix = f\" {carrier}\"\n\n    nodal_prices = network.buses_t.marginal_price[pd.Index(PROV_NAMES) + suffix]\n\n    if fig_shape[0] * fig_shape[1] &lt; len(nodal_prices.columns):\n        raise ValueError(\n            f\"Figure shape {fig_shape} is too small for {len(nodal_prices.columns)} regions. \"\n            + \"Please increase the number of subplots.\"\n        )\n    fig, axes = plt.subplots(fig_shape[0], fig_shape[1], sharex=True, sharey=True, figsize=(12, 12))\n\n    # region by region sorting of prices\n    for i, region in enumerate(nodal_prices.columns):\n        reg_pr = nodal_prices[region]\n        reg_pr.sort_values(ascending=False).reset_index(drop=True).plot(\n            ax=axes[i // 4, i % fig_shape[1]], label=region\n        )\n        axes[i // 4, i % fig_shape[1]].set_title(region, fontsize=10)\n        if logy:\n            axes[i // 4, i % fig_shape[1]].semilogy()\n        if y_lower:\n            axes[i // 4, i % fig_shape[1]].set_ylim(y_lower, reg_pr.max() * 1.2)\n        elif reg_pr.min() &gt; 1e-5 and not logy:\n            axes[i // 4, i % fig_shape[1]].set_ylim(0, reg_pr.max() * 1.2)\n    fig.tight_layout(h_pad=0.2, w_pad=0.2)\n    for ax in axes.flat:\n        # Remove all x-tick labels except the largest value\n        xticks = ax.get_xticks()\n        if len(xticks) &gt; 0:\n            ax.set_xticks([xticks[0], xticks[-1]])\n            ax.set_xticklabels([f\"{xticks[0]:.0f}\", f\"{xticks[-1]:.0f}\"])\n\n    return ax\n</code></pre>"},{"location":"reference/plot_time_series/#plot_time_series.plot_price_duration_curve","title":"<code>plot_price_duration_curve(network, carrier='AC', ax=None, figsize=(8, 8))</code>","text":"<p>Plot the price duration curve for the given carrier</p> <p>Parameters:</p> Name Type Description Default <code>network</code> <code>Network</code> <p>the pypasa network object</p> required <code>carrier</code> <code>str</code> <p>the load carrier, defaults to AC</p> <code>'AC'</code> <code>ax</code> <code>Axes</code> <p>Axes to plot on, if none fig will be created. Defaults to None.</p> <code>None</code> <code>figsize</code> <code>tuple</code> <p>size of the figure (if no ax given), defaults to (8, 8)</p> <code>(8, 8)</code> <p>Returns:</p> Type Description <code>Axes</code> <p>plt.Axes: the plotting axes</p> Source code in <code>workflow/scripts/plot_time_series.py</code> <pre><code>def plot_price_duration_curve(\n    network: pypsa.Network, carrier=\"AC\", ax: plt.Axes = None, figsize=(8, 8)\n) -&gt; plt.Axes:\n    \"\"\"Plot the price duration curve for the given carrier\n\n    Args:\n        network (pypsa.Network): the pypasa network object\n        carrier (str, optional): the load carrier, defaults to AC\n        ax (plt.Axes, optional): Axes to plot on, if none fig will be created. Defaults to None.\n        figsize (tuple, optional): size of the figure (if no ax given), defaults to (8, 8)\n\n    Returns:\n        plt.Axes: the plotting axes\n    \"\"\"\n    if not ax:\n        fig, ax = plt.subplots(figsize=figsize)\n    else:\n        fig = ax.get_figure()\n\n    ntwk_el_price = (\n        -1\n        * network.statistics.revenue(bus_carrier=carrier, aggregate_time=False, comps=\"Load\")\n        / network.statistics.withdrawal(bus_carrier=carrier, aggregate_time=False, comps=\"Load\")\n    ).T\n    ntwk_el_price.rename(columns={\"-\": \"Load\"}, inplace=True)\n    ntwk_el_price.Load.sort_values(ascending=False).reset_index(drop=True).plot(\n        title=\"Price Duration Curve\", ax=ax, lw=2\n    )\n    fig.tight_layout()\n\n    return ax\n</code></pre>"},{"location":"reference/plot_time_series/#plot_time_series.plot_price_heatmap","title":"<code>plot_price_heatmap(network, carrier='AC', log_values=False, color_map='viridis', time_range=None, ax=None)</code>","text":"<p>Plot the price heat map (region vs time) for the given carrier</p> <p>Parameters:</p> Name Type Description Default <code>network</code> <code>Network</code> <p>the pypsa network object</p> required <code>carrier</code> <code>str</code> <p>the carrier for which to get the price. Defaults to \"AC\".</p> <code>'AC'</code> <code>log_values</code> <code>bool</code> <p>whether to use log scale for the prices. Defaults to False.</p> <code>False</code> <code>color_map</code> <code>str</code> <p>the color map to use. Defaults to \"viridis\".</p> <code>'viridis'</code> <code>time_range</code> <code>Index</code> <p>the time range to plot. Defaults to None (all times).</p> <code>None</code> <code>ax</code> <code>Axes</code> <p>the plotting axis. Defaults to None (new fig).</p> <code>None</code> <p>Returns:</p> Type Description <code>Axes</code> <p>plt.Axes: the axes for plotting</p> Source code in <code>workflow/scripts/plot_time_series.py</code> <pre><code>def plot_price_heatmap(\n    network: pypsa.Network,\n    carrier=\"AC\",\n    log_values=False,\n    color_map=\"viridis\",\n    time_range: pd.Index = None,\n    ax: plt.Axes = None,\n) -&gt; plt.Axes:\n    \"\"\"Plot the price heat map (region vs time) for the given carrier\n\n    Args:\n        network (pypsa.Network): the pypsa network object\n        carrier (str, optional): the carrier for which to get the price. Defaults to \"AC\".\n        log_values (bool, optional): whether to use log scale for the prices. Defaults to False.\n        color_map (str, optional): the color map to use. Defaults to \"viridis\".\n        time_range (pd.Index, optional): the time range to plot. Defaults to None (all times).\n        ax (plt.Axes, optional): the plotting axis. Defaults to None (new fig).\n\n    Returns:\n        plt.Axes: the axes for plotting\n    \"\"\"\n\n    if not ax:\n        fig, ax = plt.subplots(figsize=(20, 8))\n    else:\n        fig = ax.get_figure()\n\n    carrier_buses = network.buses.carrier[network.buses.carrier == carrier].index.values\n    nodal_prices = network.buses_t.marginal_price[carrier_buses]\n\n    if time_range is not None:\n        # Filter nodal_prices by the given time range\n        nodal_prices = nodal_prices.loc[time_range]\n    # Normalize nodal_prices with log transformation\n    if log_values:\n        # Avoid log(0) by clipping values to a minimum of 0.1\n        normalized_prices = np.log(nodal_prices.clip(lower=0.1))\n        label = \"Log-Transformed Price [\u20ac/MWh]\"\n    else:\n        normalized_prices = nodal_prices\n        label = \"Price [\u20ac/MWh]\"\n    # Create a heatmap of normalized nodal_prices\n    plot_index = normalized_prices.index.strftime(\"%m-%d %H:%M\").to_list()\n    normalized_prices.index = plot_index\n    sns.heatmap(\n        normalized_prices.T,\n        cmap=color_map,\n        cbar_kws={\"label\": label},\n        ax=ax,\n    )\n\n    # Customize the plot\n    if log_values:\n        ax.set_title(\"Heatmap of Log-Transformed Nodal Prices\")\n    else:\n        ax.set_title(\"Heatmap of Nodal Prices\")\n\n    ax.set_xlabel(\"Time\")\n    ax.set_ylabel(\"Nodes\")\n    fig.tight_layout()\n\n    return ax\n</code></pre>"},{"location":"reference/plot_time_series/#plot_time_series.plot_regional_load_durations","title":"<code>plot_regional_load_durations(network, carrier='AC', ax=None, cmap='plasma')</code>","text":"<p>Plot the load duration curve for the given carrier stacked by region</p> <p>Parameters:</p> Name Type Description Default <code>network</code> <code>Network</code> <p>the pypasa network object</p> required <code>carrier</code> <code>str</code> <p>the load carrier, defaults to AC</p> <code>'AC'</code> <code>ax</code> <code>Axes</code> <p>axes to plot on, if none fig will be created. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Axes</code> <p>plt.Axes: the plotting axes</p> Source code in <code>workflow/scripts/plot_time_series.py</code> <pre><code>def plot_regional_load_durations(\n    network: pypsa.Network, carrier=\"AC\", ax=None, cmap=\"plasma\"\n) -&gt; plt.Axes:\n    \"\"\"Plot the load duration curve for the given carrier stacked by region\n\n    Args:\n        network (pypsa.Network): the pypasa network object\n        carrier (str, optional): the load carrier, defaults to AC\n        ax (plt.Axes, optional): axes to plot on, if none fig will be created. Defaults to None.\n\n    Returns:\n        plt.Axes: the plotting axes\n    \"\"\"\n    if not ax:\n        fig, ax = plt.subplots(figsize=(10, 8))\n    else:\n        fig = ax.get_figure()\n\n    loads_all = network.statistics.withdrawal(\n        groupby=get_location_and_carrier,\n        aggregate_time=False,\n        bus_carrier=carrier,\n        comps=\"Load\",\n    ).sum()\n    load_curve_all = loads_all.sort_values(ascending=False) / PLOT_CAP_UNITS\n    regio = network.statistics.withdrawal(\n        groupby=get_location_and_carrier,\n        aggregate_time=False,\n        bus_carrier=carrier,\n        comps=\"Load\",\n    )\n    regio = regio.droplevel(1).T\n    load_curve_regio = regio.loc[load_curve_all.index] / PLOT_CAP_UNITS\n    load_curve_regio.reset_index(drop=True).plot.area(\n        ax=ax, stacked=True, cmap=cmap, legend=True, lw=3\n    )\n    ax.set_ylabel(f\"Load [{PLOT_CAP_LABEL}]\")\n    ax.set_xlabel(\"Hours\")\n    ax.legend(\n        ncol=3,\n        loc=\"upper center\",\n        bbox_to_anchor=(0.5, -0.15),\n        fontsize=\"small\",\n        title_fontsize=\"small\",\n        fancybox=True,\n        shadow=True,\n    )\n\n    fig.tight_layout()\n\n    return ax\n</code></pre>"},{"location":"reference/plot_time_series/#plot_time_series.plot_residual_load_duration_curve","title":"<code>plot_residual_load_duration_curve(network, ax=None, vre_techs=['Onshore Wind', 'Offshore Wind', 'Solar'])</code>","text":"<p>Plot the residual load duration curve for the given carrier</p> <p>Parameters:</p> Name Type Description Default <code>network</code> <code>Network</code> <p>the pypasa network object</p> required <code>ax</code> <code>Axes</code> <p>Axes to plot on, if none fig will be created. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Axes</code> <p>plt.Axes: the plotting axes</p> Source code in <code>workflow/scripts/plot_time_series.py</code> <pre><code>def plot_residual_load_duration_curve(\n    network, ax: plt.Axes = None, vre_techs=[\"Onshore Wind\", \"Offshore Wind\", \"Solar\"]\n) -&gt; plt.Axes:\n    \"\"\"Plot the residual load duration curve for the given carrier\n\n    Args:\n        network (pypsa.Network): the pypasa network object\n        ax (plt.Axes, optional): Axes to plot on, if none fig will be created. Defaults to None.\n\n    Returns:\n        plt.Axes: the plotting axes\n    \"\"\"\n    CARRIER = \"AC\"\n    if not ax:\n        fig, ax = plt.subplots(figsize=(16, 8))\n    load = network.statistics.withdrawal(\n        groupby=get_location_and_carrier,\n        aggregate_time=False,\n        bus_carrier=CARRIER,\n        comps=\"Load\",\n    ).sum()\n\n    vre_supply = (\n        network.statistics.supply(\n            groupby=get_location_and_carrier,\n            aggregate_time=False,\n            bus_carrier=CARRIER,\n            comps=\"Generator\",\n        )\n        .groupby(level=1)\n        .sum()\n        .loc[vre_techs]\n        .sum()\n    )\n\n    residual = (load - vre_supply).sort_values(ascending=False) / PLOT_CAP_UNITS\n    residual.reset_index(drop=True).plot(ax=ax, lw=3)\n    ax.set_ylabel(f\"Residual Load [{PLOT_CAP_LABEL}]\")\n    ax.set_xlabel(\"Hours\")\n\n    return ax\n</code></pre>"},{"location":"reference/plot_time_series/#plot_time_series.plot_vre_heatmap","title":"<code>plot_vre_heatmap(n, config, color_map='magma', log_values=True, time_range=None)</code>","text":"<p>Plot the VRE generation per hour and day as a heatmap</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>Network</code> <p>the pypsa network object</p> required <code>time_range</code> <code>Index</code> <p>the time range to plot. Defaults to None (all times).</p> <code>None</code> <code>log_values</code> <code>bool</code> <p>whether to use log scale for the values. Defaults to True.</p> <code>True</code> <code>config</code> <code>dict</code> <p>the run config (snakemake.config).</p> required Source code in <code>workflow/scripts/plot_time_series.py</code> <pre><code>def plot_vre_heatmap(\n    n: pypsa.Network,\n    config: dict,\n    color_map=\"magma\",\n    log_values=True,\n    time_range: pd.Index = None,\n):\n    \"\"\"Plot the VRE generation per hour and day as a heatmap\n\n    Args:\n        n (pypsa.Network): the pypsa network object\n        time_range (pd.Index, optional): the time range to plot. Defaults to None (all times).\n        log_values (bool, optional): whether to use log scale for the values. Defaults to True.\n        config (dict, optional): the run config (snakemake.config).\n\n    \"\"\"\n\n    vres = config[\"Techs\"].get(\n        \"non_dispatchable\", [\"Offshore Wind\", \"Onshore Wind\", \"Solar\", \"Solar Residential\"]\n    )\n    vre_avail = (\n        n.statistics.supply(\n            comps=\"Generator\",\n            aggregate_time=False,\n            bus_carrier=\"AC\",\n            nice_names=False,\n            groupby=[\"location\", \"carrier\"],\n        )\n        .query(\"carrier in @vres\")\n        .T.fillna(0)\n    )\n\n    if time_range is not None:\n        vre_avail = vre_avail.loc[time_range]\n\n    for tech in vres[::-1]:\n        tech_avail = vre_avail.T.query(\"carrier == @tech\")\n        tech_avail.index = tech_avail.index.droplevel(1)\n        tech_avail = tech_avail.T\n        tech_avail.index = tech_avail.index.strftime(\"%m-%d %H:%M\")\n        if log_values:\n            # Avoid log(0) by clipping values to a minimum of 10\n            tech_avail = np.log(tech_avail.clip(lower=10))\n        fig, ax = plt.subplots()\n        sns.heatmap(tech_avail.T, ax=ax, cmap=color_map)\n        ax.set_title(f\"{tech} generation by province\")\n</code></pre>"},{"location":"reference/plot_time_series/#plot_time_series.plot_vre_timemap","title":"<code>plot_vre_timemap(network, color_map='viridis', time_range=None)</code>","text":"<p>Plot the VRE generation per hour and day as a heatmap</p> <p>Parameters:</p> Name Type Description Default <code>network</code> <code>Network</code> <p>the pypsa network object</p> required <code>color_map</code> <code>str</code> <p>the color map to use. Defaults to \"viridis\".</p> <code>'viridis'</code> <code>time_range</code> <code>Index</code> <p>the time range to plot. Defaults to None (all times).</p> <code>None</code> Source code in <code>workflow/scripts/plot_time_series.py</code> <pre><code>def plot_vre_timemap(\n    network: pypsa.Network,\n    color_map=\"viridis\",\n    time_range: pd.Index = None,\n):\n    \"\"\"Plot the VRE generation per hour and day as a heatmap\n\n    Args:\n        network (pypsa.Network): the pypsa network object\n        color_map (str, optional): the color map to use. Defaults to \"viridis\".\n        time_range (pd.Index, optional): the time range to plot. Defaults to None (all times).\n    \"\"\"\n\n    vres = [\"offwind\", \"onwind\", \"solar\"]\n    vre_avail = (\n        network.statistics.supply(\n            comps=\"Generator\", aggregate_time=False, bus_carrier=\"AC\", nice_names=False\n        )\n        .query(\"carrier in @vres\")\n        .T.fillna(0)\n    )\n    if time_range is not None:\n        vre_avail = vre_avail.loc[time_range]\n\n    vre_avail[\"day\"] = vre_avail.index.strftime(\"%d-%m\")\n    vre_avail[\"hour\"] = vre_avail.index.hour\n\n    for tech in vres:\n        pivot_ = vre_avail.pivot_table(index=\"hour\", columns=\"day\", values=tech)\n        fig, ax = plt.subplots(figsize=(12, 6))\n        sns.heatmap(pivot_.sort_index(ascending=False), cmap=color_map, ax=ax)\n        ax.set_title(f\"{tech} generation by hour and day\")\n\n        fig.tight_layout()\n</code></pre>"},{"location":"reference/prepare_existing_capacities/","title":"Prepare existing capacities","text":"<p>Functions to prepare existing assets for the network</p> <p>SHORT TERM FIX until PowerPlantMatching is implemented - required as split from add_existing_baseyear for remind compat</p>"},{"location":"reference/prepare_existing_capacities/#prepare_existing_capacities.assign_year_bins","title":"<code>assign_year_bins(df, year_bins)</code>","text":"<p>Assign a year bin to the existing capacities according to the config</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame with existing capacities and build years (DateIn)</p> required <code>year_bins</code> <code>list</code> <p>years to bin the existing capacities to</p> required <p>Returns:     pd.DataFrame: DataFrame regridded to the year bins</p> Source code in <code>workflow/scripts/prepare_existing_capacities.py</code> <pre><code>def assign_year_bins(df: pd.DataFrame, year_bins: list) -&gt; pd.DataFrame:\n    \"\"\"\n    Assign a year bin to the existing capacities according to the config\n\n    Args:\n        df (pd.DataFrame): DataFrame with existing capacities and build years (DateIn)\n        year_bins (list): years to bin the existing capacities to\n    Returns:\n        pd.DataFrame: DataFrame regridded to the year bins\n    \"\"\"\n\n    df_ = df.copy()\n    # bin by years (np.digitize)\n    df_[\"grouping_year\"] = np.take(year_bins, np.digitize(df.DateIn, year_bins, right=True))\n    return df_.fillna(0)\n</code></pre>"},{"location":"reference/prepare_existing_capacities/#prepare_existing_capacities.convert_CHP_to_poweronly","title":"<code>convert_CHP_to_poweronly(capacities)</code>","text":"<p>Convert CHP capacities to power-only capacities by removing the heat part</p> <p>Parameters:</p> Name Type Description Default <code>capacities</code> <code>DataFrame</code> <p>DataFrame with existing capacities</p> required <p>Returns:     pd.DataFrame: DataFrame with converted capacities</p> Source code in <code>workflow/scripts/prepare_existing_capacities.py</code> <pre><code>def convert_CHP_to_poweronly(capacities: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Convert CHP capacities to power-only capacities by removing the heat part\n\n    Args:\n        capacities (pd.DataFrame): DataFrame with existing capacities\n    Returns:\n        pd.DataFrame: DataFrame with converted capacities\n    \"\"\"\n    # Convert CHP to power-only by removing the heat part\n    chp_mask = capacities.Tech.str.contains(\"CHP\")\n    capacities.loc[chp_mask, \"Fueltype\"] = (\n        capacities.loc[chp_mask, \"Fueltype\"]\n        .str.replace(\"central coal CHP\", \"coal power plant\")\n        .str.replace(\"central gas CHP\", \"gas CCGT\")\n    )\n    # update the Tech field based on the converted Fueltype\n    capacities.loc[chp_mask, \"Tech\"] = (\n        capacities.loc[chp_mask, \"Fueltype\"]\n        .str.replace(\" CHP\", \"\")\n        .str.replace(\"CHP \", \" \")\n        .str.replace(\"gas \", \"\")\n        .str.replace(\"coal power plant\", \"coal\")\n    )\n    return capacities\n</code></pre>"},{"location":"reference/prepare_existing_capacities/#prepare_existing_capacities.determine_simulation_timespan","title":"<code>determine_simulation_timespan(config, year)</code>","text":"<p>Determine the simulation timespan in years (so the network object is not needed)</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict</code> <p>the snakemake config</p> required <code>year</code> <code>int</code> <p>the year to simulate</p> required <p>Returns:     int: the simulation timespan in years</p> Source code in <code>workflow/scripts/prepare_existing_capacities.py</code> <pre><code>def determine_simulation_timespan(config: dict, year: int) -&gt; int:\n    \"\"\"Determine the simulation timespan in years (so the network object is not needed)\n\n    Args:\n        config (dict): the snakemake config\n        year (int): the year to simulate\n    Returns:\n        int: the simulation timespan in years\n    \"\"\"\n\n    # make snapshots (drop leap days) -&gt; possibly do all the unpacking in the function\n    snapshot_cfg = config[\"snapshots\"]\n    snapshots = make_periodic_snapshots(\n        year=year,\n        freq=snapshot_cfg[\"freq\"],\n        start_day_hour=snapshot_cfg[\"start\"],\n        end_day_hour=snapshot_cfg[\"end\"],\n        bounds=snapshot_cfg[\"bounds\"],\n        # naive local timezone\n        tz=None,\n        end_year=None if not snapshot_cfg[\"end_year_plus1\"] else year + 1,\n    )\n\n    # load costs\n    n_years = config[\"snapshots\"][\"frequency\"] * len(snapshots) / YEAR_HRS\n\n    return n_years\n</code></pre>"},{"location":"reference/prepare_existing_capacities/#prepare_existing_capacities.fix_existing_capacities","title":"<code>fix_existing_capacities(existing_df, costs, year_bins, baseyear)</code>","text":"<p>add/fill missing dateIn, discretize lifetime to grouping year, rename columns drop plants that were retired before the smallest sim timeframe</p> <p>Parameters:</p> Name Type Description Default <code>existing_df</code> <code>DataFrame</code> <p>the existing capacities</p> required <code>costs</code> <code>DataFrame</code> <p>the technoeconomic data</p> required <code>year_bins</code> <code>list</code> <p>the year groups</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: fixed capacities</p> Source code in <code>workflow/scripts/prepare_existing_capacities.py</code> <pre><code>def fix_existing_capacities(\n    existing_df: pd.DataFrame, costs: pd.DataFrame, year_bins: list, baseyear: int\n) -&gt; pd.DataFrame:\n    \"\"\"add/fill missing dateIn, discretize lifetime to grouping year, rename columns\n    drop plants that were retired before the smallest sim timeframe\n\n    Args:\n        existing_df (pd.DataFrame): the existing capacities\n        costs (pd.DataFrame): the technoeconomic data\n        year_bins (list): the year groups\n\n\n    Returns:\n        pd.DataFrame: fixed capacities\n    \"\"\"\n    existing_df.DateIn = existing_df.DateIn.astype(int)\n    # add/fill missing dateIn\n    if \"DateOut\" not in existing_df.columns:\n        existing_df[\"DateOut\"] = np.nan\n\n    # names matching costs split across FuelType and Tech, apply to both. Fillna means no overwrite\n    lifetimes = existing_df.Fueltype.map(costs.lifetime).fillna(\n        existing_df.Tech.map(costs.lifetime)\n    )\n    if lifetimes.isna().any():\n        raise ValueError(\n            f\"Some assets have no lifetime assigned: \\n{lifetimes[lifetimes.isna()]}. \"\n            \"Please check the costs file for the missing lifetimes.\"\n        )\n    existing_df.loc[:, \"DateOut\"] = existing_df.DateOut.fillna(lifetimes) + existing_df.DateIn\n\n    existing_df[\"lifetime\"] = existing_df.DateOut - existing_df[\"grouping_year\"]\n    existing_df.rename(columns={\"cluster_bus\": \"bus\"}, inplace=True)\n\n    phased_out = existing_df[existing_df[\"DateOut\"] &lt; baseyear].index\n    existing_df.drop(phased_out, inplace=True)\n\n    # check the grouping years are appropriate\n    newer_assets = (existing_df.DateIn &gt; max(year_bins)).sum()\n    if newer_assets:\n        raise ValueError(\n            f\"There are {newer_assets} assets with build year \"\n            f\"after last power grouping year {max(year_bins)}. \"\n            \"These assets are dropped and not considered.\"\n            \"Redefine the grouping years to keep them or\"\n            \" remove pre-construction/construction/... states.\"\n        )\n\n    return existing_df\n</code></pre>"},{"location":"reference/prepare_existing_capacities/#prepare_existing_capacities.read_existing_capacities","title":"<code>read_existing_capacities(paths_dict, techs)</code>","text":"<p>Read existing capacities from csv files and format them Args:     paths_dict (dict[str, os.PathLike]): dictionary with paths to the csv files     techs (list): list of technologies to read Returns:     pd.DataFrame: DataFrame with existing capacities</p> Source code in <code>workflow/scripts/prepare_existing_capacities.py</code> <pre><code>def read_existing_capacities(paths_dict: dict[str, os.PathLike], techs: list) -&gt; pd.DataFrame:\n    \"\"\"Read existing capacities from csv files and format them\n    Args:\n        paths_dict (dict[str, os.PathLike]): dictionary with paths to the csv files\n        techs (list): list of technologies to read\n    Returns:\n        pd.DataFrame: DataFrame with existing capacities\n    \"\"\"\n    # TODO fix centralise (make a dict from start?)\n    carrier = {\n        \"coal\": \"coal power plant\",\n        \"CHP coal\": \"central coal CHP\",\n        \"CHP gas\": \"central gas CHP\",\n        \"OCGT\": \"gas OCGT\",\n        \"CCGT\": \"gas CCGT\",\n        \"solar\": \"solar\",\n        \"solar thermal\": \"central solar thermal\",\n        \"onwind\": \"onwind\",\n        \"offwind\": \"offwind\",\n        \"coal boiler\": \"central coal boiler\",\n        \"ground heat pump\": \"central ground-sourced heat pump\",\n        \"nuclear\": \"nuclear\",\n        \"biomass\": \"biomass\",\n    }\n    carrier = {k: v for k, v in carrier.items() if k in techs}\n\n    df_agg = pd.DataFrame()\n    for tech in carrier:\n        df = pd.read_csv(paths_dict[tech], index_col=0).fillna(0.0)\n        df.columns = df.columns.astype(int)\n        df = df.sort_index()\n\n        for year in df.columns:\n            for node in df.index:\n                name = f\"{node}-{tech}-{year}\"\n                capacity = df.loc[node, year]\n                if capacity &gt; 0.0:\n                    df_agg.at[name, \"Fueltype\"] = carrier[tech]\n                    df_agg.at[name, \"Tech\"] = tech\n                    df_agg.at[name, \"Capacity\"] = capacity\n                    df_agg.at[name, \"DateIn\"] = year\n                    df_agg.at[name, \"cluster_bus\"] = node\n\n    return df_agg\n</code></pre>"},{"location":"reference/prepare_network/","title":"Prepare network","text":"<p>Function suite and script to define the network to be solved. Network components are added here. Additional constraints require the linopy model and are added in the solve_network script.</p> <p>These functions are currently only for the overnight mode. Myopic pathway mode contains near         duplicates which need to merged in the future. Idem for solve_network.py</p>"},{"location":"reference/prepare_network/#prepare_network.add_H2","title":"<code>add_H2(network, config, nodes, costs, planning_year)</code>","text":"<p>Add hydrogen infrastructure including production, storage, and transport.</p> <p>Adds hydrogen electrolysis, fuel cells, turbines, storage systems, and optionally hydrogen pipelines for transport and methanation for power-to-gas</p> <p>Parameters:</p> Name Type Description Default <code>network</code> <code>Network</code> <p>PyPSA network object to which H2 comps are added</p> required <code>config</code> <code>dict</code> <p>Configuration with H2 settings (inc geo storage)</p> required <code>nodes</code> <code>Index</code> <p>nodes where hydrogen infrastructure will be installed.</p> required <code>costs</code> <code>DataFrame</code> <p>Cost database containing techno-economic parameters for hydrogen technologies and infrastructure.</p> required <code>planning_year</code> <code>int</code> <p>Planning year for cost selection and temporal alignment.</p> required Source code in <code>workflow/scripts/prepare_network.py</code> <pre><code>def add_H2(\n    network: pypsa.Network, config: dict, nodes: pd.Index, costs: pd.DataFrame, planning_year: int\n):\n    \"\"\"Add hydrogen infrastructure including production, storage, and transport.\n\n    Adds hydrogen electrolysis, fuel cells, turbines, storage systems, and\n    optionally hydrogen pipelines for transport and methanation for power-to-gas\n\n    Args:\n        network (pypsa.Network): PyPSA network object to which H2 comps are added\n        config (dict): Configuration with H2 settings (inc geo storage)\n        nodes (pd.Index): nodes where hydrogen infrastructure will be installed.\n        costs (pd.DataFrame): Cost database containing techno-economic parameters\n            for hydrogen technologies and infrastructure.\n        planning_year (int): Planning year for cost selection and temporal alignment.\n    \"\"\"\n    # TODO, does it make sense?\n    if config.get(\"heat_coupling\", False):\n        network.add(\n            \"Link\",\n            name=nodes + \" H2 Electrolysis\",\n            bus0=nodes,\n            bus1=nodes + \" H2\",\n            bus2=nodes + \" central heat\",\n            p_nom_extendable=True,\n            carrier=\"H2 Electrolysis\",\n            efficiency=costs.at[\"electrolysis\", \"efficiency\"],\n            efficiency2=costs.at[\"electrolysis\", \"efficiency-heat\"],\n            capital_cost=costs.at[\"electrolysis\", \"capital_cost\"],\n            lifetime=costs.at[\"electrolysis\", \"lifetime\"],\n        )\n    else:\n        network.add(\n            \"Link\",\n            name=nodes + \" H2 Electrolysis\",\n            bus0=nodes,\n            bus1=nodes + \" H2\",\n            p_nom_extendable=True,\n            carrier=\"H2 Electrolysis\",\n            efficiency=costs.at[\"electrolysis\", \"efficiency\"],\n            capital_cost=costs.at[\"electrolysis\", \"capital_cost\"],\n            lifetime=costs.at[\"electrolysis\", \"lifetime\"],\n        )\n\n    if \"fuel cell\" in config[\"Techs\"][\"vre_techs\"]:\n        network.add(\n            \"Link\",\n            name=nodes + \" H2 Fuel Cell\",\n            bus0=nodes + \" H2\",\n            bus1=nodes,\n            p_nom_extendable=True,\n            efficiency=costs.at[\"fuel cell\", \"efficiency\"],\n            capital_cost=costs.at[\"fuel cell\", \"efficiency\"]\n            * costs.at[\"fuel cell\", \"capital_cost\"],\n            lifetime=costs.at[\"fuel cell\", \"lifetime\"],\n            carrier=\"H2 fuel cell\",\n        )\n    if \"H2 turbine\" in config[\"Techs\"][\"vre_techs\"]:\n        network.add(\n            \"Link\",\n            name=nodes + \" H2 turbine\",\n            bus0=nodes + \" H2\",\n            bus1=nodes,\n            p_nom_extendable=True,\n            efficiency=costs.at[\"H2 turbine\", \"efficiency\"],\n            capital_cost=costs.at[\"H2 turbine\", \"efficiency\"]\n            * costs.at[\"H2 turbine\", \"capital_cost\"],\n            lifetime=costs.at[\"H2 turbine\", \"lifetime\"],\n            carrier=\"H2 turbine\",\n        )\n\n    H2_under_nodes_ = pd.Index(config[\"H2\"][\"geo_storage_nodes\"])\n    H2_type1_nodes_ = nodes.difference(H2_under_nodes_)\n    H2_under_nodes = H2_under_nodes_.intersection(nodes)\n    H2_type1_nodes = H2_type1_nodes_.intersection(nodes)\n    if not (\n        H2_under_nodes_.shape == H2_under_nodes.shape\n        and H2_type1_nodes_.shape == H2_type1_nodes.shape\n    ):\n        logger.warning(\"Some H2 storage nodes are not in the network buses\")\n\n    network.add(\n        \"Store\",\n        H2_under_nodes + \" H2 Store\",\n        bus=H2_under_nodes + \" H2\",\n        e_nom_extendable=True,\n        e_cyclic=True,\n        capital_cost=costs.at[\"hydrogen storage underground\", \"capital_cost\"],\n        lifetime=costs.at[\"hydrogen storage underground\", \"lifetime\"],\n    )\n\n    # TODO harmonize with remind (add if in techs)\n    network.add(\n        \"Store\",\n        H2_type1_nodes + \" H2 Store\",\n        bus=H2_type1_nodes + \" H2\",\n        e_nom_extendable=True,\n        e_cyclic=True,\n        capital_cost=costs.at[\"hydrogen storage tank type 1 including compressor\", \"capital_cost\"],\n        lifetime=costs.at[\"hydrogen storage tank type 1 including compressor\", \"lifetime\"],\n    )\n    if config[\"add_methanation\"]:\n        cost_year = planning_year\n        network.add(\n            \"Link\",\n            nodes + \" Sabatier\",\n            bus0=nodes + \" H2\",\n            bus1=nodes + \" gas\",\n            carrier=\"Sabatier\",\n            p_nom_extendable=True,\n            efficiency=costs.at[\"methanation\", \"efficiency\"],\n            capital_cost=costs.at[\"methanation\", \"efficiency\"]\n            * costs.at[\"methanation\", \"capital_cost\"]\n            + costs.at[\"direct air capture\", \"capital_cost\"]\n            * costs.at[\"gas\", \"co2_emissions\"]\n            * costs.at[\"methanation\", \"efficiency\"],\n            # TODO fix me\n            lifetime=costs.at[\"methanation\", \"lifetime\"],\n            marginal_cost=(400 - 5 * (int(cost_year) - 2020))\n            * costs.at[\"gas\", \"co2_emissions\"]\n            * costs.at[\"methanation\", \"efficiency\"],\n        )\n\n    if config[\"Techs\"][\"hydrogen_lines\"]:\n        edge_path = config[\"edge_paths\"].get(config[\"scenario\"][\"topology\"], None)\n        if edge_path is None:\n            raise ValueError(f\"No grid found for topology {config['scenario']['topology']}\")\n        else:\n            edges_ = pd.read_csv(\n                edge_path, sep=\",\", header=None, names=[\"bus0\", \"bus1\", \"p_nom\"]\n            ).fillna(0)\n            edges = edges_[edges_[\"bus0\"].isin(nodes) &amp; edges_[\"bus1\"].isin(nodes)]\n            if edges_.shape[0] != edges.shape[0]:\n                logger.warning(\"Some edges are not in the network buses\")\n\n        # fix this to use map with x.y\n        lengths = config[\"lines\"][\"line_length_factor\"] * np.array(\n            [\n                haversine(\n                    [network.buses.at[bus0, \"x\"], network.buses.at[bus0, \"y\"]],\n                    [network.buses.at[bus1, \"x\"], network.buses.at[bus1, \"y\"]],\n                )\n                for bus0, bus1 in edges[[\"bus0\", \"bus1\"]].values\n            ]\n        )\n\n        # TODO harmonize with remind (add if in techs)\n        cc = costs.at[\"H2 (g) pipeline\", \"capital_cost\"] * lengths\n\n        # === h2 pipeline with losses ====\n        # NB this only works if there is an equalising constraint, which is hidden in solve_ntwk\n        network.add(\n            \"Link\",\n            edges[\"bus0\"] + \"-\" + edges[\"bus1\"] + \" H2 pipeline\",\n            suffix=\" positive\",\n            bus0=edges[\"bus0\"].values + \" H2\",\n            bus1=edges[\"bus1\"].values + \" H2\",\n            bus2=edges[\"bus0\"].values,\n            carrier=\"H2 pipeline\",\n            p_nom_extendable=True,\n            p_nom=0,\n            p_nom_min=0,\n            p_min_pu=0,\n            efficiency=config[\"transmission_efficiency\"][\"H2 pipeline\"][\"efficiency_static\"]\n            * config[\"transmission_efficiency\"][\"H2 pipeline\"][\"efficiency_per_1000km\"]\n            ** (lengths / 1000),\n            efficiency2=-config[\"transmission_efficiency\"][\"H2 pipeline\"][\"compression_per_1000km\"]\n            * lengths\n            / 1e3,\n            length=lengths,\n            lifetime=costs.at[\"H2 (g) pipeline\", \"lifetime\"],\n            capital_cost=cc,\n        )\n\n        network.add(\n            \"Link\",\n            edges[\"bus0\"] + \"-\" + edges[\"bus1\"] + \" H2 pipeline\",\n            suffix=\" reversed\",\n            carrier=\"H2 pipeline\",\n            bus0=edges[\"bus1\"].values + \" H2\",\n            bus1=edges[\"bus0\"].values + \" H2\",\n            bus2=edges[\"bus1\"].values,\n            p_nom_extendable=True,\n            p_nom=0,\n            p_nom_min=0,\n            p_min_pu=0,\n            efficiency=config[\"transmission_efficiency\"][\"H2 pipeline\"][\"efficiency_static\"]\n            * config[\"transmission_efficiency\"][\"H2 pipeline\"][\"efficiency_per_1000km\"]\n            ** (lengths / 1000),\n            efficiency2=-config[\"transmission_efficiency\"][\"H2 pipeline\"][\"compression_per_1000km\"]\n            * lengths\n            / 1e3,\n            length=lengths,\n            lifetime=costs.at[\"H2 (g) pipeline\", \"lifetime\"],\n            capital_cost=0,\n        )\n</code></pre>"},{"location":"reference/prepare_network/#prepare_network.add_biomass_chp","title":"<code>add_biomass_chp(network, costs, nodes, biomass_potential, prov_centroids, add_beccs=True)</code>","text":"<p>Add biomass combined heat and power (CHP) systems to the network.</p> <p>Integrates biomass CHP technology as new-build capacity (not retrofits or co-firing with coal). Optionally includes biomass with carbon capture and storage (BECCS) for negative emissions.</p> Note <p>The carbon capture component is not currently constrained to biomass capacity</p> <p>Parameters:</p> Name Type Description Default <code>network</code> <code>Network</code> <p>The PyPSA network object to modify.</p> required <code>costs</code> <code>DataFrame</code> <p>Techno-economic data (for all techs including biomass CHP)</p> required <code>nodes</code> <code>Index</code> <p>network nodes (typically province identifiers) where biomass CHP can be installed.</p> required <code>biomass_potential</code> <code>DataFrame</code> <p>biomass feedstock potential by node, indexed by location with values in energy units.</p> required <code>prov_centroids</code> <code>GeoDataFrame</code> <p>GeoDataFrame containing the geographic coordinates (x, y) of provincial (node) centroids (for plotting).</p> required <code>add_beccs</code> <code>bool</code> <p>Whether to include biomass with carbon capture and storage technology. Defaults to True.</p> <code>True</code> Source code in <code>workflow/scripts/prepare_network.py</code> <pre><code>def add_biomass_chp(\n    network: pypsa.Network,\n    costs: pd.DataFrame,\n    nodes: pd.Index,\n    biomass_potential: pd.DataFrame,\n    prov_centroids: gpd.GeoDataFrame,\n    add_beccs: bool = True,\n):\n    \"\"\"Add biomass combined heat and power (CHP) systems to the network.\n\n    Integrates biomass CHP technology as new-build capacity (not retrofits or\n    co-firing with coal). Optionally includes biomass with carbon capture and\n    storage (BECCS) for negative emissions.\n\n    Note:\n        The carbon capture component is not currently constrained to biomass capacity\n\n    Args:\n        network (pypsa.Network): The PyPSA network object to modify.\n        costs (pd.DataFrame): Techno-economic data (for all techs including biomass CHP)\n        nodes (pd.Index): network nodes (typically province identifiers)\n            where biomass CHP can be installed.\n        biomass_potential (pd.DataFrame): biomass feedstock potential\n            by node, indexed by location with values in energy units.\n        prov_centroids (gpd.GeoDataFrame): GeoDataFrame containing the geographic\n            coordinates (x, y) of provincial (node) centroids (for plotting).\n        add_beccs (bool, optional): Whether to include biomass with carbon capture\n            and storage technology. Defaults to True.\n    \"\"\"\n\n    suffix = \" biomass\"\n    biomass_potential.index = biomass_potential.index.map(\n        lambda x: x + suffix if not x.endswith(suffix) else x\n    )\n\n    network.add(\n        \"Bus\",\n        nodes,\n        suffix=suffix,\n        x=prov_centroids.x,\n        y=prov_centroids.y,\n        carrier=\"biomass\",\n    )\n    logger.info(\"Adding biomass buses\")\n    logger.info(f\"{nodes + suffix}\")\n    logger.info(\"potentials\")\n    # aggricultural residue biomass\n    # NOTE THIS CURRENTLY DOESN'T INCLUDE TRANSPORT between nodes\n    # NOTE additional emissions from treatment/remedials are missing\n    network.add(\n        \"Store\",\n        nodes + suffix,\n        bus=nodes + suffix,\n        e_nom_extendable=False,\n        e_nom=biomass_potential,\n        e_initial=biomass_potential,\n        carrier=\"biomass\",\n    )\n    biomass_co2_intsty = estimate_co2_intensity_xing()\n    network.add(\n        \"Link\",\n        nodes + \" central biomass CHP\",\n        bus0=nodes + \" biomass\",\n        bus1=nodes,\n        bus2=nodes + \" central heat\",\n        bus3=nodes + \" CO2\",\n        p_nom_extendable=True,\n        carrier=\"biomass\",\n        efficiency=costs.at[\"biomass CHP\", \"efficiency\"],\n        efficiency2=costs.at[\"biomass CHP\", \"efficiency-heat\"],\n        efficiency3=biomass_co2_intsty,\n        capital_cost=costs.at[\"biomass CHP\", \"efficiency\"]\n        * costs.at[\"biomass CHP\", \"capital_cost\"],\n        marginal_cost=costs.at[\"biomass CHP\", \"efficiency\"]\n        * costs.at[\"biomass CHP\", \"marginal_cost\"]\n        + costs.at[\"solid biomass\", \"fuel\"],\n        lifetime=costs.at[\"biomass CHP\", \"lifetime\"],\n    )\n    if add_beccs:\n        network.add(\n            \"Link\",\n            nodes + \" central biomass CHP capture\",\n            bus0=nodes + \" CO2\",\n            bus1=nodes + \" CO2 capture\",\n            bus2=nodes,\n            p_nom_extendable=True,\n            carrier=\"CO2 capture\",\n            efficiency=costs.at[\"biomass CHP capture\", \"capture_rate\"],\n            efficiency2=-1\n            * costs.at[\"biomass CHP capture\", \"capture_rate\"]\n            * costs.at[\"biomass CHP capture\", \"electricity-input\"],\n            capital_cost=costs.at[\"biomass CHP capture\", \"capture_rate\"]\n            * costs.at[\"biomass CHP capture\", \"capital_cost\"],\n            lifetime=costs.at[\"biomass CHP capture\", \"lifetime\"],\n        )\n\n    network.add(\n        \"Link\",\n        nodes + \" decentral biomass boiler\",\n        bus0=nodes + \" biomass\",\n        bus1=nodes + \" decentral heat\",\n        p_nom_extendable=True,\n        carrier=\"biomass\",\n        efficiency=costs.at[\"biomass boiler\", \"efficiency\"],\n        capital_cost=costs.at[\"biomass boiler\", \"efficiency\"]\n        * costs.at[\"biomass boiler\", \"capital_cost\"],\n        marginal_cost=costs.at[\"biomass boiler\", \"efficiency\"]\n        * costs.at[\"biomass boiler\", \"marginal_cost\"]\n        + costs.at[\"biomass boiler\", \"pelletizing cost\"]\n        + costs.at[\"solid biomass\", \"fuel\"],\n        lifetime=costs.at[\"biomass boiler\", \"lifetime\"],\n    )\n</code></pre>"},{"location":"reference/prepare_network/#prepare_network.add_carriers","title":"<code>add_carriers(network, config, costs)</code>","text":"<p>Add various carriers to the network based on configuration settings.</p> <p>Creates carrier objects for different energy types including electricity (AC), heat, renewable energy sources, storage technologies, and fuel carriers with their associated CO2 emissions factors.</p> <p>Parameters:</p> Name Type Description Default <code>network</code> <code>Network</code> <p>The PyPSA network object to modify.</p> required <code>config</code> <code>dict</code> <p>Config dictionary containing technology specifications and settings for carriers to be added.</p> required <code>costs</code> <code>DataFrame</code> <p>Cost database containing CO2 emissions factors for fuel carriers, indexed by carrier name.</p> required Source code in <code>workflow/scripts/prepare_network.py</code> <pre><code>def add_carriers(network: pypsa.Network, config: dict, costs: pd.DataFrame):\n    \"\"\"Add various carriers to the network based on configuration settings.\n\n    Creates carrier objects for different energy types including electricity (AC),\n    heat, renewable energy sources, storage technologies, and fuel carriers with\n    their associated CO2 emissions factors.\n\n    Args:\n        network (pypsa.Network): The PyPSA network object to modify.\n        config (dict): Config dictionary containing technology specifications\n            and settings for carriers to be added.\n        costs (pd.DataFrame): Cost database containing CO2 emissions factors for\n            fuel carriers, indexed by carrier name.\n    \"\"\"\n\n    network.add(\"Carrier\", \"AC\")\n    if config.get(\"heat_coupling\", False):\n        network.add(\"Carrier\", \"heat\")\n    for carrier in config[\"Techs\"][\"vre_techs\"]:\n        network.add(\"Carrier\", carrier)\n        if carrier == \"hydroelectricity\":\n            network.add(\"Carrier\", \"hydro_inflow\")\n    for carrier in config[\"Techs\"][\"store_techs\"]:\n        network.add(\"Carrier\", carrier)\n        if carrier == \"battery\":\n            network.add(\"Carrier\", \"battery discharger\")\n\n    # add fuel carriers, emissions in # in t_CO2/MWht\n    if config[\"add_gas\"]:\n        network.add(\"Carrier\", \"gas\", co2_emissions=costs.at[\"gas\", \"co2_emissions\"])\n    if config[\"add_coal\"]:\n        network.add(\"Carrier\", \"coal\", co2_emissions=costs.at[\"coal\", \"co2_emissions\"])\n    if \"CCGT-CCS\" in config[\"Techs\"][\"conv_techs\"]:\n        network.add(\"Carrier\", \"gas ccs\", co2_emissions=costs.at[\"gas ccs\", \"co2_emissions\"])\n    if \"coal-CCS\" in config[\"Techs\"][\"conv_techs\"]:\n        network.add(\"Carrier\", \"coal ccs\", co2_emissions=costs.at[\"coal ccs\", \"co2_emissions\"])\n</code></pre>"},{"location":"reference/prepare_network/#prepare_network.add_co2_capture_support","title":"<code>add_co2_capture_support(network, nodes, prov_centroids)</code>","text":"<p>Add CO2 capture carriers and storage to the network.</p> <p>Creates the necessary CO2-related carriers, buses, and storage components to support carbon capture and storage (CCS) technologies in the network.</p> <p>Parameters:</p> Name Type Description Default <code>network</code> <code>Network</code> <p>The PyPSA network object to modify.</p> required <code>nodes</code> <code>Index</code> <p>nodes for which CO2 infrastructure will be added.</p> required <code>prov_centroids</code> <code>GeoDataFrame</code> <p>GeoDataFrame containing the geographic coordinates (x, y) of network nodes (for plotting).</p> required Source code in <code>workflow/scripts/prepare_network.py</code> <pre><code>def add_co2_capture_support(\n    network: pypsa.Network, nodes: pd.Index, prov_centroids: gpd.GeoDataFrame\n):\n    \"\"\"Add CO2 capture carriers and storage to the network.\n\n    Creates the necessary CO2-related carriers, buses, and storage components\n    to support carbon capture and storage (CCS) technologies in the network.\n\n    Args:\n        network (pypsa.Network): The PyPSA network object to modify.\n        nodes (pd.Index): nodes for which CO2 infrastructure will be added.\n        prov_centroids (gpd.GeoDataFrame): GeoDataFrame containing the geographic\n            coordinates (x, y) of network nodes (for plotting).\n    \"\"\"\n\n    network.add(\"Carrier\", \"CO2\", co2_emissions=0)\n    network.add(\n        \"Bus\",\n        nodes,\n        suffix=\" CO2\",\n        x=prov_centroids.x,\n        y=prov_centroids.y,\n        carrier=\"CO2\",\n    )\n\n    network.add(\"Store\", nodes + \" CO2\", bus=nodes + \" CO2\", carrier=\"CO2\")\n    # normally taking away from carrier generates CO2, but here we are\n    # adding CO2 stored, so the emissions will point the other way ?\n    network.add(\"Carrier\", \"CO2 capture\", co2_emissions=1)\n    network.add(\n        \"Bus\",\n        nodes,\n        suffix=\" CO2 capture\",\n        x=prov_centroids.x,\n        y=prov_centroids.y,\n        carrier=\"CO2 capture\",\n    )\n\n    network.add(\n        \"Store\",\n        nodes + \" CO2 capture\",\n        bus=nodes + \" CO2 capture\",\n        e_nom_extendable=True,\n        # TODO change to capture to improve reporting (add carrier too?)\n        carrier=\"CO2\",\n    )\n</code></pre>"},{"location":"reference/prepare_network/#prepare_network.add_conventional_generators","title":"<code>add_conventional_generators(network, nodes, config, prov_centroids, costs)</code>","text":"<p>Add conventional generation techs to the network.</p> <p>Integrates fossil fuel generators/links including gas (OCGT, CCGT), coal power plants, and carbon capture and storage (CCS) variants based on configuration settings.</p> <p>Parameters:</p> Name Type Description Default <code>network</code> <code>Network</code> <p>The PyPSA network object to modify.</p> required <code>nodes</code> <code>Index</code> <p>Index of network nodes where generators will be added.</p> required <code>config</code> <code>dict</code> <p>Config with tech choices etc</p> required <code>prov_centroids</code> <code>GeoDataFrame</code> <p>GeoDataFrame containing the geographic coordinates (x, y) of network nodes for spatial representation.</p> required <code>costs</code> <code>DataFrame</code> <p>Cost database containing techno-economic parameters</p> required Source code in <code>workflow/scripts/prepare_network.py</code> <pre><code>def add_conventional_generators(\n    network: pypsa.Network,\n    nodes: pd.Index,\n    config: dict,\n    prov_centroids: gpd.GeoDataFrame,\n    costs: pd.DataFrame,\n):\n    \"\"\"Add conventional generation techs to the network.\n\n    Integrates fossil fuel generators/links including gas (OCGT, CCGT), coal power plants,\n    and carbon capture and storage (CCS) variants based on configuration settings.\n\n    Args:\n        network (pypsa.Network): The PyPSA network object to modify.\n        nodes (pd.Index): Index of network nodes where generators will be added.\n        config (dict): Config with tech choices etc\n        prov_centroids (gpd.GeoDataFrame): GeoDataFrame containing the geographic\n            coordinates (x, y) of network nodes for spatial representation.\n        costs (pd.DataFrame): Cost database containing techno-economic parameters\n    \"\"\"\n    if config[\"add_gas\"]:\n        # add converter from fuel source\n        network.add(\n            \"Bus\",\n            nodes,\n            suffix=\" gas\",\n            x=prov_centroids.x,\n            y=prov_centroids.y,\n            carrier=\"gas\",\n            location=nodes,\n        )\n\n        network.add(\n            \"Generator\",\n            nodes,\n            suffix=\" gas fuel\",\n            bus=nodes + \" gas\",\n            carrier=\"gas\",\n            p_nom_extendable=True,\n            p_nom=1e7,\n            marginal_cost=costs.at[\"gas\", \"fuel\"],\n        )\n\n        # gas prices identical per region, pipelines ignored\n        network.add(\n            \"Store\",\n            nodes + \" gas Store\",\n            bus=nodes + \" gas\",\n            e_nom_extendable=True,\n            carrier=\"gas\",\n            e_nom=1e7,\n            e_cyclic=True,\n        )\n\n    # add gas will then be true\n    gas_techs = [\"OCGT\", \"CCGT\"]\n    for tech in gas_techs:\n        if tech in config[\"Techs\"][\"conv_techs\"]:\n            network.add(\n                \"Link\",\n                nodes,\n                suffix=f\" {tech}\",\n                bus0=nodes + \" gas\",\n                bus1=nodes,\n                marginal_cost=costs.at[tech, \"efficiency\"]\n                * costs.at[tech, \"VOM\"],  # NB: VOM is per MWel\n                capital_cost=costs.at[tech, \"efficiency\"]\n                * costs.at[tech, \"capital_cost\"],  # NB: capital cost is per MWel\n                p_nom_extendable=True,\n                efficiency=costs.at[tech, \"efficiency\"],\n                lifetime=costs.at[tech, \"lifetime\"],\n                carrier=f\"gas {tech}\",\n            )\n\n    if \"CCGT-CCS\" in config[\"Techs\"][\"conv_techs\"]:\n        network.add(\n            \"Generator\",\n            nodes,\n            suffix=\" CCGT-CCS\",\n            bus=nodes,\n            carrier=\"gas ccs\",\n            p_nom_extendable=True,\n            efficiency=costs.at[\"CCGT-CCS\", \"efficiency\"],\n            marginal_cost=costs.at[\"CCGT-CCS\", \"marginal_cost\"],\n            capital_cost=costs.at[\"CCGT-CCS\", \"efficiency\"]\n            * costs.at[\"CCGT-CCS\", \"capital_cost\"],  # NB: capital cost is per MWel\n            lifetime=costs.at[\"CCGT-CCS\", \"lifetime\"],\n            p_max_pu=0.9,  # planned and forced outages\n        )\n\n    if config[\"add_coal\"]:\n        ramps = config.get(\n            \"fossil_ramps\", {\"coal\": {\"ramp_limit_up\": np.nan, \"ramp_limit_down\": np.nan}}\n        )\n        ramps = ramps.get(\"coal\", {\"ramp_limit_up\": np.nan, \"ramp_limit_down\": np.nan})\n        ramps = {k: v * config[\"snapshots\"][\"frequency\"] for k, v in ramps.items()}\n        # this is the non sector-coupled approach\n        # for industry may have an issue in that coal feeds to chem sector\n        # no coal in Beijing - political decision\n        network.add(\n            \"Generator\",\n            nodes[nodes != \"Beijing\"],\n            suffix=\" coal power\",\n            bus=nodes[nodes != \"Beijing\"],\n            carrier=\"coal\",\n            p_nom_extendable=True,\n            efficiency=costs.at[\"coal\", \"efficiency\"],\n            marginal_cost=costs.at[\"coal\", \"marginal_cost\"],\n            capital_cost=costs.at[\"coal\", \"capital_cost\"],  # NB: capital cost is per MWel\n            lifetime=costs.at[\"coal\", \"lifetime\"],\n            ramp_limit_up=ramps[\"ramp_limit_up\"],\n            ramp_limit_down=ramps[\"ramp_limit_down\"],\n            p_max_pu=0.9,  # planned and forced outages\n        )\n\n        if \"coal-CCS\" in config[\"Techs\"][\"conv_techs\"]:\n            network.add(\n                \"Generator\",\n                nodes,\n                suffix=\" coal-CCS\",\n                bus=nodes,\n                carrier=\"coal ccs\",\n                p_nom_extendable=True,\n                efficiency=costs.at[\"coal ccs\", \"efficiency\"],\n                marginal_cost=costs.at[\"coal ccs\", \"marginal_cost\"],\n                capital_cost=costs.at[\"coal ccs\", \"capital_cost\"],  # NB: capital cost is per MWel\n                lifetime=costs.at[\"coal ccs\", \"lifetime\"],\n                p_max_pu=0.9,  # planned and forced outages\n            )\n</code></pre>"},{"location":"reference/prepare_network/#prepare_network.add_fuel_subsidies","title":"<code>add_fuel_subsidies(n, subsidy_config)</code>","text":"<p>Apply fuel subsidies to generators as a post-processing step.</p> <p>Subsidies are applied to generators based on their carrier and location. The subsidy values (in EUR/MWh fuel) are divided by efficiency to convert to electricity basis (EUR/MWhel). Links are not modified as they get their fuel from generators.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>Network</code> <p>The network object to modify.</p> required <code>subsidy_config</code> <code>dict</code> <p>Subsidy configuration dictionary with keys like \"coal\" or \"gas\", each containing a dict mapping provinces to subsidy values.</p> required Source code in <code>workflow/scripts/prepare_network.py</code> <pre><code>def add_fuel_subsidies(n: pypsa.Network, subsidy_config: dict):\n    \"\"\"Apply fuel subsidies to generators as a post-processing step.\n\n    Subsidies are applied to generators based on their carrier and location.\n    The subsidy values (in EUR/MWh fuel) are divided by efficiency to convert\n    to electricity basis (EUR/MWhel). Links are not modified as they get their\n    fuel from generators.\n\n    Args:\n        n (pypsa.Network): The network object to modify.\n        subsidy_config (dict): Subsidy configuration dictionary with keys like\n            \"coal\" or \"gas\", each containing a dict mapping provinces to subsidy values.\n    \"\"\"\n    if not subsidy_config:\n        return\n\n    carriers = subsidy_config.keys()\n\n    for carrier in carriers:\n        subs_dict = subsidy_config.get(carrier, {})\n        if not subs_dict:\n            continue\n\n        # Convert subsidy dict to Series indexed by province\n        subs = pd.Series(subs_dict, dtype=float)\n\n        # Check that subsidies are non-positive (negative = subsidy, positive would be a reward)\n        if (subs &gt; 0).any():\n            raise ValueError(\n                f\"Positive subsidy values found for carrier '{carrier}': \"\n                f\"{subs[subs &gt; 0].to_dict()}. Only zero or negative values are allowed \"\n                f\"(negative reduces marginal cost, positive would increase it).\"\n            )\n\n        # Check if location column exists\n        if \"location\" not in n.generators.columns:\n            logger.warning(\n                f\"Location column not found in generators. \"\n                f\"Cannot apply subsidies for carrier '{carrier}'.\"\n            )\n            continue\n\n        # Query generators with matching carrier and location in subsidy provinces\n        mask = n.generators.query(\n            f\"carrier == @carrier and location in @subs.index\"\n        ).index\n\n        if mask.empty:\n            logger.warning(\n                f\"No generators found with carrier '{carrier}' and locations \"\n                f\"in {list(subs.index)}. Skipping subsidy application.\"\n            )\n            continue\n\n        # Merge subsidies with generators by location\n        gen_locs = n.generators.loc[mask, \"location\"]\n        subs_to_apply = gen_locs.map(subs).fillna(0.0)\n\n        # Check if all provinces were found\n        missing_provs = set(subs.index) - set(gen_locs.unique())\n        if missing_provs:\n            logger.warning(\n                f\"Subsidies specified for provinces {missing_provs} but no \"\n                f\"generators found with carrier '{carrier}' in these provinces.\"\n            )\n\n        # Apply subsidies: divide by efficiency to convert from fuel to electricity basis\n        # Handle cases where efficiency might be NaN or missing\n        efficiencies = n.generators.loc[mask, \"efficiency\"].fillna(1.0)\n        subs_electricity = subs_to_apply / efficiencies\n\n        # Subtract subsidy from marginal cost (negative subsidy = cost reduction)\n        n.generators.loc[mask, \"marginal_cost\"] += subs_electricity\n\n        logger.info(\n            f\"Applied subsidies for carrier '{carrier}' to {len(mask)} generators \"\n            f\"in provinces {sorted(gen_locs.unique())}\"\n        )\n</code></pre>"},{"location":"reference/prepare_network/#prepare_network.add_heat_coupling","title":"<code>add_heat_coupling(network, config, nodes, prov_centroids, costs, planning_year, paths)</code>","text":"<p>Add heat sector coupling technologies and infrastructure to the network.</p> <p>Integrates heat pumps, thermal storage, heating demand, and other heat-related technologies for both centralized and decentralized heating systems.</p> <p>Parameters:</p> Name Type Description Default <code>network</code> <code>Network</code> <p>PyPSA network object to modify.</p> required <code>config</code> <code>dict</code> <p>Config containing heat coupling and demand settings</p> required <code>nodes</code> <code>Index</code> <p>network nodes for which heat infrastructure will be added.</p> required <code>prov_centroids</code> <code>GeoDataFrame</code> <p>GeoDataFrame containing the geographic coordinates (x, y) of network nodes</p> required <code>costs</code> <code>DataFrame</code> <p>Cost database containing techno-economic parameters for heat technologies and heating systems.</p> required <code>planning_year</code> <code>int</code> <p>Planning year for profile alignment and cost calculations.</p> required <code>paths</code> <code>dict</code> <p>Dictionary containing file paths to heat demand profiles, COP (coefficient of performance) data, and other heat-related data files.</p> required Source code in <code>workflow/scripts/prepare_network.py</code> <pre><code>def add_heat_coupling(\n    network: pypsa.Network,\n    config: dict,\n    nodes: pd.Index,\n    prov_centroids: gpd.GeoDataFrame,\n    costs: pd.DataFrame,\n    planning_year: int,\n    paths: dict,\n):\n    \"\"\"Add heat sector coupling technologies and infrastructure to the network.\n\n    Integrates heat pumps, thermal storage, heating demand, and other heat-related\n    technologies for both centralized and decentralized heating systems.\n\n    Args:\n        network (pypsa.Network): PyPSA network object to modify.\n        config (dict): Config containing heat coupling and demand settings\n        nodes (pd.Index): network nodes for which heat infrastructure\n            will be added.\n        prov_centroids (gpd.GeoDataFrame): GeoDataFrame containing the geographic\n            coordinates (x, y) of network nodes\n        costs (pd.DataFrame): Cost database containing techno-economic parameters\n            for heat technologies and heating systems.\n        planning_year (int): Planning year for profile alignment and cost calculations.\n        paths (dict): Dictionary containing file paths to heat demand profiles,\n            COP (coefficient of performance) data, and other heat-related data files.\n    \"\"\"\n    central_fraction = pd.read_hdf(paths[\"central_fraction\"])\n    with pd.HDFStore(paths[\"heat_demand_profile\"], mode=\"r\") as store:\n        heat_demand = store[\"heat_demand_profiles\"]\n        # TODO fix this if not working\n        heat_demand.index = heat_demand.index.tz_localize(None)\n        heat_demand = heat_demand.loc[network.snapshots]\n        # NOTE electric boilers not yet subtracted from load\n        hot_water_demand = store.get(\"hot_water_demand\")\n        hot_water_demand = hot_water_demand.loc[network.snapshots]\n\n    network.add(\n        \"Bus\",\n        nodes,\n        suffix=\" decentral heat\",\n        x=prov_centroids.x,\n        y=prov_centroids.y,\n        carrier=\"heat\",\n        location=nodes,\n    )\n\n    network.add(\n        \"Bus\",\n        nodes,\n        suffix=\" central heat\",\n        x=prov_centroids.x,\n        y=prov_centroids.y,\n        carrier=\"heat\",\n        location=nodes,\n    )\n\n    network.add(\n        \"Load\",\n        nodes,\n        suffix=\" decentral heat\",\n        bus=nodes + \" decentral heat\",\n        p_set=heat_demand[nodes].multiply(1 - central_fraction[nodes]) + hot_water_demand[nodes],\n    )\n\n    network.add(\n        \"Load\",\n        nodes,\n        carrier=\"heat\",\n        suffix=\" central heat\",\n        bus=nodes + \" central heat\",\n        p_set=heat_demand[nodes].multiply(central_fraction[nodes]),\n    )\n\n    if \"heat pump\" in config[\"Techs\"][\"vre_techs\"]:\n        logger.info(f\"loading cop profiles from {paths['cop_name']}\")\n        with pd.HDFStore(paths[\"cop_name\"], mode=\"r\") as store:\n            ashp_cop = store[\"ashp_cop_profiles\"]\n            ashp_cop.index = ashp_cop.index.tz_localize(None)\n            ashp_cop = shift_profile_to_planning_year(ashp_cop, planning_year)\n            gshp_cop = store[\"gshp_cop_profiles\"]\n            gshp_cop.index = gshp_cop.index.tz_localize(None)\n            gshp_cop = shift_profile_to_planning_year(gshp_cop, planning_year)\n\n        for cat in [\" decentral \", \" central \"]:\n            network.add(\n                \"Link\",\n                nodes,\n                suffix=cat + \"heat pump\",\n                bus0=nodes,\n                bus1=nodes + cat + \"heat\",\n                carrier=\"heat pump\",\n                efficiency=(\n                    ashp_cop.loc[network.snapshots, nodes]\n                    if config[\"time_dep_hp_cop\"]\n                    else costs.at[cat.lstrip() + \"air-sourced heat pump\", \"efficiency\"]\n                ),\n                capital_cost=costs.at[cat.lstrip() + \"air-sourced heat pump\", \"efficiency\"]\n                * costs.at[cat.lstrip() + \"air-sourced heat pump\", \"capital_cost\"],\n                marginal_cost=costs.at[cat.lstrip() + \"air-sourced heat pump\", \"efficiency\"]\n                * costs.at[cat.lstrip() + \"air-sourced heat pump\", \"marginal_cost\"],\n                p_nom_extendable=True,\n                lifetime=costs.at[cat.lstrip() + \"air-sourced heat pump\", \"lifetime\"],\n            )\n\n        network.add(\n            \"Link\",\n            nodes,\n            suffix=\" ground heat pump\",\n            bus0=nodes,\n            bus1=nodes + \" decentral heat\",\n            carrier=\"heat pump\",\n            efficiency=(\n                gshp_cop.loc[network.snapshots, nodes]\n                if config[\"time_dep_hp_cop\"]\n                else costs.at[\"decentral ground-sourced heat pump\", \"efficiency\"]\n            ),\n            marginal_cost=costs.at[cat.lstrip() + \"ground-sourced heat pump\", \"efficiency\"]\n            * costs.at[cat.lstrip() + \"ground-sourced heat pump\", \"marginal_cost\"],\n            capital_cost=costs.at[cat.lstrip() + \"ground-sourced heat pump\", \"efficiency\"]\n            * costs.at[\"decentral ground-sourced heat pump\", \"capital_cost\"],\n            p_nom_extendable=True,\n            lifetime=costs.at[\"decentral ground-sourced heat pump\", \"lifetime\"],\n        )\n\n    if \"water tanks\" in config[\"Techs\"][\"store_techs\"]:\n        for cat in [\" decentral \", \" central \"]:\n            network.add(\n                \"Bus\",\n                nodes,\n                suffix=cat + \"water tanks\",\n                x=prov_centroids.x,\n                y=prov_centroids.y,\n                carrier=\"water tanks\",\n                location=nodes,\n            )\n\n            network.add(\n                \"Link\",\n                nodes + cat + \"water tanks charger\",\n                bus0=nodes + cat + \"heat\",\n                bus1=nodes + cat + \"water tanks\",\n                carrier=\"water tanks\",\n                efficiency=costs.at[\"water tank charger\", \"efficiency\"],\n                p_nom_extendable=True,\n            )\n\n            network.add(\n                \"Link\",\n                nodes + cat + \"water tanks discharger\",\n                bus0=nodes + cat + \"water tanks\",\n                bus1=nodes + cat + \"heat\",\n                carrier=\"water tanks\",\n                efficiency=costs.at[\"water tank discharger\", \"efficiency\"],\n                p_nom_extendable=True,\n            )\n            # [HP] 180 day time constant for centralised, 3 day for decentralised\n            tes_tau = config[\"water_tanks\"][\"tes_tau\"][cat.strip()]\n            network.add(\n                \"Store\",\n                nodes + cat + \"water tank\",\n                bus=nodes + cat + \"water tanks\",\n                carrier=\"water tanks\",\n                e_cyclic=True,\n                e_nom_extendable=True,\n                standing_loss=1 - np.exp(-1 / (24.0 * tes_tau)),\n                capital_cost=costs.at[cat.lstrip() + \"water tank storage\", \"capital_cost\"],\n                lifetime=costs.at[cat.lstrip() + \"water tank storage\", \"lifetime\"],\n            )\n\n    if \"resistive heater\" in config[\"Techs\"][\"vre_techs\"]:\n        for cat in [\" decentral \", \" central \"]:\n            network.add(\n                \"Link\",\n                nodes + cat + \"resistive heater\",\n                bus0=nodes,\n                bus1=nodes + cat + \"heat\",\n                carrier=\"resistive heater\",\n                efficiency=costs.at[cat.lstrip() + \"resistive heater\", \"efficiency\"],\n                capital_cost=costs.at[cat.lstrip() + \"resistive heater\", \"efficiency\"]\n                * costs.at[cat.lstrip() + \"resistive heater\", \"capital_cost\"],\n                marginal_cost=costs.at[cat.lstrip() + \"resistive heater\", \"efficiency\"]\n                * costs.at[cat.lstrip() + \"resistive heater\", \"marginal_cost\"],\n                p_nom_extendable=True,\n                lifetime=costs.at[cat.lstrip() + \"resistive heater\", \"lifetime\"],\n            )\n\n    if (\n        \"H2 CHP\" in config[\"Techs\"][\"vre_techs\"]\n        and config[\"add_H2\"]\n        and config.get(\"heat_coupling\", False)\n    ):\n        htpr = config[\"chp_parameters\"][\"OCGT\"][\"heat_to_power\"]\n\n        network.add(\n            \"Link\",\n            nodes,\n            suffix=\" CHP H2 generator\",\n            bus0=nodes + \" H2\",\n            bus1=nodes,\n            bus2=nodes + \" central heat\",\n            p_nom_extendable=True,\n            marginal_cost=costs.at[\"central hydrogen CHP\", \"efficiency\"]\n            * costs.at[\"central hydrogen CHP\", \"VOM\"],  # NB: VOM is per MWel\n            capital_cost=costs.at[\"central hydrogen CHP\", \"efficiency\"]\n            * costs.at[\"central hydrogen CHP\", \"capital_cost\"],  # NB: capital cost is per MWel\n            efficiency=costs.at[\"central hydrogen CHP\", \"efficiency\"],\n            heat_to_power=config[\"chp_parameters\"][\"gas\"][\"heat_to_power\"],\n            lifetime=costs.at[\"central hydrogen CHP\", \"lifetime\"],\n            carrier=\"CHP H2\",\n            location=nodes,\n        )\n        network.add(\n            \"Link\",\n            nodes,\n            suffix=\" CHP H2 boiler\",\n            bus0=nodes + \" H2\",\n            bus1=nodes + \" central heat\",\n            carrier=\"CHP H2\",\n            marginal_cost=costs.at[\"central hydrogen CHP\", \"efficiency\"]\n            * costs.at[\"central hydrogen CHP\", \"VOM\"],  # NB: VOM is per MWel\n            p_nom_extendable=True,\n            # total eff will be fixed by CHP constraints\n            efficiency=config[\"chp_parameters\"][\"gas\"][\"total_eff\"],\n            lifetime=costs.at[\"central hydrogen CHP\", \"lifetime\"],\n            heat_to_power=config[\"chp_parameters\"][\"gas\"][\"heat_to_power\"],\n            location=nodes,\n        )\n\n    if \"CHP gas\" in config[\"Techs\"][\"conv_techs\"]:\n        # Extraction mode CCGT CHP -&gt; heat to power ratio is maximum\n        network.add(\n            \"Link\",\n            nodes,\n            suffix=\" CHP gas generator\",\n            bus0=nodes + \" gas\",\n            bus1=nodes,\n            bus2=nodes + \" central heat\",\n            p_nom_extendable=True,\n            marginal_cost=costs.at[\"central gas CHP CC\", \"efficiency\"]\n            * costs.at[\"central gas CHP CC\", \"VOM\"],  # NB: VOM is per MWel\n            capital_cost=costs.at[\"central gas CHP CC\", \"efficiency\"]\n            * costs.at[\"central gas CHP CC\", \"capital_cost\"],  # NB: capital cost is per MWel\n            efficiency=costs.at[\"central gas CHP CC\", \"efficiency\"],\n            heat_to_power=config[\"chp_parameters\"][\"gas\"][\"heat_to_power\"],\n            lifetime=costs.at[\"central gas CHP CC\", \"lifetime\"],\n            carrier=\"CHP gas\",\n            location=nodes,\n        )\n\n        network.add(\n            \"Link\",\n            nodes,\n            suffix=\" CHP gas boiler\",\n            bus0=nodes + \" gas\",\n            bus1=nodes + \" central heat\",\n            carrier=\"CHP gas\",\n            marginal_cost=costs.at[\"central gas CHP CC\", \"efficiency\"]\n            * costs.at[\"central gas CHP CC\", \"VOM\"],  # NB: VOM is per MWel\n            p_nom_extendable=True,\n            # total eff will be fixed by CHP constraints\n            efficiency=config[\"chp_parameters\"][\"gas\"][\"total_eff\"],\n            lifetime=costs.at[\"central gas CHP CC\", \"lifetime\"],\n            heat_to_power=config[\"chp_parameters\"][\"gas\"][\"heat_to_power\"],\n            location=nodes,\n        )\n\n    if \"CHP OCGT gas\" in config[\"Techs\"][\"conv_techs\"]:\n        # OCGT CHPs have fixed heat to power ratio\n        # F*eta_tot = Qout + Pout = (1+htpr)*Pout = (1+1/htpr)*Qout\n        # eta_el = Pout/F = eta_tot/(1+htpr) etc\n        eta_tot = config[\"chp_parameters\"][\"OCGT\"][\"total_eff\"]\n        htpr = config[\"chp_parameters\"][\"OCGT\"][\"heat_to_power\"]\n\n        network.add(\n            \"Link\",\n            nodes,\n            suffix=\" CHP OCGT gas\",\n            bus0=nodes + \" gas\",\n            bus1=nodes,\n            bus2=nodes + \" central heat\",\n            p_nom_extendable=True,\n            marginal_cost=costs.at[\"central gas CHP\", \"efficiency\"]\n            * costs.at[\"central gas CHP\", \"VOM\"],  # NB: VOM is per MWel\n            capital_cost=costs.at[\"central gas CHP\", \"efficiency\"]\n            * costs.at[\"central gas CHP\", \"capital_cost\"],  # NB: capital cost is per MWel\n            efficiency=eta_tot / (1 + htpr),\n            efficiency2=eta_tot / (1 + 1 / htpr),\n            lifetime=costs.at[\"central gas CHP\", \"lifetime\"],\n            carrier=\"CHP OCGT gas\",\n        )\n\n    if \"CHP coal\" in config[\"Techs\"][\"conv_techs\"]:\n        logger.info(\"Adding CHP coal to network\")\n        # Extraction mode super critical CHP -&gt; heat to power ratio is maximum\n        # in ideal model there would be plant level \"commitment\" of extraction mode\n        # to reflect that real plants have a min heat output in extraction mode\n        # however, over many plants can avoid this (remembering no heat in condensing mode)\n\n        network.add(\n            \"Bus\",\n            nodes,\n            suffix=\" coal fuel\",\n            x=prov_centroids.x,\n            y=prov_centroids.y,\n            carrier=\"coal\",\n            location=nodes,\n        )\n\n        network.add(\n            \"Generator\",\n            nodes + \" coal fuel\",\n            bus=nodes + \" coal fuel\",\n            carrier=\"coal\",\n            p_nom_extendable=False,\n            p_nom=1e8,\n            marginal_cost=costs.at[\"coal\", \"fuel\"],\n        )\n\n        # NOTE CHP + generator | boiler is a key word for the constraint\n        network.add(\n            \"Link\",\n            name=nodes[nodes != \"Beijing\"],\n            suffix=\" CHP coal generator\",\n            bus0=nodes[nodes != \"Beijing\"] + \" coal fuel\",\n            bus1=nodes[nodes != \"Beijing\"],\n            p_nom_extendable=True,\n            marginal_cost=costs.at[\"central coal CHP\", \"efficiency\"]\n            * costs.at[\"central coal CHP\", \"VOM\"],  # NB: VOM is per MWel\n            capital_cost=costs.at[\"central coal CHP\", \"efficiency\"]\n            * costs.at[\"central coal CHP\", \"capital_cost\"],  # NB: capital cost is per MWel\n            efficiency=costs.at[\"central coal CHP\", \"efficiency\"],\n            heat_to_power=config[\"chp_parameters\"][\"coal\"][\"heat_to_power\"],\n            lifetime=costs.at[\"central coal CHP\", \"lifetime\"],\n            carrier=\"CHP coal\",\n            location=nodes[nodes != \"Beijing\"],\n        )\n\n        network.add(\n            \"Link\",\n            nodes[nodes != \"Beijing\"],\n            suffix=\" central CHP coal boiler\",\n            bus0=nodes[nodes != \"Beijing\"] + \" coal fuel\",\n            bus1=nodes[nodes != \"Beijing\"] + \" central heat\",\n            carrier=\"CHP coal\",\n            p_nom_extendable=True,\n            marginal_cost=costs.at[\"central coal CHP\", \"efficiency\"]\n            * costs.at[\n                \"central coal CHP\", \"VOM\"\n            ],  # NB: VOM is per MWel            # total eff will be fixed by CHP constraints\n            efficiency=config[\"chp_parameters\"][\"coal\"][\"total_eff\"],\n            lifetime=costs.at[\"central coal CHP\", \"lifetime\"],\n            heat_to_power=config[\"chp_parameters\"][\"coal\"][\"heat_to_power\"],\n            location=nodes[nodes != \"Beijing\"],\n        )\n\n    if \"coal boiler\" in config[\"Techs\"][\"conv_techs\"]:\n        use_hist_eff = config[\"use_historical_efficiency\"].get(\"coal_boiler\", False)\n        for cat in [\"decentral\", \"central\"]:\n            eff = (\n                costs.at[f\"{cat} coal boiler\", \"hist_efficiency\"]\n                if use_hist_eff\n                else costs.at[f\"{cat} coal boiler\", \"efficiency\"]\n            )\n            network.add(\n                \"Link\",\n                nodes[nodes != \"Beijing\"] + f\" {cat} coal boiler\",\n                p_nom_extendable=True,\n                bus0=nodes[nodes != \"Beijing\"] + \" coal fuel\",\n                bus1=nodes[nodes != \"Beijing\"] + f\" {cat} heat\",\n                efficiency=eff,\n                marginal_cost=eff * costs.at[f\"{cat} coal boiler\", \"VOM\"],\n                capital_cost=eff * costs.at[f\"{cat} coal boiler\", \"capital_cost\"],\n                lifetime=costs.at[f\"{cat} coal boiler\", \"lifetime\"],\n                carrier=f\"coal boiler {cat}\",\n            )\n\n    if \"gas boiler\" in config[\"Techs\"][\"conv_techs\"]:\n        for cat in [\"decentral\", \"central\"]:\n            network.add(\n                \"Link\",\n                nodes + cat + \"gas boiler\",\n                p_nom_extendable=True,\n                bus0=nodes + \" gas\",\n                bus1=nodes + f\" {cat} heat\",\n                efficiency=costs.at[f\"{cat} gas boiler\", \"efficiency\"],\n                marginal_cost=costs.at[f\"{cat} gas boiler\", \"VOM\"],\n                capital_cost=costs.at[f\"{cat} gas boiler\", \"efficiency\"]\n                * costs.at[f\"{cat} gas boiler\", \"capital_cost\"],\n                lifetime=costs.at[f\"{cat} gas boiler\", \"lifetime\"],\n                carrier=f\"gas boiler {cat}\",\n            )\n\n    if \"solar thermal\" in config[\"Techs\"][\"vre_techs\"]:\n        # this is the amount of heat collected in W per m^2, accounting\n        # for efficiency\n        with pd.HDFStore(paths[\"solar_thermal_name\"], mode=\"r\") as store:\n            # 1e3 converts from W/m^2 to MW/(1000m^2) = kW/m^2\n            solar_thermal = config[\"solar_cf_correction\"] * store[\"solar_thermal_profiles\"] / 1e3\n\n        solar_thermal.index = solar_thermal.index.tz_localize(None)\n        solar_thermal = shift_profile_to_planning_year(solar_thermal, planning_year)\n        solar_thermal = solar_thermal.loc[network.snapshots]\n\n        for cat in [\" decentral \", \" central \"]:\n            network.add(\n                \"Generator\",\n                nodes,\n                suffix=cat + \"solar thermal\",\n                bus=nodes + cat + \"heat\",\n                carrier=\"solar thermal\",\n                p_nom_extendable=True,\n                capital_cost=costs.at[cat.lstrip() + \"solar thermal\", \"capital_cost\"],\n                p_max_pu=solar_thermal[nodes].clip(1.0e-4),\n                lifetime=costs.at[cat.lstrip() + \"solar thermal\", \"lifetime\"],\n            )\n</code></pre>"},{"location":"reference/prepare_network/#prepare_network.add_hydro","title":"<code>add_hydro(network, config, nodes, prov_shapes, costs, planning_horizons)</code>","text":"<p>Add the hydropower plants (dams) to the network. Due to the spillage/basin calculations these have real locations not just nodes. WARNING: the node is assigned based on the damn province name (turbine link)         NOT future proof</p> <p>Parameters:</p> Name Type Description Default <code>network</code> <code>Network</code> <p>the network object</p> required <code>config</code> <code>dict</code> <p>the yaml config</p> required <code>nodes</code> <code>Index</code> <p>the buses</p> required <code>prov_shapes</code> <code>GeoDataFrame</code> <p>the province shapes GDF</p> required <code>costs</code> <code>DataFrame</code> <p>the costs dataframe</p> required <code>planning_horizons</code> <code>int</code> <p>the year</p> required Source code in <code>workflow/scripts/prepare_network.py</code> <pre><code>def add_hydro(\n    network: pypsa.Network,\n    config: dict,\n    nodes: pd.Index,\n    prov_shapes: gpd.GeoDataFrame,\n    costs: pd.DataFrame,\n    planning_horizons: int,\n):\n    \"\"\"Add the hydropower plants (dams) to the network.\n    Due to the spillage/basin calculations these have real locations not just nodes.\n    WARNING: the node is assigned based on the damn province name (turbine link)\n            NOT future proof\n\n    Args:\n        network (pypsa.Network): the network object\n        config (dict): the yaml config\n        nodes (pd.Index): the buses\n        prov_shapes (gpd.GeoDataFrame): the province shapes GDF\n        costs (pd.DataFrame): the costs dataframe\n        planning_horizons (int): the year\n    \"\"\"\n\n    logger.info(\"\\tAdding dam cascade\")\n\n    # load dams\n    df = pd.read_csv(config[\"hydro_dams\"][\"dams_path\"], index_col=0)\n    points = df.apply(lambda row: Point(row.Lon, row.Lat), axis=1)\n    dams = gpd.GeoDataFrame(df, geometry=points, crs=CRS)\n    # store all info, then filter by selected nodes\n    dam_provinces = dams.Province\n    all_dams = dams.index.values\n    dams = dams[dams.Province.isin(nodes)]\n\n    logger.debug(f\"Hydro dams in {nodes} provinces: {dams.index}\")\n\n    hourly_rng = pd.date_range(\n        config[\"hydro_dams\"][\"inflow_date_start\"],\n        config[\"hydro_dams\"][\"inflow_date_end\"],\n        freq=\"1h\",  # THIS IS THE INFLOW RES\n        inclusive=\"left\",\n    )\n    # TODO implement inflow calc, understand resolution (seems daily!)\n    inflow = pd.read_pickle(config[\"hydro_dams\"][\"inflow_path\"])\n    # select inflow year\n    hourly_rng = hourly_rng[hourly_rng.year == INFLOW_DATA_YR]\n    inflow = inflow.loc[inflow.index.year == INFLOW_DATA_YR]\n    inflow = inflow.reindex(hourly_rng, fill_value=0)\n    inflow.columns = all_dams  # TODO dangerous\n    # select only the dams in the network\n    inflow = inflow.loc[:, inflow.columns.map(dam_provinces).isin(nodes)]\n    inflow = shift_profile_to_planning_year(inflow, planning_horizons)\n    inflow = inflow.loc[network.snapshots]\n    # m^3/KWh -&gt; m^3/MWh\n    water_consumption_factor = dams.loc[:, \"Water_consumption_factor_avg\"] * 1e3\n\n    #######\n    # ### Add hydro stations as buses\n    network.add(\n        \"Bus\",\n        dams.index,\n        suffix=\" station\",\n        carrier=\"stations\",\n        x=dams[\"geometry\"].to_crs(\"+proj=cea\").centroid.to_crs(prov_shapes.crs).x,\n        y=dams[\"geometry\"].to_crs(\"+proj=cea\").centroid.to_crs(prov_shapes.crs).y,\n        location=dams[\"Province\"],\n    )\n\n    dam_buses = network.buses[network.buses.carrier == \"stations\"]\n\n    # ===== add hydro reservoirs as stores ======\n    initial_capacity = pd.read_pickle(config[\"hydro_dams\"][\"reservoir_initial_capacity_path\"])\n    effective_capacity = pd.read_pickle(config[\"hydro_dams\"][\"reservoir_effective_capacity_path\"])\n    initial_capacity.index = all_dams\n    effective_capacity.index = all_dams\n    initial_capacity = initial_capacity / water_consumption_factor\n    effective_capacity = effective_capacity / water_consumption_factor\n\n    # select relevant dams in nodes\n    effective_capacity = effective_capacity.loc[\n        effective_capacity.index.map(dam_provinces).isin(nodes)\n    ]\n    initial_capacity = initial_capacity.loc[initial_capacity.index.map(dam_provinces).isin(nodes)]\n\n    network.add(\n        \"Store\",\n        dams.index,\n        suffix=\" reservoir\",\n        bus=dam_buses.index,\n        e_nom=effective_capacity,\n        e_initial=initial_capacity,\n        e_cyclic=True,\n        # TODO fix all config[\"costs\"]\n        marginal_cost=config[\"hydro\"][\"marginal_cost\"][\"reservoir\"],\n    )\n\n    # add hydro turbines to link stations to provinces\n    network.add(\n        \"Link\",\n        dams.index,\n        suffix=\" turbines\",\n        bus0=dam_buses.index,\n        bus1=dams[\"Province\"],\n        carrier=\"hydroelectricity\",\n        p_nom=10 * dams[\"installed_capacity_10MW\"],\n        capital_cost=(\n            costs.at[\"hydro\", \"capital_cost\"] if config[\"hydro\"][\"hydro_capital_cost\"] else 0\n        ),\n        efficiency=1,\n        location=dams[\"Province\"],\n        p_nom_extendable=False,\n    )\n\n    # ===  add rivers to link station to station\n    dam_edges = pd.read_csv(config[\"hydro_dams\"][\"damn_flows_path\"], delimiter=\",\")\n    in_nodes = dam_edges.bus0.map(dam_provinces).isin(nodes) &amp; dam_edges.end_bus.map(\n        dam_provinces\n    ).isin(nodes)\n    dam_edges = dam_edges[in_nodes]\n\n    # === normal flow ====\n    for row in dam_edges.iterrows():\n        bus0 = row[1].bus0 + \" turbines\"\n        bus2 = row[1].end_bus + \" station\"\n        network.links.at[bus0, \"bus2\"] = bus2\n        network.links.at[bus0, \"efficiency2\"] = 1.0\n\n    # === spillage ====\n    # TODO WHY EXTENDABLE - weather year?\n    for row in dam_edges.iterrows():\n        bus0 = row[1].bus0 + \" station\"\n        bus1 = row[1].end_bus + \" station\"\n        network.add(\n            \"Link\",\n            f\"{bus0}-{bus1}\" + \" spillage\",\n            bus0=bus0,\n            bus1=bus1,\n            p_nom_extendable=True,\n        )\n\n    dam_ends = [\n        dam\n        for dam in np.unique(dams.index.values)\n        if dam not in dam_edges[\"bus0\"]\n        or dam not in dam_edges[\"end_bus\"]\n        or (dam in dam_edges[\"end_bus\"].values &amp; dam not in dam_edges[\"bus0\"])\n    ]\n    # need some kind of sink to absorb spillage (e,g ocean).\n    # here hack by flowing to existing bus with 0 efficiency (lose)\n    # TODO make more transparent -&gt; generator with neg sign and 0 c0st\n    for bus0 in dam_ends:\n        network.add(\n            \"Link\",\n            bus0 + \" spillage\",\n            bus0=bus0 + \" station\",\n            bus1=bus0 + \" station\",\n            p_nom_extendable=True,\n            efficiency=0.0,\n        )\n\n    # add inflow as generators\n    # only feed into hydro stations which are the first of a cascade\n    inflow_stations = [\n        dam for dam in np.unique(dams.index.values) if dam not in dam_edges[\"end_bus\"].values\n    ]\n\n    for inflow_station in inflow_stations:\n        # p_nom = 1 and p_max_pu &amp; p_min_pu = p_pu, compulsory inflow\n        p_nom = (inflow / water_consumption_factor)[inflow_station].max()\n        p_pu = (inflow / water_consumption_factor)[inflow_station] / p_nom\n        p_pu.index = network.snapshots\n        network.add(\n            \"Generator\",\n            inflow_station + \" inflow\",\n            bus=inflow_station + \" station\",\n            carrier=\"hydro_inflow\",\n            p_max_pu=p_pu.clip(1.0e-6),\n            # p_min_pu=p_pu.clip(1.0e-6),\n            p_nom=p_nom,\n        )\n\n        # p_nom*p_pu = XXX m^3 then use turbines efficiency to convert to power\n\n    # ======= add other existing hydro power (not lattitude resolved) ===\n    hydro_p_nom = pd.read_hdf(config[\"hydro_dams\"][\"p_nom_path\"]).loc[nodes]\n    hydro_p_max_pu = (\n        pd.read_hdf(\n            config[\"hydro_dams\"][\"p_max_pu_path\"],\n            key=config[\"hydro_dams\"][\"p_max_pu_key\"],\n        ).tz_localize(None)\n    )[nodes]\n\n    hydro_p_max_pu = shift_profile_to_planning_year(hydro_p_max_pu, planning_horizons)\n    # sort buses (columns) otherwise stuff will break\n    hydro_p_max_pu.sort_index(axis=1, inplace=True)\n\n    hydro_p_max_pu = hydro_p_max_pu.loc[snapshots]\n    hydro_p_max_pu.index = network.snapshots\n\n    logger.info(\"\\tAdding extra hydro capacity (regionally aggregated)\")\n\n    network.add(\n        \"Generator\",\n        nodes,\n        suffix=\" hydroelectricity\",\n        bus=nodes,\n        carrier=\"hydroelectricity\",\n        p_nom=hydro_p_nom,\n        p_nom_min=hydro_p_nom,\n        p_nom_extendable=False,\n        p_max_pu=hydro_p_max_pu,\n        capital_cost=(\n            costs.at[\"hydro\", \"capital_cost\"] if config[\"hydro\"][\"hydro_capital_cost\"] else 0\n        ),\n    )\n</code></pre>"},{"location":"reference/prepare_network/#prepare_network.add_voltage_links","title":"<code>add_voltage_links(network, config)</code>","text":"<p>Add high-voltage transmission links (HVDC/AC) to the network.</p> <p>Creates transmission infrastructure between network nodes based on topology config. Supports both lossy and lossless transmission models with configurable efficiency parameters and security margins.</p> <p>Parameters:</p> Name Type Description Default <code>network</code> <code>Network</code> <p>The PyPSA network object to modify.</p> required <code>config</code> <code>dict</code> <p>Config containing transmission topology, line parameters, efficiency settings, and security margins.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the specified topology file is not found.</p> Source code in <code>workflow/scripts/prepare_network.py</code> <pre><code>def add_voltage_links(network: pypsa.Network, config: dict):\n    \"\"\"Add high-voltage transmission links (HVDC/AC) to the network.\n\n    Creates transmission infrastructure between network nodes based on topology config.\n    Supports both lossy and lossless transmission models with\n    configurable efficiency parameters and security margins.\n\n    Args:\n        network (pypsa.Network): The PyPSA network object to modify.\n        config (dict): Config containing transmission topology,\n            line parameters, efficiency settings, and security margins.\n\n    Raises:\n        ValueError: If the specified topology file is not found.\n    \"\"\"\n\n    represented_hours = network.snapshot_weightings.sum()[0]\n    n_years = represented_hours / 8760.0\n\n    # determine topology\n    edge_path = config[\"edge_paths\"].get(config[\"scenario\"][\"topology\"], None)\n    if edge_path is None:\n        raise ValueError(f\"No grid found for topology {config['scenario']['topology']}\")\n    else:\n        edges_ = pd.read_csv(\n            edge_path, sep=\",\", header=None, names=[\"bus0\", \"bus1\", \"p_nom\"]\n        ).fillna(0)\n        edges = edges_[edges_[\"bus0\"].isin(PROV_NAMES) &amp; edges_[\"bus1\"].isin(PROV_NAMES)]\n        if edges_.shape[0] != edges.shape[0]:\n            logger.warning(\"Some edges are not in the network\")\n    # fix this to use map with x.y\n    lengths = config[\"lines\"][\"line_length_factor\"] * np.array(\n        [\n            haversine(\n                [network.buses.at[bus0, \"x\"], network.buses.at[bus0, \"y\"]],\n                [network.buses.at[bus1, \"x\"], network.buses.at[bus1, \"y\"]],\n            )\n            for bus0, bus1 in edges[[\"bus0\", \"bus1\"]].values\n        ]\n    )\n\n    # get for backward compatibility\n    security_config = config.get(\"security\", {\"line_security_margin\": 70})\n    line_margin = security_config.get(\"line_security_margin\", 70) / 100\n\n    line_cost = (\n        lengths * costs.at[\"HVDC overhead\", \"capital_cost\"] * FOM_LINES * n_years\n    ) + costs.at[\n        \"HVDC inverter pair\", \"capital_cost\"\n    ]  # /MW\n\n    # ==== lossy transport model (split into 2) ====\n    # NB this only works if there is an equalising constraint, which is hidden in solve_ntwk\n    if config[\"line_losses\"]:\n        network.add(\n            \"Link\",\n            edges[\"bus0\"] + \"-\" + edges[\"bus1\"],\n            bus0=edges[\"bus0\"].values,\n            bus1=edges[\"bus1\"].values,\n            suffix=\" positive\",\n            p_nom_extendable=True,\n            p_nom=edges[\"p_nom\"].values,\n            p_nom_min=edges[\"p_nom\"].values,\n            p_min_pu=0,\n            p_max_pu=line_margin,\n            efficiency=config[\"transmission_efficiency\"][\"DC\"][\"efficiency_static\"]\n            * config[\"transmission_efficiency\"][\"DC\"][\"efficiency_per_1000km\"] ** (lengths / 1000),\n            length=lengths,\n            capital_cost=line_cost,\n            carrier=\"AC\",  # Fake - actually DC\n        )\n        # 0 len for reversed in case line limits are specified in km. Limited in constraints to fwdcap\n        network.add(\n            \"Link\",\n            edges[\"bus0\"] + \"-\" + edges[\"bus1\"],\n            bus0=edges[\"bus1\"].values,\n            bus1=edges[\"bus0\"].values,\n            suffix=\" reversed\",\n            p_nom_extendable=True,\n            p_nom=edges[\"p_nom\"].values,\n            p_nom_min=edges[\"p_nom\"].values,\n            p_min_pu=0,\n            p_max_pu=line_margin,\n            efficiency=config[\"transmission_efficiency\"][\"DC\"][\"efficiency_static\"]\n            * config[\"transmission_efficiency\"][\"DC\"][\"efficiency_per_1000km\"] ** (lengths / 1000),\n            length=0,\n            capital_cost=0,\n            carrier=\"AC\",\n        )\n    # lossless transport model\n    else:\n        network.add(\n            \"Link\",\n            edges[\"bus0\"] + \"-\" + edges[\"bus1\"],\n            p_nom=edges[\"p_nom\"].values,\n            p_nom_min=edges[\"p_nom\"].values,\n            bus0=edges[\"bus0\"].values,\n            bus1=edges[\"bus1\"].values,\n            p_nom_extendable=True,\n            p_min_pu=-1,\n            length=lengths,\n            capital_cost=line_cost,\n            carrier=\"AC\",\n        )\n</code></pre>"},{"location":"reference/prepare_network/#prepare_network.add_wind_and_solar","title":"<code>add_wind_and_solar(network, techs, paths, year, costs)</code>","text":"<p>Add wind and solar generators with resource grade differentiation.</p> <p>Add ariable renewable energy (VRE) generators for each tech and quality grade Loads capacity factors and potential from profiles and adds generators.</p> <p>Parameters:</p> Name Type Description Default <code>network</code> <code>Network</code> <p>PyPSA network to which generators will be added.</p> required <code>techs</code> <code>list</code> <p>renewable energy technologies to add. Supported technologies are [\"solar\", \"onwind\", \"offwind\"].</p> required <code>paths</code> <code>dict</code> <p>Dictionary containing file paths to renewable energy profiles with keys like 'profile_solar', 'profile_onwind', etc.</p> required <code>year</code> <code>int</code> <p>Planning year for which profiles should be aligned.</p> required <code>costs</code> <code>DataFrame</code> <p>Technoeconomic data incl. renewable technologies</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If unsupported technologies are specified or if paths not specified</p> Source code in <code>workflow/scripts/prepare_network.py</code> <pre><code>def add_wind_and_solar(\n    network: pypsa.Network,\n    techs: list[str],\n    paths: dict,\n    year: int,\n    costs: pd.DataFrame,\n):\n    \"\"\"Add wind and solar generators with resource grade differentiation.\n\n    Add ariable renewable energy (VRE) generators for each tech and quality grade\n    Loads capacity factors and potential from profiles and adds generators.\n\n    Args:\n        network (pypsa.Network): PyPSA network to which generators will be added.\n        techs (list): renewable energy technologies to add. Supported\n            technologies are [\"solar\", \"onwind\", \"offwind\"].\n        paths (dict): Dictionary containing file paths to renewable energy profiles\n            with keys like 'profile_solar', 'profile_onwind', etc.\n        year (int): Planning year for which profiles should be aligned.\n        costs (pd.DataFrame): Technoeconomic data incl. renewable technologies\n\n    Raises:\n        ValueError: If unsupported technologies are specified or if paths not specified\n    \"\"\"\n\n    unsupported = set(techs).difference({\"solar\", \"onwind\", \"offwind\"})\n    if unsupported:\n        raise ValueError(f\"Carrier(s) {unsupported} not wind or solar pv\")\n    prof_paths = {f\"profile_{tech}\": paths[f\"profile_{tech}\"] for tech in techs}\n    if len(prof_paths) != len(techs):\n        raise ValueError(f\"Paths do not correspond to techs  ({prof_paths} vs {techs})\")\n\n    for tech in techs:\n        # load the renewable profiles\n        logger.info(f\"Attaching {tech} to network\")\n        with xr.open_dataset(prof_paths[f\"profile_{tech}\"]) as ds:\n            if ds.indexes[\"bus\"].empty:\n                continue\n            if \"year\" in ds.indexes:\n                ds = ds.sel(year=ds.year.min(), drop=True)\n\n            timestamps = pd.DatetimeIndex(ds.time)\n\n            def shift_weather_to_planning_yr(t):\n                \"\"\"Shift weather data to planning year.\"\"\"\n                return t.replace(year=int(year))\n\n            timestamps = timestamps.map(shift_weather_to_planning_yr)\n            ds = ds.assign_coords(time=timestamps)\n\n            mask = ds.time.isin(network.snapshots)\n            ds = ds.sel(time=mask)\n\n            if not len(ds.time) == len(network.snapshots):\n                err = f\"{len(ds.time)} and {len(network.snapshots)}\"\n                raise ValueError(\"Mismatch in profile and network timestamps \" + err)\n            ds = ds.stack(bus_bin=[\"bus\", \"bin\"])\n\n        # remove low potential bins\n        cutoff = config.get(\"renewable_potential_cutoff\", 0)\n        ds = ds.where(ds[\"p_nom_max\"] &gt; cutoff, drop=True)\n\n        # bins represent renewable generation grades\n        def flatten(t):\n            \"\"\"Flatten tuple to string with ' grade' separator.\"\"\"\n            return \" grade\".join(map(str, t))\n\n        buses = ds.indexes[\"bus_bin\"].get_level_values(\"bus\")\n        bus_bins = ds.indexes[\"bus_bin\"].map(flatten)\n\n        p_nom_max = ds[\"p_nom_max\"].to_pandas()\n        p_nom_max.index = p_nom_max.index.map(flatten)\n\n        p_max_pu = ds[\"profile\"].to_pandas()\n        p_max_pu.columns = p_max_pu.columns.map(flatten)\n\n        # add renewables\n        network.add(\n            \"Generator\",\n            bus_bins,\n            suffix=f\" {tech}\",\n            bus=buses,\n            carrier=tech,\n            p_nom_extendable=True,\n            p_nom_max=p_nom_max,\n            capital_cost=costs.at[tech, \"capital_cost\"],\n            marginal_cost=costs.at[tech, \"marginal_cost\"],\n            p_max_pu=p_max_pu,\n            lifetime=costs.at[tech, \"lifetime\"],\n        )\n</code></pre>"},{"location":"reference/prepare_network/#prepare_network.prepare_network","title":"<code>prepare_network(config, costs, snapshots, biomass_potential=None, paths=None)</code>","text":"<p>Prepares/makes the network object for overnight mode according to config &amp; at 1 node per region/province</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict</code> <p>the snakemake config</p> required <code>costs</code> <code>DataFrame</code> <p>the costs dataframe (anualised capex and marginal costs)</p> required <code>snapshots</code> <code>date_range</code> <p>the snapshots for the network</p> required <code>biomass_potential</code> <code>(Optional, DataFrame)</code> <p>biomass potential dataframe. Defaults to None.</p> <code>None</code> <code>paths</code> <code>(Optional, dict)</code> <p>the paths to the data files. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Network</code> <p>pypsa.Network: the pypsa network object</p> Source code in <code>workflow/scripts/prepare_network.py</code> <pre><code>def prepare_network(\n    config: dict,\n    costs: pd.DataFrame,\n    snapshots: pd.date_range,\n    biomass_potential: pd.DataFrame = None,\n    paths: dict = None,\n) -&gt; pypsa.Network:\n    \"\"\"Prepares/makes the network object for overnight mode according to config &amp;\n    at 1 node per region/province\n\n    Args:\n        config (dict): the snakemake config\n        costs (pd.DataFrame): the costs dataframe (anualised capex and marginal costs)\n        snapshots (pd.date_range): the snapshots for the network\n        biomass_potential (Optional, pd.DataFrame): biomass potential dataframe. Defaults to None.\n        paths (Optional, dict): the paths to the data files. Defaults to None.\n\n    Returns:\n        pypsa.Network: the pypsa network object\n    \"\"\"\n\n    # determine whether gas/coal to be added depending on specified conv techs\n    config[\"add_gas\"] = (\n        True\n        if [tech for tech in config[\"Techs\"][\"conv_techs\"] if (\"gas\" in tech or \"CGT\" in tech)]\n        else False\n    )\n    config[\"add_coal\"] = (\n        True if [tech for tech in config[\"Techs\"][\"conv_techs\"] if \"coal\" in tech] else False\n    )\n\n    planning_horizons = snakemake.wildcards[\"planning_horizons\"]\n\n    # Build the Network object, which stores all other objects\n    network = pypsa.Network()\n    network.set_snapshots(snapshots)\n    network.snapshot_weightings[:] = config[\"snapshots\"][\"frequency\"]\n    # load graph\n    nodes = pd.Index(PROV_NAMES)\n    # toso soft code\n    countries = [\"CN\"] * len(nodes)\n\n    # TODO check crs projection correct\n    # load provinces\n    prov_shapes = read_province_shapes(paths[\"province_shape\"])\n    prov_centroids = prov_shapes.to_crs(\"+proj=cea\").centroid.to_crs(CRS)\n\n    # add AC buses\n    network.add(\n        \"Bus\", nodes, x=prov_centroids.x, y=prov_centroids.y, location=nodes, country=countries\n    )\n\n    # add carriers\n    add_carriers(network, config, costs)\n\n    # load electricity demand data\n    with pd.HDFStore(paths[\"elec_load\"], mode=\"r\") as store:\n        load = store[\"load\"].loc[network.snapshots, PROV_NAMES]\n\n    network.add(\"Load\", nodes, bus=nodes, p_set=load[nodes])\n\n    # NOTE: EV components are now handled by add_sectors rule (add_sectors.py)\n    # This allows the workflow to run with or without sector coupling\n    logger.info(\"EV components will be added by add_sectors rule if sector coupling is enabled\")\n\n    ws_carriers = [c for c in config[\"Techs\"][\"vre_techs\"] if c.find(\"wind\") &gt;= 0 or c == \"solar\"]\n    add_wind_and_solar(network, ws_carriers, paths, planning_horizons, costs)\n\n    add_conventional_generators(network, nodes, config, prov_centroids, costs)\n\n    # nuclear is brownfield\n    if \"nuclear\" in config[\"Techs\"][\"vre_techs\"]:\n        nuclear_nodes = pd.Index(NUCLEAR_EXTENDABLE)\n        network.add(\n            \"Generator\",\n            nuclear_nodes,\n            suffix=\" nuclear\",\n            p_nom_extendable=True,\n            p_max_pu=config[\"nuclear_reactors\"][\"p_max_pu\"],\n            p_min_pu=config[\"nuclear_reactors\"][\"p_min_pu\"],\n            bus=nuclear_nodes,\n            carrier=\"nuclear\",\n            efficiency=costs.at[\"nuclear\", \"efficiency\"],\n            capital_cost=costs.at[\"nuclear\", \"capital_cost\"],  # NB: capital cost is per MWel\n            marginal_cost=costs.at[\"nuclear\", \"marginal_cost\"],\n            lifetime=costs.at[\"nuclear\", \"lifetime\"],\n        )\n\n    # TODO add coal CC? no retrofit option\n\n    if \"PHS\" in config[\"Techs\"][\"store_techs\"]:\n        # pure pumped hydro storage, fixed, 6h energy by default, no inflow\n        cc = costs.at[\"PHS\", \"capital_cost\"]\n\n        network.add(\n            \"StorageUnit\",\n            nodes,\n            suffix=\" PHS\",\n            bus=nodes,\n            carrier=\"PHS\",\n            p_nom_extendable=True,\n            max_hours=config[\"hydro\"][\"PHS_max_hours\"],\n            efficiency_store=np.sqrt(costs.at[\"PHS\", \"efficiency\"]),\n            efficiency_dispatch=np.sqrt(costs.at[\"PHS\", \"efficiency\"]),\n            cyclic_state_of_charge=True,\n            capital_cost=cc,\n            marginal_cost=0.0,\n        )\n\n    if config[\"add_hydro\"]:\n        logger.info(\"Adding hydro to network\")\n        add_hydro(network, config, nodes, prov_centroids, costs, planning_horizons)\n\n    if config[\"add_H2\"]:\n        logger.info(\"Adding H2 buses to network\")\n        # do beore heat coupling to avoid warning\n        network.add(\n            \"Bus\",\n            nodes,\n            suffix=\" H2\",\n            x=prov_centroids.x,\n            y=prov_centroids.y,\n            carrier=\"H2\",\n            location=nodes,\n        )\n\n    if config.get(\"heat_coupling\", False):\n        logger.info(\"Adding heat and CHP to the network\")\n        add_heat_coupling(network, config, nodes, prov_centroids, costs, planning_horizons, paths)\n\n        if config[\"add_biomass\"]:\n            logger.info(\"Adding biomass to network\")\n            add_co2_capture_support(network, nodes, prov_centroids)\n            add_biomass_chp(\n                network,\n                costs,\n                nodes,\n                biomass_potential[nodes],\n                prov_centroids,\n                add_beccs=\"beccs\" in config[\"Techs\"][\"vre_techs\"],\n            )\n\n    if config[\"add_H2\"]:\n        logger.info(\"Adding H2 to network\")\n        add_H2(network, config, nodes, costs, yr)\n\n    if \"battery\" in config[\"Techs\"][\"store_techs\"]:\n        network.add(\n            \"Bus\",\n            nodes,\n            suffix=\" battery\",\n            x=prov_centroids.x,\n            y=prov_centroids.y,\n            carrier=\"battery\",\n            location=nodes,\n        )\n\n        # TODO Why no standing loss?: test with\n        min_charge = config[\"electricity\"].get(\"min_charge\", {\"battery\": 0})\n        min_charge = min_charge.get(\"battery\", 0)\n        network.add(\n            \"Store\",\n            nodes + \" battery\",\n            bus=nodes + \" battery\",\n            e_cyclic=True,\n            e_nom_extendable=True,\n            e_min_pu=min_charge,\n            capital_cost=costs.at[\"battery storage\", \"capital_cost\"],\n            lifetime=costs.at[\"battery storage\", \"lifetime\"],\n        )\n\n        # TODO understand/remove sources, data should not be in code\n        # Sources:\n        # [HP]: Henning, Palzer http://www.sciencedirect.com/science/article/pii/S1364032113006710\n        # [B]: Budischak et al. http://www.sciencedirect.com/science/article/pii/S0378775312014759\n\n        network.add(\n            \"Link\",\n            nodes + \" battery charger\",\n            bus0=nodes,\n            bus1=nodes + \" battery\",\n            efficiency=costs.at[\"battery inverter\", \"efficiency\"] ** 0.5,\n            capital_cost=costs.at[\"battery inverter\", \"efficiency\"]\n            * costs.at[\"battery inverter\", \"capital_cost\"],\n            p_nom_extendable=True,\n            carrier=\"battery\",\n            lifetime=costs.at[\"battery inverter\", \"lifetime\"],\n        )\n\n        network.add(\n            \"Link\",\n            nodes + \" battery discharger\",\n            bus0=nodes + \" battery\",\n            bus1=nodes,\n            efficiency=costs.at[\"battery inverter\", \"efficiency\"] ** 0.5,\n            marginal_cost=0.0,\n            p_nom_extendable=True,\n            carrier=\"battery discharger\",\n        )\n\n    # ============= add lines =========\n    # The lines are implemented according to the transport model (no KVL) and without losses.\n    # see Neumann et al 10.1016/j.apenergy.2022.118859\n    # TODO make not lossless optional (? - increases computing cost)\n\n    if not config[\"no_lines\"]:\n        add_voltage_links(network, config)\n\n    assign_locations(network)\n\n    # Apply fuel subsidies as post-processing step\n    subsidy_config = config.get(\"subsidies\", {})\n    if subsidy_config and subsidy_config.get(\"enabled\", True):\n        # Remove 'enabled' key before passing to add_fuel_subsidies\n        subsidy_config_clean = {k: v for k, v in subsidy_config.items() if k != \"enabled\"}\n        if subsidy_config_clean:\n            add_fuel_subsidies(network, subsidy_config_clean)\n\n    return network\n</code></pre>"},{"location":"reference/readers/","title":"Readers","text":"<p>File reading support functions for PyPSA-China-PIK workflow.</p> <p>This module provides functions for reading and processing yearly load projections from REMIND data, with support for sector coupling (electric vehicles) and  flexible data format handling.</p>"},{"location":"reference/readers/#readers.aggregate_sectoral_loads","title":"<code>aggregate_sectoral_loads(yearly_proj, config)</code>","text":"<p>Aggregate REMIND load sectors according to the model configuration.</p> <p>Sectors that are NOT enabled for independent modeling will be aggregated  into the main electricity load. For example, if EV sector is not enabled  as an independent sector (enabled: false), its load will be added to the  AC load in the aggregation.</p> <p>Parameters:</p> Name Type Description Default <code>yearly_proj</code> <code>DataFrame</code> <p>REMIND output with columns ['province', 'sector', '2020', '2025', ...]. Each row represents one sector's load for one province across all years.</p> required <code>config</code> <code>dict</code> <p>Configuration dict with structure: - sectors.electric_vehicles.enabled: bool (whether to model EV as independent sector) - sectors.sector_mapping.base: list (always-included sectors, e.g., ['ac']) - sectors.sector_mapping.electric_vehicles: list (EV sectors, e.g., ['ev_pass', 'ev_freight'])</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with provinces as index and years as columns, containing total annual</p> <code>DataFrame</code> <p>load (MWh) summed across sectors that should be aggregated.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If sector_mapping is missing or no matching sectors found.</p> Source code in <code>workflow/scripts/readers.py</code> <pre><code>def aggregate_sectoral_loads(yearly_proj: pd.DataFrame, config: dict) -&gt; pd.DataFrame:\n    \"\"\"Aggregate REMIND load sectors according to the model configuration.\n\n    Sectors that are NOT enabled for independent modeling will be aggregated \n    into the main electricity load. For example, if EV sector is not enabled \n    as an independent sector (enabled: false), its load will be added to the \n    AC load in the aggregation.\n\n    Args:\n        yearly_proj: REMIND output with columns ['province', 'sector', '2020', '2025', ...].\n            Each row represents one sector's load for one province across all years.\n        config: Configuration dict with structure:\n            - sectors.electric_vehicles.enabled: bool (whether to model EV as independent sector)\n            - sectors.sector_mapping.base: list (always-included sectors, e.g., ['ac'])\n            - sectors.sector_mapping.electric_vehicles: list (EV sectors, e.g., ['ev_pass', 'ev_freight'])\n\n    Returns:\n        DataFrame with provinces as index and years as columns, containing total annual\n        load (MWh) summed across sectors that should be aggregated.\n\n    Raises:\n        ValueError: If sector_mapping is missing or no matching sectors found.\n    \"\"\"\n    sectors_cfg = config.get(\"sectors\", {})\n    mapping = sectors_cfg.get(\"sector_mapping\", {})\n\n    if not mapping:\n        raise ValueError(\"Missing sector_mapping configuration\")\n\n    # Always include base sectors (e.g., AC)\n    sectors_to_include = set(mapping.get(\"base\", []))\n\n    # For each optional sector, if it's NOT enabled for independent modeling,\n    # aggregate it into the main load\n    # Electric vehicles\n    if not sectors_cfg.get(\"electric_vehicles\", {}).get(\"enabled\", False):\n        if \"electric_vehicles\" in mapping:\n            sectors_to_include.update(mapping[\"electric_vehicles\"])\n\n    # Heat coupling (if exists in the future)\n    if not sectors_cfg.get(\"heat_coupling\", {}).get(\"enabled\", False):\n        if \"heat_coupling\" in mapping:\n            sectors_to_include.update(mapping[\"heat_coupling\"])\n\n    # Filter data to only include selected sectors\n    filtered = yearly_proj[yearly_proj[\"sector\"].isin(sectors_to_include)].copy()\n    if filtered.empty:\n        raise ValueError(\n            f\"No sector data found. \"\n            f\"Requested sectors: {sectors_to_include}, \"\n            f\"Available sectors: {yearly_proj['sector'].unique().tolist()}\"\n        )\n\n    # Aggregate by province and year\n    year_cols = [c for c in filtered.columns if c.isdigit()]\n    result = filtered.groupby(\"province\")[year_cols].sum()\n    return result\n</code></pre>"},{"location":"reference/readers/#readers.read_yearly_load_projections","title":"<code>read_yearly_load_projections(file_path='resources/data/load/Province_Load_2020_2060.csv', conversion=1.0, config=None)</code>","text":"<p>Read and process yearly load projections from CSV files.</p> <p>Supports both simple load data and REMIND sector-coupled data with  electric vehicle integration. Automatically detects data format and applies appropriate processing.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>PathLike</code> <p>Path to the yearly projections CSV file. Defaults to \"resources/data/load/Province_Load_2020_2060.csv\".</p> <code>'resources/data/load/Province_Load_2020_2060.csv'</code> <code>conversion</code> <code>float</code> <p>Conversion factor to apply to the data (e.g., to MWh). Defaults to 1.0.</p> <code>1.0</code> <code>config</code> <code>dict</code> <p>Configuration dictionary for sector processing. Required when processing REMIND data with sector columns. Should contain 'sectors' and 'sector_mapping' keys.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: Processed load projections data with: - Province names as index (for simple data) or columns - Year columns as integers - Data converted by the conversion factor</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If required columns are missing or configuration is invalid</p> <code>FileNotFoundError</code> <p>If the input file does not exist</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Simple load data\n&gt;&gt;&gt; data = read_yearly_load_projections(\"simple_load.csv\")\n</code></pre> <pre><code>&gt;&gt;&gt; # REMIND data with electric vehicles\n&gt;&gt;&gt; config = {\n...     \"sectors\": {\"electric_vehicles\": True},\n...     \"sector_mapping\": {\n...         \"base\": [\"ac\"],\n...         \"electric_vehicles\": [\"ev_freight\", \"ev_pass\"]\n...     }\n... }\n&gt;&gt;&gt; data = read_yearly_load_projections(\"remind_data.csv\", config=config)\n</code></pre> Source code in <code>workflow/scripts/readers.py</code> <pre><code>def read_yearly_load_projections(\n    file_path: os.PathLike = \"resources/data/load/Province_Load_2020_2060.csv\",\n    conversion: float = 1.0,\n    config: dict = None,\n) -&gt; pd.DataFrame:\n    \"\"\"Read and process yearly load projections from CSV files.\n\n    Supports both simple load data and REMIND sector-coupled data with \n    electric vehicle integration. Automatically detects data format and\n    applies appropriate processing.\n\n    Args:\n        file_path (os.PathLike): Path to the yearly projections CSV file.\n            Defaults to \"resources/data/load/Province_Load_2020_2060.csv\".\n        conversion (float): Conversion factor to apply to the data (e.g., to MWh).\n            Defaults to 1.0.\n        config (dict, optional): Configuration dictionary for sector processing.\n            Required when processing REMIND data with sector columns.\n            Should contain 'sectors' and 'sector_mapping' keys.\n\n    Returns:\n        pd.DataFrame: Processed load projections data with:\n            - Province names as index (for simple data) or columns\n            - Year columns as integers\n            - Data converted by the conversion factor\n\n    Raises:\n        ValueError: If required columns are missing or configuration is invalid\n        FileNotFoundError: If the input file does not exist\n\n    Examples:\n        &gt;&gt;&gt; # Simple load data\n        &gt;&gt;&gt; data = read_yearly_load_projections(\"simple_load.csv\")\n\n        &gt;&gt;&gt; # REMIND data with electric vehicles\n        &gt;&gt;&gt; config = {\n        ...     \"sectors\": {\"electric_vehicles\": True},\n        ...     \"sector_mapping\": {\n        ...         \"base\": [\"ac\"],\n        ...         \"electric_vehicles\": [\"ev_freight\", \"ev_pass\"]\n        ...     }\n        ... }\n        &gt;&gt;&gt; data = read_yearly_load_projections(\"remind_data.csv\", config=config)\n    \"\"\"\n    # Read the CSV file\n    df = pd.read_csv(file_path)\n\n    # Standardize province column name\n    province_candidates = [\"province\", \"region\", \"Unnamed: 0\"]\n    province_col = next((col for col in province_candidates if col in df.columns), None)\n\n    if province_col is None:\n        raise ValueError(\n            f\"No province column found in {file_path}. \"\n            f\"Expected one of: {province_candidates}\"\n        )\n\n    if province_col != \"province\":\n        df = df.rename(columns={province_col: \"province\"})\n\n    # Process data based on whether it contains sector information\n    if \"sector\" in df.columns:\n        if config is None:\n            raise ValueError(\n                \"Data file contains sector column but no config provided. \"\n                \"Please provide config with 'sectors' and 'sector_mapping' keys.\"\n            )\n        df = aggregate_sectoral_loads(df, config)\n    else:\n        # Simple data format - set province as index\n        df = df.set_index(\"province\")\n\n    # Convert year columns to integers for consistency\n    year_cols = {col: int(col) for col in df.columns if col.isdigit()}\n    df = df.rename(columns=year_cols)\n\n    # Apply conversion factor\n    return df * conversion\n</code></pre>"},{"location":"reference/readers_geospatial/","title":"Readers geospatial","text":"<p>File reading support functions</p>"},{"location":"reference/readers_geospatial/#readers_geospatial.read_offshore_province_shapes","title":"<code>read_offshore_province_shapes(shape_file, index_name='province')</code>","text":"<p>Read the offshore province shape files (based on the eez)</p> <p>Parameters:</p> Name Type Description Default <code>shape_file</code> <code>PathLike</code> <p>the path to the .shp file &amp; co</p> required <code>index_name</code> <code>str</code> <p>the name of the index column. Defaults to \"province\".</p> <code>'province'</code> <p>Returns:</p> Type Description <code>GeoDataFrame</code> <p>gpd.GeoDataFrame: the offshore province shapes as a GeoDataFrame</p> Source code in <code>workflow/scripts/readers_geospatial.py</code> <pre><code>def read_offshore_province_shapes(\n    shape_file: os.PathLike, index_name=\"province\"\n) -&gt; gpd.GeoDataFrame:\n    \"\"\"Read the offshore province shape files (based on the eez)\n\n    Args:\n        shape_file (os.PathLike): the path to the .shp file &amp; co\n        index_name (str, optional): the name of the index column. Defaults to \"province\".\n\n    Returns:\n        gpd.GeoDataFrame: the offshore province shapes as a GeoDataFrame\n    \"\"\"\n\n    offshore_regional = gpd.read_file(shape_file).set_index(index_name)\n    offshore_regional = offshore_regional.reindex(OFFSHORE_WIND_NODES).rename_axis(\"bus\")\n    if offshore_regional.geometry.isnull().any():\n        empty_geoms = offshore_regional[offshore_regional.geometry.isnull()].index.to_list()\n        raise ValueError(\n            f\"There are empty geometries in offshore_regional {empty_geoms}, offshore wind will fail\"\n        )\n\n    return offshore_regional\n</code></pre>"},{"location":"reference/readers_geospatial/#readers_geospatial.read_pop_density","title":"<code>read_pop_density(path, clip_shape=None, crs=CRS, chunks=25, var_name='pop_density')</code>","text":"<p>Read raster data, clip it to a clip_shape and convert it to a GeoDataFrame</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>PathLike</code> <p>the target path for the raster data (tif)</p> required <code>clip_shape</code> <code>GeoSeries</code> <p>the shape to clip the data. Defaults to None.</p> <code>None</code> <code>crs</code> <code>int</code> <p>the coordinate system. Defaults to 4326.</p> <code>CRS</code> <code>var_name</code> <code>str</code> <p>the variable name. Defaults to \"var\".</p> <code>'pop_density'</code> <code>chunks</code> <code>int</code> <p>the chunk size for the raster data. Defaults to 25.</p> <code>25</code> <p>Returns:</p> Type Description <code>GeoDataFrame</code> <p>gpd.GeoDataFrame: the raster data for the aoi</p> Source code in <code>workflow/scripts/readers_geospatial.py</code> <pre><code>def read_pop_density(\n    path: os.PathLike,\n    clip_shape: gpd.GeoSeries = None,\n    crs=CRS,\n    chunks=25,\n    var_name=\"pop_density\",\n) -&gt; gpd.GeoDataFrame:\n    \"\"\"Read raster data, clip it to a clip_shape and convert it to a GeoDataFrame\n\n    Args:\n        path (os.PathLike): the target path for the raster data (tif)\n        clip_shape (gpd.GeoSeries, optional): the shape to clip the data. Defaults to None.\n        crs (int, optional): the coordinate system. Defaults to 4326.\n        var_name (str, optional): the variable name. Defaults to \"var\".\n        chunks (int, optional): the chunk size for the raster data. Defaults to 25.\n\n    Returns:\n        gpd.GeoDataFrame: the raster data for the aoi\n    \"\"\"\n\n    ds = read_raster(path, clip_shape, var_name, plot=False)\n    ds = ds.where(ds &gt; 0)\n\n    df = ds.to_dataframe(var_name)\n    df.reset_index(inplace=True)\n\n    # Convert the DataFrame to a GeoDataFrame\n    return gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df.x, df.y), crs=crs)\n</code></pre>"},{"location":"reference/readers_geospatial/#readers_geospatial.read_province_shapes","title":"<code>read_province_shapes(shape_file)</code>","text":"<p>Read the province shape files</p> <p>Parameters:</p> Name Type Description Default <code>shape_file</code> <code>PathLike</code> <p>the path to the .shp file &amp; co</p> required <p>Returns:</p> Type Description <code>GeoDataFrame</code> <p>gpd.GeoDataFrame: the province shapes as a GeoDataFrame</p> Source code in <code>workflow/scripts/readers_geospatial.py</code> <pre><code>def read_province_shapes(shape_file: os.PathLike) -&gt; gpd.GeoDataFrame:\n    \"\"\"Read the province shape files\n\n    Args:\n        shape_file (os.PathLike): the path to the .shp file &amp; co\n\n    Returns:\n        gpd.GeoDataFrame: the province shapes as a GeoDataFrame\n    \"\"\"\n\n    prov_shapes = gpd.GeoDataFrame.from_file(shape_file)\n    prov_shapes = prov_shapes.to_crs(CRS)\n    prov_shapes.set_index(\"province\", inplace=True)\n    # TODO: does this make sense? reindex after?\n    if not (prov_shapes.sort_index().index == sorted(PROV_NAMES)).all():\n        missing = f\"Missing provinces: {set(PROV_NAMES) - set(prov_shapes.index)}\"\n        raise ValueError(f\"Province names do not match expected names: missing {missing}\")\n\n    return prov_shapes\n</code></pre>"},{"location":"reference/readers_geospatial/#readers_geospatial.read_raster","title":"<code>read_raster(path, clip_shape=None, var_name='var', chunks=60, plot=False)</code>","text":"<p>Read raster data and optionally clip it to a given shape.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>PathLike</code> <p>The path to the raster file.</p> required <code>clip_shape</code> <code>GeoSeries</code> <p>The shape to clip the raster data. Defaults to None.</p> <code>None</code> <code>var_name</code> <code>str</code> <p>The variable name to assign to the raster data. Defaults to \"var\".</p> <code>'var'</code> <code>chunks</code> <code>int</code> <p>The chunk size for the raster data. Defaults to 60.</p> <code>60</code> <code>plot</code> <code>bool</code> <p>Whether to plot the raster data. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>DataArray</code> <code>DataArray</code> <p>The raster data as an xarray DataArray.</p> Source code in <code>workflow/scripts/readers_geospatial.py</code> <pre><code>def read_raster(\n    path: os.PathLike,\n    clip_shape: gpd.GeoSeries = None,\n    var_name=\"var\",\n    chunks=60,\n    plot=False,\n) -&gt; DataArray:\n    \"\"\"Read raster data and optionally clip it to a given shape.\n\n    Args:\n        path (os.PathLike): The path to the raster file.\n        clip_shape (gpd.GeoSeries, optional): The shape to clip the raster data. Defaults to None.\n        var_name (str, optional): The variable name to assign to the raster data. Defaults to \"var\".\n        chunks (int, optional): The chunk size for the raster data. Defaults to 60.\n        plot (bool, optional): Whether to plot the raster data. Defaults to False.\n\n    Returns:\n        DataArray: The raster data as an xarray DataArray.\n    \"\"\"\n    ds = rioxarray.open_rasterio(path, chunks=chunks, default_name=\"pop_density\")\n    ds = ds.rename(var_name)\n\n    if clip_shape is not None:\n        ds = ds.rio.clip(clip_shape.geometry)\n\n    if plot:\n        ds.plot()\n\n    return ds\n</code></pre>"},{"location":"reference/solve_network/","title":"Solve network","text":"<p>Functions to add constraints and prepare the network for the solver. Associated with the <code>solve_networks</code> rule in the Snakefile.</p>"},{"location":"reference/solve_network/#solve_network.add_battery_constraints","title":"<code>add_battery_constraints(n)</code>","text":"<p>Add constraint ensuring that charger = discharger, i.e. 1 * charger_size - efficiency * discharger_size = 0</p> Source code in <code>workflow/scripts/solve_network.py</code> <pre><code>def add_battery_constraints(n: pypsa.Network):\n    \"\"\"\n    Add constraint ensuring that charger = discharger, i.e.\n    1 * charger_size - efficiency * discharger_size = 0\n    \"\"\"\n    if not n.links.p_nom_extendable.any():\n        return\n\n    discharger_bool = n.links.index.str.contains(\"battery discharger\")\n    charger_bool = n.links.index.str.contains(\"battery charger\")\n\n    dischargers_ext = n.links[discharger_bool].query(\"p_nom_extendable\").index\n    chargers_ext = n.links[charger_bool].query(\"p_nom_extendable\").index\n\n    eff = n.links.efficiency[dischargers_ext].values\n    lhs = n.model[\"Link-p_nom\"].loc[chargers_ext] - n.model[\"Link-p_nom\"].loc[dischargers_ext] * eff\n\n    n.model.add_constraints(lhs == 0, name=\"Link-charger_ratio\")\n</code></pre>"},{"location":"reference/solve_network/#solve_network.add_chp_constraints","title":"<code>add_chp_constraints(n)</code>","text":"<p>Add constraints to couple the heat and electricity output of CHP plants.   Simplified treatment of extraction plant with Cb/Cv coeffs as per DKEA catalogue.   Ignore the minimum electric power for heat output (aggregate over all units).</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>Network</code> <p>the pypsa network object to which's model the constraints are added</p> required Source code in <code>workflow/scripts/solve_network.py</code> <pre><code>def add_chp_constraints(n: pypsa.Network):\n    \"\"\"Add constraints to couple the heat and electricity output of CHP plants.\n      Simplified treatment of extraction plant with Cb/Cv coeffs as per DKEA catalogue.\n      Ignore the minimum electric power for heat output (aggregate over all units).\n\n\n    Args:\n        n (pypsa.Network): the pypsa network object to which's model the constraints are added\n    \"\"\"\n    electric = n.links.index.str.contains(\"CHP\") &amp; n.links.index.str.contains(\"generator\")\n    heat = n.links.index.str.contains(\"CHP\") &amp; n.links.index.str.contains(\"boiler\")\n\n    electric_ext = n.links[electric].query(\"p_nom_extendable\").index\n    heat_ext = n.links[heat].query(\"p_nom_extendable\").index\n\n    electric_fix = n.links[electric].query(\"~p_nom_extendable\").index\n    heat_fix = n.links[heat].query(\"~p_nom_extendable\").index\n\n    p = n.model[\"Link-p\"]  # dimension: [time, link]\n\n    # output ratio between heat and electricity and top_iso_fuel_line for extendable\n    if not electric_ext.empty:\n        p_nom = n.model[\"Link-p_nom\"]\n\n        lhs = (\n            p_nom.loc[electric_ext]\n            * (n.links.heat_to_power * n.links.efficiency)[electric_ext].values\n            - p_nom.loc[heat_ext] * n.links.efficiency[heat_ext].values\n        )\n        n.model.add_constraints(lhs == 0, name=\"chplink-fix_p_nom_ratio\")\n\n        rename = {\"Link-ext\": \"Link\"}\n        lhs = p.loc[:, electric_ext] + p.loc[:, heat_ext] - p_nom.rename(rename).loc[electric_ext]\n        n.model.add_constraints(lhs &lt;= 0, name=\"chplink-top_iso_fuel_line_ext\")\n\n    # top_iso_fuel_line for fixed\n    if not electric_fix.empty:\n        lhs = p.loc[:, electric_fix] + p.loc[:, heat_fix]\n        rhs = n.links.p_nom[electric_fix]\n        n.model.add_constraints(lhs &lt;= rhs, name=\"chplink-top_iso_fuel_line_fix\")\n\n    # max heat to power ratio (simplified extraction plant w/o min heat output in extraction mode)\n    # heat can drop to zero to represent condensation mode\n    if not n.links[electric].index.empty:\n        lhs = p.loc[:, heat] * n.links.efficiency[heat] - p.loc[:, electric] * n.links.efficiency[\n            electric\n        ] * n.links.heat_to_power[electric].fillna(1)\n        n.model.add_constraints(lhs &lt;= 0, name=\"chplink-backpressure\")\n</code></pre>"},{"location":"reference/solve_network/#solve_network.add_chp_constraints_new_attempt","title":"<code>add_chp_constraints_new_attempt(n)</code>","text":"<p>Add constraints to couple the heat and electricity output of CHP plants.   Simplified treatment of extraction plant with max heat to power ratio.   Ignore the minimum electric power for heat output (aggregate over all units).</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>Network</code> <p>the pypsa network object to which's model the constraints are added</p> required Source code in <code>workflow/scripts/solve_network.py</code> <pre><code>def add_chp_constraints_new_attempt(n: pypsa.Network):\n    \"\"\"Add constraints to couple the heat and electricity output of CHP plants.\n      Simplified treatment of extraction plant with max heat to power ratio.\n      Ignore the minimum electric power for heat output (aggregate over all units).\n\n\n    Args:\n        n (pypsa.Network): the pypsa network object to which's model the constraints are added\n    \"\"\"\n\n    elec = n.links.query(\"index.str.contains('chp', case=False) &amp; index.str.contains('generator')\")\n    heat = n.links.query(\"index.str.contains('chp', case=False) &amp; index.str.contains('boiler')\")\n\n    if not elec.shape == heat.shape:\n        raise ValueError(\n            \"Incorrect definition of CHP units, check link names.\"\n            \" Each 'CHP generator' needs an associated 'CHP boiler'.\"\n        )\n\n    # elec = n.links.index.str.contains(\"CHP\") &amp; n.links.index.str.contains(\"generator\")\n    # heat = n.links.index.str.contains(\"CHP\") &amp; n.links.index.str.contains(\"boiler\")\n\n    elec_ext = sorted(elec.query(\"p_nom_extendable\").index)\n    heat_ext = sorted(heat.query(\"p_nom_extendable\").index)\n\n    elec_fix = elec.query(\"~p_nom_extendable\").index\n    heat_fix = heat.query(\"~p_nom_extendable\").index\n\n    p = n.model[\"Link-p\"]  # dimension: [time, link]\n    p_nom = n.model[\"Link-p_nom\"]\n\n    # maximum capacity of heat set by ratio between heat and electricity\n    if not elec.query(\"p_nom_extendable\").empty:\n        # maximising heat power under constraints that Pel/eta_el + Pth/eta_th &lt;= p_nom_link\n        # &amp; noting that pypsa link powers are defined by input (fuel)\n        # yields: p_nom_heat = p_nom_el * htpr / (htpr+eta_h/eta_el)\n        htpr = n.links.p_nom_ratio[elec_ext].values\n        alpha = n.links.efficiency[heat_ext].values / n.links.efficiency[elec_ext].values\n        lhs = p_nom.loc[elec_ext] * htpr / (htpr + alpha) - p_nom.loc[heat_ext]\n        n.model.add_constraints(lhs == 0, name=\"chplink-fix_max_heat\")\n\n        # top_iso_fuel_line ext (is it still relevant?)\n        rename = {\"Link-ext\": \"Link\"}\n        lhs = p.loc[:, elec_ext] + p.loc[:, heat_ext] - p_nom.rename(rename).loc[elec_ext]\n        n.model.add_constraints(lhs &lt;= 0, name=\"chplink-top_iso_fuel_line_ext\")\n\n    # max heat to power ratio\n    grouper_heat = pd.concat([heat.location, heat.build_year], axis=1)\n    grouper_elec = pd.concat([elec.location, elec.build_year], axis=1)\n    p_heat = n.model[\"Link-p\"].loc[:, heat.index]\n    p_elec = n.model[\"Link-p\"].loc[:, elec.index]\n    lhs = (p_heat * heat.efficiency).groupby(grouper_heat).sum()\n    rhs = (p_elec * elec.efficiency * elec.p_nom_ratio.fillna(1)).groupby(grouper_elec).sum()\n    n.model.add_constraints(lhs &lt;= rhs, name=\"chplink-max_heat_to_power\")\n\n    # top_iso_fuel_line for fixed (is it still relevant?)\n    if not elec_fix.empty:\n        lhs = p.loc[:, elec_fix] + p.loc[:, heat_fix]\n        rhs = n.links.p_nom[elec_fix]\n        n.model.add_constraints(lhs &lt;= rhs, name=\"chplink-top_iso_fuel_line_fix\")\n</code></pre>"},{"location":"reference/solve_network/#solve_network.add_co2_constraints_prices","title":"<code>add_co2_constraints_prices(network, co2_control)</code>","text":"<p>Add co2 constraints or prices</p> <p>Parameters:</p> Name Type Description Default <code>network</code> <code>Network</code> <p>the network to which prices or constraints are to be added</p> required <code>co2_control</code> <code>dict</code> <p>the config</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>unrecognised co2 control option</p> Source code in <code>workflow/scripts/solve_network.py</code> <pre><code>def add_co2_constraints_prices(network: pypsa.Network, co2_control: dict):\n    \"\"\"Add co2 constraints or prices\n\n    Args:\n        network (pypsa.Network): the network to which prices or constraints are to be added\n        co2_control (dict): the config\n\n    Raises:\n        ValueError: unrecognised co2 control option\n    \"\"\"\n\n    if co2_control[\"control\"] is None:\n        pass\n    elif co2_control[\"control\"] == \"price\":\n        logger.info(\"Adding CO2 price to marginal costs of generators and storage units\")\n        add_emission_prices(network, emission_prices={\"co2\": co2_control[\"co2_pr_or_limit\"]})\n\n    elif co2_control[\"control\"].startswith(\"budget\"):\n        co2_limit = co2_control[\"co2_pr_or_limit\"]\n        logger.info(\"Adding CO2 constraint based on scenario {co2_limit}\")\n        network.add(\n            \"GlobalConstraint\",\n            \"co2_limit\",\n            type=\"primary_energy\",\n            carrier_attribute=\"co2_emissions\",\n            sense=\"&lt;=\",\n            constant=co2_limit,\n        )\n    else:\n        logger.error(f\"Unhandled CO2 control config {co2_control} due to unknown control.\")\n        raise ValueError(f\"Unhandled CO2 control config {co2_control} due to unknown control\")\n</code></pre>"},{"location":"reference/solve_network/#solve_network.add_emission_prices","title":"<code>add_emission_prices(n, emission_prices={'co2': 0.0}, exclude_co2=False)</code>","text":"<p>From pypsa-eur: add GHG price to marginal costs of generators and storage units</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>Network</code> <p>the pypsa network</p> required <code>emission_prices</code> <code>dict</code> <p>emission prices per GHG. Defaults to {\"co2\": 0.0}.</p> <code>{'co2': 0.0}</code> <code>exclude_co2</code> <code>bool</code> <p>do not charge for CO2 emissions. Defaults to False.</p> <code>False</code> Source code in <code>workflow/scripts/solve_network.py</code> <pre><code>def add_emission_prices(n: pypsa.Network, emission_prices={\"co2\": 0.0}, exclude_co2=False):\n    \"\"\"From pypsa-eur: add GHG price to marginal costs of generators and storage units\n\n    Args:\n        n (pypsa.Network): the pypsa network\n        emission_prices (dict, optional): emission prices per GHG. Defaults to {\"co2\": 0.0}.\n        exclude_co2 (bool, optional): do not charge for CO2 emissions. Defaults to False.\n    \"\"\"\n    if exclude_co2:\n        emission_prices.pop(\"co2\")\n    em_price = (\n        pd.Series(emission_prices).rename(lambda x: x + \"_emissions\")\n        * n.carriers.filter(like=\"_emissions\")\n    ).sum(axis=1)\n\n    n.meta.update({\"emission_prices\": emission_prices})\n\n    gen_em_price = n.generators.carrier.map(em_price) / n.generators.efficiency\n\n    n.generators[\"marginal_cost\"] += gen_em_price\n    n.generators_t[\"marginal_cost\"] += gen_em_price[n.generators_t[\"marginal_cost\"].columns]\n    # storage units su\n    su_em_price = n.storage_units.carrier.map(em_price) / n.storage_units.efficiency_dispatch\n    n.storage_units[\"marginal_cost\"] += su_em_price\n\n    logger.info(\"Added emission prices to marginal costs of generators and storage units\")\n    logger.info(f\"\\tEmission prices: {emission_prices}\")\n</code></pre>"},{"location":"reference/solve_network/#solve_network.add_land_use_constraint","title":"<code>add_land_use_constraint(n, planning_horizons)</code>","text":"<p>Add land use constraints for renewable energy potential. This ensures that the brownfield +  greenfield vre installations for each generator tech do not exceed the technical potential.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>Network</code> <p>the network object to add the constraints to</p> required <code>planning_horizons</code> <code>str | int</code> <p>the planning horizon year as string</p> required Source code in <code>workflow/scripts/solve_network.py</code> <pre><code>def add_land_use_constraint(n: pypsa.Network, planning_horizons: str | int) -&gt; None:\n    \"\"\"\n    Add land use constraints for renewable energy potential. This ensures that the brownfield +\n     greenfield vre installations for each generator tech do not exceed the technical potential.\n\n    Args:\n        n (pypsa.Network): the network object to add the constraints to\n        planning_horizons (str | int): the planning horizon year as string\n    \"\"\"\n    # warning: this will miss existing offwind which is not classed AC-DC and has carrier 'offwind'\n\n    for carrier in [\n        \"solar\",\n        \"solar thermal\",\n        \"onwind\",\n        \"offwind\",\n        \"offwind-ac\",\n        \"offwind-dc\",\n        \"offwind-float\",\n    ]:\n        ext_i = (n.generators.carrier == carrier) &amp; ~n.generators.p_nom_extendable\n        grouper = n.generators.loc[ext_i].index.str.replace(f\" {carrier}.*$\", \"\", regex=True)\n        existing = n.generators.loc[ext_i, \"p_nom\"].groupby(grouper).sum()\n        existing.index += f\" {carrier}\"\n        n.generators.loc[existing.index, \"p_nom_max\"] -= existing\n\n    # check if existing capacities are larger than technical potential\n    existing_large = n.generators[n.generators[\"p_nom_min\"] &gt; n.generators[\"p_nom_max\"]].index\n    if len(existing_large):\n        logger.warning(\n            f\"Existing capacities larger than technical potential for {existing_large},\\\n                        adjust technical potential to existing capacities\"\n        )\n        n.generators.loc[existing_large, \"p_nom_max\"] = n.generators.loc[\n            existing_large, \"p_nom_min\"\n        ]\n\n    n.generators[\"p_nom_max\"] = n.generators[\"p_nom_max\"].clip(lower=0)\n</code></pre>"},{"location":"reference/solve_network/#solve_network.add_nuclear_expansion_constraints","title":"<code>add_nuclear_expansion_constraints(n)</code>","text":"<p>Add nuclear expansion limit constraint if configured.</p> <p>This function adds a global constraint limiting the total capacity of all extendable nuclear generators. The limit is based on max_annual_capacity_addition.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>Network</code> <p>the network object</p> required Source code in <code>workflow/scripts/solve_network.py</code> <pre><code>def add_nuclear_expansion_constraints(n: pypsa.Network):\n    \"\"\"\n    Add nuclear expansion limit constraint if configured.\n\n    This function adds a global constraint limiting the total capacity of all\n    extendable nuclear generators. The limit is based on max_annual_capacity_addition.\n\n    Args:\n        n (pypsa.Network): the network object\n    \"\"\"\n    nuclear_config = n.config.get(\"nuclear_reactors\", {})\n\n    if not nuclear_config.get(\"enable_growth_limit\", True):\n        return\n\n    nuclear_gens_ext = n.generators[\n        (n.generators.carrier == \"nuclear\") &amp; (n.generators.p_nom_extendable == True)\n    ].index\n\n    if len(nuclear_gens_ext) == 0:\n        return\n\n    limit = nuclear_config.get(\"max_annual_capacity_addition\")\n    if limit is None:\n        return\n\n    n.model.add_constraints(\n        n.model[\"Generator-p_nom\"].loc[nuclear_gens_ext].sum() &lt;= limit,\n        name=\"nuclear_expansion_limit\",\n    )\n</code></pre>"},{"location":"reference/solve_network/#solve_network.add_operational_reserve_margin","title":"<code>add_operational_reserve_margin(n, config)</code>","text":"<p>Build operational reserve margin constraints based on the formulation given in https://genxproject.github.io/GenX.jl/stable/Model_Reference/core/#GenX.operational_reserves_core!-Tuple{JuMP.Model,%20Dict,%20Dict}</p> <p>The constraint is network wide and not at each node!</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>Network</code> <p>the network object to optimize</p> required <code>config</code> <code>dict</code> <p>the configuration dictionary</p> required Example <p>config.yaml requires to specify operational_reserve: operational_reserve:     activate: true     epsilon_load: 0.02 # percentage of load at each snapshot     epsilon_vres: 0.02 # percentage of VRES at each snapshot     contingency: 400000 # MW</p> Source code in <code>workflow/scripts/solve_network.py</code> <pre><code>def add_operational_reserve_margin(n: pypsa.network, config):\n    \"\"\"\n    Build operational reserve margin constraints based on the formulation given in\n    https://genxproject.github.io/GenX.jl/stable/Model_Reference/core/#GenX.operational_reserves_core!-Tuple{JuMP.Model,%20Dict,%20Dict}\n\n    The constraint is network wide and not at each node!\n\n    Args:\n        n (pypsa.Network): the network object to optimize\n        config (dict): the configuration dictionary\n\n    Example:\n        config.yaml requires to specify operational_reserve:\n        operational_reserve:\n            activate: true\n            epsilon_load: 0.02 # percentage of load at each snapshot\n            epsilon_vres: 0.02 # percentage of VRES at each snapshot\n            contingency: 400000 # MW\n    \"\"\"\n    reserve_config = config[\"operational_reserve\"]\n    VRE_TECHS = config[\"Techs\"].get(\"non_dispatchable\", [\"onwind\", \"offwind\", \"solar\"])  # noqa F841\n    EPSILON_LOAD, EPSILON_VRES = reserve_config[\"epsilon_load\"], reserve_config[\"epsilon_vres\"]\n    CONTINGENCY = float(reserve_config[\"contingency\"])\n\n    # AC producers\n    ac_mask = n.generators.bus.map(n.buses.carrier) == \"AC\"\n    ac_buses = n.buses.query(\"carrier =='AC'\").index  # noqa: F841\n    attached_carriers = filter_carriers(n, \"AC\")  # noqa: F841\n    # conceivably a link could have a negative efficiency and flow towards bus0 - don't consider\n    prod_links = n.links.query(\"carrier in @_attached_carriers &amp; not bus0 in @_ac_buses\")\n    transport_links = prod_links.bus0.map(n.buses.carrier) == prod_links.bus1.map(n.buses.carrier)\n    prod_links = prod_links.loc[transport_links == False]\n    prod_gen = n.generators.loc[ac_mask]\n    producers_all = prod_links.index.append(prod_gen.index)\n    producers_all.name = \"Producers-p\"\n\n    # RSERVES\n    n.model.add_variables(0, np.inf, coords=[n.snapshots, prod_gen.index], name=\"Generator-r\")\n    n.model.add_variables(0, np.inf, coords=[n.snapshots, prod_links.index], name=\"Link-r\")\n\n    # Define Reserve and weigh VRES by their mean availability (\"capacity credit\")\n    vres_gen = prod_gen.query(\"carrier in @_VRE_TECHS\")\n    non_vre = prod_gen.index.difference(vres_gen.index)\n    # full capacity credit for non-VRE producers (questionable, maybe should be weighted by availability)\n    summed_reserve = (n.model[\"Link-r\"] * prod_links.efficiency).sum(\"Link\") + n.model[\n        \"Generator-r\"\n    ].loc[:, non_vre].sum(\"Generator\")\n\n    # VRE capacity credit &amp; margin reqs\n    ext_idx = vres_gen.query(\"p_nom_extendable\").index\n    avail = n.generators_t.p_max_pu.loc[:, vres_gen.index]\n    vres_idx = avail.columns\n\n    if not vres_gen.empty:\n        # Reserve score based on actual avail (perfect foresight) not mean/expected avail\n        vre_reserve_score = (n.model[\"Generator-r\"].loc[:, vres_gen.index] * avail).sum(\"Generator\")\n        summed_reserve += vre_reserve_score\n    if not ext_idx.empty and not vres_idx.empty:\n        # reqs from brownfield VRE generators. epsilon is the margin for VRES forecast error\n        avail_factor = n.generators_t.p_max_pu[ext_idx]\n        p_nom_vres = n.model[\"Generator-p_nom\"].loc[ext_idx].rename({\"Generator-ext\": \"Generator\"})\n        vre_req_ext = (p_nom_vres * (EPSILON_VRES * xr.DataArray(avail_factor))).sum(\"Generator\")\n    else:\n        vre_req_ext = 0\n\n    if not vres_idx.empty:\n        # reqs extendable VRE generators\n        avail_factor = n.generators_t.p_max_pu[vres_idx.difference(ext_idx)]\n        renewable_capacity = n.generators.p_nom[vres_idx.difference(ext_idx)]\n        vre_req_fix = (avail_factor * renewable_capacity).sum(axis=1)\n    else:\n        vre_req_fix = 0\n\n    lhs = summed_reserve - vre_req_ext\n    # Right-hand-side\n    demand = get_as_dense(n, \"Load\", \"p_set\").sum(axis=1)\n    rhs = EPSILON_LOAD * demand + EPSILON_VRES * vre_req_fix + CONTINGENCY\n\n    n.model.add_constraints(lhs &gt;= rhs, name=\"Reserve-margin\")\n\n    # Need additional constraints (reserve + dispatch &lt;= p_nom): gen_r + gen_p &lt;= gen_p_nom (capacity)\n    to_constrain = {\"Link\": prod_links, \"Generator\": prod_gen}\n    for component, producer in to_constrain.items():\n        logger.info(f\"adding secondary reserve constraint for {component}s\")\n\n        fix_i = producer.query(\"p_nom_extendable==False\").index\n        ext_i = producer.query(\"p_nom_extendable==True\").index\n\n        dispatch = n.model[f\"{component}-p\"].loc[:, producer.index]\n        reserve = n.model[f\"{component}-r\"].loc[:, ext_i.union(fix_i)]\n\n        capacity_variable = n.model[f\"{component}-p_nom\"].loc[ext_i]\n        capacity_variable = capacity_variable.rename({f\"{component}-ext\": f\"{component}\"})\n        capacity_fixed = getattr(n, component.lower() + \"s\").p_nom[fix_i]\n\n        p_max_pu = get_as_dense(n, f\"{component}\", \"p_max_pu\")\n\n        lhs = dispatch + reserve\n        # MAY have to check what happens in case pmaxpu is not defined for all items\n        rhs = capacity_variable * p_max_pu[ext_i] + (p_max_pu[fix_i] * capacity_fixed)\n        n.model.add_constraints(\n            lhs - rhs.loc[lhs.indexes] &lt;= 0, name=f\"{component}-p-reserve-upper\"\n        )\n</code></pre>"},{"location":"reference/solve_network/#solve_network.add_remind_paid_off_constraints","title":"<code>add_remind_paid_off_constraints(n)</code>","text":"<p>Paid-off components can be placed wherever PyPSA wants but have a total limit.</p> <p>Add constraints to ensure that the paid off capacity from REMIND is not exceeded across the network &amp; that it does not exceed the technical potential.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>Network</code> <p>the network object to which's model the constraints are added</p> required Source code in <code>workflow/scripts/solve_network.py</code> <pre><code>def add_remind_paid_off_constraints(n: pypsa.Network) -&gt; None:\n    \"\"\"\n    Paid-off components can be placed wherever PyPSA wants but have a total limit.\n\n    Add constraints to ensure that the paid off capacity from REMIND is not\n    exceeded across the network &amp; that it does not exceed the technical potential.\n\n    Args:\n        n (pypsa.Network): the network object to which's model the constraints are added\n    \"\"\"\n\n    if not n.config[\"run\"].get(\"is_remind_coupled\", False):\n        logger.info(\"Skipping paid off constraints as REMIND is not coupled\")\n        return\n\n    # In coupled-mode components (Generators, Links,..) have limits p/e_nom_rcl &amp; a tech_group\n    # These columns are added by `add_existing_baseyear.add_paid_off_capacity`.\n    # p/e_nom_rcl is the availale paid-off capacity per tech group and is nan for non paid-off (usual) generators.\n    # rcl is a legacy name from Aodenweller\n    for component in [\"Generator\", \"Link\", \"Store\"]:\n        prefix = \"e\" if component == \"Store\" else \"p\"\n        paid_off_col = f\"{prefix}_nom_max_rcl\"\n\n        paid_off = getattr(n, component.lower() + \"s\").copy()\n        # if there are no paid_off components\n        if paid_off_col not in paid_off.columns:\n            continue\n        else:\n            paid_off.dropna(subset=[paid_off_col], inplace=True)\n\n        paid_off_totals = paid_off.set_index(\"tech_group\")[paid_off_col].drop_duplicates()\n\n        # LHS: p_nom per technology grp &lt; totals\n        groupers = [paid_off[\"tech_group\"]]\n        grouper_lhs = xr.DataArray(pd.MultiIndex.from_arrays(groupers), dims=[f\"{component}-ext\"])\n        p_nom_groups = (\n            n.model[f\"{component}-{prefix}_nom\"].loc[paid_off.index].groupby(grouper_lhs).sum()\n        )\n\n        # get indices to sort RHS. the grouper is multi-indexed (legacy from PyPSA-Eur)\n        idx = p_nom_groups.indexes[\"group\"]\n        idx = [x[0] for x in idx]\n\n        # Add constraint\n        if not p_nom_groups.empty:\n            n.model.add_constraints(\n                p_nom_groups &lt;= paid_off_totals[idx].values,\n                name=f\"paidoff_cap_totals_{component.lower()}\",\n            )\n\n    # === ensure normal e/p_nom_max is respected for (paid_off + normal) components\n    # e.g. if PV has 100MW tech potential at nodeA, paid_off+normal p_nom_opt &lt;100MW\n    for component in [\"Generator\", \"Link\", \"Store\"]:\n        paidoff_comp = getattr(n, component.lower() + \"s\").copy()\n\n        prefix = \"e\" if component == \"Store\" else \"p\"\n        paid_off_col = f\"{prefix}_nom_max_rcl\"\n        # if there are no paid_off components\n        if paid_off_col not in paidoff_comp.columns:\n            continue\n        else:\n            paidoff_comp.dropna(subset=[paid_off_col], inplace=True)\n\n        # techs that only exist as paid-off don't have usual counterparts\n        remind_only = n.config[\"existing_capacities\"].get(\"remind_only_tech_groups\", [])  # noqa: F841\n        paidoff_comp = paidoff_comp.query(\"tech_group not in @remind_only\")\n\n        if paidoff_comp.empty:\n            continue\n\n        # find equivalent usual (not paid-off) components\n        usual_comps_idx = paidoff_comp.index.str.replace(\"_paid_off\", \"\")\n        usual_comps = getattr(n, component.lower() + \"s\").loc[usual_comps_idx].copy()\n        usual_comps = usual_comps[~usual_comps.p_nom_max.isin([np.inf, np.nan])]\n\n        paid_off_wlimits = paidoff_comp.loc[usual_comps.index + \"_paid_off\"]\n        to_constrain = pd.concat([usual_comps, paid_off_wlimits], axis=0)\n        if to_constrain.empty:\n            continue\n        to_constrain.rename_axis(index=f\"{component}-ext\", inplace=True)\n        # otherwise n.model query will fail. This is needed in case freeze_compoents was used\n        # it is fine so long as p_nom is zero for the frozen components\n        to_constrain = to_constrain.query(\"p_nom_extendable==True\")\n        to_constrain[\"grouper\"] = to_constrain.index.str.replace(\"_paid_off\", \"\")\n\n        grouper = xr.DataArray(to_constrain.grouper, dims=[f\"{component}-ext\"])\n\n        lhs = n.model[f\"{component}-{prefix}_nom\"].loc[to_constrain.index].groupby(grouper).sum()\n        # RHS\n        idx = lhs.indexes[\"grouper\"]\n\n        if not lhs.empty:\n            n.model.add_constraints(\n                lhs &lt;= usual_comps.loc[idx].p_nom_max.values,\n                name=f\"constrain_paidoff&amp;usual_{component}_potential\",\n            )\n</code></pre>"},{"location":"reference/solve_network/#solve_network.add_transmission_constraints","title":"<code>add_transmission_constraints(n)</code>","text":"<p>Add constraint ensuring that transmission lines p_nom are the same for both directions, i.e. p_nom positive = p_nom negative</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>Network</code> <p>the network object to optimize</p> required Source code in <code>workflow/scripts/solve_network.py</code> <pre><code>def add_transmission_constraints(n: pypsa.Network):\n    \"\"\"\n    Add constraint ensuring that transmission lines p_nom are the same for both directions, i.e.\n    p_nom positive = p_nom negative\n\n    Args:\n        n (pypsa.Network): the network object to optimize\n    \"\"\"\n\n    if not n.links.p_nom_extendable.any():\n        return\n\n    positive_bool = n.links.index.str.contains(\"positive\")\n    negative_bool = n.links.index.str.contains(\"reversed\")\n\n    positive_ext = n.links[positive_bool].query(\"p_nom_extendable\").index\n    negative_ext = n.links[negative_bool].query(\"p_nom_extendable\").index\n\n    lhs = n.model[\"Link-p_nom\"].loc[positive_ext]\n    rhs = n.model[\"Link-p_nom\"].loc[negative_ext]\n\n    n.model.add_constraints(lhs == rhs, name=\"Link-transmission\")\n</code></pre>"},{"location":"reference/solve_network/#solve_network.add_water_tank_charger_constraints","title":"<code>add_water_tank_charger_constraints(n, config)</code>","text":"<p>Add constraint ensuring that centra water tank charger = discharger &amp; limit p_nom/e_nom ratio, i.e.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>Network</code> <p>the network object to optimize</p> required <code>config</code> <code>dict</code> <p>the snakemake configuration dictionary</p> required Source code in <code>workflow/scripts/solve_network.py</code> <pre><code>def add_water_tank_charger_constraints(n: pypsa.Network, config: dict):\n    \"\"\"\n    Add constraint ensuring that centra water tank charger = discharger &amp; limit p_nom/e_nom ratio, i.e.\n\n    Args:\n        n (pypsa.Network): the network object to optimize\n        config (dict): the snakemake configuration dictionary\n    \"\"\"\n\n    discharger_bool = n.links.index.str.contains(\n        \"water tanks discharger &amp; not index.str.contains('decentral')\"\n    )\n    charger_bool = n.links.index.str.contains(\n        \"water tanks charger &amp; not index.str.contains('decentral')\"\n    )\n\n    dischargers_ext = n.links[discharger_bool].query(\"p_nom_extendable\").index\n    chargers_ext = n.links[charger_bool].query(\"p_nom_extendable\").index\n\n    eff = n.links.efficiency[dischargers_ext].values\n    lhs = n.model[\"Link-p_nom\"].loc[chargers_ext] - n.model[\"Link-p_nom\"].loc[dischargers_ext] * eff\n\n    n.model.add_constraints(lhs == 0, name=\"Link-water-tank-charger_ratio\")\n\n    # limit the p_nom/e_nom ratio\n    central_tanks = n.stores.query(\"carrier == 'water tanks' &amp; not index.str.contains('decentral')\")\n    central_dischargers_ext = n.links.query(\n        \"carrier == 'water tanks' and index.str.contains('discharger') and not index.str.contains('decentral')\"\n    ).query(\"p_nom_extendable\")\n\n    grouper_s = central_tanks.location.rename_axis(\"Store-ext\")\n    grouper_l = central_dischargers_ext.rename_axis(\"Link-ext\").location\n    p_nom_over_e_nom = config[\"water_tanks\"].get(\"p_nom_over_e_nom\", 0.2)\n    lhs = (\n        n.model[\"Store-e_nom\"].loc[central_tanks.index].groupby(grouper_s).sum() * p_nom_over_e_nom\n        - n.model[\"Link-p_nom\"].loc[central_dischargers_ext.index].groupby(grouper_l).sum()\n    )\n    n.model.add_constraints(lhs &gt;= 0, name=\"Central_Water_Tank_p_nom_over_e_nom\")\n\n    # limit the p_nom/e_nom ratio to 1 for decentral tanks\n    decentral_tanks = n.stores.query(\"carrier == 'water tanks' &amp; index.str.contains('decentral')\")\n    decentral_dischargers_ext = n.links.query(\n        \"carrier == 'water tanks' and index.str.contains('discharger') and index.str.contains('decentral')\"\n    ).query(\"p_nom_extendable\")\n    decentral_chargers_ext = n.links.query(\n        \"carrier == 'water tanks' and not index.str.contains('discharger') and index.str.contains('decentral')\"\n    ).query(\"p_nom_extendable\")\n\n    grouper_s = decentral_tanks.location.rename_axis(\"Store-ext\")\n    grouper_l = decentral_dischargers_ext.rename_axis(\"Link-ext\").location\n    lhs = (\n        n.model[\"Store-e_nom\"].loc[decentral_tanks.index].groupby(grouper_s).sum()\n        - n.model[\"Link-p_nom\"].loc[decentral_dischargers_ext.index].groupby(grouper_l).sum()\n    )\n\n    n.model.add_constraints(lhs &gt;= 0, name=\"Decentral_Water_Tank_p_nom_over_e_nom\")\n    lhs = (\n        n.model[\"Store-e_nom\"].loc[decentral_tanks.index].groupby(grouper_s).sum()\n        - n.model[\"Link-p_nom\"].loc[decentral_dischargers_ext.index].groupby(grouper_l).sum()\n    )\n    grouper_l = decentral_chargers_ext.rename_axis(\"Link-ext\").location\n    lhs = (\n        n.model[\"Store-e_nom\"].loc[decentral_tanks.index].groupby(grouper_s).sum()\n        - n.model[\"Link-p_nom\"].loc[decentral_chargers_ext.index].groupby(grouper_l).sum()\n    )\n    n.model.add_constraints(lhs &gt;= 0, name=\"Decentral_Water_Tank_p_nom_over_e_nom_charger\")\n</code></pre>"},{"location":"reference/solve_network/#solve_network.calc_nuclear_expansion_limit","title":"<code>calc_nuclear_expansion_limit(n, config, planning_year, network_path)</code>","text":"<p>Calculate and apply the nuclear expansion limit from configuration.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>Network</code> <p>the network object</p> required <code>config</code> <code>dict</code> <p>full configuration dictionary (mutated in place)</p> required <code>planning_year</code> <code>int</code> <p>target planning horizon year</p> required <code>network_path</code> <code>str</code> <p>path to the current network file, used to locate base year</p> required Source code in <code>workflow/scripts/solve_network.py</code> <pre><code>def calc_nuclear_expansion_limit(\n    n: pypsa.Network,\n    config: dict,\n    planning_year: int,\n    network_path: str,\n) -&gt; None:\n    \"\"\"\n    Calculate and apply the nuclear expansion limit from configuration.\n\n    Args:\n        n (pypsa.Network): the network object\n        config (dict): full configuration dictionary (mutated in place)\n        planning_year (int): target planning horizon year\n        network_path (str): path to the current network file, used to locate base year\n    \"\"\"\n    nuclear_cfg = config.setdefault(\"nuclear_reactors\", {})\n\n    if not nuclear_cfg.get(\"enable_growth_limit\", True):\n        return\n\n    annual_addition = nuclear_cfg.get(\"max_annual_capacity_addition\")\n    base_year = nuclear_cfg.get(\"base_year\", 2020)\n    n_years = planning_year - base_year\n\n    if not annual_addition or n_years &lt;= 0:\n        return\n\n    base_capacity = nuclear_cfg.get(\"base_capacity\")\n    if base_capacity is None:\n        base_path = network_path.replace(f\"ntwk_{planning_year}.nc\", f\"ntwk_{base_year}.nc\")\n        if os.path.exists(base_path):\n            n_base = pypsa.Network(base_path)\n            base_capacity = n_base.generators[n_base.generators.carrier == \"nuclear\"][\"p_nom\"].sum()\n        else:\n            base_capacity = n.generators[n.generators.carrier == \"nuclear\"][\"p_nom\"].sum()\n\n    max_capacity = base_capacity + annual_addition * n_years\n    nuclear_gens_ext = n.generators[\n        (n.generators.carrier == \"nuclear\") &amp; (n.generators.p_nom_extendable == True)\n    ].index\n\n    if len(nuclear_gens_ext) &gt; 0:\n        logger.info(\n            f\"Nuclear expansion limit for {planning_year}: {max_capacity:.0f} MW \"\n            f\"[{base_capacity:.0f} + {annual_addition:.0f} \u00d7 {n_years} years]\"\n        )\n</code></pre>"},{"location":"reference/solve_network/#solve_network.extra_functionality","title":"<code>extra_functionality(n, _)</code>","text":"<p>Add supplementary constraints to the network model. <code>pypsa.linopf.network_lopf</code>. If you want to enforce additional custom constraints, this is a good location to add them. The arguments <code>opts</code> and <code>snakemake.config</code> are expected to be attached to the network.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>Network</code> <p>the network object to optimize</p> required <code>_</code> <p>dummy for compatibility with pypsa solve</p> required Source code in <code>workflow/scripts/solve_network.py</code> <pre><code>def extra_functionality(n: pypsa.Network, _) -&gt; None:\n    \"\"\"\n    Add supplementary constraints to the network model. ``pypsa.linopf.network_lopf``.\n    If you want to enforce additional custom constraints, this is a good location to add them.\n    The arguments ``opts`` and ``snakemake.config`` are expected to be attached to the network.\n\n    Args:\n        n (pypsa.Network): the network object to optimize\n        _: dummy for compatibility with pypsa solve\n    \"\"\"\n    config = n.config\n    add_battery_constraints(n)\n    add_transmission_constraints(n)\n    add_nuclear_expansion_constraints(n)\n\n    if config[\"heat_coupling\"]:\n        add_water_tank_charger_constraints(n, config)\n        add_chp_constraints(n)\n        # add_chp_constraints_new_attempt(n)\n    if config[\"run\"].get(\"is_remind_coupled\", False):\n        logger.info(\"Adding remind paid off constraints\")\n        add_remind_paid_off_constraints(n)\n\n    reserve = config.get(\"operational_reserve\", {})\n    if reserve.get(\"activate\", False):\n        logger.info(\"Adding operational reserve margin constraints\")\n        add_operational_reserve_margin(n, config)\n\n    logger.info(\"Added extra functionality to the network model\")\n</code></pre>"},{"location":"reference/solve_network/#solve_network.freeze_components","title":"<code>freeze_components(n, config, exclude=['H2 turbine'])</code>","text":"<p>Set p_nom_extendable=False for the components in the network. Applies to vre_techs and conventional technologies not in the exclude list.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>Network</code> <p>the network object</p> required <code>config</code> <code>dict</code> <p>the configuration dictionary</p> required <code>exclude</code> <code>list</code> <p>list of technologies to exclude from freezing. Defaults to [\"OCGT\"]</p> <code>['H2 turbine']</code> Source code in <code>workflow/scripts/solve_network.py</code> <pre><code>def freeze_components(n: pypsa.Network, config: dict, exclude: list = [\"H2 turbine\"]):\n    \"\"\"Set p_nom_extendable=False for the components in the network.\n    Applies to vre_techs and conventional technologies not in the exclude list.\n\n    Args:\n        n (pypsa.Network): the network object\n        config (dict): the configuration dictionary\n        exclude (list, optional): list of technologies to exclude from freezing.\n            Defaults to [\"OCGT\"]\n    \"\"\"\n\n    # Freeze VRE and conventional techs\n    freeze = config[\"Techs\"][\"vre_techs\"] + config[\"Techs\"][\"conv_techs\"]\n    freeze = [f for f in freeze if f not in exclude]\n    if \"coal boiler\" in freeze:\n        freeze += [\"coal boiler central\", \"coal boiler decentral\"]\n    if \"gas boiler\" in freeze:\n        freeze += [\"gas boiler central\", \"gas boiler decentral\"]\n\n    # very ugly -&gt; how to make more robust?\n    to_fix = {\n        \"OCGT\": \"gas OCGT\",\n        \"CCGT\": \"gas CCGT\",\n        \"CCGT-CCS\": \"gas ccs\",\n        \"coal power plant\": \"coal\",\n        \"coal-CCS\": \"coal ccs\",\n    }\n    freeze += [to_fix[k] for k in to_fix if k in freeze]\n\n    for comp in [\"generators\", \"links\"]:\n        query = \"carrier in @freeze &amp; p_nom_extendable == True\"\n        components = getattr(n, comp)\n        # p_nom_max_rcl.isna(): exclude paid_off as needed\n        if \"p_nom_max_rcl\" in components.columns:\n            query += \" &amp; p_nom_max_rcl.isna()\"\n        mask = components.query(query).index\n        components.loc[mask, \"p_nom_extendable\"] = False\n</code></pre>"},{"location":"reference/solve_network/#solve_network.prepare_network","title":"<code>prepare_network(n, solve_opts, config, plan_year, co2_pathway)</code>","text":"<p>Prepare the network for the solver,</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>Network</code> <p>the network object to optimize</p> required <code>solve_opts</code> <code>dict</code> <p>solving options</p> required <code>config</code> <code>dict</code> <p>the snakemake configuration dictionary</p> required <code>plan_year</code> <code>int</code> <p>planning horizon year for which network is solved</p> required <code>co2_pathway</code> <code>str</code> <p>the CO2 pathway name to use</p> required <p>Returns:</p> Type Description <code>Network</code> <p>pypsa.Network: network object with additional constraints</p> Source code in <code>workflow/scripts/solve_network.py</code> <pre><code>def prepare_network(\n    n: pypsa.Network, solve_opts: dict, config: dict, plan_year: int, co2_pathway: str\n) -&gt; pypsa.Network:\n    \"\"\"Prepare the network for the solver,\n\n    Args:\n        n (pypsa.Network): the network object to optimize\n        solve_opts (dict): solving options\n        config (dict): the snakemake configuration dictionary\n        plan_year (int): planning horizon year for which network is solved\n        co2_pathway (str): the CO2 pathway name to use\n\n    Returns:\n        pypsa.Network: network object with additional constraints\n    \"\"\"\n\n    co2_opts = ConfigManager(config).fetch_co2_restriction(co2_pathway, int(plan_year))\n    add_co2_constraints_prices(n, co2_opts)\n\n    if \"clip_p_max_pu\" in solve_opts:\n        for df in (n.generators_t.p_max_pu, n.storage_units_t.inflow):\n            df.where(df &gt; solve_opts[\"clip_p_max_pu\"], other=0.0, inplace=True)\n\n    # TODO duplicated with freeze components\n    if solve_opts.get(\"load_shedding\"):\n        n.add(\"Carrier\", \"Load Shedding\")\n        buses_i = n.buses.query(\"carrier == 'AC'\").index\n        n.add(\n            \"Generator\",\n            buses_i,\n            \" load\",\n            bus=buses_i,\n            carrier=\"Load Shedding\",\n            marginal_cost=solve_opts.get(\"voll\", 1e5),  # EUR/MWh\n            # intersect between macroeconomic and surveybased\n            # willingness to pay\n            # http://journal.frontiersin.org/article/10.3389/fenrg.2015.00055/full\n            p_nom=1e6,  # MW\n        )\n\n    if solve_opts.get(\"noisy_costs\"):\n        for t in n.iterate_components(n.one_port_components):\n            # if 'capital_cost' in t.df:\n            #    t.df['capital_cost'] += 1e1 + 2.*(np.random.random(len(t.df)) - 0.5)\n            if \"marginal_cost\" in t.df:\n                t.df[\"marginal_cost\"] += 1e-2 + 2e-3 * (np.random.random(len(t.df)) - 0.5)\n\n        for t in n.iterate_components([\"Line\", \"Link\"]):\n            t.df[\"capital_cost\"] += (1e-1 + 2e-2 * (np.random.random(len(t.df)) - 0.5)) * t.df[\n                \"length\"\n            ]\n\n    if config[\"run\"].get(\"is_remind_coupled\", False) &amp; (\n        config[\"existing_capacities\"].get(\"freeze_new\", False)\n    ):\n        freeze_components(\n            n,\n            config,\n            exclude=config[\"existing_capacities\"].get(\"never_freeze\", []),\n        )\n\n    if solve_opts.get(\"nhours\"):\n        nhours = solve_opts[\"nhours\"]\n        n.set_snapshots(n.snapshots[:nhours])\n        n.snapshot_weightings[:] = YEAR_HRS / nhours\n\n    if config[\"existing_capacities\"].get(\"add\", False):\n        add_land_use_constraint(n, plan_year)\n\n    return n\n</code></pre>"},{"location":"reference/solve_network/#solve_network.set_transmission_limit","title":"<code>set_transmission_limit(n, kind, factor, n_years=1)</code>","text":"<p>Set global transimission limit constraints - adapted from pypsa-eur</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>Network</code> <p>the network object</p> required <code>kind</code> <code>str</code> <p>the kind of limit to set, either 'c' for cost or 'v' for volume or l for length</p> required <code>factor</code> <code>float or str</code> <p>the factor to apply to the base year quantity, per year</p> required <code>n_years</code> <code>int</code> <p>the number of years to consider for the limit. Defaults to 1.</p> <code>1</code> Source code in <code>workflow/scripts/solve_network.py</code> <pre><code>def set_transmission_limit(n: pypsa.Network, kind: str, factor: float, n_years=1):\n    \"\"\"\n    Set global transimission limit constraints - adapted from pypsa-eur\n\n    Args:\n        n (pypsa.Network): the network object\n        kind (str): the kind of limit to set, either 'c' for cost or 'v' for volume or l for length\n        factor (float or str): the factor to apply to the base year quantity, per year\n        n_years (int, optional): the number of years to consider for the limit. Defaults to 1.\n    \"\"\"\n    logger.info(\n        f\"Adding global transmission limit for {kind} with factor {factor}/year &amp; {n_years} years\"\n    )\n    links_dc = n.links.query(\"carrier in ['AC','DC']\").index\n    # links_dc_rev = n.links.query(\"carrier in ['AC','DC'] &amp; Link.str.contains('reverse')\").index\n\n    _lines_s_nom = (\n        np.sqrt(3)\n        * n.lines.type.map(n.line_types.i_nom)\n        * n.lines.num_parallel\n        * n.lines.bus0.map(n.buses.v_nom)\n    )\n    lines_s_nom = n.lines.s_nom.where(n.lines.type == \"\", _lines_s_nom)\n\n    col = \"capital_cost\" if kind == \"c\" else \"length\"\n    ref = lines_s_nom @ n.lines[col] + n.links.loc[links_dc, \"p_nom\"] @ n.links.loc[links_dc, col]\n\n    if factor == \"opt\" or float(factor) ** n_years &gt; 1.0:\n        n.lines[\"s_nom_min\"] = lines_s_nom\n        n.lines[\"s_nom_extendable\"] = True\n\n        n.links.loc[links_dc, \"p_nom_extendable\"] = True\n\n    elif float(factor) ** n_years == 1.0:\n        # if factor is 1.0, then we do not need to extend\n        n.lines[\"s_nom_min\"] = lines_s_nom\n        n.lines[\"s_nom_extendable\"] = False\n        n.links.loc[links_dc, \"p_nom_extendable\"] = False\n\n        # factor = 1 + 1e-7  # to avoid numerical issues with the constraints\n\n    elif float(factor) ** n_years &lt; 1.0:\n        n.lines[\"s_nom_min\"] = 0\n        n.links.loc[links_dc, \"p_nom_min\"] = 0\n        # n.links.loc[links_dc_rev, \"p_nom_min\"] = 0\n\n    if factor != \"opt\":\n        con_type = \"expansion_cost\" if kind == \"c\" else \"volume_expansion\"\n        rhs = float(factor) ** n_years * ref\n        logger.info(\n            f\"Adding global transmission limit for {kind} to {float(factor) ** n_years} current value\"\n        )\n        n.add(\n            \"GlobalConstraint\",\n            f\"l{kind}_limit\",\n            type=f\"transmission_{con_type}_limit\",\n            sense=\"&lt;=\",\n            constant=rhs,\n            carrier_attribute=\"AC, DC\",\n        )\n</code></pre>"},{"location":"reference/solve_network/#solve_network.solve_network","title":"<code>solve_network(n, config, solving, opts='', **kwargs)</code>","text":"<p>Perform the optimisation Args:     n (pypsa.Network): the pypsa network object     config (dict): the configuration dictionary     solving (dict): the solving configuration dictionary     opts (str): optional wildcards such as ll (not used in pypsa-china)</p> <p>Returns:</p> Type Description <code>Network</code> <p>pypsa.Network: the optimized network</p> Source code in <code>workflow/scripts/solve_network.py</code> <pre><code>def solve_network(\n    n: pypsa.Network, config: dict, solving: dict, opts: str = \"\", **kwargs\n) -&gt; pypsa.Network:\n    \"\"\"Perform the optimisation\n    Args:\n        n (pypsa.Network): the pypsa network object\n        config (dict): the configuration dictionary\n        solving (dict): the solving configuration dictionary\n        opts (str): optional wildcards such as ll (not used in pypsa-china)\n\n    Returns:\n        pypsa.Network: the optimized network\n    \"\"\"\n    set_of_options = solving[\"solver\"][\"options\"]\n    solver_options = solving[\"solver_options\"][set_of_options] if set_of_options else {}\n    solver_name = solving[\"solver\"][\"name\"]\n    cf_solving = solving[\"options\"]\n    track_iterations = cf_solving.get(\"track_iterations\", False)\n    min_iterations = cf_solving.get(\"min_iterations\", 4)\n    max_iterations = cf_solving.get(\"max_iterations\", 6)\n    transmission_losses = cf_solving.get(\"transmission_losses\", 0)\n\n    # add to network for extra_functionality\n    n.config = config\n    n.opts = opts\n\n    skip_iterations = cf_solving.get(\"skip_iterations\", False)\n    if not n.lines.s_nom_extendable.any():\n        skip_iterations = True\n        logger.info(\"No expandable lines found. Skipping iterative solving.\")\n\n    if skip_iterations:\n        status, condition = n.optimize(\n            solver_name=solver_name,\n            transmission_losses=transmission_losses,\n            extra_functionality=extra_functionality,\n            **solver_options,\n            **kwargs,\n        )\n    else:\n        status, condition = n.optimize.optimize_transmission_expansion_iteratively(\n            solver_name=solver_name,\n            track_iterations=track_iterations,\n            min_iterations=min_iterations,\n            max_iterations=max_iterations,\n            transmission_losses=transmission_losses,\n            extra_functionality=extra_functionality,\n            **solver_options,\n            **kwargs,\n        )\n\n    if status != \"ok\":\n        logger.warning(f\"Solving status '{status}' with termination condition '{condition}'\")\n    if \"infeasible\" in condition:\n        raise RuntimeError(\"Solving status 'infeasible'\")\n\n    return n\n</code></pre>"},{"location":"reference/remind_coupling/disaggregate_data/","title":"Disaggregate data","text":"<p>generic disaggregation development Split steps into:</p> <ul> <li>ETL</li> <li>disagg (also an ETL op)</li> </ul> <p>to be rebalanced with the remind_coupling package</p>"},{"location":"reference/remind_coupling/disaggregate_data/#remind_coupling.disaggregate_data.add_possible_techs_to_paidoff","title":"<code>add_possible_techs_to_paidoff(paidoff, tech_groups)</code>","text":"<p>Add possible PyPSA technologies to the paid off capacities DataFrame. The paidoff capacities are grouped in case the Remind-PyPSA tecg mapping is not 1:1 but the network needs to add PyPSA techs. A constraint is added so the paid off caps per group are not exceeded.</p> <p>Parameters:</p> Name Type Description Default <code>paidoff</code> <code>DataFrame</code> <p>DataFrame with paid off capacities</p> required <p>Returns:     pd.DataFrame: paid off techs with list of PyPSA technologies Example:     &gt;&gt; tech_groups         PyPSA_tech, group         coal CHP, coal         coal, coal     &gt;&gt; add_possible_techs_to_paidoff(paidoff, tech_groups)     &gt;&gt; paidoff         tech_group, paid_off_capacity, techs         coal, 1000, ['coal CHP', 'coal']</p> Source code in <code>workflow/scripts/remind_coupling/disaggregate_data.py</code> <pre><code>def add_possible_techs_to_paidoff(paidoff: pd.DataFrame, tech_groups: pd.Series) -&gt; pd.DataFrame:\n    \"\"\"Add possible PyPSA technologies to the paid off capacities DataFrame.\n    The paidoff capacities are grouped in case the Remind-PyPSA tecg mapping is not 1:1\n    but the network needs to add PyPSA techs.\n    A constraint is added so the paid off caps per group are not exceeded.\n\n    Args:\n        paidoff (pd.DataFrame): DataFrame with paid off capacities\n    Returns:\n        pd.DataFrame: paid off techs with list of PyPSA technologies\n    Example:\n        &gt;&gt; tech_groups\n            PyPSA_tech, group\n            coal CHP, coal\n            coal, coal\n        &gt;&gt; add_possible_techs_to_paidoff(paidoff, tech_groups)\n        &gt;&gt; paidoff\n            tech_group, paid_off_capacity, techs\n            coal, 1000, ['coal CHP', 'coal']\n    \"\"\"\n    df = tech_groups.reset_index()\n    possibilities = df.groupby(\"group\").PyPSA_tech.apply(lambda x: list(x.unique()))\n    paidoff[\"techs\"] = paidoff.tech_group.map(possibilities)\n    return paidoff\n</code></pre>"},{"location":"reference/remind_coupling/disaggregate_data/#remind_coupling.disaggregate_data.disagg_load_using_ref","title":"<code>disagg_load_using_ref(data, reference_data, reference_year, sector_coupling_enabled=False)</code>","text":"<p>Spatially disaggregate the load using regional/nodal reference data.</p> <p>Automatically chooses between single-sector (electric-only) and multi-sector disaggregation based on sector_coupling_enabled parameter.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>DataFrame containing the load data</p> required <code>reference_data</code> <code>DataFrame</code> <p>DataFrame containing the reference data</p> required <code>reference_year</code> <code>int | str</code> <p>Year to use for disaggregation</p> required <code>sector_coupling_enabled</code> <code>bool</code> <p>Whether to use multi-sector disaggregation</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: Disaggregated load data (Region x Year) or (Province x Sector x Year)</p> Source code in <code>workflow/scripts/remind_coupling/disaggregate_data.py</code> <pre><code>@register_etl(\"disagg_load_ref\")\ndef disagg_load_using_ref(\n    data: pd.DataFrame,\n    reference_data: pd.DataFrame,\n    reference_year: int | str,\n    sector_coupling_enabled: bool = False,\n) -&gt; pd.DataFrame:\n    \"\"\"Spatially disaggregate the load using regional/nodal reference data.\n\n    Automatically chooses between single-sector (electric-only) and multi-sector disaggregation\n    based on sector_coupling_enabled parameter.\n\n    Args:\n        data (pd.DataFrame): DataFrame containing the load data\n        reference_data (pd.DataFrame): DataFrame containing the reference data\n        reference_year (int | str): Year to use for disaggregation\n        sector_coupling_enabled (bool): Whether to use multi-sector disaggregation\n\n    Returns:\n        pd.DataFrame: Disaggregated load data (Region x Year) or (Province x Sector x Year)\n    \"\"\"\n\n    if sector_coupling_enabled:\n        logger.info(\"Sector coupling enabled - using multi-sector disaggregation\")\n        return _disagg_multisector_load(data, reference_data, reference_year)\n    else:\n        logger.info(\"Sector coupling disabled - using total electricity load disaggregation\")\n        return _disagg_total_load(data, reference_data, reference_year)\n</code></pre>"},{"location":"reference/remind_coupling/ev_refshare_extrapolator/","title":"Ev refshare extrapolator","text":"<p>EV provincial disaggregation share extrapolator.</p> <p>Extrapolates future provincial EV shares using a Gompertz model fitted to historical vehicle ownership, GDP, and population data. These shares are used to spatially disaggregate national REMIND EV demand to provincial level.</p>"},{"location":"reference/remind_coupling/ev_refshare_extrapolator/#remind_coupling.ev_refshare_extrapolator.GompertzModel","title":"<code>GompertzModel</code>","text":"<p>Simplified Gompertz model for vehicle ownership prediction.</p> <p>Parameters:</p> Name Type Description Default <code>saturation_level</code> <code>float</code> <p>Maximum vehicle ownership per 1000 people (default: 500)</p> <code>500</code> <code>alpha</code> <code>float</code> <p>Fixed Gompertz parameter (default: -5.58)</p> <code>-5.58</code> Source code in <code>workflow/scripts/remind_coupling/ev_refshare_extrapolator.py</code> <pre><code>class GompertzModel:\n    \"\"\"Simplified Gompertz model for vehicle ownership prediction.\n\n    Args:\n        saturation_level: Maximum vehicle ownership per 1000 people (default: 500)\n        alpha: Fixed Gompertz parameter (default: -5.58)\n    \"\"\"\n\n    def __init__(self, saturation_level: float = 500, alpha: float = -5.58):\n        self.saturation_level = saturation_level\n        self.alpha = alpha\n        self.beta = None\n        self.fitted = False\n\n    def gompertz_function(self, pgdp: np.ndarray, beta: float) -&gt; np.ndarray:\n        \"\"\"Calculate Gompertz function values for vehicle ownership prediction.\n\n        Args:\n            pgdp: Per-capita GDP values\n            beta: Fitted parameter\n\n        Returns:\n            np.ndarray: Predicted vehicle ownership per capita\n        \"\"\"\n        return self.saturation_level * np.exp(self.alpha * np.exp(beta * pgdp))\n\n    def fit_model(self, pgdp_data: np.ndarray, vehicle_data: np.ndarray) -&gt; bool:\n        \"\"\"Fit Gompertz model beta parameter to historical data.\n\n        Args:\n            pgdp_data: Historical per-capita GDP data\n            vehicle_data: Historical vehicle ownership data\n\n        Returns:\n            bool: True if fitting successful, False otherwise\n        \"\"\"\n        try:\n\n            def objective_function(pgdp, beta):\n                return self.gompertz_function(pgdp, beta)\n\n            popt, _ = curve_fit(\n                objective_function, pgdp_data, vehicle_data, p0=[-0.0001], bounds=([-1], [0])\n            )\n            self.beta = popt[0]\n            self.fitted = True\n            logger.info(f\"Gompertz model fitted successfully - \u03b1: {self.alpha}, \u03b2: {self.beta:.6f}\")\n            return True\n        except Exception as e:\n            logger.error(f\"Gompertz model fitting failed: {e}\")\n            return False\n\n    def predict_vehicles(self, pgdp: float, population: float) -&gt; float:\n        \"\"\"Predict total vehicle numbers using fitted model.\n\n        Args:\n            pgdp: Per-capita GDP\n            population: Population (in 10,000 persons)\n\n        Returns:\n            float: Predicted vehicles (in 10,000 vehicles)\n        \"\"\"\n        if not self.fitted:\n            raise ValueError(\"Model not fitted\")\n\n        vehicle_per_capita = self.gompertz_function(np.array([pgdp]), self.beta)[0]\n        return vehicle_per_capita * population / 1000  # output in 10,000 vehicles\n</code></pre>"},{"location":"reference/remind_coupling/ev_refshare_extrapolator/#remind_coupling.ev_refshare_extrapolator.GompertzModel.fit_model","title":"<code>fit_model(pgdp_data, vehicle_data)</code>","text":"<p>Fit Gompertz model beta parameter to historical data.</p> <p>Parameters:</p> Name Type Description Default <code>pgdp_data</code> <code>ndarray</code> <p>Historical per-capita GDP data</p> required <code>vehicle_data</code> <code>ndarray</code> <p>Historical vehicle ownership data</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if fitting successful, False otherwise</p> Source code in <code>workflow/scripts/remind_coupling/ev_refshare_extrapolator.py</code> <pre><code>def fit_model(self, pgdp_data: np.ndarray, vehicle_data: np.ndarray) -&gt; bool:\n    \"\"\"Fit Gompertz model beta parameter to historical data.\n\n    Args:\n        pgdp_data: Historical per-capita GDP data\n        vehicle_data: Historical vehicle ownership data\n\n    Returns:\n        bool: True if fitting successful, False otherwise\n    \"\"\"\n    try:\n\n        def objective_function(pgdp, beta):\n            return self.gompertz_function(pgdp, beta)\n\n        popt, _ = curve_fit(\n            objective_function, pgdp_data, vehicle_data, p0=[-0.0001], bounds=([-1], [0])\n        )\n        self.beta = popt[0]\n        self.fitted = True\n        logger.info(f\"Gompertz model fitted successfully - \u03b1: {self.alpha}, \u03b2: {self.beta:.6f}\")\n        return True\n    except Exception as e:\n        logger.error(f\"Gompertz model fitting failed: {e}\")\n        return False\n</code></pre>"},{"location":"reference/remind_coupling/ev_refshare_extrapolator/#remind_coupling.ev_refshare_extrapolator.GompertzModel.gompertz_function","title":"<code>gompertz_function(pgdp, beta)</code>","text":"<p>Calculate Gompertz function values for vehicle ownership prediction.</p> <p>Parameters:</p> Name Type Description Default <code>pgdp</code> <code>ndarray</code> <p>Per-capita GDP values</p> required <code>beta</code> <code>float</code> <p>Fitted parameter</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Predicted vehicle ownership per capita</p> Source code in <code>workflow/scripts/remind_coupling/ev_refshare_extrapolator.py</code> <pre><code>def gompertz_function(self, pgdp: np.ndarray, beta: float) -&gt; np.ndarray:\n    \"\"\"Calculate Gompertz function values for vehicle ownership prediction.\n\n    Args:\n        pgdp: Per-capita GDP values\n        beta: Fitted parameter\n\n    Returns:\n        np.ndarray: Predicted vehicle ownership per capita\n    \"\"\"\n    return self.saturation_level * np.exp(self.alpha * np.exp(beta * pgdp))\n</code></pre>"},{"location":"reference/remind_coupling/ev_refshare_extrapolator/#remind_coupling.ev_refshare_extrapolator.GompertzModel.predict_vehicles","title":"<code>predict_vehicles(pgdp, population)</code>","text":"<p>Predict total vehicle numbers using fitted model.</p> <p>Parameters:</p> Name Type Description Default <code>pgdp</code> <code>float</code> <p>Per-capita GDP</p> required <code>population</code> <code>float</code> <p>Population (in 10,000 persons)</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>Predicted vehicles (in 10,000 vehicles)</p> Source code in <code>workflow/scripts/remind_coupling/ev_refshare_extrapolator.py</code> <pre><code>def predict_vehicles(self, pgdp: float, population: float) -&gt; float:\n    \"\"\"Predict total vehicle numbers using fitted model.\n\n    Args:\n        pgdp: Per-capita GDP\n        population: Population (in 10,000 persons)\n\n    Returns:\n        float: Predicted vehicles (in 10,000 vehicles)\n    \"\"\"\n    if not self.fitted:\n        raise ValueError(\"Model not fitted\")\n\n    vehicle_per_capita = self.gompertz_function(np.array([pgdp]), self.beta)[0]\n    return vehicle_per_capita * population / 1000  # output in 10,000 vehicles\n</code></pre>"},{"location":"reference/remind_coupling/ev_refshare_extrapolator/#remind_coupling.ev_refshare_extrapolator.extrapolate_reference","title":"<code>extrapolate_reference(years, input_files, output_dir, config=None)</code>","text":"<p>Extrapolate provincial EV disaggregation shares using Gompertz model.</p> <p>Parameters:</p> Name Type Description Default <code>years</code> <code>list</code> <p>Target years for projections (e.g., [2030, 2040, 2050])</p> required <code>input_files</code> <code>dict</code> <p>Dictionary of input data file paths with keys: - 'historical_gdp': Historical GDP by province - 'historical_pop': Historical population by province - 'historical_cars': Historical vehicle ownership by province - 'ssp2_pop': SSP2 future population projections - 'ssp2_gdp': SSP2 future GDP projections</p> required <code>output_dir</code> <code>str</code> <p>Output directory for results (CSV files will be saved here)</p> required <code>config</code> <code>dict</code> <p>Gompertz model parameters from  sectors.electric_vehicles.gompertz configuration: - 'saturation_level': Maximum vehicles per 1000 people (default: 500) - 'alpha': Fixed Gompertz parameter (default: -5.58)</p> <code>None</code> Outputs <p>Saves two CSV files to output_dir: - ev_passenger_shares.csv: Provincial shares of passenger EV demand - ev_freight_shares.csv: Provincial shares of freight EV demand</p> Source code in <code>workflow/scripts/remind_coupling/ev_refshare_extrapolator.py</code> <pre><code>def extrapolate_reference(years: list, input_files: dict, output_dir: str, config: dict = None):\n    \"\"\"Extrapolate provincial EV disaggregation shares using Gompertz model.\n\n    Args:\n        years (list): Target years for projections (e.g., [2030, 2040, 2050])\n        input_files (dict): Dictionary of input data file paths with keys:\n            - 'historical_gdp': Historical GDP by province\n            - 'historical_pop': Historical population by province\n            - 'historical_cars': Historical vehicle ownership by province\n            - 'ssp2_pop': SSP2 future population projections\n            - 'ssp2_gdp': SSP2 future GDP projections\n        output_dir (str): Output directory for results (CSV files will be saved here)\n        config (dict, optional): Gompertz model parameters from \n            sectors.electric_vehicles.gompertz configuration:\n            - 'saturation_level': Maximum vehicles per 1000 people (default: 500)\n            - 'alpha': Fixed Gompertz parameter (default: -5.58)\n\n    Outputs:\n        Saves two CSV files to output_dir:\n        - ev_passenger_shares.csv: Provincial shares of passenger EV demand\n        - ev_freight_shares.csv: Provincial shares of freight EV demand\n    \"\"\"\n    logger.info(\"Extrapolating provincial EV disaggregation shares using Gompertz model\")\n\n    model = GompertzModel(\n        saturation_level=config.get(\"saturation_level\", 500)\n        if config\n        else 500,  # Nature Geoscience: https://doi.org/10.1038/s41561-023-01350-9\n        alpha=config.get(\"alpha\", -5.58)\n        if config\n        else -5.58,  # Energy Policy: https://doi.org/10.1016/j.enpol.2011.01.043\n    )\n\n    historical_data = _load_historical_data(input_files)\n    pgdp_data = historical_data[\"pgdp\"].values\n    vehicle_data = historical_data[\"vehicle_per_capita\"].values\n    model.fit_model(pgdp_data, vehicle_data)\n\n    future_data = _load_future_data(input_files, years)\n\n    future_data[\"vehicles\"] = future_data.apply(\n        lambda row: model.predict_vehicles(row[\"pgdp\"], row[\"population\"]), axis=1\n    )\n\n    shares_by_year = {}\n    for year in years:\n        year_data = future_data[future_data[\"year\"] == year]\n        total_vehicles = year_data[\"vehicles\"].sum()\n        shares = year_data[\"vehicles\"] / total_vehicles\n        shares_by_year[year] = dict(zip(year_data[\"province\"], shares))\n\n    shares_df = pd.DataFrame(shares_by_year)\n    shares_df.to_csv(f\"{output_dir}/ev_passenger_shares.csv\")\n    shares_df.to_csv(f\"{output_dir}/ev_freight_shares.csv\")\n\n    logger.info(f\"EV disaggregation shares saved to {output_dir}\")\n</code></pre>"},{"location":"reference/remind_coupling/extrapolate_regional_references/","title":"Extrapolate regional references","text":"<p>Extrapolate regional disaggregation reference shares for different sectors.</p> <p>This script generates provincial share/ratio data used to spatially disaggregate REMIND national-level outputs to the provincial level. For example, EV passenger shares indicate what fraction of national EV demand belongs to each province.</p> <p>This provides a general framework to coordinate the extrapolation of sector-specific reference share files. Each sector has its own specialized module implementing the  <code>extrapolate_reference</code> function.</p>"},{"location":"reference/remind_coupling/extrapolate_regional_references/#remind_coupling.extrapolate_regional_references.SectorReferenceGenerator","title":"<code>SectorReferenceGenerator</code>","text":"<p>General framework for generating sectoral disaggregation shares.</p> <p>Coordinates the extrapolation of provincial share/ratio data for different sectors (e.g., EV, heat). These shares are used to spatially disaggregate REMIND national outputs to provincial level.</p> Source code in <code>workflow/scripts/remind_coupling/extrapolate_regional_references.py</code> <pre><code>class SectorReferenceGenerator:\n    \"\"\"General framework for generating sectoral disaggregation shares.\n\n    Coordinates the extrapolation of provincial share/ratio data for different\n    sectors (e.g., EV, heat). These shares are used to spatially disaggregate\n    REMIND national outputs to provincial level.\n    \"\"\"\n\n    def __init__(self, config: dict = None):\n        \"\"\"Initialize the generator.\n\n        Args:\n            config (dict): Configuration dictionary with sector-specific settings.\n        \"\"\"\n        self.config = config or {}\n        self.sector_modules = {}\n        self._load_sector_modules()\n\n    def _load_sector_modules(self):\n        \"\"\"Load sector-specific modules for generating disaggregation shares.\"\"\"\n        supported_sectors = {\n            \"ev\": \"ev_refshare_extrapolator\",\n        }\n\n        for sector, module_name in supported_sectors.items():\n            try:\n                module = importlib.import_module(module_name)\n                self.sector_modules[sector] = module\n                logger.info(f\"Loaded {sector} sector module\")\n            except ImportError as e:\n                logger.warning(f\"Could not load {sector} sector module: {e}\")\n\n    def extrapolate_references(self, years: list[int], input_files: dict[str, str], output_dir: str):\n        \"\"\"Extrapolate provincial disaggregation shares for all available sectors.\n\n        Generates reference share files that indicate what fraction of national-level\n        sectoral demand/activity belongs to each province. For example, EV passenger\n        shares show the provincial distribution of passenger EV demand.\n\n        Args:\n            years (list[int]): List of target years for projections (e.g., [2020, 2025, 2030]).\n            input_files (dict[str, str]): Dictionary mapping data types to file paths \n                (e.g., historical GDP, population, sector-specific data).\n            output_dir (str): Directory to save extrapolated reference share files.\n        \"\"\"\n        logger.info(\"Extrapolating sectoral disaggregation shares\")\n\n        output_path = Path(output_dir)\n        output_path.mkdir(parents=True, exist_ok=True)\n\n        for sector, module in self.sector_modules.items():\n            try:\n                logger.info(f\"Extrapolating disaggregation shares for {sector} sector\")\n\n                if hasattr(module, \"extrapolate_reference\"):\n                    module.extrapolate_reference(\n                        years=years,\n                        input_files=input_files,\n                        output_dir=str(output_path),\n                        config=self.config.get(sector, {}),\n                    )\n                else:\n                    logger.error(f\"Sector module {sector} missing extrapolate_reference function\")\n\n            except Exception as e:\n                logger.error(f\"Failed to extrapolate disaggregation shares for {sector}: {e}\")\n\n        logger.info(\"Sectoral disaggregation shares extrapolation completed\")\n</code></pre>"},{"location":"reference/remind_coupling/extrapolate_regional_references/#remind_coupling.extrapolate_regional_references.SectorReferenceGenerator.__init__","title":"<code>__init__(config=None)</code>","text":"<p>Initialize the generator.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict</code> <p>Configuration dictionary with sector-specific settings.</p> <code>None</code> Source code in <code>workflow/scripts/remind_coupling/extrapolate_regional_references.py</code> <pre><code>def __init__(self, config: dict = None):\n    \"\"\"Initialize the generator.\n\n    Args:\n        config (dict): Configuration dictionary with sector-specific settings.\n    \"\"\"\n    self.config = config or {}\n    self.sector_modules = {}\n    self._load_sector_modules()\n</code></pre>"},{"location":"reference/remind_coupling/extrapolate_regional_references/#remind_coupling.extrapolate_regional_references.SectorReferenceGenerator.extrapolate_references","title":"<code>extrapolate_references(years, input_files, output_dir)</code>","text":"<p>Extrapolate provincial disaggregation shares for all available sectors.</p> <p>Generates reference share files that indicate what fraction of national-level sectoral demand/activity belongs to each province. For example, EV passenger shares show the provincial distribution of passenger EV demand.</p> <p>Parameters:</p> Name Type Description Default <code>years</code> <code>list[int]</code> <p>List of target years for projections (e.g., [2020, 2025, 2030]).</p> required <code>input_files</code> <code>dict[str, str]</code> <p>Dictionary mapping data types to file paths  (e.g., historical GDP, population, sector-specific data).</p> required <code>output_dir</code> <code>str</code> <p>Directory to save extrapolated reference share files.</p> required Source code in <code>workflow/scripts/remind_coupling/extrapolate_regional_references.py</code> <pre><code>def extrapolate_references(self, years: list[int], input_files: dict[str, str], output_dir: str):\n    \"\"\"Extrapolate provincial disaggregation shares for all available sectors.\n\n    Generates reference share files that indicate what fraction of national-level\n    sectoral demand/activity belongs to each province. For example, EV passenger\n    shares show the provincial distribution of passenger EV demand.\n\n    Args:\n        years (list[int]): List of target years for projections (e.g., [2020, 2025, 2030]).\n        input_files (dict[str, str]): Dictionary mapping data types to file paths \n            (e.g., historical GDP, population, sector-specific data).\n        output_dir (str): Directory to save extrapolated reference share files.\n    \"\"\"\n    logger.info(\"Extrapolating sectoral disaggregation shares\")\n\n    output_path = Path(output_dir)\n    output_path.mkdir(parents=True, exist_ok=True)\n\n    for sector, module in self.sector_modules.items():\n        try:\n            logger.info(f\"Extrapolating disaggregation shares for {sector} sector\")\n\n            if hasattr(module, \"extrapolate_reference\"):\n                module.extrapolate_reference(\n                    years=years,\n                    input_files=input_files,\n                    output_dir=str(output_path),\n                    config=self.config.get(sector, {}),\n                )\n            else:\n                logger.error(f\"Sector module {sector} missing extrapolate_reference function\")\n\n        except Exception as e:\n            logger.error(f\"Failed to extrapolate disaggregation shares for {sector}: {e}\")\n\n    logger.info(\"Sectoral disaggregation shares extrapolation completed\")\n</code></pre>"},{"location":"reference/remind_coupling/extrapolate_regional_references/#remind_coupling.extrapolate_regional_references.main","title":"<code>main()</code>","text":"<p>Main entry point for generating provincial disaggregation shares.</p> Source code in <code>workflow/scripts/remind_coupling/extrapolate_regional_references.py</code> <pre><code>def main():\n    \"\"\"Main entry point for generating provincial disaggregation shares.\"\"\"\n    if \"snakemake\" not in globals():\n        snakemake = setup._mock_snakemake(\n            \"generate_sector_references\",\n            config_files=\"resources/tmp/remind_coupled_cg.yaml\",\n        )\n    else:\n        snakemake = globals()[\"snakemake\"]\n\n    configure_logging(snakemake)\n    logger.info(\"Starting generation of sectoral disaggregation shares\")\n\n    params = snakemake.params\n    years = params.years\n    config = params.gompertz_config\n\n    input_files = {\n        \"historical_gdp\": snakemake.input.historical_gdp,\n        \"historical_pop\": snakemake.input.historical_pop,\n        \"historical_cars\": snakemake.input.historical_cars,\n        \"ssp2_pop\": snakemake.input.ssp2_pop,\n        \"ssp2_gdp\": snakemake.input.ssp2_gdp,\n    }\n\n    output_dir = Path(snakemake.output.ev_passenger_shares).parent\n\n    generator = SectorReferenceGenerator(config)\n    generator.extrapolate_references(years, input_files, str(output_dir))\n\n    logger.info(\"Generation of sectoral disaggregation shares finished\")\n</code></pre>"},{"location":"reference/remind_coupling/generic_etl/","title":"Generic etl","text":"<p>generic etl development, to be rebalanced with the remind_coupling package</p> <p>The ETL operations are governed by the config file. Allowed fields are defined by the rpycpl.etl.Transformation class and are     name: str     method: Optional[str]     frames: Dict[str, Any]     params: Dict[str, Any]     filters: Dict[str, Any     kwargs: Dict[str, Any]     dependencies: Dict[str, Any]</p> <p>The sequence of operations matters: Dependencies represents previous step outputs.</p>"},{"location":"reference/remind_coupling/generic_etl/#remind_coupling.generic_etl.ETLRunner","title":"<code>ETLRunner</code>","text":"<p>Container class to execute ETL steps.</p> Source code in <code>workflow/scripts/remind_coupling/generic_etl.py</code> <pre><code>class ETLRunner:\n    \"\"\"Container class to execute ETL steps.\"\"\"\n\n    @staticmethod\n    def run(\n        step: Transformation,\n        frames: dict[str, pd.DataFrame],\n        previous_outputs: dict[str, Any] = None,\n        **kwargs,\n    ) -&gt; pd.DataFrame:\n        \"\"\"Run the ETL step using the provided frames and extra arguments.\n\n        Args:\n            step (Transformation): The ETL step to run.\n            frames (dict): Dictionary of loaded frames.\n            previous_outputs (dict, optional): Dictionary of outputs from previous\n                steps that can be used as inputs.\n            **kwargs: Additional arguments for the ETL method.\n\n        Returns:\n            pd.DataFrame: The result of the ETL step.\n        \"\"\"\n        method = step.name if not step.method else step.method\n        func = ETL_REGISTRY.get(method)\n        if not func:\n            raise ValueError(f\"ETL method '{method}' not found in registry.\")\n\n        # Handle dependencies on previous outputs if specified in the step\n        if hasattr(step, \"dependencies\") and step.dependencies and previous_outputs:\n            for output_key in step.dependencies:\n                if output_key in previous_outputs:\n                    # Add the dependency to frames with the specified key\n                    frames[output_key] = previous_outputs[output_key]\n                else:\n                    msg = f\"Dependency '{output_key}' not found in previous outputs\"\n                    msg += f\" for step '{step.name}'\"\n                    raise ValueError(msg)\n\n        kwargs.update(step.kwargs)\n        if kwargs:\n            return func(frames, **kwargs)\n        else:\n            return func(frames)\n</code></pre>"},{"location":"reference/remind_coupling/generic_etl/#remind_coupling.generic_etl.ETLRunner.run","title":"<code>run(step, frames, previous_outputs=None, **kwargs)</code>  <code>staticmethod</code>","text":"<p>Run the ETL step using the provided frames and extra arguments.</p> <p>Parameters:</p> Name Type Description Default <code>step</code> <code>Transformation</code> <p>The ETL step to run.</p> required <code>frames</code> <code>dict</code> <p>Dictionary of loaded frames.</p> required <code>previous_outputs</code> <code>dict</code> <p>Dictionary of outputs from previous steps that can be used as inputs.</p> <code>None</code> <code>**kwargs</code> <p>Additional arguments for the ETL method.</p> <code>{}</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: The result of the ETL step.</p> Source code in <code>workflow/scripts/remind_coupling/generic_etl.py</code> <pre><code>@staticmethod\ndef run(\n    step: Transformation,\n    frames: dict[str, pd.DataFrame],\n    previous_outputs: dict[str, Any] = None,\n    **kwargs,\n) -&gt; pd.DataFrame:\n    \"\"\"Run the ETL step using the provided frames and extra arguments.\n\n    Args:\n        step (Transformation): The ETL step to run.\n        frames (dict): Dictionary of loaded frames.\n        previous_outputs (dict, optional): Dictionary of outputs from previous\n            steps that can be used as inputs.\n        **kwargs: Additional arguments for the ETL method.\n\n    Returns:\n        pd.DataFrame: The result of the ETL step.\n    \"\"\"\n    method = step.name if not step.method else step.method\n    func = ETL_REGISTRY.get(method)\n    if not func:\n        raise ValueError(f\"ETL method '{method}' not found in registry.\")\n\n    # Handle dependencies on previous outputs if specified in the step\n    if hasattr(step, \"dependencies\") and step.dependencies and previous_outputs:\n        for output_key in step.dependencies:\n            if output_key in previous_outputs:\n                # Add the dependency to frames with the specified key\n                frames[output_key] = previous_outputs[output_key]\n            else:\n                msg = f\"Dependency '{output_key}' not found in previous outputs\"\n                msg += f\" for step '{step.name}'\"\n                raise ValueError(msg)\n\n    kwargs.update(step.kwargs)\n    if kwargs:\n        return func(frames, **kwargs)\n    else:\n        return func(frames)\n</code></pre>"},{"location":"reference/remind_coupling/generic_etl/#remind_coupling.generic_etl.RemindLoader","title":"<code>RemindLoader</code>","text":"<p>Load Remind symbol tables from csvs or gdx</p> Source code in <code>workflow/scripts/remind_coupling/generic_etl.py</code> <pre><code>class RemindLoader:\n    \"\"\"Load Remind symbol tables from csvs or gdx\"\"\"\n\n    def __init__(self, remind_dir: PathLike, backend=\"csv\"):\n        self.remind_dir = remind_dir\n\n        supported_backends = [\"csv\", \"gdx\"]\n        if backend not in supported_backends:\n            raise ValueError(f\"Backend {backend} not supported. Available: {supported_backends}\")\n        self.backend = backend\n\n    def _group_split_frames(self, keys, pattern: str = r\"_part\\d+$\") -&gt; dict[str, list[str]]:\n        r\"\"\"Chat gpt regex magic to group split frames\n        Args:\n            keys (list): list of keys\n            pattern (str, optional): regex pattern to group split frames by. Defaults to r\"_part\\\\d+$\".\"\n        Returns:\n            dict[str, list[str]]: dictionary with base name as key and list of keys as value\n        \"\"\"\n        grouped = {}\n        for k in keys:\n            base = re.sub(pattern, \"\", k)\n            grouped.setdefault(base, []).append(k)\n        return grouped\n\n    def load_frames_csv(self, frames: dict[str, str]) -&gt; dict[str, pd.DataFrame]:\n        \"\"\"Remind Frames to read\n        Args:\n            frames (dict): (param: remind_symbol_name) to read\n        Returns:\n            dict[str, pd.DataFrame]: dictionary (param: dataframe)\n        \"\"\"\n        paths = {k: os.path.join(self.remind_dir, v + \".csv\") for k, v in frames.items() if v}\n        return {k: read_remind_csv(v) for k, v in paths.items()}\n\n    def load_frames_gdx(\n        self, frames: dict[str, str], gdx_file: PathLike\n    ) -&gt; dict[str, pd.DataFrame]:\n        \"\"\"Load frames from GDX file.\n\n        Args:\n            frames (dict[str, str]): Dictionary mapping parameter names to REMIND symbol names.\n            gdx_file (PathLike): Path to the GDX file.\n\n        Returns:\n            dict[str, pd.DataFrame]: Dictionary of loaded DataFrames.\n\n        Raises:\n            NotImplementedError: GDX loading not implemented yet.\n        \"\"\"\n        raise NotImplementedError(\"GDX loading not implemented yet\")\n\n    def merge_split_frames(self, frames: dict[str, pd.DataFrame]) -&gt; dict[str, pd.DataFrame]:\n        \"\"\"In case several REMIND parameters are needed, group them by their base name\n        Args:\n            frames (dict): Dictionary with all dataframes\n        Example:\n            frames = {eta: 'pm_dataeta', eta_part2: 'pm_eta_conv'}\n            merge_split_frames(frames)\n            &gt;&gt; {eta: pd.concat([pm_dataeta, pm_eta_conv], axis=0).drop_duplicates()}\n        \"\"\"\n\n        grouped = self._group_split_frames(frames)\n        unmerged = {k: v for k, v in grouped.items() if len(v) &gt; 1}\n        merged = {k: pd.concat([frames[v] for v in multi], axis=0) for k, multi in unmerged.items()}\n        merged = {k: v.drop_duplicates().reset_index(drop=True) for k, v in merged.items()}\n\n        to_drop = [item for sublist in unmerged.values() for item in sublist]\n        frames = {k: v for k, v in frames.items() if k not in to_drop}\n        frames.update(merged)\n        return frames\n\n    def auto_load(\n        self, frames: dict[str, str], filters: dict[str, str] = None\n    ) -&gt; dict[str, pd.DataFrame]:\n        \"\"\"Automatically load, merge, and filter frames in one step.\n\n        Args:\n            frames: Dictionary mapping parameter names to REMIND symbol names.\n            filters: Optional dictionary of filter expressions to apply to frames.\n\n        Returns:\n            Dictionary of processed DataFrames ready for transformation.\n        \"\"\"\n        # Load raw frames\n        if self.backend == \"gdx\":\n            raw_frames = self.load_frames_gdx(frames, os.path.join(self.remind_dir, \"gdx\"))\n        elif self.backend == \"csv\":\n            raw_frames = self.load_frames_csv(frames)\n\n        # Merge split frames\n        processed_frames = self.merge_split_frames(raw_frames)\n\n        # Apply filters if any\n        if filters:\n            for k, filter_expr in filters.items():\n                if k in processed_frames:\n                    processed_frames[k] = processed_frames[k].query(filter_expr)\n                else:\n                    logger.warning(f\"Filter specified for non-existent frame: {k}\")\n\n        return processed_frames\n</code></pre>"},{"location":"reference/remind_coupling/generic_etl/#remind_coupling.generic_etl.RemindLoader.auto_load","title":"<code>auto_load(frames, filters=None)</code>","text":"<p>Automatically load, merge, and filter frames in one step.</p> <p>Parameters:</p> Name Type Description Default <code>frames</code> <code>dict[str, str]</code> <p>Dictionary mapping parameter names to REMIND symbol names.</p> required <code>filters</code> <code>dict[str, str]</code> <p>Optional dictionary of filter expressions to apply to frames.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, DataFrame]</code> <p>Dictionary of processed DataFrames ready for transformation.</p> Source code in <code>workflow/scripts/remind_coupling/generic_etl.py</code> <pre><code>def auto_load(\n    self, frames: dict[str, str], filters: dict[str, str] = None\n) -&gt; dict[str, pd.DataFrame]:\n    \"\"\"Automatically load, merge, and filter frames in one step.\n\n    Args:\n        frames: Dictionary mapping parameter names to REMIND symbol names.\n        filters: Optional dictionary of filter expressions to apply to frames.\n\n    Returns:\n        Dictionary of processed DataFrames ready for transformation.\n    \"\"\"\n    # Load raw frames\n    if self.backend == \"gdx\":\n        raw_frames = self.load_frames_gdx(frames, os.path.join(self.remind_dir, \"gdx\"))\n    elif self.backend == \"csv\":\n        raw_frames = self.load_frames_csv(frames)\n\n    # Merge split frames\n    processed_frames = self.merge_split_frames(raw_frames)\n\n    # Apply filters if any\n    if filters:\n        for k, filter_expr in filters.items():\n            if k in processed_frames:\n                processed_frames[k] = processed_frames[k].query(filter_expr)\n            else:\n                logger.warning(f\"Filter specified for non-existent frame: {k}\")\n\n    return processed_frames\n</code></pre>"},{"location":"reference/remind_coupling/generic_etl/#remind_coupling.generic_etl.RemindLoader.load_frames_csv","title":"<code>load_frames_csv(frames)</code>","text":"<p>Remind Frames to read Args:     frames (dict): (param: remind_symbol_name) to read Returns:     dict[str, pd.DataFrame]: dictionary (param: dataframe)</p> Source code in <code>workflow/scripts/remind_coupling/generic_etl.py</code> <pre><code>def load_frames_csv(self, frames: dict[str, str]) -&gt; dict[str, pd.DataFrame]:\n    \"\"\"Remind Frames to read\n    Args:\n        frames (dict): (param: remind_symbol_name) to read\n    Returns:\n        dict[str, pd.DataFrame]: dictionary (param: dataframe)\n    \"\"\"\n    paths = {k: os.path.join(self.remind_dir, v + \".csv\") for k, v in frames.items() if v}\n    return {k: read_remind_csv(v) for k, v in paths.items()}\n</code></pre>"},{"location":"reference/remind_coupling/generic_etl/#remind_coupling.generic_etl.RemindLoader.load_frames_gdx","title":"<code>load_frames_gdx(frames, gdx_file)</code>","text":"<p>Load frames from GDX file.</p> <p>Parameters:</p> Name Type Description Default <code>frames</code> <code>dict[str, str]</code> <p>Dictionary mapping parameter names to REMIND symbol names.</p> required <code>gdx_file</code> <code>PathLike</code> <p>Path to the GDX file.</p> required <p>Returns:</p> Type Description <code>dict[str, DataFrame]</code> <p>dict[str, pd.DataFrame]: Dictionary of loaded DataFrames.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>GDX loading not implemented yet.</p> Source code in <code>workflow/scripts/remind_coupling/generic_etl.py</code> <pre><code>def load_frames_gdx(\n    self, frames: dict[str, str], gdx_file: PathLike\n) -&gt; dict[str, pd.DataFrame]:\n    \"\"\"Load frames from GDX file.\n\n    Args:\n        frames (dict[str, str]): Dictionary mapping parameter names to REMIND symbol names.\n        gdx_file (PathLike): Path to the GDX file.\n\n    Returns:\n        dict[str, pd.DataFrame]: Dictionary of loaded DataFrames.\n\n    Raises:\n        NotImplementedError: GDX loading not implemented yet.\n    \"\"\"\n    raise NotImplementedError(\"GDX loading not implemented yet\")\n</code></pre>"},{"location":"reference/remind_coupling/generic_etl/#remind_coupling.generic_etl.RemindLoader.merge_split_frames","title":"<code>merge_split_frames(frames)</code>","text":"<p>In case several REMIND parameters are needed, group them by their base name Args:     frames (dict): Dictionary with all dataframes Example:     frames = {eta: 'pm_dataeta', eta_part2: 'pm_eta_conv'}     merge_split_frames(frames)     &gt;&gt; {eta: pd.concat([pm_dataeta, pm_eta_conv], axis=0).drop_duplicates()}</p> Source code in <code>workflow/scripts/remind_coupling/generic_etl.py</code> <pre><code>def merge_split_frames(self, frames: dict[str, pd.DataFrame]) -&gt; dict[str, pd.DataFrame]:\n    \"\"\"In case several REMIND parameters are needed, group them by their base name\n    Args:\n        frames (dict): Dictionary with all dataframes\n    Example:\n        frames = {eta: 'pm_dataeta', eta_part2: 'pm_eta_conv'}\n        merge_split_frames(frames)\n        &gt;&gt; {eta: pd.concat([pm_dataeta, pm_eta_conv], axis=0).drop_duplicates()}\n    \"\"\"\n\n    grouped = self._group_split_frames(frames)\n    unmerged = {k: v for k, v in grouped.items() if len(v) &gt; 1}\n    merged = {k: pd.concat([frames[v] for v in multi], axis=0) for k, multi in unmerged.items()}\n    merged = {k: v.drop_duplicates().reset_index(drop=True) for k, v in merged.items()}\n\n    to_drop = [item for sublist in unmerged.values() for item in sublist]\n    frames = {k: v for k, v in frames.items() if k not in to_drop}\n    frames.update(merged)\n    return frames\n</code></pre>"},{"location":"reference/remind_coupling/make_pypsa_config/","title":"Make pypsa config","text":"<p>Script to create a PyPSA config file based on the REMIND output/config. NB: Needs to be run before the coupled PyPSA run.</p> Example"},{"location":"reference/remind_coupling/make_pypsa_config/#remind_coupling.make_pypsa_config--config-file-name-needs-to-match-the-output-of-the-snakemake-rule","title":"!! config file name needs to match the output of the snakemake rule","text":"<p><code>snakemake config_file_name -f --cores=1 # makes config_file_name</code> <code>snakemake --configfile=config_file_name</code> # the run</p>"},{"location":"reference/remind_coupling/make_pypsa_config/#remind_coupling.make_pypsa_config.get_currency_conversion","title":"<code>get_currency_conversion(template_cfg)</code>","text":"<p>Get the currency conversion factor from the template config.</p> <p>Parameters:</p> Name Type Description Default <code>template_cfg</code> <code>dict</code> <p>The template configuration dictionary.</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>The currency conversion factor.</p> Source code in <code>workflow/scripts/remind_coupling/make_pypsa_config.py</code> <pre><code>def get_currency_conversion(template_cfg: dict) -&gt; float:\n    \"\"\"Get the currency conversion factor from the template config.\n\n    Args:\n        template_cfg (dict): The template configuration dictionary.\n\n    Returns:\n        float: The currency conversion factor.\n    \"\"\"\n    # ugly stuff\n    etl_cfg = template_cfg[\"remind_etl\"][\"etl_steps\"]\n    techec_cfg = [step for step in etl_cfg if step[\"name\"] == \"technoeconomic_data\"][0]\n    return techec_cfg[\"kwargs\"][\"currency_conversion\"]\n</code></pre>"},{"location":"reference/remind_coupling/make_pypsa_config/#remind_coupling.make_pypsa_config.read_remind_em_prices","title":"<code>read_remind_em_prices(remind_outp_dir, region)</code>","text":"<p>Read relevant REMIND data from the output directory.</p> <p>Parameters:</p> Name Type Description Default <code>remind_outp_dir</code> <code>PathLike</code> <p>Path to the REMIND output directory.</p> required <code>region</code> <code>str</code> <p>Remind region to filter the data by.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Dictionary containing REMIND data.</p> Source code in <code>workflow/scripts/remind_coupling/make_pypsa_config.py</code> <pre><code>def read_remind_em_prices(remind_outp_dir: os.PathLike, region: str) -&gt; dict:\n    \"\"\"\n    Read relevant REMIND data from the output directory.\n\n    Args:\n        remind_outp_dir (os.PathLike): Path to the REMIND output directory.\n        region (str): Remind region to filter the data by.\n\n    Returns:\n        dict: Dictionary containing REMIND data.\n    \"\"\"\n\n    co2_p = (\n        (\n            coupl_utils.read_remind_csv(os.path.join(remind_outp_dir, \"pm_taxCO2eq.csv\"))\n            .query(\"region == @region\")\n            .drop(columns=[\"region\"])\n            .set_index(\"year\")\n        )\n        * 1000\n        / MW_CO2\n        * MW_C\n    )  # gC to ton CO2\n\n    # get remind version\n    with open(os.path.join(remind_outp_dir, \"c_model_version.csv\")) as f:\n        remind_v = f.read().split(\"\\n\")[1].replace(\",\", \"\").replace(\" \", \"\")\n    # get remind run name\n    with open(os.path.join(remind_outp_dir, \"c_expname.csv\")) as f:\n        remind_exp_name = f.read().split(\"\\n\")[1].replace(\",\", \"\").replace(\" \", \"\")\n\n    return {\"co2_prices\": co2_p, \"version\": remind_v, \"expname\": remind_exp_name}\n</code></pre>"},{"location":"reference/remind_coupling/setup/","title":"Setup","text":"<p>Setup scripts/remind as standalone for dev</p> <ul> <li>add paths</li> <li>mock snakemake</li> </ul>"},{"location":"reference/remind_coupling/setup/#remind_coupling.setup.setup_paths","title":"<code>setup_paths()</code>","text":"<p>Add the scripts directory to the Python path to enable direct imports from workflow/scripts subdirectories without relative imports. (for debugging) Call this at the beginning of any script that needs to import from sibling modules.</p> Source code in <code>workflow/scripts/remind_coupling/setup.py</code> <pre><code>def setup_paths():\n    \"\"\"\n    Add the scripts directory to the Python path to enable direct imports\n    from workflow/scripts subdirectories without relative imports. (for debugging)\n    Call this at the beginning of any script that needs to import from sibling modules.\n    \"\"\"\n    scripts_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n    if scripts_dir not in sys.path:\n        sys.path.insert(0, scripts_dir)\n</code></pre>"},{"location":"tutorials/coupling/","title":"REMIND-coupling","text":"<p>Please get in touch with us if you are interested in coupling with REMIND.</p>"},{"location":"tutorials/running/","title":"Running the workflow","text":""},{"location":"tutorials/running/#snakefile-config","title":"Snakefile &amp; config","text":"<p>PyPSA-China execution is controlled by the snakemake workflow manager. The <code>snakefile</code> should be understood as a control file. All implemented functionalities are accessible via the config and CLI args, the control should only be edited if you need new features (or to fix bugs). </p> <p>The workflow is intended to be managed via the config files rather than the CLI argumnents. As explained below, the config files allow the control of nearly all aspects of the PyPSA-China execution.  </p>"},{"location":"tutorials/running/#default-run-local","title":"Default run (local)","text":"<p>If the pypsa-china environment is  installed &amp; activated, you can lauch run a with the default settings  launch default run<pre><code>cd &lt;my_install_location&gt;\nsnakemake\n</code></pre> This is a large job! It is unwise to run it locally or on login nodes.</p>"},{"location":"tutorials/running/#custom-config-options","title":"custom config options","text":"<p>The default config may not fit your solver or preferences. Rather than overwriting it, which may cause merge isues with future versions, it is recommended to make a small <code>my_config.yaml</code>.</p> <p>Snakemake overwrites configs in order that they are added. Your config file then only needs the subset of options you want to overwrite and can be executed using/</p> launch custom run<pre><code>cd &lt;workflow_root_folder&gt;\nsnakemake --configfile=&lt;configs/my_changes.yml&gt; &lt;optional_additional_snakemake_opts&gt;\n</code></pre> <p>In the config you can specify the solver you would like to use. By default gurobi is chosen.</p>"},{"location":"tutorials/running/#dry-runs","title":"dry runs","text":"<p>The <code>-n</code> flag from snakemake allows to start a \"dry run\", which is a mock run that will show what rules would be executed. We recommend always running with this flag first before launching an actual run.</p> <pre><code>cd &lt;workflow_root_folder&gt;\nsnakemake -n &lt;additional_options&gt;\n</code></pre>"},{"location":"tutorials/running/#snakemake-command-line-options","title":"Snakemake command line options","text":"<p>You can find the full list of options here. The most important ones are summarised in the tricks page</p>"},{"location":"tutorials/running/#running-a-module-in-standalone","title":"Running a module in standalone","text":"<p>You can run any of the modules as standalone python thanks to the <code>mock_snakemake</code> function.</p>"},{"location":"tutorials/running/#remotehpc-execution-with-profiles","title":"Remote/hpc execution with profiles","text":"<p>The <code>--profile</code> arg allows you to specify execution snakemake options via a yaml config file. This is a better alternative to the CLI in many cases. </p> <p>The <code>--profile</code> is especially useful for specifying and controlling remote execution, for example on an HPC. You will find a slurm HPC example in the config under <code>pik_hpc_profile/</code>. This allows you to set the compute resources per rule. Note that the profile must be called <code>config.yaml</code> </p> <p>The profile can also include any other snakemake flag, such as re-run conditions and verbosity.</p> <p>To execute the workflow on a remote resource, e.g. a slurm cluster you can do the below from a login node:</p> <p><code>bash title=\"execute remotely` cd &lt;workflow_root_folder&gt; snakemake --profile=&lt;configs/my_profile_parent_dir&gt; &lt;optional_additional_snakemake_opts&gt;</code></p>"},{"location":"tutorials/running/#running-the-examples","title":"running the examples","text":"<p>EXAMPLES ARE CURRENTLY UNAVAILABLE</p>"},{"location":"tutorials/running/#development-and-debugging","title":"Development and debugging","text":"<p>It is possible to run each script as standalone using the <code>mock_snakemake</code> helper utility. The python file will run the main script, reading the Snakefile.</p>"},{"location":"tutorials/running/#specific-settings","title":"specific settings","text":"<p>You can edit the wildcards in mocksnakemake. You can also mock passing a configfile ontop of defaults by adding it to the snakefile (add configfile:\"my_config\" after the default configs)  </p>"},{"location":"tutorials/running/#questions","title":"Questions?","text":"<p>Please contact us if needed. Note that pypsa-China-PIK is currently under active development and we recommend waiting until the alpha or first stable release.</p>"},{"location":"tutorials/snakemake_tricks/","title":"Snakemake tricks","text":""},{"location":"tutorials/snakemake_tricks/#good-to-know","title":"Good to know","text":"<p>Snakemake workflow execution is based on input files. If inputs to a rule are missing, outdated or changed, snakemake will work out the compute graph needed to build all required data.</p> <p>The first rule in the snajefile is the default target rule. Pseudo rules such as <code>plot_all</code> can be put at the top to call a whole workflow. Otherwise the target rule can be specified with <code>-t</code>.</p> <p>Snakemake executes lazily based on change criteria, such as changes in input data. Most of the decision are based on timestamps. You can specify change criterias via a profile or the cli</p>"},{"location":"tutorials/snakemake_tricks/#useful-command-line-arguments","title":"Useful command line arguments","text":"<ul> <li><code>--touch</code> : update all the timestamps of previously runned files. Good if you don't want to re-run a partially completed workflow</li> <li><code>-n</code>: dry run - see what would be computed</li> <li><code>-t</code>: specify the target rule</li> <li><code>f</code>: specify the target output file</li> </ul>"},{"location":"tutorials/snakemake_tricks/#dag-printout","title":"DAG printout","text":"<p>It is possible to visualise the workflow with  <code>snakemake results/dag/rules_graph.png -f</code></p>"}]}