# A cluster profile for the PIK HPC
# Profile allows to activeate the gurobi license via an ssh tunnel and solve pypsa
# adapted from jdblischak/smk-simple-slurm/ & jhemmp https://github.com/aodenweller/pypsa-eur/blob/remind_develop_pypsa_v0.10.0/cluster_config/config.yaml
# PIK-2024

# define the executor
executor: cluster-generic
jobs: 500

# configure smk
restart-times: 1
max-jobs-per-second: 10
max-status-checks-per-second: 1
# local-cores: 1
latency-wait: 60
# retries: 2
keep-going: False
rerun-incomplete: True
printshellcmds: True
# scheduler: greedy
# use-conda: True
rerun-trigger:
- code
- input
- mtime
- params
verbose: false

# define the cluster submission command (slurm wrapper)
cluster-generic-submit-cmd:
  mkdir -p logs/logs_slurm/{rule} &&
  sbatch
    --partition={resources.partition}
    --qos={resources.qos}
    --cpus-per-task={threads}
    --mem={resources.mem_mb}
    --job-name={rule}-{wildcards}-smk
    --output=logs/logs_slurm/{rule}/%j-{rule}-{wildcards}.out
    --parsable
    --export=PATH,LD_LIBRARY_PATH,GUROBI_HOME
cluster-generic-cancel-cmd: scancel

# now define the resources
default-resources:
  partition: standard
  qos: short
  mem_mb: 2000
  cores: 8

set-resources:
  solve_network_myopic:
    mem_mb: 80000
    time: 300
  solve_networks:
    mem_mb: 80000
    qos : short
    partition: standard
    time : 300
  build_cutout:
    time: 500
    cores: 4
    partition: io
    qos: io
  plot_network:
    mem_mb: 8000
    cores: 2
  retrieve_build_up_raster:
    time: 300
    cores: 1
    partition: io # login
    qos: io
  retrieve_Grass_raster:
    time: 300
    cores: 1
    partition: io
    qos: io
  retrieve_Bare_raster:
    time: 300
    cores: 1
    partition: io
    qos: io
  retrieve_Shrubland_raster:
    time: 300
    cores: 1
    partition: io
    qos: io
