{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to the PyPSA-China (PIK version) documentation!","text":"<p>This is the documentation for the China Python Power System Analysis (PyPSA-China) Model maintained by the Potsdam Institute for Climate Impact Studies' Energy Transition Lab. PyPSA-China-PIK is an open model to simulate the future of energy in China at provincial level. Currently, electricity and heat are covered. The model can also be partially coupled to the REMIND Integrated Assesment Model to obtain multi-sectoral demand pathways.</p>"},{"location":"#workflow","title":"Workflow","text":"<p>PyPSA-China-PIK is a workflow built around the PyPSA energy system modelling framework. The workflow consists of gathering and preparing relevant data, formulating the problem as a PyPSA network object, minimising the system costs using a solver and post-processing the data. The workflow is managed by Snakemake.  </p>"},{"location":"#capabilities","title":"Capabilities","text":"Capability Description Sectors Models both electricity and heat demand and supply Open Data &amp; Code Fully open-source model and datasets Provincial Resolution Simulates energy systems at the provincial level across China Renewables &amp; Hydro Models renewables and hydroelectricity availability based on historic data using Atlite. Renewables can be split by capacity factor grades Storage Long and short-term duration storage with hydrogen, pumped hydro and batteries. Coupling with REMIND Can be partially coupled to the REMIND IAM for multi-sectoral analysis Cost Optimization Minimizes system costs using a solver of your choice Flexible Workflow Managed by Snakemake for reproducible and automated analysis Customizable Modular structure can easily be customized Post-processing Tools Detailed results analysis and visualization <p>The model has been validated against short term energy trends. Todo: add figure. </p>"},{"location":"#key-packages","title":"Key packages","text":"<p>You may want to look into the PyPSA documentation, the snakemake documentation and the atlite documentation. Atlite is used to generate</p>"},{"location":"#authors-and-credits","title":"Authors and Credits","text":"<p>This version has is maintained by the PIK team. It is not yet published, please contact us in case you are interested in using the model.</p> <p>The model is based on the PyPSA-EUR work by the Tom Brown Group, originally adapted to China by Hailiang Liu et al for their study of hydro-power in china and extended by Xiaowei Zhou et al for their  \"Multi-energy system horizon planning: Early decarbonisation in China avoids stranded assets\" paper. It has received significant upgrades.</p>"},{"location":"configuration/","title":"Configuration Reference","text":"<p>This is documentation for the PyPSA-China configuration (<code>config/default_config.yaml</code> &amp; <code>config/technology_config.yaml</code>). The configuration file controls various aspects of the PyPSA-China energy system modeling workflow.</p>"},{"location":"configuration/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Run Configuration</li> <li>File Paths</li> <li>Grid Topology</li> <li>Scenarios</li> <li>CO2 Scenarios</li> <li>Time Snapshots</li> <li>Logging</li> <li>Data Fetch Toggles</li> <li>Atlite Weather Settings</li> <li>Renewable Energy Technologies</li> <li>Heat Demand</li> <li>Reporting</li> <li>Bus and Carriers</li> <li>Technology Categories</li> <li>Sectors &amp; Component</li> <li>Hydro Dams</li> <li>Hydrogen Storage</li> <li>Solving</li> <li>Transmission Lines</li> <li>Security Constraints</li> <li>Existing Capacities</li> <li>Regions</li> <li>Input/Output Settings</li> <li>Transmission Efficiency</li> <li>Combined Heat and Power (CHP) Parameters</li> <li>Solar Technology Parameters</li> <li>Heat Pump Configuration</li> <li>Thermal Energy Storage</li> <li>Electricity Sector Configuration</li> <li>Hydroelectric Power</li> <li>Fossil Fuel Ramping Constraints</li> <li>Nuclear Reactor Parameters</li> <li>Economic Parameters</li> </ul>"},{"location":"configuration/#usage-notes","title":"Usage Notes","text":"<ol> <li> <p>File Organization: Configuration files should be placed in the <code>config/</code> directory.</p> </li> <li> <p>Customization: Do not edit <code>default_config.yaml</code>. Overwrite the variables you need in <code>my_config.yaml</code>. See running section</p> </li> <li> <p>Technology Configuration: Additional technology parameters are defined in separate files in  <code>config/technology_config.yaml</code> </p> </li> <li> <p>Solver Selection: Remember to select a solver that is installed.</p> </li> </ol>"},{"location":"configuration/#run-configuration","title":"Run Configuration","text":"<pre><code>run:\n  name: unamed_run\n  is_remind_coupled: true\nforesight: \"overnight\"\n</code></pre> <ul> <li><code>run.name</code>: Identifier for the model run. Used for organizing results and logs.</li> <li><code>run.is_remind_coupled</code>: Boolean indicating whether the model run is coupled with REMIND. Set to <code>false</code> for standalone PyPSA-China runs. True overwrites numerous settings.</li> <li><code>foresight</code>: [\"overnight\"|\"myopic\"] Set to <code>\"overnight\"</code> for perfect foresight within one time horizon. Each horizon is solved independently. Set to <code>myopic</code> to obtain a sequential pathway of overnight horizons, with the result of the previous horizon acting as the brownfield for the next horizon.</li> </ul>"},{"location":"configuration/#file-paths","title":"File Paths","text":"<p><pre><code>paths:\n  results_dir: \"results/\"\n  costs_dir: \"resources/data/costs/default\"\n  yearly_regional_load: \n    ac: \"resources/data/load/Provincial_Load_2020_2060_MWh.csv\"\n    ac_to_mwh: 1\n</code></pre> - <code>results_dir</code>: Directory where model results are stored - <code>costs_dir</code>: Directory containing technology &amp; cost data - <code>yearly_regional_load.ac</code>: Path to provincial electricity load data file - <code>yearly_regional_load.ac_to_mwh</code>: Conversion factor to MWh (set to 1 if load file is already in MWh)</p>"},{"location":"configuration/#grid-topology","title":"Grid Topology","text":"<p><pre><code>edge_paths:\n  current: \"resources/data/grids/edges_current.csv\"\n  current+FCG: \"resources/data/grids/edges_current_FCG.csv\"\n  current+Neighbor: \"resources/data/grids/edges_current_neighbor.csv\"\n</code></pre> Paths to named transmission line topology files for different scenarios. Predefined   - <code>\"current\"</code>: Existing transmission network   - <code>\"current+FCG\"</code>: Fully connected grid (represents current expansion plans)   - <code>\"current+Neighbor\"</code>: current + all neighbours connected</p>"},{"location":"configuration/#scenario-configuration","title":"Scenario Configuration","text":"<p>A scenario is a set of time horizons with a carbon reduction pathway. The variations become snakemake wildcards.</p> <pre><code>scenario: \n  co2_pathway: [\"exp175default\"]\n  topology: \"current+FCG\"\n  planning_horizons: [year_list]\n  heating_demand: [\"positive\"]\n</code></pre> <ul> <li><code>co2_pathway</code>: List of named CO2 emission scenarios to model</li> <li><code>topology</code>: Grid topology configuration. Must correspond to an edge_path</li> <li><code>planning_horizons</code>: Years to model</li> <li><code>heating_demand</code>: Heating demand scenarios (will be overhauled)</li> </ul>"},{"location":"configuration/#co2-scenarios","title":"CO2 Scenarios","text":"<p>Emission reduction pathways. <code>scenario.co2_pathway</code> entries must be defined here.</p> <pre><code>co2_scenarios: \n  exp175default: # pathway name\n    control: \"reduction\"\n    pathway:\n      '2020': 0.0\n      '2025': 0.22623418\n      # ... additional years\n</code></pre> <p>Defines CO2 emission reduction scenarios: - <code>control</code>: Control mechanism (<code>\"price\"</code>, <code>\"reduction\"</code>, <code>\"budget\"</code>, or <code>None</code>) - <code>pathway</code>: Yearly values for the price, reduction or budget. Ignored if <code>control=None</code>.</p>"},{"location":"configuration/#snapshots","title":"Snapshots","text":"<p>Snapshots are the modelled timestamps</p> <p><pre><code>snapshots:\n  start: \"01-01 00:00\"\n  end: \"12-31 23:00\"\n  bounds: 'both'\n  freq: '5h'\n  frequency: 5.\n  end_year_plus1: false\n</code></pre> Controls the temporal resolution of the model: - <code>start</code>: Start date and time (MM-DD HH:MM format) - <code>end</code>: End date and time - <code>bounds</code>: Include start/end points (<code>'both'</code>, <code>'left'</code>, <code>'right'</code>) - <code>freq</code>: Frequency string for pandas (e.g., <code>'5h'</code> for 5-hour intervals) - <code>frequency</code>: weight of a snapshot (eg 5 means that the generation for that timestamp will be multiplifed by 5 to get actual MWh values &amp; costs) - <code>end_year_plus1</code>: bool: not single year?</p> <p>Time sampling</p> <p>Lower resolutions are currently dumbly sampled as 1 in n. Nonetheless it is possible to get mixes and costs close to those of the 1hr resolution.</p>"},{"location":"configuration/#logging","title":"Logging","text":"<pre><code>logging_level: INFO\nlogging:\n  level: INFO\n  format: '%(levelname)s:%(name)s:%(message)s'\n</code></pre> <ul> <li><code>logging_level</code>: Global logging level</li> <li><code>logging.level</code>: Detailed logging level</li> <li><code>logging.format</code>: Log message format string</li> </ul>"},{"location":"configuration/#data-fetch-toggles","title":"Data fetch toggles","text":"<pre><code>enable:\n  build_cutout: false\n  retrieve_cutout: false\n  retrieve_raster: false\n</code></pre> <p>Controls whether the data fetch workflow steps are enabled: - <code>build_cutout</code>: Build new weather data cutouts - <code>retrieve_cutout</code>: Download existing cutouts (not in correct file structure, must be manually copied) - <code>retrieve_raster</code>: Download existing raster data</p>"},{"location":"configuration/#atlite-weather","title":"Atlite Weather","text":"<pre><code>atlite:\n  cutout_name: \"China-2020c\"\n  freq: \"h\"\n  nprocesses: 1\n  show_progress: true\n  monthly_requests: true\n  cutouts:\n    China-2020c:\n      module: era5\n      dx: 0.25\n      dy: 0.25\n      weather_year: 2020\n</code></pre> <p>Configuration for weather data processing: - <code>cutout_name</code>: Name of cutout (searched for in <code>resources/cutouts/</code>) - <code>freq</code>: Temporal frequency for weather data - <code>nprocesses</code>: Number of parallel processes - <code>show_progress</code>: Display progress bars - <code>monthly_requests</code>: Split requests by month - <code>cutouts</code>: config(s) for one or more cutout names   - <code>module</code>: Weather data source (e.g., <code>era5</code>)   - <code>dx</code>/<code>dy</code>: Spatial resolution in degrees   - <code>weather_year</code>: weather year to fetch/use</p>"},{"location":"configuration/#renewable-energy-technologies-atlite","title":"Renewable Energy Technologies Atlite","text":""},{"location":"configuration/#wind","title":"Wind","text":"<pre><code>renewable:\n  onwind | offwind: \n    cutout: cutout-name\n    resource:\n      method: wind\n      turbine: model\n    resource_classes:\n      min_cf_delta: 0.05\n      n: 3\n    capacity_per_sqkm: 3\n    potential: simple\n    natura: false\n    clip_p_max_pu: 1.e-2\n    min_p_nom_max: 1.e-2\n</code></pre>"},{"location":"configuration/#solar-pv","title":"Solar PV","text":"<pre><code>  solar:\n    cutout: cutout-name\n    resource:\n      method: pv\n      panel: model\n      orientation:\n        slope: 35.\n        azimuth: 180.\n    resource_classes:\n      min_cf_delta: 0.02\n      n: 2\n    capacity_per_sqkm: 5.1\n    potential: simple\n    correction_factor: 0.85\n    natura: false\n    clip_p_max_pu: 1.e-2\n    min_p_nom_max: 1.e-2\n</code></pre> <p>Common renewable parameters: - <code>cutout</code>: Weather data cutout name to use - <code>resource.method</code>: Resource calculation method - <code>resource.turbine | panel</code>: Technology specification - <code>resource_classes</code>: Capacity factor binning to reduce provincial aggregation effects   - <code>n</code>: Number of resource bins   - <code>min_cf_delta</code>: Minimum capacity factor difference between bins - <code>capacity_per_sqkm</code>: Max installable Power density (MW/km\u00b2) - <code>potential</code>: Potential calculation method (<code>simple</code> or <code>conservative</code>) - <code>correction_factor</code>: Technology-specific correction factor - <code>natura</code>: Consider nature protection areas - <code>max_depth</code>: Maximum water depth for offshore wind (m) - <code>clip_p_max_pu</code>: Minimum capacity factor threshold - <code>min_p_nom_max</code>: Minimum installable capacity threshold</p> <p><pre><code>renewable_potential_cutoff: 200  # MW\n</code></pre> Skip locations with potential below this threshold to reduce problem size.</p>"},{"location":"configuration/#heat-demand","title":"Heat Demand","text":"<pre><code>heat_demand:\n  start_day: \"01-04\"\n  end_day: \"30-09\"\n  heating_start_temp: 15.0\n  heating_lin_slope: 1\n  heating_offet: 0\nsolar_thermal_angle: 45\n</code></pre> <ul> <li><code>start_day</code>/<code>end_day</code>: Heating season dates (DD-MM format)</li> <li><code>heating_start_temp</code>: Temperature threshold for heating demand (\u00b0C)</li> <li><code>heating_lin_slope</code>: Linear relationship slope</li> <li><code>heating_offset</code>: Linear model offset</li> <li><code>solar_thermal_angle</code>: Solar thermal collector angle (degrees)</li> </ul>"},{"location":"configuration/#reporting","title":"Reporting","text":"<pre><code>reporting:\n  adjust_link_capacities_by_efficiency: true\n</code></pre> <ul> <li><code>adjust_link_capacities_by_efficiency</code>: PyPSA links capacities are in input. Typical reporting for AC is in output capacitiy. If true Adjust link capacities by efficiency for consistent AC-side reporting.</li> </ul>"},{"location":"configuration/#bus-and-carrier-configuration","title":"Bus and Carrier Configuration","text":"<pre><code>bus_suffix: [\"\",\" central heat\",\" decentral heat\",\" gas\",\" coal\"]\nbus_carrier: {\n    \"\": \"AC\",\n    \" central heat\": \"heat\",\n    \" decentral heat\": \"heat\",\n    \" gas\": \"gas\",\n    \" coal\": \"coal\",\n}\n</code></pre> <p>Defines bus types and their corresponding energy carriers: - <code>bus_suffix</code>: List of bus name suffixes - <code>bus_carrier</code>: Mapping of suffixes to carrier types</p>"},{"location":"configuration/#technology-categories","title":"Technology Categories","text":"<pre><code>Techs:\n  vre_techs: [\"onwind\",\"offwind\",\"solar\",\"solar thermal\",\"hydroelectricity\", \"nuclear\",\"biomass\",\"beccs\",\"heat pump\",\"resistive heater\",\"Sabatier\",\"H2 CHP\", \"fuel cell\"]\n  conv_techs: [\"OCGT\", \"CCGT\", \"CHP gas\", \"gas boiler\",\"coal boiler\",\"coal power plant\",\"CHP coal\"]\n  store_techs: [\"H2\",\"battery\",\"water tanks\",\"PHS\"]\n  coal_cc: true\n  hydrogen_lines: true\n</code></pre> <p>Technology categorization: - <code>vre_techs</code>: Variable renewable energy technologies - <code>conv_techs</code>: Conventional generation technologies - <code>store_techs</code>: Storage technologies - <code>coal_cc</code>: Enable coal with carbon capture retrofit (myopic only). Coal carbon capture new-builds are controled via \"vre_techs\" for overnight. - <code>hydrogen_lines</code>: Enable hydrogen transmission lines</p>"},{"location":"configuration/#sector-and-component-switches","title":"Sector and component Switches","text":"<pre><code>heat_coupling: false\nadd_biomass: True\nadd_hydro: True\nadd_H2: True\nadd_methanation: True\nline_losses: True\nno_lines: False\n</code></pre> <p>Control which components to include in the model: - <code>heat_coupling</code>: Enable heat sector coupling - <code>add_biomass</code>: Include biomass technologies - <code>add_hydro</code>: Include hydroelectric power - <code>add_H2</code>: Include hydrogen technologies (and pipelines) - <code>add_methanation</code>: Include methanation processes - <code>line_losses</code>: Model transmission line losses - <code>no_lines</code>: Disable transmission lines (autartik)</p>"},{"location":"configuration/#hydro-dams","title":"Hydro Dams","text":"<pre><code>hydro_dams:\n  dams_path: \"resources/data/hydro/dams_large.csv\"\n  inflow_path: \"resources/data/hydro/daily_hydro_inflow_per_dam_1979_2016_m3.pickle\"\n  inflow_date_start: \"1979-01-01\"\n  inflow_date_end: \"2017-01-01\"\n  reservoir_initial_capacity_path: \"resources/data/hydro/reservoir_initial_capacity.pickle\"\n  reservoir_effective_capacity_path: \"resources/data/hydro/reservoir_effective_capacity.pickle\"\n  river_links_stations: \"\"\n  p_nom_path: \"resources/data/p_nom/hydro_p_nom.h5\"\n  p_max_pu_path: \"resources/data/p_nom/hydro_p_max_pu.h5\"\n  p_max_pu_key: \"hydro_p_max_pu\"\n  damn_flows_path: \"resources/data/hydro/dam_flow_links.csv\"\n</code></pre> <p>Hydroelectric dam configuration: - <code>dams_path</code>: CSV file with dam locations and characteristics - <code>inflow_path</code>: Historical inflow data - <code>inflow_date_start</code>/<code>inflow_date_end</code>: Date range for inflow data - <code>reservoir_*_capacity_path</code>: Reservoir capacity data files - <code>p_nom_path</code>: Installed capacity data - <code>p_max_pu_path</code>: Maximum capacity factor time series - <code>damn_flows_path</code>: Dam flow connections</p>"},{"location":"configuration/#hydrogen-storage","title":"Hydrogen Storage","text":"<pre><code>H2:\n  geo_storage_nodes: [\"Sichuan\", \"Chongqing\", \"Hubei\", \"Jiangxi\", \"Anhui\", \"Jiangsu\", \"Shandong\", \"Guangdong\"]\n</code></pre> <ul> <li><code>geo_storage_nodes</code>: Provinces with geological hydrogen storage potential</li> </ul>"},{"location":"configuration/#solving-configuration","title":"Solving Configuration","text":""},{"location":"configuration/#general-options","title":"General Options","text":"<pre><code>solving:\n  options:\n    formulation: kirchhoff\n    load_shedding: false\n    voll: 1e5\n    noisy_costs: false\n    min_iterations: 4\n    max_iterations: 6\n    clip_p_max_pu: 0.01\n    skip_iterations: false\n    track_iterations: false\n</code></pre> <ul> <li><code>formulation</code>: Network formulation (<code>kirchhoff</code> or <code>angles</code>)</li> <li><code>load_shedding</code>: Allow unserved energy</li> <li><code>voll</code>: Value of lost load (EUR/MWh)</li> <li><code>noisy_costs</code>: Add noise to costs for degeneracy handling</li> <li><code>min_iterations</code>/<code>max_iterations</code>: Iteration bounds for iterative solving</li> <li><code>clip_p_max_pu</code>: Minimum capacity factor threshold</li> <li><code>skip_iterations</code>/<code>track_iterations</code>: Iteration control</li> </ul>"},{"location":"configuration/#transmission-lines","title":"Transmission Lines","text":"<pre><code>lines:\n  line_length_factor: 1.25\n  expansion:\n    transmission_limit: vopt\n    base_year: 2020\n</code></pre> <ul> <li><code>line_length_factor</code>: Line length increase factor, representing impossibility to build perfectly straight</li> <li><code>expansion.transmission_limit</code>: Transmission expansion constraint</li> <li><code>[v]opt</code>: Optimal (unconstrained)</li> <li><code>v1.03</code>: Volume-constrained (3% increase limit PER YEAR not horizon)</li> <li><code>c1.03</code>: Cost-constrained (3% increase limit PER YEAR not horizon)</li> <li><code>expansion.base_year</code>: Reference year for expansion limits (to calculate max exp). Should be year of input network topology file (which has capacities)</li> </ul>"},{"location":"configuration/#security-constraints","title":"Security Constraints","text":"<pre><code>security:\n  line_margin: 70  # Max percent of line capacity\n</code></pre> <ul> <li><code>line_margin</code>: Security margin for transmission lines (% of capacity)</li> </ul>"},{"location":"configuration/#existing-capacities","title":"Existing Capacities","text":"<pre><code>existing_capacities:\n  add: True\n  grouping_years: [1980,1985, 1990, 1995, 2000, 2005, 2010, 2015, 2019, 2020, 2025, 2030, 2035, 2040, 2045, 2050, 2055, 2060]\n  threshold_capacity: 1\n  techs: ['coal','CHP coal', 'CHP gas', 'OCGT', 'CCGT', 'solar', 'solar thermal', 'onwind', 'offwind','coal boiler','ground heat pump','nuclear']\n</code></pre> <p>Configuration for incorporating existing power plant capacities: - <code>add</code>: Include existing GEM capacities in the model. These are retired only if they reach end of life (determined based on the costs tech config) - <code>grouping_years</code>: Years for capacity grouping - <code>threshold_capacity</code>: Minimum capacity threshold - <code>techs</code>: Technologies to include from existing capacity data</p>"},{"location":"configuration/#region-configuration","title":"Region Configuration","text":"<pre><code>fetch_regions:\n  simplify_tol: 0.5\n</code></pre> <ul> <li><code>simplify_tol</code>: Tolerance for region boundary simplification</li> </ul>"},{"location":"configuration/#inputoutput-settings","title":"Input/Output Settings","text":"<pre><code>io:\n  nc_compression:\n    level: 4\n    zlib: True\n</code></pre> <p>Controls compression settings for NetCDF output files: - <code>level</code>: Compression level (0-9, higher = more compression) - <code>zlib</code>: Enable zlib compression</p>"},{"location":"configuration/#technology-config","title":"TECHNOLOGY CONFIG","text":"<p>The below are in <code>config/technology_config.yaml</code></p>"},{"location":"configuration/#transmission-efficiency","title":"Transmission Efficiency","text":"<pre><code>transmission_efficiency:\n  DC:\n    efficiency_static: 0.98\n    efficiency_per_1000km: 0.977\n  H2 pipeline:\n    efficiency_static: 1\n    efficiency_per_1000km: 0.979\n    compression_per_1000km: 0.019\n</code></pre> <p>Defines transmission efficiency parameters for different carriers &amp; technologies:</p>"},{"location":"configuration/#dc-transmission","title":"DC Transmission","text":"<ul> <li><code>efficiency_static</code>: Base efficiency for DC transmission lines (98%)</li> <li><code>efficiency_per_1000km</code>: Distance-dependent efficiency factor per 1000 km (97.7% per 1000 km)</li> </ul>"},{"location":"configuration/#hydrogen-pipeline","title":"Hydrogen Pipeline","text":"<ul> <li><code>efficiency_static</code>: Base efficiency for hydrogen pipelines (100% - no static losses)</li> <li><code>efficiency_per_1000km</code>: Distance-dependent efficiency factor per 1000 km (97.9% per 1000 km)</li> <li><code>compression_per_1000km</code>: Energy required for compression per 1000 km (1.9% of transported energy)</li> </ul> <p>The total efficiency for transmission links is calculated as: <pre><code>Total efficiency = efficiency_static \u00d7 (efficiency_per_1000km)^(distance_km/1000)\n</code></pre></p>"},{"location":"configuration/#combined-heat-and-power-chp-parameters","title":"Combined Heat and Power (CHP) Parameters","text":"<p><pre><code>chp_parameters:\n  eff_th: 0.5304\n</code></pre> CHP is treated either as a pre-defined values or using a back pressure coeficient (see DK Energy catalogue). For coal CHP use the back pressure (variable ratio). For gas CHP use pre-defined parameter.</p> <p>TODO this is legacy, check implentation is OK. CHP often runs heat first in China but code seems to maintain electric eff</p>"},{"location":"configuration/#solar-technology-parameters","title":"Solar Technology Parameters","text":"<pre><code>solar_cf_correction: 0.85\n</code></pre> <ul> <li><code>solar_cf_correction</code>: Correction factor applied to solar capacity factors (85%)</li> </ul> <p>This factor accounts for various real-world effects that reduce solar Thermal performance compared to theoretical values. NOT APPLIED TO PV</p>"},{"location":"configuration/#heat-pump-configuration","title":"Heat Pump Configuration","text":"<pre><code>time_dep_hp_cop: True\n</code></pre> <ul> <li><code>time_dep_hp_cop</code>: Enable time-dependent coefficient of performance (COP) for heat pumps</li> </ul> <p>When enabled, heat pump efficiency varies with ambient temperature conditions throughout the year.</p>"},{"location":"configuration/#thermal-energy-storage","title":"Thermal Energy Storage","text":"<pre><code>water_tanks:\n  tes_tau:\n    decentral: 3. # days\n    central: 180 # days\n</code></pre> <p>Standing loss parameters for thermal energy storage (water tanks): - <code>decentral</code>: Time constant for decentralized thermal storage (3 days) - <code>central</code>: Time constant for centralized thermal storage (180 days)</p> <p>The time constant (tau) determines the rate of thermal losses.</p>"},{"location":"configuration/#electricity-sector-configuration","title":"Electricity Sector Configuration","text":"<p>Partially legacy and to be revised</p> <pre><code>electricity:\n  max_hours:\n    battery: 6\n    H2: 168\n  min_charge:\n    battery: 0.1 # fraction of e_nom\n</code></pre>"},{"location":"configuration/#storage-parameters","title":"Storage Parameters","text":"<ul> <li><code>max_hours.battery</code>: Maximum storage duration for batteries (6 hours)</li> <li><code>max_hours.H2</code>: Maximum storage duration for hydrogen storage (168 hours = 1 week)</li> <li><code>min_charge.battery</code>: Minimum state of charge for batteries (10% of nominal energy capacity)</li> </ul>"},{"location":"configuration/#hydroelectric-power","title":"Hydroelectric Power","text":"<pre><code>hydro:\n  hydro_capital_cost: True\n  marginal_cost:\n    reservoir: 0\n  PHS_max_hours: 24 # hours\n</code></pre> <ul> <li><code>hydro_capital_cost</code>: Include capital costs for hydroelectric plants</li> <li><code>marginal_cost.reservoir</code>: Marginal cost of storage (defaults to zeo)</li> <li><code>PHS_max_hours</code>: Maximum storage duration for pumped hydro storage (24 hours)</li> </ul>"},{"location":"configuration/#fossil-fuel-ramping-constraints","title":"Fossil Fuel Ramping Constraints","text":"<p>Operational ramping constraints for fossil fuel power plants:</p> <pre><code>fossil_ramps:\n  tech:\n    ramp_limit_up: 0.5 # fraction of p_nom per hour\n    ramp_limit_down: 0.5 # fraction of p_nom per hour\n</code></pre> <ul> <li><code>ramp_limit_up</code>: Maximum upward ramping rate (50% of nominal capacity per hour)</li> <li><code>ramp_limit_down</code>: Maximum downward ramping rate (50% of nominal capacity per hour)</li> </ul>"},{"location":"configuration/#nuclear-reactor-parameters","title":"Nuclear Reactor Parameters","text":"<p>Instead of solving unit commitment problem, which is computationally expensive, can stylise baseload generation. The upper limit reflects planned and unplanned outages</p> <p><pre><code>nuclear_reactors:\n  p_max_pu: 0.88 # fraction of p_nom, after IEAE\n  p_min_pu: 0.7 # fraction of p_nom\n</code></pre> Operational constraints for nuclear power plants: - <code>p_max_pu</code>: Maximum power output (% of nominal capacity) - <code>p_min_pu</code>: Minimum power output (% of nominal capacity)</p>"},{"location":"configuration/#economic-parameters","title":"Economic Parameters","text":"<pre><code>costs:\n  discountrate: 0.06\n  social_discount_rate: 0.02\n  USD2013_to_EUR2013: 0.9189\n  marginal_cost:\n    hydro: 0.\n  pv_utility_fraction: 1\n</code></pre>"},{"location":"configuration/#discount-rates","title":"Discount Rates","text":"<ul> <li><code>discountrate</code>: Financial discount rate for investment decisions (6%)</li> <li><code>social_discount_rate</code>: Social discount rate for welfare analysis (2%)</li> </ul> <p>The financial discount rate is used for technology investment decisions, while the social discount rate is applied for broader economic impact assessments.</p>"},{"location":"configuration/#currency-conversion","title":"Currency Conversion","text":"<ul> <li><code>USD2013_to_EUR2013</code>: Exchange rate from USD to EUR for 2013 prices (0.9189 EUR/USD) - used for technoeconomic conversion. Weakpoint, only one currency. Also tech costs are now in Euro2015 from DK EA</li> </ul>"},{"location":"configuration/#solar-pv-configuration","title":"Solar PV Configuration","text":"<ul> <li><code>pv_utility_fraction</code>: Fraction of solar PV that is utility-scale (100%)</li> </ul> <p>This parameter distinguishes between utility-scale and residential/distributed solar installations, affecting cost assumptions and grid integration characteristics.</p>"},{"location":"Tutorials/coupling/","title":"REMIND-coupling","text":"<p>Please get in touch with us if you are interested in coupling with REMIND.</p>"},{"location":"Tutorials/running/","title":"Running the workflow","text":""},{"location":"Tutorials/running/#snakefile-config","title":"Snakefile &amp; config","text":"<p>PyPSA-China execution is controlled by the snakemake workflow manager. The <code>snakefile</code> should be understood as a control file. All implemented functionalities are accessible via the config and CLI args, the control should only be edited if you need new features (or to fix bugs). </p> <p>The workflow is intended to be managed via the config files rather than the CLI argumnents. As explained below, the config files allow the control of nearly all aspects of the PyPSA-China execution.  </p>"},{"location":"Tutorials/running/#default-run-local","title":"Default run (local)","text":"<p>If the pypsa-china environment is  installed &amp; activated, you can lauch run a with the default settings  launch default run<pre><code>cd &lt;my_install_location&gt;\nsnakemake\n</code></pre> This is a large job! It is unwise to run it locally or on login nodes.</p>"},{"location":"Tutorials/running/#custom-config-options","title":"custom config options","text":"<p>The default config may not fit your solver or preferences. Rather than overwriting it, which may cause merge isues with future versions, it is recommended to make a small <code>my_config.yaml</code>.</p> <p>Snakemake overwrites configs in order that they are added. Your config file then only needs the subset of options you want to overwrite and can be executed using/</p> launch custom run<pre><code>cd &lt;workflow_root_folder&gt;\nsnakemake --configfile=&lt;configs/my_changes.yml&gt; &lt;optional_additional_snakemake_opts&gt;\n</code></pre> <p>In the config you can specify the solver you would like to use. By default gurobi is chosen.</p>"},{"location":"Tutorials/running/#dry-runs","title":"dry runs","text":"<p>The <code>-n</code> flag from snakemake allows to start a \"dry run\", which is a mock run that will show what rules would be executed. We recommend always running with this flag first before launching an actual run.</p> <pre><code>cd &lt;workflow_root_folder&gt;\nsnakemake -n &lt;additional_options&gt;\n</code></pre>"},{"location":"Tutorials/running/#snakemake-command-line-options","title":"Snakemake command line options","text":"<p>You can find the full list of options here. The most important ones are summarised in the tricks page</p>"},{"location":"Tutorials/running/#running-a-module-in-standalone","title":"Running a module in standalone","text":"<p>You can run any of the modules as standalone python thanks to the <code>mock_snakemake</code> function.</p>"},{"location":"Tutorials/running/#remotehpc-execution-with-profiles","title":"Remote/hpc execution with profiles","text":"<p>The <code>--profile</code> arg allows you to specify execution snakemake options via a yaml config file. This is a better alternative to the CLI in many cases. </p> <p>The <code>--profile</code> is especially useful for specifying and controlling remote execution, for example on an HPC. You will find a slurm HPC example in the config under <code>pik_hpc_profile/</code>. This allows you to set the compute resources per rule. Note that the profile must be called <code>config.yaml</code> </p> <p>The profile can also include any other snakemake flag, such as re-run conditions and verbosity.</p> <p>To execute the workflow on a remote resource, e.g. a slurm cluster you can do the below from a login node:</p> <p><code>bash title=\"execute remotely` cd &lt;workflow_root_folder&gt; snakemake --profile=&lt;configs/my_profile_parent_dir&gt; &lt;optional_additional_snakemake_opts&gt;</code></p>"},{"location":"Tutorials/running/#running-the-examples","title":"running the examples","text":"<p>EXAMPLES ARE CURRENTLY UNAVAILABLE</p>"},{"location":"Tutorials/running/#development-and-debugging","title":"Development and debugging","text":"<p>It is possible to run each script as standalone using the <code>mock_snakemake</code> helper utility. The python file will run the main script, reading the Snakefile.</p>"},{"location":"Tutorials/running/#specific-settings","title":"specific settings","text":"<p>You can edit the wildcards in mocksnakemake. You can also mock passing a configfile ontop of defaults by adding it to the snakefile (add configfile:\"my_config\" after the default configs)  </p>"},{"location":"Tutorials/running/#questions","title":"Questions?","text":"<p>Please contact us if needed. Note that pypsa-China-PIK is currently under active development and we recommend waiting until the alpha or first stable release.</p>"},{"location":"Tutorials/snakemake_tricks/","title":"Snakemake tricks","text":""},{"location":"Tutorials/snakemake_tricks/#good-to-know","title":"Good to know","text":"<p>Snakemake workflow execution is based on input files. If inputs to a rule are missing, outdated or changed, snakemake will work out the compute graph needed to build all required data.</p> <p>The first rule in the snajefile is the default target rule. Pseudo rules such as <code>plot_all</code> can be put at the top to call a whole workflow. Otherwise the target rule can be specified with <code>-t</code>.</p> <p>Snakemake executes lazily based on change criteria, such as changes in input data. Most of the decision are based on timestamps. You can specify change criterias via a profile or the cli</p>"},{"location":"Tutorials/snakemake_tricks/#useful-command-line-arguments","title":"Useful command line arguments","text":"<ul> <li><code>--touch</code> : update all the timestamps of previously runned files. Good if you don't want to re-run a partially completed workflow</li> <li><code>-n</code>: dry run - see what would be computed</li> <li><code>-t</code>: specify the target rule</li> <li><code>f</code>: specify the target output file</li> </ul>"},{"location":"Tutorials/snakemake_tricks/#dag-printout","title":"DAG printout","text":"<p>It is possible to visualise the workflow with  <code>snakemake results/dag/rules_graph.png -f</code></p>"},{"location":"installation/quick_start/","title":"Quick Start","text":"<p>System requirements</p> <p>With the low-resolution settings, PyPSA-China-PIK will run on a local machine or laptop. Solving a full year at hourly resolution will require a high performance cluster or server with around 50GB of RAM - depending on your settings.</p>"},{"location":"installation/quick_start/#installation","title":"Installation","text":"<ol> <li> <p>Install the conda package manager </p> <p>It may be possible to run the workflow with a better manager such as <code>uv</code> but it will not work out of the box</p> Unix/MacOs/WSLWindows <p>Conda tips</p> <p>Conda is a rather large install with a GUI and features not required by PyPSA-China. You may prefer to use the lighter miniconda, which is our recommendation. </p> <p>You can check whether you already have conda with <code>which anaconda</code> or <code>which conda</code>. Newer condas have a faster dependcy solver - as the package is rather large we strongly recommend you update to <code>v&gt; 2024.10</code>.</p> <p>Conda tips</p> <p>Conda is a rather large install with a GUI and features not required by PyPSA-China. You may prefer to use the lighter miniconda, which is our recommendation</p> <p>You can check whether you already have conda with <code>where anaconda</code> or <code>where conda</code>. Newer condas have a faster dependcy solver - as the package is rather large we strongly recommend you update to <code>v&gt; 2024.9</code>.</p> </li> <li> <p>Setup the environment: This can take a long time </p> Unix/MacOS/WSL install dependencies<pre><code>cd &lt;workflow_location&gt;\nconda env create --file=workflow/envs/environment.yaml\n</code></pre> <ol> <li>Activate environment</li> </ol> Unix/MacOS/WSLWindows activate environment<pre><code>source activate pypsa-china\n</code></pre> activate environment<pre><code>conda activate pypsa-china\n</code></pre> <ol> <li>Fetch data herefore have to run data fetches</li> <li>Install a Solver: e.g. gurobi HiGHS or cplex. The current default configuration uses gurobi.</li> <li>Run locally</li> </ol> </li> </ol>"},{"location":"installation/quick_start/#testing-the-installation","title":"Testing the installation","text":"Unix/MacOS/WSLWindows Test install<pre><code>source activate pypsa-china\ncd &lt;workflow_location&gt;\npytest tests/integration\n</code></pre> Test install<pre><code>conda activate pypsa-china\ncd &lt;workflow_location&gt;\npytest tests/integration\n</code></pre>"},{"location":"reference/SUMMARY/","title":"SUMMARY","text":""},{"location":"reference/SUMMARY/#reference-guide","title":"Reference guide","text":"<p>This is the code documentation extracted from docstrings.</p>"},{"location":"reference/add_brownfield/","title":"Add brownfield","text":"<p>Functions for myopic pathway network generation snakemake rules</p> <p>Add assets from previous planning horizon network solution to network to solve for the current planning horizon.</p> <p>Usage: - use via a snakemake rule - debug: use as standalone with mock_snakemake (reads snakefile)</p>"},{"location":"reference/add_brownfield/#add_brownfield.add_brownfield","title":"<code>add_brownfield(n, n_p, year)</code>","text":"<p>Add paid for assets as p_nom to the current network</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>Network</code> <p>next network to prep &amp; optimize in the planning horizon</p> required <code>n_p</code> <code>Network</code> <p>previous optimized network</p> required <code>year</code> <code>int</code> <p>the planning year</p> required Source code in <code>workflow/scripts/add_brownfield.py</code> <pre><code>def add_brownfield(n: pypsa.Network, n_p: pypsa.Network, year: int):\n    \"\"\"Add paid for assets as p_nom to the current network\n\n    Args:\n        n (pypsa.Network): next network to prep &amp; optimize in the planning horizon\n        n_p (pypsa.Network): previous optimized network\n        year (int): the planning year\n    \"\"\"\n\n    logger.info(\"Adding brownfield\")\n    # electric transmission grid set optimised capacities of previous as minimum\n    n.lines.s_nom_min = n_p.lines.s_nom_opt\n    # dc_i = n.links[n.links.carrier==\"DC\"].index\n    # n.links.loc[dc_i, \"p_nom_min\"] = n_p.links.loc[dc_i, \"p_nom_opt\"]\n    # update links\n    n.links.loc[(n.links.length &gt; 0) &amp; (n.links.lifetime == np.inf), \"p_nom\"] = n_p.links.loc[\n        (n_p.links.carrier == \"AC\") &amp; (n_p.links.build_year == 0), \"p_nom_opt\"\n    ]\n    n.links.loc[(n.links.length &gt; 0) &amp; (n.links.lifetime == np.inf), \"p_nom_min\"] = n_p.links.loc[\n        (n_p.links.carrier == \"AC\") &amp; (n_p.links.build_year == 0), \"p_nom_opt\"\n    ]\n\n    if year == 2025:\n        add_build_year_to_new_assets(n_p, 2020)\n\n    for c in n_p.iterate_components([\"Link\", \"Generator\", \"Store\"]):\n\n        attr = \"e\" if c.name == \"Store\" else \"p\"\n\n        # first, remove generators, links and stores that track\n        # CO2 or global EU values since these are already in n\n        n_p.mremove(c.name, c.df.index[c.df.lifetime == np.inf])\n        # remove assets whose build_year + lifetime &lt; year\n        n_p.mremove(c.name, c.df.index[c.df.build_year + c.df.lifetime &lt; year])\n        # remove assets if their optimized nominal capacity is lower than a threshold\n        # since CHP heat Link is proportional to CHP electric Link, ensure threshold is compatible\n        chp_heat = c.df.index[(c.df[attr + \"_nom_extendable\"] &amp; c.df.index.str.contains(\"CHP\"))]\n\n        threshold = snakemake.config[\"existing_capacities\"][\"threshold_capacity\"]\n\n        if not chp_heat.empty:\n            threshold_chp_heat = (\n                threshold * c.df.loc[chp_heat].efficiency2 / c.df.loc[chp_heat].efficiency\n            )\n            n_p.mremove(\n                c.name,\n                chp_heat[c.df.loc[chp_heat, attr + \"_nom_opt\"] &lt; threshold_chp_heat],\n            )\n\n        n_p.mremove(\n            c.name,\n            c.df.index[\n                c.df[attr + \"_nom_extendable\"]\n                &amp; ~c.df.index.isin(chp_heat)\n                &amp; (c.df[attr + \"_nom_opt\"] &lt; threshold)\n            ],\n        )\n\n        # copy over assets but fix their capacity\n        c.df[attr + \"_nom\"] = c.df[attr + \"_nom_opt\"]\n        c.df[attr + \"_nom_extendable\"] = False\n        c.df[attr + \"_nom_max\"] = np.inf\n\n        n.import_components_from_dataframe(c.df, c.name)\n\n        # copy time-dependent\n        selection = n.component_attrs[c.name].type.str.contains(\"series\") &amp; n.component_attrs[\n            c.name\n        ].status.str.contains(\"Input\")\n\n        for tattr in n.component_attrs[c.name].index[selection]:\n            n.import_series_from_dataframe(c.pnl[tattr].set_index(n.snapshots), c.name, tattr)\n\n    for tech in [\"onwind\", \"offwind\", \"solar\"]:\n        ds_tech = xr.open_dataset(snakemake.input[\"profile_\" + tech])\n        p_nom_max_initial = ds_tech[\"p_nom_max\"].to_pandas()\n\n        if tech == \"offwind\":\n            for node in OFFSHORE_WIND_NODES:\n                n.generators.loc[\n                    (n.generators.bus == node)\n                    &amp; (n.generators.carrier == tech)\n                    &amp; (n.generators.build_year == year),\n                    \"p_nom_max\",\n                ] = (\n                    p_nom_max_initial[node]\n                    - n_p.generators[\n                        (n_p.generators.bus == node) &amp; (n_p.generators.carrier == tech)\n                    ].p_nom_opt.sum()\n                )\n        else:\n            for node in PROV_NAMES:\n                n.generators.loc[\n                    (n.generators.bus == node)\n                    &amp; (n.generators.carrier == tech)\n                    &amp; (n.generators.build_year == year),\n                    \"p_nom_max\",\n                ] = (\n                    p_nom_max_initial[node]\n                    - n_p.generators[\n                        (n_p.generators.bus == node) &amp; (n_p.generators.carrier == tech)\n                    ].p_nom_opt.sum()\n                )\n\n    n.generators.loc[(n.generators.p_nom_max &lt; 0), \"p_nom_max\"] = 0\n\n    # retrofit coal power plant with carbon capture\n    n.generators.loc[n.generators.carrier == \"coal power plant\", \"p_nom_extendable\"] = True\n    n.generators.loc[\n        n.generators.index.str.contains(\"retrofit\") &amp; ~n.generators.index.str.contains(str(year)),\n        \"p_nom_extendable\",\n    ] = False\n</code></pre>"},{"location":"reference/add_electricity/","title":"Add electricity","text":"<p>Misc collection of functions supporting network prep     still to be cleaned up</p>"},{"location":"reference/add_electricity/#add_electricity.add_missing_carriers","title":"<code>add_missing_carriers(n, carriers)</code>","text":"<p>Function to add missing carriers to the network without raising errors.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>Network</code> <p>the pypsa network object</p> required <code>carriers</code> <code>list | set</code> <p>a list of carriers that should be included</p> required Source code in <code>workflow/scripts/add_electricity.py</code> <pre><code>def add_missing_carriers(n: pypsa.Network, carriers: list | set) -&gt; None:\n    \"\"\"Function to add missing carriers to the network without raising errors.\n\n    Args:\n        n (pypsa.Network): the pypsa network object\n        carriers (list | set): a list of carriers that should be included\n    \"\"\"\n    missing_carriers = set(carriers) - set(n.carriers.index)\n    if len(missing_carriers) &gt; 0:\n        n.add(\"Carrier\", missing_carriers)\n</code></pre>"},{"location":"reference/add_electricity/#add_electricity.calculate_annuity","title":"<code>calculate_annuity(lifetime, discount_rate)</code>","text":"<p>Calculate the annuity factor for an asset with lifetime n years and discount rate of r, e.g. annuity(20, 0.05) * 20 = 1.6</p> <p>Parameters:</p> Name Type Description Default <code>lifetime</code> <code>int</code> <p>ecomic asset lifetime for discounting/NPV calc</p> required <code>discount_rate</code> <code>float</code> <p>the WACC</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>the annuity factor</p> Source code in <code>workflow/scripts/add_electricity.py</code> <pre><code>def calculate_annuity(lifetime: int, discount_rate: float) -&gt; float:\n    \"\"\"Calculate the annuity factor for an asset with lifetime n years and\n    discount rate of r, e.g. annuity(20, 0.05) * 20 = 1.6\n\n    Args:\n        lifetime (int): ecomic asset lifetime for discounting/NPV calc\n        discount_rate (float): the WACC\n\n    Returns:\n        float: the annuity factor\n    \"\"\"\n    r = discount_rate\n    n = lifetime\n\n    if isinstance(r, pd.Series):\n        if r.any() &lt; 0:\n            raise ValueError(\"Discount rate must be positive\")\n        if r.any() &lt; 0:\n            raise ValueError(\"Discount rate must be positive\")\n        return pd.Series(1 / n, index=r.index).where(r == 0, r / (1.0 - 1.0 / (1.0 + r) ** n))\n    elif r &lt; 0:\n        raise ValueError(\"Discount rate must be positive\")\n    elif r &lt; 0:\n        raise ValueError(\"Discount rate must be positive\")\n    elif r &gt; 0:\n        return r / (1.0 - 1.0 / (1.0 + r) ** n)\n    else:\n        return 1 / n\n</code></pre>"},{"location":"reference/add_electricity/#add_electricity.load_costs","title":"<code>load_costs(tech_costs, cost_config, elec_config, cost_year, n_years, econ_lifetime=40)</code>","text":"<p>Calculate the anualised capex costs and OM costs for the technologies based on the input data</p> <p>Parameters:</p> Name Type Description Default <code>tech_costs</code> <code>PathLike</code> <p>the csv containing the costs</p> required <code>cost_config</code> <code>dict</code> <p>the snakemake pypsa-china cost config</p> required <code>elec_config</code> <code>dict</code> <p>the snakemake pypsa-china electricity config</p> required <code>cost_year</code> <code>int</code> <p>the year for which the costs are retrived</p> required <code>n_years</code> <code>int</code> <p>the # of years represented by the snapshots/investment period</p> required <code>econ_lifetime</code> <code>int</code> <p>the max lifetime over which to discount. Defaults to 40.</p> <code>40</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: costs dataframe in [CURRENCY] per MW_ ... or per MWh_ ...</p> Source code in <code>workflow/scripts/add_electricity.py</code> <pre><code>def load_costs(\n    tech_costs: PathLike,\n    cost_config: dict,\n    elec_config: dict,\n    cost_year: int,\n    n_years: int,\n    econ_lifetime=40,\n) -&gt; pd.DataFrame:\n    \"\"\"Calculate the anualised capex costs and OM costs for the technologies based on the input data\n\n    Args:\n        tech_costs (PathLike): the csv containing the costs\n        cost_config (dict): the snakemake pypsa-china cost config\n        elec_config (dict): the snakemake pypsa-china electricity config\n        cost_year (int): the year for which the costs are retrived\n        n_years (int): the # of years represented by the snapshots/investment period\n        econ_lifetime (int, optional): the max lifetime over which to discount. Defaults to 40.\n\n    Returns:\n        pd.DataFrame: costs dataframe in [CURRENCY] per MW_ ... or per MWh_ ...\n    \"\"\"\n\n    # set all asset costs and other parameters\n    costs = pd.read_csv(tech_costs, index_col=list(range(3))).sort_index()\n    costs.fillna(\" \", inplace=True)\n    # correct units to MW and EUR\n    costs.loc[costs.unit.str.contains(\"/kW\"), \"value\"] *= 1e3\n    costs.loc[costs.unit.str.contains(\"USD\"), \"value\"] *= cost_config[\"USD2013_to_EUR2013\"]\n    costs.loc[costs.unit.str.contains(\"USD\"), \"value\"] *= cost_config[\"USD2013_to_EUR2013\"]\n\n    cost_year = float(cost_year)\n    costs = (\n        costs.loc[idx[:, cost_year, :], \"value\"]\n        .unstack(level=2)\n        .groupby(\"technology\")\n        .sum(min_count=1)\n    )\n\n    # TODO set default lifetime as option\n    if \"discount rate\" not in costs.columns:\n        costs.loc[:, \"discount rate\"] = cost_config[\"discountrate\"]\n    costs = costs.fillna(\n        {\n            \"CO2 intensity\": 0,\n            \"FOM\": 0,\n            \"VOM\": 0,\n            \"discount rate\": cost_config[\"discountrate\"],\n            \"efficiency\": 1,\n            \"fuel\": 0,\n            \"investment\": 0,\n            \"lifetime\": 25,\n        }\n    )\n\n    discount_period = costs[\"lifetime\"].apply(lambda x: min(x, econ_lifetime))\n    costs[\"capital_cost\"] = (\n        (calculate_annuity(discount_period, costs[\"discount rate\"]) + costs[\"FOM\"] / 100.0)\n        * costs[\"investment\"]\n        * n_years\n    )\n\n    costs.at[\"OCGT\", \"fuel\"] = costs.at[\"gas\", \"fuel\"]\n    costs.at[\"CCGT\", \"fuel\"] = costs.at[\"gas\", \"fuel\"]\n    costs.at[\"CCGT-CCS\", \"fuel\"] = costs.at[\"gas\", \"fuel\"]\n\n    costs[\"marginal_cost\"] = costs[\"VOM\"] + costs[\"fuel\"] / costs[\"efficiency\"]\n\n    costs = costs.rename(columns={\"CO2 intensity\": \"co2_emissions\"})\n\n    costs.at[\"OCGT\", \"co2_emissions\"] = costs.at[\"gas\", \"co2_emissions\"]\n    costs.at[\"CCGT\", \"co2_emissions\"] = costs.at[\"gas\", \"co2_emissions\"]\n\n    def costs_for_storage(store, link1, link2=None, max_hours=1.0):\n        capital_cost = link1[\"capital_cost\"] + max_hours * store[\"capital_cost\"]\n        if link2 is not None:\n            capital_cost += link2[\"capital_cost\"]\n        return pd.Series(dict(capital_cost=capital_cost, marginal_cost=0.0, co2_emissions=0.0))\n\n    max_hours = elec_config[\"max_hours\"]\n    costs.loc[\"battery\"] = costs_for_storage(\n        costs.loc[\"battery storage\"], costs.loc[\"battery inverter\"], max_hours=max_hours[\"battery\"]\n    )\n\n    for attr in (\"marginal_cost\", \"capital_cost\"):\n        overwrites = cost_config.get(attr)\n        overwrites = cost_config.get(attr)\n        if overwrites is not None:\n            overwrites = pd.Series(overwrites)\n            costs.loc[overwrites.index, attr] = overwrites\n\n    return costs\n</code></pre>"},{"location":"reference/add_electricity/#add_electricity.sanitize_carriers","title":"<code>sanitize_carriers(n, config)</code>","text":"<p>Sanitize the carrier information in a PyPSA Network object.</p> <p>The function ensures that all unique carrier names are present in the network's carriers attribute, and adds nice names and colors for each carrier according to the provided configuration dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>Network</code> <p>PyPSA Network object representing the electrical power system.</p> required <code>config</code> <code>dict</code> <p>A dictionary containing configuration information, specifically the    \"plotting\" key with \"nice_names\" and \"tech_colors\" keys for carriers.</p> required Source code in <code>workflow/scripts/add_electricity.py</code> <pre><code>def sanitize_carriers(n: pypsa.Network, config: dict) -&gt; None:\n    \"\"\"Sanitize the carrier information in a PyPSA Network object.\n\n    The function ensures that all unique carrier names are present in the network's\n    carriers attribute, and adds nice names and colors for each carrier according\n    to the provided configuration dictionary.\n\n    Args:\n        n (pypsa.Network): PyPSA Network object representing the electrical power system.\n        config (dict): A dictionary containing configuration information, specifically the\n               \"plotting\" key with \"nice_names\" and \"tech_colors\" keys for carriers.\n    \"\"\"\n    # update default nice names w user settings\n    nice_names = NICE_NAMES_DEFAULT.update(config[\"plotting\"].get(\"nice_names\", {}))\n    for c in n.iterate_components():\n        if \"carrier\" in c.df:\n            add_missing_carriers(n, c.df.carrier)\n\n    # sort the nice names to match carriers and fill missing with \"ugly\" names\n    carrier_i = n.carriers.index\n    nice_names = pd.Series(nice_names).reindex(carrier_i).fillna(carrier_i.to_series())\n    # replace empty nice names with nice names\n    n.carriers.nice_name.where(n.carriers.nice_name != \"\", nice_names, inplace=True)\n\n    # TODO make less messy, avoid using map\n    tech_colors = config[\"plotting\"][\"tech_colors\"]\n    colors = pd.Series(tech_colors).reindex(carrier_i)\n    # try to fill missing colors with tech_colors after renaming\n    missing_colors_i = colors[colors.isna()].index\n    colors[missing_colors_i] = missing_colors_i.map(lambda x: rename_techs(x, nice_names)).map(\n        tech_colors\n    )\n    if colors.isna().any():\n        missing_i = list(colors.index[colors.isna()])\n        logger.warning(f\"tech_colors for carriers {missing_i} not defined in config.\")\n    n.carriers[\"color\"] = n.carriers.color.where(n.carriers.color != \"\", colors)\n</code></pre>"},{"location":"reference/add_existing_baseyear/","title":"Add existing baseyear","text":"<p>Functions to add brownfield capacities to the network for a reference year</p>"},{"location":"reference/add_existing_baseyear/#add_existing_baseyear.add_build_year_to_new_assets","title":"<code>add_build_year_to_new_assets(n, baseyear)</code>","text":"<p>add a build year to new assets</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>Network</code> <p>the network</p> required <code>baseyear</code> <code>int</code> <p>year in which optimized assets are built</p> required Source code in <code>workflow/scripts/add_existing_baseyear.py</code> <pre><code>def add_build_year_to_new_assets(n: pypsa.Network, baseyear: int):\n    \"\"\"add a build year to new assets\n\n    Args:\n        n (pypsa.Network): the network\n        baseyear (int): year in which optimized assets are built\n    \"\"\"\n\n    # Give assets with lifetimes and no build year the build year baseyear\n    for c in n.iterate_components([\"Link\", \"Generator\", \"Store\"]):\n        attr = \"e\" if c.name == \"Store\" else \"p\"\n\n        assets = c.df.index[(c.df.lifetime != np.inf) &amp; (c.df[attr + \"_nom_extendable\"] is True)]\n\n        # add -baseyear to name\n        renamed = pd.Series(c.df.index, c.df.index)\n        renamed[assets] += \"-\" + str(baseyear)\n        c.df.rename(index=renamed, inplace=True)\n\n        assets = c.df.index[\n            (c.df.lifetime != np.inf)\n            &amp; (c.df[attr + \"_nom_extendable\"] is True)\n            &amp; (c.df.build_year == 0)\n        ]\n        c.df.loc[assets, \"build_year\"] = baseyear\n\n        # rename time-dependent\n        selection = n.component_attrs[c.name].type.str.contains(\"series\") &amp; n.component_attrs[\n            c.name\n        ].status.str.contains(\"Input\")\n        for attr in n.component_attrs[c.name].index[selection]:\n            c.pnl[attr].rename(columns=renamed, inplace=True)\n</code></pre>"},{"location":"reference/add_existing_baseyear/#add_existing_baseyear.add_existing_vre_capacities","title":"<code>add_existing_vre_capacities(n, costs, vre_caps, config)</code>","text":"<p>Add existing VRE capacities to the network and distribute them by vre grade potential. Adapted from pypsa-eur but the VRE capacities are province resolved.</p> <p>NOTE that using this function requires adding the land-use constraint in solve_network so   that the existing capacities are subtracted from the available potential</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>Network</code> <p>the network</p> required <code>costs</code> <code>DataFrame</code> <p>costs of the technologies</p> required <code>vre_caps</code> <code>DataFrame</code> <p>existing VRE capacities in MW</p> required <code>config</code> <code>dict</code> <p>snakemake configuration dictionary</p> required <p>Returns:     pd.DataFrame: DataFrame with existing VRE capacities distributed by CF grade</p> Source code in <code>workflow/scripts/add_existing_baseyear.py</code> <pre><code>def add_existing_vre_capacities(\n    n: pypsa.Network,\n    costs: pd.DataFrame,\n    vre_caps: pd.DataFrame,\n    config: dict,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Add existing VRE capacities to the network and distribute them by vre grade potential.\n    Adapted from pypsa-eur but the VRE capacities are province resolved.\n\n    NOTE that using this function requires adding the land-use constraint in solve_network so\n      that the existing capacities are subtracted from the available potential\n\n    Args:\n        n (pypsa.Network): the network\n        costs (pd.DataFrame): costs of the technologies\n        vre_caps (pd.DataFrame): existing VRE capacities in MW\n        config (dict): snakemake configuration dictionary\n    Returns:\n        pd.DataFrame: DataFrame with existing VRE capacities distributed by CF grade\n\n    \"\"\"\n\n    tech_map = {\"solar\": \"PV\", \"onwind\": \"Onshore\", \"offwind-ac\": \"Offshore\", \"offwind\": \"Offshore\"}\n    tech_map = {k: tech_map[k] for k in tech_map if k in config[\"Techs\"][\"vre_techs\"]}\n\n    grouped_vre = vre_caps.groupby([\"Tech\", \"bus\", \"DateIn\"]).Capacity.sum()\n    vre_df = grouped_vre.unstack().reset_index()\n    df_agg = pd.DataFrame()\n\n    for carrier in tech_map:\n\n        df = vre_df[vre_df.Tech == carrier].drop(columns=[\"Tech\"])\n        df.set_index(\"bus\", inplace=True)\n        df.columns = df.columns.astype(int)\n\n        # fetch existing vre generators (n grade bins per node)\n        gen_i = n.generators.query(\"carrier == @carrier\").index\n        carrier_gens = n.generators.loc[gen_i]\n        res_capacities = []\n        # for each bus, distribute the vre capacities by grade potential - best first\n        for bus, group in carrier_gens.groupby(\"bus\"):\n            if bus not in df.index:\n                continue\n            res_capacities.append(distribute_vre_by_grade(group.p_nom_max, df.loc[bus]))\n\n        if res_capacities:\n            res_capacities = pd.concat(res_capacities, axis=0)\n\n            for year in df.columns:\n                for gen in res_capacities.index:\n                    bus_bin = re.sub(f\" {carrier}.*\", \"\", gen)\n                    bus, bin_id = bus_bin.rsplit(\" \", maxsplit=1)\n                    name = f\"{bus_bin} {carrier}-{int(year)}\"\n                    capacity = res_capacities.loc[gen, year]\n                    if capacity &gt; 0.0:\n                        cost_key = carrier.split(\"-\", maxsplit=1)[0]\n                        df_agg.at[name, \"Fueltype\"] = carrier\n                        df_agg.at[name, \"Capacity\"] = capacity\n                        df_agg.at[name, \"DateIn\"] = int(year)\n                        df_agg.at[name, \"grouping_year\"] = int(year)\n                        df_agg.at[name, \"lifetime\"] = costs.at[cost_key, \"lifetime\"]\n                        df_agg.at[name, \"DateOut\"] = year + costs.at[cost_key, \"lifetime\"] - 1\n                        df_agg.at[name, \"bus\"] = bus\n                        df_agg.at[name, \"resource_class\"] = bin_id\n\n    if df_agg.empty:\n        return df_agg\n\n    df_agg.loc[:, \"Tech\"] = df_agg.Fueltype\n    return df_agg\n</code></pre>"},{"location":"reference/add_existing_baseyear/#add_existing_baseyear.add_paid_off_capacity","title":"<code>add_paid_off_capacity(network, paid_off_caps, costs, cutoff=100)</code>","text":"<p>Add capacities that have been paid off to the network. This is intended for REMIND coupling, where (some of) the REMIND investments can be freely allocated to the optimal node. NB: an additional constraing is needed to ensure that the capacity is not exceeded.</p> <p>Parameters:</p> Name Type Description Default <code>network</code> <code>Network</code> <p>the network to which the capacities are added.</p> required <code>paid_off_caps</code> <code>DataFrame</code> <p>DataFrame with paid off capacities &amp; columns [tech_group, Capacity, techs]</p> required <code>costs</code> <code>DataFrame</code> <p>techno-economic data for the technologies</p> required <code>cutoff</code> <code>int</code> <p>minimum capacity to be considered. Defaults to 100 MW.</p> <code>100</code> Source code in <code>workflow/scripts/add_existing_baseyear.py</code> <pre><code>def add_paid_off_capacity(\n    network: pypsa.Network, paid_off_caps: pd.DataFrame, costs: pd.DataFrame, cutoff=100\n):\n    \"\"\"\n    Add capacities that have been paid off to the network. This is intended\n    for REMIND coupling, where (some of) the REMIND investments can be freely allocated\n    to the optimal node. NB: an additional constraing is needed to ensure that\n    the capacity is not exceeded.\n\n    Args:\n        network (pypsa.Network): the network to which the capacities are added.\n        paid_off_caps (pd.DataFrame): DataFrame with paid off capacities &amp; columns\n            [tech_group, Capacity, techs]\n        costs (pd.DataFrame): techno-economic data for the technologies\n        cutoff (int, optional): minimum capacity to be considered. Defaults to 100 MW.\"\"\"\n\n    paid_off = paid_off_caps.reset_index()\n\n    # explode tech list per tech group (constraint will apply to group)\n    paid_off.techs = paid_off.techs.apply(to_list)\n    paid_off = paid_off.explode(\"techs\")\n    paid_off[\"carrier\"] = paid_off.techs.str.replace(\"'\", \"\")\n    paid_off.set_index(\"carrier\", inplace=True)\n    # clip small capacities\n    paid_off[\"p_nom_max\"] = paid_off.Capacity.apply(lambda x: 0 if x &lt; cutoff else x)\n    paid_off.drop(columns=[\"Capacity\", \"techs\"], inplace=True)\n    paid_off = paid_off.query(\"p_nom_max &gt; 0\")\n\n    component_settings = {\n        \"Generator\": {\n            \"join_col\": \"carrier\",\n            \"attrs_to_fix\": [\"p_min_pu\", \"p_max_pu\"],\n        },\n        \"Link\": {\n            \"join_col\": \"carrier\",\n            \"attrs_to_fix\": [\"p_min_pu\", \"p_max_pu\", \"efficiency\", \"efficiency2\"],\n        },\n        \"Store\": {\n            \"join_col\": \"carrier\",\n            \"attrs_to_fix\": [],\n        },\n    }\n\n    # TODO make a centralised setting or update cpl config\n    rename_carriers = {\"OCGT\": \"gas OCGT\", \"CCGT\": \"gas CCGT\"}\n    paid_off.rename(rename_carriers, inplace=True)\n\n    for component, settings in component_settings.items():\n        prefix = \"e\" if component == \"Store\" else \"p\"\n        paid_off_comp = paid_off.rename(columns={\"p_nom_max\": f\"{prefix}_nom_max\"})\n\n        # exclude brownfield capacities\n        df = getattr(network, component.lower() + \"s\").query(f\"{prefix}_nom_extendable == True\")\n        # join will add the tech_group and p_nom_max_rcl columns, used later for constraints\n        # rcl is legacy name from Adrian for region country limit\n        paid = df.join(paid_off_comp, on=[settings[\"join_col\"]], how=\"right\", rsuffix=\"_rcl\")\n        paid.dropna(subset=[f\"{prefix}_nom_max\", f\"{prefix}_nom_max_rcl\"], inplace=True)\n        paid = paid.loc[paid.index.dropna()]\n        if paid.empty:\n            continue\n\n        # REMIND cap is in output, PyPSA link in input\n        if component == \"Link\":\n            paid.loc[:, \"p_nom_max_rcl\"] /= paid.loc[:, \"efficiency\"]\n\n        paid.index += \"_paid_off\"\n        # set permissive options for the paid-off capacities (constraint per group added to model later)\n        paid[\"capital_cost\"] = 0\n        paid[f\"{prefix}_nom_min\"] = 0.0\n        paid[f\"{prefix}_nom\"] = 0.0\n        paid[f\"{prefix}_nom_max\"] = np.inf\n\n        # add to the network\n        network.add(component, paid.index, **paid)\n        # now add the dynamic attributes not carried over by n.add (per unit avail etc)\n        for missing_attr in settings[\"attrs_to_fix\"]:\n            df_t = getattr(network, component.lower() + \"s_t\")[missing_attr]\n            if not df_t.empty:\n                base_cols = [\n                    x for x in paid.index.str.replace(\"_paid_off\", \"\") if x in df_t.columns\n                ]\n                df_t.loc[:, pd.Index(base_cols) + \"_paid_off\"] = df_t[base_cols].rename(\n                    columns=lambda x: x + \"_paid_off\"\n                )\n\n    if \"biomass\" in paid_off.index and \"biomass\" not in network.generators.carrier.unique():\n        _add_paidoff_biomass(\n            network,\n            costs,\n            paid_off.loc[\"biomass\", \"p_nom_max\"],\n            tech_group=paid_off.loc[\"biomass\", \"tech_group\"],\n        )\n</code></pre>"},{"location":"reference/add_existing_baseyear/#add_existing_baseyear.add_power_capacities_installed_before_baseyear","title":"<code>add_power_capacities_installed_before_baseyear(n, costs, config, installed_capacities)</code>","text":"<p>Add existing power capacities to the network</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>Network</code> <p>the network</p> required <code>costs</code> <code>DataFrame</code> <p>techno-economic data</p> required <code>config</code> <code>dict</code> <p>configuration dictionary</p> required <code>installed_capacities</code> <code>DataFrame</code> <p>installed capacities in MW</p> required Source code in <code>workflow/scripts/add_existing_baseyear.py</code> <pre><code>def add_power_capacities_installed_before_baseyear(\n    n: pypsa.Network,\n    costs: pd.DataFrame,\n    config: dict,\n    installed_capacities: pd.DataFrame,\n):\n    \"\"\"\n    Add existing power capacities to the network\n\n    Args:\n        n (pypsa.Network): the network\n        costs (pd.DataFrame): techno-economic data\n        config (dict): configuration dictionary\n        installed_capacities (pd.DataFrame): installed capacities in MW\n    \"\"\"\n\n    logger.info(\"adding power capacities installed before baseyear\")\n\n    df = installed_capacities.copy()\n    # fix fuel type CHP order to match network\n    df[\"tech_clean\"] = df[\"Fueltype\"].str.replace(r\"^CHP (.+)$\", r\"\\1 CHP\", regex=True)\n    df[\"tech_clean\"] = df[\"tech_clean\"].str.replace(\"central \", \"\")\n    df[\"tech_clean\"] = df[\"tech_clean\"].str.replace(\"decentral \", \"\")\n\n    # TODO fix this based on config / centralise / other\n    carrier_map = {\n        \"coal\": \"coal\",\n        \"coal power plant\": \"coal\",\n        \"CHP coal\": \"CHP coal\",\n        \"coal CHP\": \"CHP coal\",\n        \"CHP gas\": \"CHP gas\",\n        \"gas CHP\": \"CHP gas\",\n        \"gas OCGT\": \"gas OCGT\",\n        \"gas CCGT\": \"gas CCGT\",\n        \"solar\": \"solar\",\n        \"solar thermal\": \"solar thermal\",\n        \"onwind\": \"onwind\",\n        \"offwind\": \"offwind\",\n        \"coal boiler\": \"coal boiler\",\n        \"ground-sourced heat pump\": \"heat pump\",\n        \"ground heat pump\": \"heat pump\",\n        \"air heat pump\": \"heat pump\",\n        \"nuclear\": \"nuclear\",\n    }\n    costs_map = {\n        \"coal power plant\": \"coal\",\n        \"coal CHP\": \"central coal CHP\",\n        \"gas CHP\": \"central gas CHP\",\n        \"gas OCGT\": \"OCGT\",\n        \"gas CCGT\": \"CCGT\",\n        \"solar\": \"solar\",\n        \"solar thermal\": \"central solar thermal\",\n        \"onwind\": \"onwind\",\n        \"offwind\": \"offwind\",\n        \"coal boiler\": \"central coal boier\",\n        \"heat pump\": \"central ground-sourced heat pump\",\n        \"ground-sourced heat pump\": \"central ground-sourced heat pump\",\n        \"nuclear\": \"nuclear\",\n    }\n\n    # add techs that may have a direct match to the technoecon data\n    missing_techs = {k: k for k in df.Fueltype.unique() if k not in costs_map}\n    costs_map.update(missing_techs)\n\n    if \"resource_class\" not in df.columns:\n        df[\"resource_class\"] = \"\"\n    else:\n        df.resource_class.fillna(\"\", inplace=True)\n    df.grouping_year = df.grouping_year.astype(int)\n    if config[\"existing_capacities\"].get(\"collapse_years\", False):\n        df.grouping_year = \"brownwfield\"\n\n    df_ = df.pivot_table(\n        index=[\"grouping_year\", \"tech_clean\", \"resource_class\"],\n        columns=\"bus\",\n        values=\"Capacity\",\n        aggfunc=\"sum\",\n    )\n\n    df_.fillna(0, inplace=True)\n\n    defined_carriers = n.carriers.index.unique().to_list()\n    vre_carriers = [\"solar\", \"onwind\", \"offwind\"]\n    vre_carriers = [\"solar\", \"onwind\", \"offwind\"]\n\n    # TODO do we really need to loop over the years? / so many things?\n    # something like df_.unstack(level=0) would be more efficient\n    for grouping_year, generator, resource_grade in df_.index:\n        build_year = 0 if grouping_year == \"brownwfield\" else grouping_year\n        logger.info(f\"Adding existing generator {generator} with year grp {grouping_year}\")\n        if not carrier_map.get(generator, \"missing\") in defined_carriers:\n            logger.warning(\n                f\"Carrier {carrier_map.get(generator, None)} for {generator} not defined in network - added anyway\"\n            )\n        elif costs_map.get(generator) is None:\n            raise ValueError(f\"{generator} not defined in technoecon map - check costs_map\")\n\n        # capacity is the capacity in MW at each node for this\n        capacity = df_.loc[grouping_year, generator]\n        if capacity.values.max() == 0:\n            continue\n        # fix index for network.add (merge grade to name)\n        capacity = capacity.unstack()\n        capacity = capacity[~capacity.isna()]\n        capacity = capacity[capacity &gt; config[\"existing_capacities\"][\"threshold_capacity\"]].dropna()\n        buses = capacity.index.get_level_values(0)\n        capacity.index = (\n            capacity.index.get_level_values(0) + \" \" + capacity.index.get_level_values(1)\n        )\n        capacity.index = capacity.index.str.rstrip() + \" \" + costs_map[generator]\n\n        costs_key = costs_map[generator]\n\n        if generator in vre_carriers:\n            mask = n.generators_t.p_max_pu.columns.map(n.generators.carrier) == generator\n            p_max_pu = n.generators_t.p_max_pu.loc[:, mask]\n            n.add(\n                \"Generator\",\n                capacity.index,\n                suffix=f\"-{grouping_year}\",\n                bus=buses,\n                carrier=carrier_map[generator],\n                p_nom=capacity,\n                p_nom_min=capacity,\n                p_nom_extendable=False,\n                marginal_cost=costs.at[costs_key, \"marginal_cost\"],\n                efficiency=costs.at[costs_key, \"efficiency\"],\n                p_max_pu=p_max_pu[capacity.index],\n                build_year=build_year,\n                lifetime=costs.at[costs_key, \"lifetime\"],\n                location=buses,\n            )\n\n        elif generator in [\"nuclear\", \"coal power plant\", \"biomass\", \"oil\"]:\n            n.add(\n                \"Generator\",\n                capacity.index,\n                suffix=\"-\" + str(grouping_year),\n                bus=buses,\n                carrier=carrier_map[generator],\n                p_nom=capacity,\n                p_nom_min=capacity,\n                p_nom_extendable=False,\n                p_max_pu=config[\"nuclear_reactors\"][\"p_max_pu\"] if generator == \"nuclear\" else 1,\n                p_min_pu=config[\"nuclear_reactors\"][\"p_min_pu\"] if generator == \"nuclear\" else 0,\n                marginal_cost=costs.at[costs_key, \"marginal_cost\"],\n                efficiency=costs.at[costs_key, \"efficiency\"],\n                build_year=build_year,\n                lifetime=costs.at[costs_key, \"lifetime\"],\n                location=buses,\n            )\n\n        # TODO this does not add the carrier to the list\n        elif generator in [\"gas CCGT\", \"gas OCGT\"]:\n            bus0 = buses + \" gas\"\n            carrier_ = carrier_map[generator]\n            # ugly fix to register the carrier. Emissions for sub carrier are 0: they are accounted for at gas bus\n            n.carriers.loc[carrier_] = {\n                \"co2_emissions\": 0,\n                \"color\": snakemake.config[\"plotting\"][\"tech_colors\"][carrier_],\n                \"nice_name\": snakemake.config[\"plotting\"][\"nice_names\"][carrier_],\n                \"max_growth\": np.inf,\n                \"max_relative_growth\": 0,\n            }\n            # now add link - carrier should exist\n            n.add(\n                \"Link\",\n                capacity.index,\n                suffix=\"-\" + str(grouping_year),\n                bus0=bus0,\n                bus1=buses,\n                carrier=carrier_map[generator],\n                marginal_cost=costs.at[costs_key, \"efficiency\"]\n                * costs.at[costs_key, \"VOM\"],  # NB: VOM is per MWel\n                # NB: fixed cost is per MWel\n                p_nom=capacity / costs.at[costs_key, \"efficiency\"],\n                p_nom_min=capacity / costs.at[costs_key, \"efficiency\"],\n                p_nom_extendable=False,\n                efficiency=costs.at[costs_key, \"efficiency\"],\n                build_year=build_year,\n                lifetime=costs.at[costs_key, \"lifetime\"],\n                location=buses,\n            )\n        elif generator in [\n            \"solar thermal\",\n            \"CHP coal\",\n            \"CHP gas\",\n            \"heat pump\",\n            \"coal boiler\",\n        ] and not config.get(\"heat_coupling\", False):\n            logger.info(f\"Skipped {generator} because heat coupling is not activated\")\n\n        elif generator == \"solar thermal\":\n            p_max_pu = n.generators_t.p_max_pu[capacity.index + \" central \" + generator]\n            p_max_pu.columns = capacity.index\n            n.add(\n                \"Generator\",\n                capacity.index,\n                suffix=f\"-{str(grouping_year)}\",\n                bus=buses + \" central heat\",\n                carrier=carrier_map[generator],\n                p_nom=capacity,\n                p_nom_min=capacity,\n                p_nom_extendable=False,\n                marginal_cost=costs.at[\"central \" + generator, \"marginal_cost\"],\n                p_max_pu=p_max_pu,\n                build_year=build_year,\n                lifetime=costs.at[\"central \" + generator, \"lifetime\"],\n                location=buses,\n            )\n\n        elif generator == \"CHP coal\":\n            bus0 = buses + \" coal\"\n            # TODO soft-code efficiency !!\n            hist_efficiency = 0.37\n            n.add(\n                \"Link\",\n                capacity.index,\n                suffix=f\"-{str(grouping_year)}\",\n                bus0=bus0,\n                bus1=capacity.index,\n                carrier=carrier_map[generator],\n                marginal_cost=hist_efficiency\n                * costs.at[\"central coal CHP\", \"VOM\"],  # NB: VOM is per MWel\n                p_nom=capacity / hist_efficiency,\n                p_nom_min=capacity / hist_efficiency,\n                p_nom_extendable=False,\n                efficiency=hist_efficiency,\n                p_nom_ratio=1.0,\n                c_b=0.75,\n                build_year=build_year,\n                lifetime=costs.at[\"central coal CHP\", \"lifetime\"],\n                location=buses,\n            )\n\n            n.add(\n                \"Link\",\n                capacity.index,\n                suffix=f\" boiler-{str(grouping_year)}\",\n                bus0=bus0,\n                bus1=capacity.index + \" central heat\",\n                carrier=carrier_map[generator],\n                marginal_cost=hist_efficiency\n                * costs.at[\"central coal CHP\", \"VOM\"],  # NB: VOM is per MWel\n                p_nom=capacity / hist_efficiency * costs.at[\"central coal CHP\", \"c_v\"],\n                p_nom_min=capacity / hist_efficiency * costs.at[\"central coal CHP\", \"c_v\"],\n                p_nom_extendable=False,\n                efficiency=hist_efficiency / costs.at[\"central coal CHP\", \"c_v\"],\n                build_year=build_year,\n                lifetime=costs.at[\"central coal CHP\", \"lifetime\"],\n                location=buses,\n            )\n\n        elif generator == \"CHP gas\":\n            hist_efficiency = 0.37\n            bus0 = buses + \" gas\"\n            n.add(\n                \"Link\",\n                capacity.index,\n                suffix=f\"-{str(grouping_year)}\",\n                bus0=bus0,\n                bus1=capacity.index,\n                carrier=carrier_map[generator],\n                marginal_cost=hist_efficiency\n                * costs.at[\"central gas CHP\", \"VOM\"],  # NB: VOM is per MWel\n                capital_cost=hist_efficiency\n                * costs.at[\"central gas CHP\", \"capital_cost\"],  # NB: fixed cost is per MWel,\n                p_nom=capacity / hist_efficiency,\n                p_nom_min=capacity / hist_efficiency,\n                p_nom_extendable=False,\n                efficiency=hist_efficiency,\n                p_nom_ratio=1.0,\n                c_b=costs.at[\"central gas CHP\", \"c_b\"],\n                build_year=build_year,\n                lifetime=costs.at[\"central gas CHP\", \"lifetime\"],\n                location=buses,\n            )\n            n.add(\n                \"Link\",\n                capacity.index,\n                suffix=f\" boiler-{str(grouping_year)}\",\n                bus0=bus0,\n                bus1=capacity.index + \" central heat\",\n                carrier=carrier_map[generator],\n                marginal_cost=hist_efficiency\n                * costs.at[\"central gas CHP\", \"VOM\"],  # NB: VOM is per MWel\n                p_nom=capacity / hist_efficiency * costs.at[\"central gas CHP\", \"c_v\"],\n                p_nom_min=capacity / hist_efficiency * costs.at[\"central gas CHP\", \"c_v\"],\n                p_nom_extendable=False,\n                efficiency=hist_efficiency / costs.at[\"central gas CHP\", \"c_v\"],\n                build_year=build_year,\n                lifetime=costs.at[\"central gas CHP\", \"lifetime\"],\n                location=buses,\n            )\n\n        elif generator == \"coal boiler\":\n            bus0 = buses + \" coal\"\n            for cat in [\" central \"]:\n                n.add(\n                    \"Link\",\n                    capacity.index,\n                    suffix=\"\" + cat + generator + \"-\" + str(grouping_year),\n                    bus0=bus0,\n                    bus1=capacity.index + cat + \"heat\",\n                    carrier=carrier_map[generator],\n                    marginal_cost=costs.at[cat.lstrip() + generator, \"efficiency\"]\n                    * costs.at[cat.lstrip() + generator, \"VOM\"],\n                    capital_cost=costs.at[cat.lstrip() + generator, \"efficiency\"]\n                    * costs.at[cat.lstrip() + generator, \"capital_cost\"],\n                    p_nom=capacity / costs.at[cat.lstrip() + generator, \"efficiency\"],\n                    p_nom_min=capacity / costs.at[cat.lstrip() + generator, \"efficiency\"],\n                    p_nom_extendable=False,\n                    efficiency=costs.at[cat.lstrip() + generator, \"efficiency\"],\n                    build_year=build_year,\n                    lifetime=costs.at[cat.lstrip() + generator, \"lifetime\"],\n                    location=buses,\n                )\n\n        # TODO fix read operation in func, fix snakemake in function, make air pumps?\n        elif generator == \"heat pump\":\n            # TODO separate the read operation from the add operation\n            with pd.HDFStore(snakemake.input.cop_name, mode=\"r\") as store:\n                gshp_cop = store[\"gshp_cop_profiles\"]\n                gshp_cop.index = gshp_cop.index.tz_localize(None)\n                gshp_cop = shift_profile_to_planning_year(\n                    gshp_cop, snakemake.wildcards.planning_horizons\n                )\n                gshp_cop = gshp_cop.loc[n.snapshots]\n            n.add(\n                \"Link\",\n                capacity.index,\n                suffix=\"-\" + str(grouping_year),\n                bus0=capacity.index,\n                bus1=capacity.index + \" central heat\",\n                carrier=\"heat pump\",\n                efficiency=(\n                    gshp_cop[capacity.index]\n                    if config[\"time_dep_hp_cop\"]\n                    else costs.at[\"decentral ground-sourced heat pump\", \"efficiency\"]\n                ),\n                capital_cost=costs.at[\"decentral ground-sourced heat pump\", \"efficiency\"]\n                * costs.at[\"decentral ground-sourced heat pump\", \"capital_cost\"],\n                marginal_cost=costs.at[\"decentral ground-sourced heat pump\", \"efficiency\"]\n                * costs.at[\"decentral ground-sourced heat pump\", \"marginal_cost\"],\n                p_nom=capacity / costs.at[\"decentral ground-sourced heat pump\", \"efficiency\"],\n                p_nom_min=capacity / costs.at[\"decentral ground-sourced heat pump\", \"efficiency\"],\n                p_nom_extendable=False,\n                build_year=build_year,\n                lifetime=costs.at[\"decentral ground-sourced heat pump\", \"lifetime\"],\n                location=buses,\n            )\n\n        else:\n            logger.warning(\n                f\"Skipped existing capacitity for {generator}\"\n                + \" - tech not implemented as existing capacity\"\n            )\n\n    return\n</code></pre>"},{"location":"reference/add_existing_baseyear/#add_existing_baseyear.distribute_vre_by_grade","title":"<code>distribute_vre_by_grade(cap_by_year, grade_capacities)</code>","text":"<p>distribute vre capacities by grade potential, use up better grades first</p> <p>Parameters:</p> Name Type Description Default <code>cap_by_year</code> <code>Series</code> <p>the vre tech potential p_nom_max added per year</p> required <code>grade_capacities</code> <code>Series</code> <p>the vre grade potential for the tech and bus</p> required <p>Returns:     pd.DataFrame: DataFrame with the distributed vre capacities (shape: years x buses)</p> Source code in <code>workflow/scripts/add_existing_baseyear.py</code> <pre><code>def distribute_vre_by_grade(cap_by_year: pd.Series, grade_capacities: pd.Series) -&gt; pd.DataFrame:\n    \"\"\"distribute vre capacities by grade potential, use up better grades first\n\n    Args:\n        cap_by_year (pd.Series): the vre tech potential p_nom_max added per year\n        grade_capacities (pd.Series): the vre grade potential for the tech and bus\n    Returns:\n        pd.DataFrame: DataFrame with the distributed vre capacities (shape: years x buses)\n    \"\"\"\n\n    availability = cap_by_year.sort_index(ascending=False)\n    to_distribute = grade_capacities.fillna(0).sort_index()\n    n_years = len(to_distribute)\n    n_sources = len(availability)\n\n    # To store allocation per year per source (shape: sources x years)\n    allocation = np.zeros((n_sources, n_years), dtype=int)\n    remaining = availability.values\n\n    for j in range(n_years):\n        needed = to_distribute.values[j]\n        cumsum = np.cumsum(remaining)\n        used_up = cumsum &lt; needed\n        cutoff = np.argmax(cumsum &gt;= needed)\n\n        allocation[used_up, j] = remaining[used_up]\n\n        if needed &gt; (cumsum[cutoff - 1] if cutoff &gt; 0 else 0):\n            allocation[cutoff, j] = needed - (cumsum[cutoff - 1] if cutoff &gt; 0 else 0)\n\n        # Subtract what was used from availability\n        remaining -= allocation[:, j]\n\n    return pd.DataFrame(data=allocation, columns=grade_capacities.index, index=availability.index)\n</code></pre>"},{"location":"reference/build_biomass_potential/","title":"Build biomass potential","text":""},{"location":"reference/build_biomass_potential/#build_biomass_potential.build_biomass_potential_xing","title":"<code>build_biomass_potential_xing(biomass_potentials_path)</code>","text":"<p>build potential from Xing et al. https://doi.org/10.1038/s41467-021-23282-x</p> <p>Parameters:</p> Name Type Description Default <code>biomass_potentials_path</code> <code>PathLike</code> <p>the path to the Xing SI data (xlsx).</p> required Source code in <code>workflow/scripts/build_biomass_potential.py</code> <pre><code>def build_biomass_potential_xing(biomass_potentials_path: PathLike):\n    \"\"\"build potential from Xing et al. https://doi.org/10.1038/s41467-021-23282-x\n\n    Args:\n        biomass_potentials_path (PathLike, optional): the path to the Xing SI data (xlsx).\n    \"\"\"\n\n    df = read_xing_si_data(biomass_potentials_path)\n\n    # select only relevant part of potential\n    df = df[df.columns[df.columns.str.contains(\"Agricultural residues burnt as waste\")]].sum(axis=1)\n\n    # convert t biomass yr-1 to MWh, heat content is from paper reference 92\n    heat_content = 19  # GJ (t biomass\u22121)\n    heat_content *= 1000 / 3600  # GJ/t -&gt; MWh\n    df = df * heat_content\n\n    return df\n</code></pre>"},{"location":"reference/build_biomass_potential/#build_biomass_potential.estimate_co2_intensity_xing","title":"<code>estimate_co2_intensity_xing()</code>","text":"<p>Estimate the biomass Co2 intensity from the Xing paper</p> <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>the biomass co2 intensity in t/MWhth</p> Source code in <code>workflow/scripts/build_biomass_potential.py</code> <pre><code>def estimate_co2_intensity_xing() -&gt; float:\n    \"\"\"Estimate the biomass Co2 intensity from the Xing paper\n\n    Returns:\n        float: the biomass co2 intensity in t/MWhth\n    \"\"\"\n\n    biomass_potential_tot = 3.04  # Gt\n    embodied_co2_tot = 5.24  # Gt\n    heat_content = 19 * 1000 / 3600  # GJ/t -&gt; MWh_th/t\n    unit_co2 = embodied_co2_tot / biomass_potential_tot  # t CO2/t biomass\n    co2_intens = unit_co2 / heat_content  # t CO2/MWh_th\n\n    return co2_intens\n</code></pre>"},{"location":"reference/build_biomass_potential/#build_biomass_potential.read_xing_si_data","title":"<code>read_xing_si_data(biomass_potentials_path)</code>","text":"<p>read and prepare the xing SI data</p> <p>Parameters:</p> Name Type Description Default <code>biomass_potentials_path</code> <code>PathLike</code> <p>the path to the Xing SI data (xlsx).</p> required Source code in <code>workflow/scripts/build_biomass_potential.py</code> <pre><code>def read_xing_si_data(biomass_potentials_path: PathLike):\n    \"\"\"read and prepare the xing SI data\n\n    Args:\n        biomass_potentials_path (PathLike): the path to the Xing SI data (xlsx).\n    \"\"\"\n    # data is indexed by province and county\n    df = pd.read_excel(biomass_potentials_path, sheet_name=\"supplementary data 1\")\n    df = df.groupby(\"Province name\").sum()\n\n    df = df.rename(index={\"Inner-Monglia\": \"InnerMongolia\", \"Anhui \": \"Anhui\"})\n    df = df.add_suffix(\" biomass\")\n\n    return df\n</code></pre>"},{"location":"reference/build_cop_profiles/","title":"Build cop profiles","text":"<p>Snakemake rule script to calculate the heat pump coefficient of performance  with atlite</p>"},{"location":"reference/build_cop_profiles/#build_cop_profiles.build_cop_profiles","title":"<code>build_cop_profiles(pop_map, cutout, output_path)</code>","text":"<p>Build COP time profiles with atlite and write outputs to output_path as hf5</p> <p>Parameters:</p> Name Type Description Default <code>pop_map</code> <code>DataFrame</code> <p>the population map (node resolution)</p> required <code>cutout</code> <code>cutout</code> <p>the atlite cutout (weather data)</p> required <code>output_path</code> <code>PathLike</code> <p>the path to write the output to as hdf5</p> required Source code in <code>workflow/scripts/build_cop_profiles.py</code> <pre><code>def build_cop_profiles(pop_map: pd.DateOffset, cutout: atlite.cutout, output_path: os.PathLike):\n    \"\"\"Build COP time profiles with atlite and write outputs to output_path as hf5\n\n    Args:\n        pop_map (pd.DataFrame): the population map (node resolution)\n        cutout (atlite.cutout): the atlite cutout (weather data)\n        output_path (os.PathLike): the path to write the output to as hdf5\n    \"\"\"\n\n    pop_matrix = sp.sparse.csr_matrix(pop_map.T)\n    index = pop_map.columns\n    index.name = \"provinces\"\n\n    soil_temp = cutout.soil_temperature(matrix=pop_matrix, index=index)\n    soil_temp[\"time\"] = (\n        pd.DatetimeIndex(soil_temp[\"time\"].values, tz=\"UTC\")\n        .tz_convert(TIMEZONE)\n        .tz_localize(None)\n        .values\n    )\n\n    with pd.HDFStore(snakemake.input.temp, mode=\"r\") as store:\n        temp = store[\"temperature\"]\n\n    source_T = temp\n    source_soil_T = soil_temp.to_pandas().divide(pop_map.sum())\n\n    # quadratic regression based on Staffell et al. (2012)\n    # https://doi.org/10.1039/C2EE22653G\n\n    sink_T = 55.0  # Based on DTU / large area radiators\n\n    delta_T = sink_T - source_T\n\n    # TODO make this user set and document\n    # For ASHP\n    def ashp_cop(d):\n        return 6.81 - 0.121 * d + 0.000630 * d**2\n\n    cop = ashp_cop(delta_T)\n\n    delta_soil_T = sink_T - source_soil_T\n\n    # TODO make this user set and document\n    # For GSHP\n    def gshp_cop(d):\n        return 8.77 - 0.150 * d + 0.000734 * d**2\n\n    cop_soil = gshp_cop(delta_soil_T)\n\n    with pd.HDFStore(output_path, mode=\"w\", complevel=4) as store:\n        store[\"ashp_cop_profiles\"] = cop\n        store[\"gshp_cop_profiles\"] = cop_soil\n</code></pre>"},{"location":"reference/build_cutout/","title":"Build cutout","text":"<p>Functions to download ERA5/SARAH data and build the atlite cutout for the atlite. These functions linked to the build_cutout rule.</p>"},{"location":"reference/build_cutout/#build_cutout.cutout_timespan","title":"<code>cutout_timespan(config, weather_year)</code>","text":"<p>build the cutout timespan. Note that the coutout requests are in UTC (TBC)</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict</code> <p>the snakemake config</p> required <code>weather_year</code> <code>dict</code> <p>the coutout weather year</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <code>list</code> <p>end and start of the cutout timespan</p> Source code in <code>workflow/scripts/build_cutout.py</code> <pre><code>def cutout_timespan(config: dict, weather_year: int) -&gt; list:\n    \"\"\"build the cutout timespan. Note that the coutout requests are in UTC (TBC)\n\n    Args:\n        config (dict): the snakemake config\n        weather_year (dict): the coutout weather year\n\n    Returns:\n        tuple: end and start of the cutout timespan\n    \"\"\"\n    snapshot_cfg = config[\"snapshots\"]\n    # make snapshots for TZ and then convert to naive UTC for atlite\n    snapshots = (\n        make_periodic_snapshots(\n            year=weather_year,\n            freq=snapshot_cfg[\"freq\"],\n            start_day_hour=snapshot_cfg[\"start\"],\n            end_day_hour=snapshot_cfg[\"end\"],\n            bounds=snapshot_cfg[\"bounds\"],\n            # here we need to convert UTC to local\n            tz=TIMEZONE,\n            end_year=(None if not snapshot_cfg[\"end_year_plus1\"] else weather_year + 1),\n        )\n        .tz_convert(\"UTC\")\n        .tz_localize(None)\n    )\n\n    return [snapshots[0], snapshots[-1]]\n</code></pre>"},{"location":"reference/build_load_profiles/","title":"Build load profiles","text":"<p>Functions for the rules to build the hourly heat and load demand profiles - electricity load profiles are based on scaling an hourly base year profile to yearly future     projections - daily heating demand is based on the degree day approx (from atlite) &amp; upscaled hourly based   on an intraday profile (for Denmark by default, see snakefile)</p>"},{"location":"reference/build_load_profiles/#build_load_profiles.build_daily_heat_demand_profiles","title":"<code>build_daily_heat_demand_profiles(heat_demand_config, atlite_heating_hr_shift, switch_month_day=True)</code>","text":"<p>build the heat demand profile according to forecast demans</p> <p>Parameters:</p> Name Type Description Default <code>heat_demand_config</code> <code>dict</code> <p>the heat demand configuration</p> required <code>atlite_heating_hr_shift</code> <code>int</code> <p>the hour shift for heating demand, needed due to imperfect timezone handling in atlite</p> required <code>switch_month_day</code> <code>bool</code> <p>whether to switch month &amp; day from heat_demand_config. Defaults to True.</p> <code>True</code> <p>Returns:     pd.DataFrame: regional daily heating demand with April to Sept forced to 0</p> Source code in <code>workflow/scripts/build_load_profiles.py</code> <pre><code>def build_daily_heat_demand_profiles(\n    heat_demand_config: dict,\n    atlite_heating_hr_shift: int,\n    switch_month_day: bool = True,\n) -&gt; pd.DataFrame:\n    \"\"\"build the heat demand profile according to forecast demans\n\n    Args:\n        heat_demand_config (dict): the heat demand configuration\n        atlite_heating_hr_shift (int): the hour shift for heating demand, needed due to imperfect\n            timezone handling in atlite\n        switch_month_day (bool, optional): whether to switch month &amp; day from heat_demand_config.\n            Defaults to True.\n    Returns:\n        pd.DataFrame: regional daily heating demand with April to Sept forced to 0\n    \"\"\"\n    with pd.HDFStore(snakemake.input.population_map, mode=\"r\") as store:\n        pop_map = store[\"population_gridcell_map\"]\n\n    cutout = atlite.Cutout(snakemake.input.cutout)\n    atlite_year = get_cutout_params(snakemake.config)[\"weather_year\"]\n\n    pop_matrix = sp.sparse.csr_matrix(pop_map.T)\n    index = pop_map.columns\n    index.name = \"provinces\"\n\n    # TODO clarify a bit here, maybe the po_matrix should be normalised earlier?\n    # unclear whether it's per cap or not\n    total_hd = cutout.heat_demand(\n        matrix=pop_matrix,\n        index=index,\n        threshold=heat_demand_config[\"heating_start_temp\"],\n        a=heat_demand_config[\"heating_lin_slope\"],\n        constant=heat_demand_config[\"heating_offet\"],\n        # hack to bring it back to local from UTC\n        hour_shift=atlite_heating_hr_shift,\n    )\n\n    regonal_daily_hd = total_hd.to_pandas().divide(pop_map.sum())\n    # input given as dd-mm but loc as yyyy-mm-dd\n    if switch_month_day:\n        start_day = \"{}-{}\".format(*heat_demand_config[\"start_day\"].split(\"-\")[::-1])\n        end_day = \"{}-{}\".format(*heat_demand_config[\"end_day\"].split(\"-\")[::-1])\n    else:\n        start_day = heat_demand_config[\"start_day\"]\n        end_day = heat_demand_config[\"end_day\"]\n    regonal_daily_hd.loc[f\"{atlite_year}-{start_day}\":f\"{atlite_year}-{end_day}\"] = 0\n\n    return regonal_daily_hd\n</code></pre>"},{"location":"reference/build_load_profiles/#build_load_profiles.build_heat_demand_profile","title":"<code>build_heat_demand_profile(daily_hd, hot_water_per_day, snapshots, planning_horizons)</code>","text":"<p>Downscale the daily heat demand to hourly heat demand using pre-defined intraday profiles THIS FUNCTION ALSO MAKES PROJECTIONS FOR HEATING DEMAND - WHICH IS NOT THE CORRECT PLACE</p> <p>Parameters:</p> Name Type Description Default <code>daily_hd</code> <code>DataFrame</code> <p>the day resolved heat demand for each region (atlite time axis)</p> required <code>hot_water_per_day</code> <code>DataFrame</code> <p>the day resolved hot water demand for each region</p> required <code>snapshots</code> <code>date_range</code> <p>the snapshots for the planning year</p> required <code>planning_horizons</code> <code>int | str</code> <p>the planning year</p> required <p>Returns:</p> Type Description <code>tuple[DataFrame, DataFrame, object, DataFrame]</code> <p>tuple[pd.DataFrame, pd.DataFrame, object, pd.DataFrame]: heat, space_heat, space_heating_per_hdd, water_heat demands</p> Source code in <code>workflow/scripts/build_load_profiles.py</code> <pre><code>def build_heat_demand_profile(\n    daily_hd: pd.DataFrame,\n    hot_water_per_day: pd.DataFrame,\n    snapshots: pd.date_range,\n    planning_horizons: int | str,\n) -&gt; tuple[pd.DataFrame, pd.DataFrame, object, pd.DataFrame]:\n    \"\"\"Downscale the daily heat demand to hourly heat demand using pre-defined intraday profiles\n    THIS FUNCTION ALSO MAKES PROJECTIONS FOR HEATING DEMAND - WHICH IS NOT THE CORRECT PLACE\n\n    Args:\n        daily_hd (DataFrame): the day resolved heat demand for each region (atlite time axis)\n        hot_water_per_day (DataFrame): the day resolved hot water demand for each region\n        snapshots (pd.date_range): the snapshots for the planning year\n        planning_horizons (int | str): the planning year\n\n    Returns:\n        tuple[pd.DataFrame, pd.DataFrame, object, pd.DataFrame]:\n            heat, space_heat, space_heating_per_hdd, water_heat demands\n    \"\"\"\n    # TODO - very strange, why would this be needed unless atlite is buggy\n    daily_hd_uniq = daily_hd[~daily_hd.index.duplicated(keep=\"first\")]\n    # hourly resolution regional demand (but wrong data, it's just ffill)\n    heat_demand_hourly = shift_profile_to_planning_year(\n        daily_hd_uniq, planning_yr=planning_horizons\n    ).reindex(index=snapshots, method=\"ffill\")\n\n    # ===== downscale to hourly =======\n    intraday_profiles = pd.read_csv(snakemake.input.intraday_profiles, index_col=0)\n\n    # TODO, does this work with variable frequency?\n    intraday_year_profiles = downscale_time_data(\n        dt_index=heat_demand_hourly.index,\n        weekly_profile=(\n            list(intraday_profiles[\"weekday\"]) * 5 + list(intraday_profiles[\"weekend\"]) * 2\n        ),\n        regional_tzs=pd.Series(index=PROV_NAMES, data=list(REGIONAL_GEO_TIMEZONES.values())),\n    )\n\n    # TWh -&gt; MWh\n    space_heat_demand_total = pd.read_csv(snakemake.input.space_heat_demand, index_col=0) * 1e6\n    space_heat_demand_total = space_heat_demand_total.squeeze()\n\n    # ==== SCALE TO FUTURE DEMAND ======\n    # TODO soft-code ref/base year or find a better variable name\n    # TODO remind coupling: fix this kind of stuff or make separate fn\n    # Belongs outside of this function really and in main\n    factor = make_heat_demand_projections(\n        planning_horizons, snakemake.wildcards[\"heating_demand\"], ref_year=REF_YEAR\n    )\n    # WOULD BE NICER TO SUM THAN TO WEIGH OR TO directly build the profile with the freq\n    # TODO, does this work with variable frequency?\n    space_heating_per_hdd = (space_heat_demand_total * factor) / (\n        heat_demand_hourly.sum() * snakemake.config[\"snapshots\"][\"frequency\"]\n    )\n\n    space_heat_demand = intraday_year_profiles.mul(heat_demand_hourly).mul(space_heating_per_hdd)\n    water_heat_demand = intraday_year_profiles.mul(hot_water_per_day / 24.0)\n\n    heat_demand = space_heat_demand + water_heat_demand\n\n    return heat_demand, space_heat_demand, space_heating_per_hdd, water_heat_demand\n</code></pre>"},{"location":"reference/build_load_profiles/#build_load_profiles.build_hot_water_per_day","title":"<code>build_hot_water_per_day(planning_horizons)</code>","text":"<p>Make projections for the hot water demand increase and scale the ref year value NB: the ref year value is for 2008 not 2020 -&gt; incorrect</p> <p>Parameters:</p> Name Type Description Default <code>planning_horizons</code> <code>int | str</code> <p>the planning year to which demand will be projected</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>if the config projection type is not supported</p> <p>Returns:</p> Type Description <code>Series</code> <p>pd.Series: regional hot water demand per day</p> Source code in <code>workflow/scripts/build_load_profiles.py</code> <pre><code>def build_hot_water_per_day(planning_horizons: int | str) -&gt; pd.Series:\n    \"\"\"Make projections for the hot water demand increase and scale the ref year value\n    NB: the ref year value is for 2008 not 2020 -&gt; incorrect\n\n    Args:\n        planning_horizons (int | str): the planning year to which demand will be projected\n\n    Raises:\n        ValueError: if the config projection type is not supported\n\n    Returns:\n        pd.Series: regional hot water demand per day\n    \"\"\"\n\n    with pd.HDFStore(snakemake.input.population, mode=\"r\") as store:\n        population_count = store[\"population\"]\n\n    unit_hot_water_start_yr = UNIT_HOT_WATER_START_YEAR\n    unit_hot_water_end_yr = UNIT_HOT_WATER_END_YEAR\n\n    if snakemake.wildcards[\"heating_demand\"] == \"positive\":\n\n        def func(x, a, b):\n            return a * x + b\n\n        x = np.array([START_YEAR, END_YEAR])\n        y = np.array([unit_hot_water_start_yr, unit_hot_water_end_yr])\n        popt, pcov = curve_fit(func, x, y)\n\n        unit_hot_water = func(int(planning_horizons), *popt)\n\n    elif snakemake.wildcards[\"heating_demand\"] == \"constant\":\n        unit_hot_water = unit_hot_water_start_yr\n\n    elif snakemake.wildcards[\"heating_demand\"] == \"mean\":\n\n        def lin_func(x: np.array, a: float, b: float) -&gt; np.array:\n            return a * x + b\n\n        x = np.array([START_YEAR, END_YEAR])\n        y = np.array([unit_hot_water_start_yr, unit_hot_water_end_yr])\n        popt, pcov = curve_fit(lin_func, x, y)\n        popt, pcov = curve_fit(lin_func, x, y)\n\n        unit_hot_water = (lin_func(int(planning_horizons), *popt) + UNIT_HOT_WATER_START_YEAR) / 2\n    else:\n        raise ValueError(f\"Invalid heating demand type {snakemake.wildcards['heating_demand']}\")\n    # MWh per day per region\n    hot_water_per_day = unit_hot_water * population_count / 365.0\n\n    return hot_water_per_day\n</code></pre>"},{"location":"reference/build_load_profiles/#build_load_profiles.downscale_time_data","title":"<code>downscale_time_data(dt_index, weekly_profile, regional_tzs)</code>","text":"<p>Make hourly resolved data profiles based on exogenous weekdays and weekend profiles. This fn takes into account that the profiles are in local time and that regions may  have different timezones.</p> <p>Parameters:</p> Name Type Description Default <code>dt_index</code> <code>DatetimeIndex</code> <p>the snapshots (in network local naive time) but hourly res.</p> required <code>weekly_profile</code> <code>Iterable</code> <p>the weekly profile as a list of 7*24 entries.</p> required <code>regional_tzs</code> <code>Series</code> <p>regional geographical timezones for profiles. Defaults to pd.Series(index=PROV_NAMES, data=list(REGIONAL_GEO_TIMEZONES.values())).</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: Regionally resolved profiles for each snapshot hour rperesented by dt_index</p> Source code in <code>workflow/scripts/build_load_profiles.py</code> <pre><code>def downscale_time_data(\n    dt_index: pd.DatetimeIndex,\n    weekly_profile: Iterable,\n    regional_tzs: pd.Series,\n) -&gt; pd.DataFrame:\n    \"\"\"Make hourly resolved data profiles based on exogenous weekdays and weekend profiles.\n    This fn takes into account that the profiles are in local time and that regions may\n     have different timezones.\n\n    Args:\n        dt_index (DatetimeIndex): the snapshots (in network local naive time) but hourly res.\n        weekly_profile (Iterable): the weekly profile as a list of 7*24 entries.\n        regional_tzs (pd.Series, optional): regional geographical timezones for profiles.\n            Defaults to pd.Series(index=PROV_NAMES, data=list(REGIONAL_GEO_TIMEZONES.values())).\n\n    Returns:\n        pd.DataFrame: Regionally resolved profiles for each snapshot hour rperesented by dt_index\n    \"\"\"\n    weekly_profile = pd.Series(weekly_profile, range(24 * 7))\n    # make a dataframe with timestamps localized to the network TIMEZONE timestamps\n    all_times = pd.DataFrame(\n        dict(zip(PROV_NAMES, [dt_index.tz_localize(TIMEZONE)] * len(PROV_NAMES))),\n        index=dt_index.tz_localize(TIMEZONE),\n        columns=PROV_NAMES,\n    )\n    # then localize to regional time. _dt ensures index is not changed\n    week_hours = all_times.apply(\n        lambda col: col.dt.tz_convert(regional_tzs[col.name]).tz_localize(None)\n    )\n    # then convert into week hour &amp; map to the intraday heat demand profile (based on local time)\n    return week_hours.apply(lambda col: col.dt.weekday * 24 + col.dt.hour).apply(\n        lambda col: col.map(weekly_profile)\n    )\n</code></pre>"},{"location":"reference/build_load_profiles/#build_load_profiles.make_heat_demand_projections","title":"<code>make_heat_demand_projections(planning_year, projection_name, ref_year=REF_YEAR)</code>","text":"<p>Make projections for heating demand</p> <p>Parameters:</p> Name Type Description Default <code>projection_name</code> <code>str</code> <p>name of projection</p> required <code>planning_year</code> <code>int</code> <p>year to project to</p> required <code>ref_year</code> <code>int</code> <p>reference year. Defaults to REF_YEAR.</p> <code>REF_YEAR</code> <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>scaling factor relative to base year for heating demand</p> Source code in <code>workflow/scripts/build_load_profiles.py</code> <pre><code>def make_heat_demand_projections(\n    planning_year: int, projection_name: str, ref_year=REF_YEAR\n) -&gt; float:\n    \"\"\"Make projections for heating demand\n\n    Args:\n        projection_name (str): name of projection\n        planning_year (int): year to project to\n        ref_year (int, optional): reference year. Defaults to REF_YEAR.\n\n    Returns:\n        float: scaling factor relative to base year for heating demand\n    \"\"\"\n    years = np.array([1985, 1990, 1995, 2000, 2005, 2010, 2015, 2020, 2060])\n\n    if projection_name == \"positive\":\n\n        def func(x, a, b, c, d):\n            \"\"\"Cubic polynomial fit to proj\"\"\"\n            return a * x**3 + b * x**2 + c * x + d\n\n        # TODO soft code\n        # heated area projection in China\n        # 2060: 6.02 * 36.52 * 1e4 north population * floor_space_per_capita in city\n        heated_area = np.array(\n            [2742, 21263, 64645, 110766, 252056, 435668, 672205, 988209, 2198504]\n        )  # 10000 m2\n\n        # Perform curve fitting\n        popt, pcov = curve_fit(func, years, heated_area)\n        factor = func(int(planning_year), *popt) / func(REF_YEAR, *popt)\n\n    elif projection_name == \"constant\":\n        factor = 1.0\n\n    else:\n        raise ValueError(f\"Invalid heating demand projection {projection_name}\")\n    return factor\n</code></pre>"},{"location":"reference/build_load_profiles/#build_load_profiles.prepare_hourly_load_data","title":"<code>prepare_hourly_load_data(hourly_load_p, prov_codes_p)</code>","text":"<p>Read the hourly demand data and prepare it for use in the model</p> <p>Parameters:</p> Name Type Description Default <code>hourly_load_p</code> <code>PathLike</code> <p>raw elec data from zenodo, see readme in data.</p> required <code>prov_codes_p</code> <code>PathLike</code> <p>province mapping for data.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: the hourly demand data with the right province names, in TWh/hr</p> Source code in <code>workflow/scripts/build_load_profiles.py</code> <pre><code>def prepare_hourly_load_data(\n    hourly_load_p: os.PathLike,\n    prov_codes_p: os.PathLike,\n) -&gt; pd.DataFrame:\n    \"\"\"Read the hourly demand data and prepare it for use in the model\n\n    Args:\n        hourly_load_p (os.PathLike, optional): raw elec data from zenodo, see readme in data.\n        prov_codes_p (os.PathLike, optional): province mapping for data.\n\n    Returns:\n        pd.DataFrame: the hourly demand data with the right province names, in TWh/hr\n    \"\"\"\n    TO_TWh = 1e-6\n    hourly = pd.read_csv(hourly_load_p)\n    hourly_TWh = hourly.drop(columns=[\"Time Series\"]) * TO_TWh\n    prov_codes = pd.read_csv(prov_codes_p)\n    prov_codes.set_index(\"Code\", inplace=True)\n    hourly_TWh.columns = hourly_TWh.columns.map(prov_codes[\"Full name\"])\n    return hourly_TWh\n</code></pre>"},{"location":"reference/build_load_profiles/#build_load_profiles.project_elec_demand","title":"<code>project_elec_demand(hourly_demand_base_yr_MWh, yearly_projections_MWh, year=2020)</code>","text":"<p>project the hourly demand to the future years</p> <p>Parameters:</p> Name Type Description Default <code>hourly_demand_base_yr_MWh</code> <code>DataFrame</code> <p>the hourly demand in the base year</p> required <code>yearly_projections_MWh</code> <code>DataFrame</code> <p>the yearly projections</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: the projected hourly demand</p> Source code in <code>workflow/scripts/build_load_profiles.py</code> <pre><code>def project_elec_demand(\n    hourly_demand_base_yr_MWh: pd.DataFrame,\n    yearly_projections_MWh: pd.DataFrame,\n    year=2020,\n) -&gt; pd.DataFrame:\n    \"\"\"project the hourly demand to the future years\n\n    Args:\n        hourly_demand_base_yr_MWh (pd.DataFrame): the hourly demand in the base year\n        yearly_projections_MWh (pd.DataFrame): the yearly projections\n\n    Returns:\n        pd.DataFrame: the projected hourly demand\n    \"\"\"\n    hourly_load_profile = hourly_demand_base_yr_MWh.loc[:, PROV_NAMES]\n    # normalise the hourly load\n    hourly_load_profile /= hourly_load_profile.sum(axis=0)\n\n    yearly_projections_MWh = yearly_projections_MWh.T.loc[int(year), PROV_NAMES]\n    hourly_load_projected = yearly_projections_MWh.multiply(hourly_load_profile)\n\n    if len(hourly_load_projected) == 8784:\n        # rm feb 29th\n        hourly_load_projected.drop(hourly_load_projected.index[1416:1440], inplace=True)\n    elif len(hourly_load_projected) != 8760:\n        raise ValueError(\"The length of the hourly load is not 8760 or 8784 (leap year, dropped)\")\n\n    snapshots = make_periodic_snapshots(\n        year=year,\n        freq=\"1h\",\n        start_day_hour=\"01-01 00:00:00\",\n        end_day_hour=\"12-31 23:00\",\n        bounds=\"both\",\n    )\n\n    hourly_load_projected.index = snapshots\n    return hourly_load_projected\n</code></pre>"},{"location":"reference/build_population/","title":"Build population","text":"<p>Rules for building the population data by region</p>"},{"location":"reference/build_population/#build_population.build_population","title":"<code>build_population(data_path=None)</code>","text":"<p>Build the population data by region</p> <p>Parameters:</p> Name Type Description Default <code>data_path</code> <code>PathLike</code> <p>the path to the pop csv. Defaults to None.</p> <code>None</code> Source code in <code>workflow/scripts/build_population.py</code> <pre><code>def build_population(data_path: os.PathLike = None):\n    \"\"\"Build the population data by region\n\n    Args:\n        data_path (os.PathLike, optional): the path to the pop csv. Defaults to None.\n    \"\"\"\n\n    if data_path is None:\n        data_path = snakemake.input.population\n\n    population = YEARBOOK_DATA2POP * load_pop_csv(csv_path=data_path)\n    population.name = \"population\"\n    population.to_hdf(snakemake.output.population, key=population.name)\n</code></pre>"},{"location":"reference/build_population/#build_population.load_pop_csv","title":"<code>load_pop_csv(csv_path)</code>","text":"<p>Load the national bureau of statistics of China population (Yearbook - Population, table 2.5 pop at year end by Region)</p> <p>Parameters:</p> Name Type Description Default <code>csv_path</code> <code>Pathlike</code> <p>the csv path</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: the population for constants.POP_YEAR by province</p> <p>Raises:     ValueError: if the province names are not as expected</p> Source code in <code>workflow/scripts/build_population.py</code> <pre><code>def load_pop_csv(csv_path: os.PathLike) -&gt; pd.DataFrame:\n    \"\"\"Load the national bureau of statistics of China population\n    (Yearbook - Population, table 2.5 pop at year end by Region)\n\n    Args:\n        csv_path (os.Pathlike): the csv path\n\n    Returns:\n        pd.DataFrame: the population for constants.POP_YEAR by province\n    Raises:\n        ValueError: if the province names are not as expected\n    \"\"\"\n\n    df = pd.read_csv(csv_path, index_col=0, header=0)\n    df = df.apply(pd.to_numeric)\n    df = df[POP_YEAR][df.index.isin(PROV_NAMES)]\n    if not sorted(df.index.to_list()) == sorted(PROV_NAMES):\n        raise ValueError(\n            f\"Province names do not match {sorted(df.index.to_list())} != {sorted(PROV_NAMES)}\"\n        )\n    return df\n</code></pre>"},{"location":"reference/build_population_gridcell_map/","title":"Build population gridcell map","text":""},{"location":"reference/build_population_gridcell_map/#build_population_gridcell_map.build_gridded_population","title":"<code>build_gridded_population(prov_pop_path, pop_density_raster_path, cutout_path, province_shape_path, gridded_pop_out)</code>","text":"<p>Build a gridded population DataFrame by matching population density to the cutout grid cells. This DataFrame is a sparse matrix of the population and shape BusesxCutout_gridcells   where buses are the provinces</p> <p>Parameters:</p> Name Type Description Default <code>prov_pop_path</code> <code>PathLike</code> <p>Path to the province population count file (hdf5).</p> required <code>pop_density_raster_path</code> <code>PathLike</code> <p>Path to the population density raster file.</p> required <code>cutout_path</code> <code>PathLike</code> <p>Path to the cutout file containing the grid.</p> required <code>province_shape_path</code> <code>PathLike</code> <p>Path to the province shape file.</p> required <code>gridded_pop_out</code> <code>PathLike</code> <p>output file path.</p> required Source code in <code>workflow/scripts/build_population_gridcell_map.py</code> <pre><code>def build_gridded_population(\n    prov_pop_path: PathLike,\n    pop_density_raster_path: PathLike,\n    cutout_path: PathLike,\n    province_shape_path: PathLike,\n    gridded_pop_out: PathLike,\n):\n    \"\"\"Build a gridded population DataFrame by matching population density to the cutout grid cells.\n    This DataFrame is a sparse matrix of the population and shape BusesxCutout_gridcells\n      where buses are the provinces\n\n    Args:\n        prov_pop_path (PathLike): Path to the province population count file (hdf5).\n        pop_density_raster_path (PathLike): Path to the population density raster file.\n        cutout_path (PathLike): Path to the cutout file containing the grid.\n        province_shape_path (PathLike): Path to the province shape file.\n        gridded_pop_out (PathLike): output file path.\n    \"\"\"\n\n    with pd.HDFStore(prov_pop_path, mode=\"r\") as store:\n        pop_province = store[\"population\"]\n\n    prov_poly = read_province_shapes(province_shape_path)\n    pop_density = read_pop_density(pop_density_raster_path, prov_poly, crs=CRS)\n\n    cutout = atlite.Cutout(cutout_path)\n    grid_points = cutout.grid\n    # this is in polygons but need points for sjoin with pop dnesity to work\n    grid_points.to_crs(3857, inplace=True)\n    grid_points[\"geometry\"] = grid_points.centroid\n    grid_points.to_crs(CRS, inplace=True)\n\n    # match cutout grid to province\n    # cutout_pts_in_prov = gpd.tools.sjoin(grid_points, prov_poly,\n    # how=\"left\", predicate=\"intersects\")\n    # TODO: do you want to dropna here?\n    cutout_pts_in_prov = gpd.tools.sjoin(\n        grid_points, prov_poly, how=\"left\", predicate=\"intersects\"\n    )  # .dropna()\n    cutout_pts_in_prov.rename(\n        columns={\"index_right\": \"province_index\", \"province\": \"province_name\"},\n        inplace=True,\n    )\n\n    # match cutout grid to province\n    cutout_pts_in_prov = gpd.tools.sjoin(grid_points, prov_poly, how=\"left\", predicate=\"intersects\")\n    cutout_pts_in_prov.rename(\n        columns={\"index_right\": \"province_index\", \"province\": \"province_name\"},\n        inplace=True,\n    )\n    # cutout_pts_in_prov.dropna(inplace=True)\n\n    # TODO CRS, think about whether this makes sense or need grid interp\n    merged = gpd.tools.sjoin_nearest(\n        cutout_pts_in_prov.to_crs(3857), pop_density.to_crs(3857), how=\"inner\"\n    )\n    merged = merged.to_crs(CRS)\n    # points outside china are NaN, need to rename to keep the index cutout after agg\n    # otherwise the spare matrix will not match the cutoutpoints\n    #  (smarter would be to change the cutout)\n    merged.fillna({\"province_name\": \"OutsideChina\"}, inplace=True)\n\n    points_in_provinces = pd.DataFrame(index=cutout_pts_in_prov.index)\n    # normalise pop per province and make a loc_id/province table\n    points_in_provinces = (\n        merged.groupby(\"province_name\")[\"pop_density\"]\n        .apply(lambda x: x / x.sum())\n        .unstack(fill_value=0.0)\n        .T\n    )\n    # now get rid of the outside china \"province\"\n    points_in_provinces.drop(columns=\"OutsideChina\", inplace=True)\n    points_in_provinces.index.name = \"\"\n    points_in_provinces.fillna(0.0, inplace=True)\n\n    points_in_provinces *= pop_province\n\n    with pd.HDFStore(gridded_pop_out, mode=\"w\", complevel=4) as store:\n        store[\"population_gridcell_map\"] = points_in_provinces\n</code></pre>"},{"location":"reference/build_population_gridcell_map/#build_population_gridcell_map.build_population_map","title":"<code>build_population_map(prov_pop_path, pop_density_raster_path, cutout_path, province_shape_path, gridded_pop_out)</code>","text":"<p>Build a gridded population DataFrame by matching population density to the cutout grid cells. This DataFrame is a sparse matrix of the population and shape BusesxCutout_gridcells   where buses are the provinces</p> <p>Parameters:</p> Name Type Description Default <code>prov_pop_path</code> <code>PathLike</code> <p>Path to the province population count file (hdf5).</p> required <code>pop_density_raster_path</code> <code>PathLike</code> <p>Path to the population density raster file.</p> required <code>cutout_path</code> <code>PathLike</code> <p>Path to the cutout file containing the grid.</p> required <code>province_shape_path</code> <code>PathLike</code> <p>Path to the province shape file.</p> required <code>gridded_pop_out</code> <code>PathLike</code> <p>output file path.</p> required Source code in <code>workflow/scripts/build_population_gridcell_map.py</code> <pre><code>def build_population_map(\n    prov_pop_path: PathLike,\n    pop_density_raster_path: PathLike,\n    cutout_path: PathLike,\n    province_shape_path: PathLike,\n    gridded_pop_out: PathLike,\n):\n    \"\"\"Build a gridded population DataFrame by matching population density to the cutout grid cells.\n    This DataFrame is a sparse matrix of the population and shape BusesxCutout_gridcells\n      where buses are the provinces\n\n    Args:\n        prov_pop_path (PathLike): Path to the province population count file (hdf5).\n        pop_density_raster_path (PathLike): Path to the population density raster file.\n        cutout_path (PathLike): Path to the cutout file containing the grid.\n        province_shape_path (PathLike): Path to the province shape file.\n        gridded_pop_out (PathLike): output file path.\n    \"\"\"\n\n    # =============== load data ===================\n    with pd.HDFStore(prov_pop_path, mode=\"r\") as store:\n        pop_province_count = store[\"population\"]\n\n    # CFSR points and Provinces\n    pop_ww = load_cfrs_data(pop_density_raster_path)\n\n    prov_poly = gpd.read_file(province_shape_path)[[\"province\", \"geometry\"]]\n    prov_poly.set_index(\"province\", inplace=True)\n    prov_poly = prov_poly.reindex(PROV_NAMES)\n    prov_poly.reset_index(inplace=True)\n\n    # load renewable profiles &amp; grid &amp; extract gridpoints\n    cutout = atlite.Cutout(cutout_path)\n    grid_points = cutout.grid\n    grid_points.to_crs(3857, inplace=True)\n    grid_points[\"geometry\"] = grid_points.centroid\n    grid_points.to_crs(CRS, inplace=True)\n\n    # match cutout grid to province\n    cutout_pts_in_prov = gpd.tools.sjoin(grid_points, prov_poly, how=\"left\", predicate=\"intersects\")\n    cutout_pts_in_prov.rename(\n        columns={\"index_right\": \"province_index\", \"province\": \"province_name\"},\n        inplace=True,\n    )\n\n    # Province masks merged with population density\n    # TODO: THIS REQUIRES EXPLANATION - can't just use random crs :||\n    cutout_pts_in_prov = cutout_pts_in_prov.to_crs(3857)\n    pop_ww = pop_ww.to_crs(3857)\n\n    merged = gpd.tools.sjoin_nearest(cutout_pts_in_prov, pop_ww, how=\"inner\")\n    merged = merged.to_crs(CRS)\n\n    # normalised pop distribution per province\n    # need an extra province for points not in the province, otherwise lose cutout grid index\n    merged.fillna({\"province_name\": \"OutsideChina\"}, inplace=True)\n    points_in_provinces = pd.DataFrame(index=cutout_pts_in_prov.index)\n    points_in_provinces = (\n        merged.groupby(\"province_name\")[\"pop_density\"]\n        .apply(lambda x: x / x.sum())\n        .unstack(fill_value=0.0)\n        .T\n    )\n    # Cleanup the matrix: get rid of the outside china \"province\" et\n    points_in_provinces.drop(columns=\"OutsideChina\", inplace=True)\n    points_in_provinces.index.name = \"\"\n    points_in_provinces.fillna(0.0, inplace=True)\n\n    # go from normalised distribution to head count\n    points_in_provinces *= pop_province_count\n    with pd.HDFStore(gridded_pop_out, mode=\"w\", complevel=4) as store:\n        store[\"population_gridcell_map\"] = points_in_provinces\n</code></pre>"},{"location":"reference/build_population_gridcell_map/#build_population_gridcell_map.load_cfrs_data","title":"<code>load_cfrs_data(target)</code>","text":"<p>load  CFRS_grid.nc type files into a geodatafram</p> <p>Parameters:</p> Name Type Description Default <code>target</code> <code>PathLike</code> <p>the abs path</p> required <p>Returns:</p> Type Description <code>GeoDataFrame</code> <p>gpd.GeoDataFrame: the data in gdf</p> Source code in <code>workflow/scripts/build_population_gridcell_map.py</code> <pre><code>def load_cfrs_data(target: PathLike) -&gt; gpd.GeoDataFrame:\n    \"\"\"load  CFRS_grid.nc type files into a geodatafram\n\n    Args:\n        target (PathLike): the abs path\n\n    Returns:\n        gpd.GeoDataFrame: the data in gdf\n    \"\"\"\n    pop_density = xr.open_dataarray(target).to_dataset(name=\"pop_density\")\n    pop_ww = xarr_to_gdf(pop_density, var_name=\"pop_density\")  # TODO is the CRS correct?\n\n    return pop_ww\n</code></pre>"},{"location":"reference/build_population_gridcell_map/#build_population_gridcell_map.xarr_to_gdf","title":"<code>xarr_to_gdf(xarr, var_name, x_var='x', y_var='y', crs=CRS)</code>","text":"<p>convert an xarray to GDF</p> <p>Parameters:</p> Name Type Description Default <code>xarr</code> <code>DataArray</code> <p>the input array</p> required <code>var_name</code> <code>str</code> <p>the array variable to be converted.</p> required <code>x_var</code> <code>str</code> <p>the x dimension. Defaults to \"x\".</p> <code>'x'</code> <code>y_var</code> <code>str</code> <p>the y dimension. Defaults to \"y\".</p> <code>'y'</code> <code>crs</code> <code>_type_</code> <p>the crs. Defaults to CRS.</p> <code>CRS</code> <p>Returns:</p> Type Description <code>GeoDataFrame</code> <p>gpd.GeoDataFrame: geodata frame in chosen CRS</p> Source code in <code>workflow/scripts/build_population_gridcell_map.py</code> <pre><code>def xarr_to_gdf(\n    xarr: xr.DataArray, var_name: str, x_var=\"x\", y_var=\"y\", crs=CRS\n) -&gt; gpd.GeoDataFrame:\n    \"\"\"convert an xarray to GDF\n\n    Args:\n        xarr (xr.DataArray): the input array\n        var_name (str): the array variable to be converted.\n        x_var (str, optional): the x dimension. Defaults to \"x\".\n        y_var (str, optional): the y dimension. Defaults to \"y\".\n        crs (_type_, optional): the crs. Defaults to CRS.\n\n    Returns:\n        gpd.GeoDataFrame: geodata frame in chosen CRS\n    \"\"\"\n    df = xarr.to_dataframe()\n    df.reset_index(inplace=True)\n    return gpd.GeoDataFrame(\n        df[var_name], geometry=gpd.points_from_xy(df[x_var], df[y_var]), crs=crs\n    )\n</code></pre>"},{"location":"reference/build_province_shapes/","title":"Build province shapes","text":"<p>Functions to get the province shapes.</p>"},{"location":"reference/build_province_shapes/#build_province_shapes.fetch_natural_earth_records","title":"<code>fetch_natural_earth_records(country_iso2_code='CN')</code>","text":"<p>fetch the province/state level (1st admin level) from the         NATURAL_EARTH data store and make a file</p> <p>Parameters:</p> Name Type Description Default <code>country_iso2_code</code> <code>str</code> <p>the country code (iso_a2) for which  provincial records will be extracted. None will not filter (untestetd) Defaults to 'CN'</p> <code>'CN'</code> <p>Returns:     Records: the natural earth records</p> Source code in <code>workflow/scripts/build_province_shapes.py</code> <pre><code>def fetch_natural_earth_records(country_iso2_code=\"CN\") -&gt; object:\n    \"\"\"fetch the province/state level (1st admin level) from the\n            NATURAL_EARTH data store and make a file\n\n    Args:\n        country_iso2_code (str, optional): the country code (iso_a2) for which\n             provincial records will be extracted. None will not filter (untestetd) Defaults to 'CN'\n    Returns:\n        Records: the natural earth records\n    \"\"\"\n\n    shpfilename = shpreader.natural_earth(\n        resolution=NATURAL_EARTH_RESOLUTION,\n        category=\"cultural\",\n        name=NATURAL_EARTH_DATA_SET,\n    )\n    reader = shpreader.Reader(shpfilename)\n    logger.info(\"Succesfully downloaded natural earth shapefiles\")\n    provinces_states = reader.records()\n\n    def filter_country_code(records: object, target_iso_a2_code=\"CN\") -&gt; list:\n        \"\"\"filter provincial/state (admin level 1) records for one country\n\n        Args:\n            records (shpreader.Reader.records): the records object from cartopy\n                    shpreader for natural earth dataset\n            target_iso_a2_code (str, optional): the country code (iso_a2) for which\n                    provincial records will be extracted. Defaults to 'CN'.\n\n        Returns:\n            list: records list\n        \"\"\"\n        results = []\n        for rec in records:\n            if rec.attributes[\"iso_a2\"] == target_iso_a2_code:\n                results.append(rec)\n\n        return results\n\n    # TODO test with none\n    if country_iso2_code is not None:\n        provinces_states = filter_country_code(\n            provinces_states, target_iso_a2_code=country_iso2_code\n        )\n\n    return provinces_states\n</code></pre>"},{"location":"reference/build_province_shapes/#build_province_shapes.records_to_data_frame","title":"<code>records_to_data_frame(records)</code>","text":"<p>dump irrelevant info and make records into a GeoDataFrame that matches the PROV_NAMES</p> <p>Parameters:</p> Name Type Description Default <code>records</code> <code>object</code> <p>the cartopy shpread records from natural earth</p> required <p>Returns:</p> Type Description <code>GeoDataFrame</code> <p>gpd.GeoDataFrame: the cleaned up &amp; sorted data in a format that can be saved</p> Source code in <code>workflow/scripts/build_province_shapes.py</code> <pre><code>def records_to_data_frame(records: object) -&gt; gpd.GeoDataFrame:\n    \"\"\"dump irrelevant info and make records into a GeoDataFrame that matches the PROV_NAMES\n\n    Args:\n        records (object): the cartopy shpread records from natural earth\n\n    Returns:\n        gpd.GeoDataFrame: the cleaned up &amp; sorted data in a format that can be saved\n    \"\"\"\n\n    records[0].attributes[\"name\"]\n    d = {\"province\": [r.attributes[\"name_en\"] for r in records]}\n    geo = [r.geometry for r in records]\n    gdf = gpd.GeoDataFrame(d, geometry=geo)\n    gdf.sort_values(by=\"province\", inplace=True)\n    # remove white spaces\n    gdf[\"province\"] = gdf.province.str.replace(\" \", \"\")\n\n    filtered = gdf[gdf.province.isin(PROV_NAMES)]\n\n    if not filtered.province.to_list() == sorted(PROV_NAMES):\n        raise ValueError(\n            \"Built cut-out does not have the right provinces\"\n            + \"- do your province lists have white spaces?\"\n        )\n\n    return filtered\n</code></pre>"},{"location":"reference/build_province_shapes/#build_province_shapes.save_province_data","title":"<code>save_province_data(provinces_gdf, crs=CRS, output_file=DEFAULT_SHAPE_OUTPATH)</code>","text":"<p>save to file</p> <p>Parameters:</p> Name Type Description Default <code>provinces_gdf</code> <code>GeoDataFrame</code> <p>the cleaned up province records</p> required <code>crs</code> <code>int</code> <p>the crs in epsg format. Defaults to CRS.</p> <code>CRS</code> <code>output_file</code> <code>pathlike</code> <p>the output path. defaults to DEFAULT_SHAPE_OUTPATH</p> <code>DEFAULT_SHAPE_OUTPATH</code> Source code in <code>workflow/scripts/build_province_shapes.py</code> <pre><code>def save_province_data(\n    provinces_gdf: gpd.GeoDataFrame,\n    crs: int = CRS,\n    output_file: os.PathLike = DEFAULT_SHAPE_OUTPATH,\n):\n    \"\"\"save to file\n\n    Args:\n        provinces_gdf (GeoDataFrame): the cleaned up province records\n        crs (int, optional): the crs in epsg format. Defaults to CRS.\n        output_file (os.pathlike): the output path. defaults to DEFAULT_SHAPE_OUTPATH\n    \"\"\"\n    provinces_gdf.set_crs(epsg=crs, inplace=True)  # WGS84\n    provinces_gdf.to_file(os.path.abspath(output_file))\n</code></pre>"},{"location":"reference/build_renewable_potential/","title":"Build renewable potential","text":"<p>Functions associated with the build_renewable_potential rule. - Temporal Profiles are built based on the atlite cutout - Potentials are built based on the atlite cutout and raster data (land availability)</p>"},{"location":"reference/build_renewable_potential/#build_renewable_potential.make_offshore_wind_profile","title":"<code>make_offshore_wind_profile(offwind_config, cutout, outp_path)</code>","text":"<p>Make the offwind geographical potentials and per unit availability time series for   each raster cell ! Somewhat compute intensive !</p> <p>Parameters:</p> Name Type Description Default <code>offwind_config</code> <code>dict</code> <p>the configuration for the offshore wind</p> required <code>cutout</code> <code>Cutout</code> <p>the atlite cutout</p> required <code>outp_path</code> <code>PathLike</code> <p>the output path for the raster date</p> required Source code in <code>workflow/scripts/build_renewable_potential.py</code> <pre><code>def make_offshore_wind_profile(offwind_config: dict, cutout: atlite.Cutout, outp_path: PathLike):\n    \"\"\"Make the offwind geographical potentials and per unit availability time series for\n      each raster cell\n    ! Somewhat compute intensive !\n\n\n    Args:\n        offwind_config (dict): the configuration for the offshore wind\n        cutout (atlite.Cutout): the atlite cutout\n        outp_path (PathLike): the output path for the raster date\n    \"\"\"\n    offwind_resource = offwind_config[\"resource\"]\n    offwind_correction_factor = offwind_config.get(\n        \"correction_factor\", DEFAULT_OFFSHORE_WIND_CORR_FACTOR\n    )\n    offwind_capacity_per_sqkm = offwind_config[\"capacity_per_sqkm\"]\n    if offwind_correction_factor != 1.0:\n        logger.info(f\"offwind_correction_factor is set as {offwind_correction_factor}\")\n\n    offwind_provinces = OFFSHORE_WIND_NODES\n\n    EEZ_province_shp = gpd.read_file(snakemake.input[\"offshore_province_shapes\"]).set_index(\n        \"province\"\n    )\n    EEZ_province_shp = EEZ_province_shp.reindex(offwind_provinces).rename_axis(\"bus\")\n    if EEZ_province_shp.geometry.isnull().any():\n        empty_geoms = EEZ_province_shp[EEZ_province_shp.geometry.isnull()].index.to_list()\n        raise ValueError(\n            f\"There are empty geometries in EEZ_province_shp {empty_geoms}, offshore wind will fail\"\n        )\n    EEZ_country = gpd.GeoDataFrame(\n        geometry=[EEZ_province_shp.unary_union],\n        crs=EEZ_province_shp.crs,\n        index=[\"country\"],\n    )\n\n    excluder_offwind = ExclusionContainer(crs=3035, res=500)\n\n    if \"max_depth\" in offwind_config:\n        func = functools.partial(np.greater, -offwind_config[\"max_depth\"])\n        excluder_offwind.add_raster(snakemake.input.gebco, codes=func, crs=CRS, nodata=-1000)\n\n    if offwind_config[\"natura\"]:\n        nat1 = gpd.read_file(snakemake.input[\"natura1\"])\n        nat2 = gpd.read_file(snakemake.input[\"natura2\"])\n        nat3 = gpd.read_file(snakemake.input[\"natura3\"])\n\n        protected_shp = gpd.GeoDataFrame(pd.concat([nat1, nat2, nat3], ignore_index=True))\n        protected_shp = gpd.GeoDataFrame(protected_shp.geometry)\n\n        protected_Marine_shp = gpd.tools.overlay(protected_shp, EEZ_country, how=\"intersection\")\n\n        # this is to avoid atlite complaining about parallelisation\n        logger.info(\"Creating tmp directory for protected marine shapefile\")\n        logger.info(f\"parent exists: {os.path.isdir(os.path.dirname(os.path.dirname(TMP)))}\")\n        if not os.path.isdir(os.path.dirname(os.path.dirname(TMP))):\n            mkdir(os.path.dirname(os.path.dirname(TMP)))\n        if not os.path.isdir(os.path.dirname(TMP)):\n            mkdir(os.path.dirname(TMP))\n        protected_Marine_shp.to_file(TMP)\n        excluder_offwind.add_geometry(TMP)\n\n    kwargs = dict(nprocesses=nprocesses, disable_progressbar=noprogress)\n    if noprogress:\n        logger.info(\"Calculate offwind landuse availabilities...\")\n        start = time.time()\n        offwind_matrix = cutout.availabilitymatrix(EEZ_province_shp, excluder_offwind, **kwargs)\n        duration = time.time() - start\n        logger.info(f\"Completed offwind availability calculation ({duration:2.2f}s)\")\n    else:\n        offwind_matrix = cutout.availabilitymatrix(EEZ_province_shp, excluder_offwind, **kwargs)\n\n    offwind_potential = offwind_capacity_per_sqkm * offwind_matrix.sum(\"bus\") * area\n\n    offwind_func = getattr(cutout, offwind_resource.pop(\"method\"))\n    offwind_resource[\"dask_kwargs\"] = {\"num_workers\": nprocesses}  # ?\n    offwind_capacity_factor = offwind_correction_factor * offwind_func(\n        capacity_factor=True, **offwind_resource\n    )\n    offwind_layout = offwind_capacity_factor * area * offwind_capacity_per_sqkm\n    offwind_profile, offwind_capacities = offwind_func(\n        matrix=offwind_matrix.stack(spatial=[\"y\", \"x\"]),\n        layout=offwind_layout,\n        index=EEZ_province_shp.index,\n        per_unit=True,\n        return_capacity=True,\n        **offwind_resource,\n    )\n\n    logger.info(\"Calculating offwind maximal capacity per bus (method 'simple')\")\n\n    offwind_p_nom_max = offwind_capacity_per_sqkm * offwind_matrix @ area\n\n    offwind_ds = xr.merge(\n        [\n            (offwind_correction_factor * offwind_profile).rename(\"profile\"),\n            offwind_capacities.rename(\"weight\"),\n            offwind_p_nom_max.rename(\"p_nom_max\"),\n            offwind_potential.rename(\"potential\"),\n        ]\n    )\n\n    offwind_ds = offwind_ds.sel(\n        bus=(\n            (offwind_ds[\"profile\"].mean(\"time\") &gt; offwind_config.get(\"min_p_max_pu\", 0.0))\n            &amp; (offwind_ds[\"p_nom_max\"] &gt; offwind_config.get(\"min_p_nom_max\", 0.0))\n        )\n    )\n\n    if \"clip_p_max_pu\" in offwind_config:\n        min_p_max_pu = offwind_config[\"clip_p_max_pu\"]\n        offwind_ds[\"profile\"] = offwind_ds[\"profile\"].where(\n            offwind_ds[\"profile\"] &gt;= min_p_max_pu, 0\n        )\n    # shift back from UTC to network time\n    offwind_ds[\"time\"] = (\n        pd.DatetimeIndex(offwind_ds[\"time\"], tz=\"UTC\").tz_convert(TIMEZONE).tz_localize(None).values\n    )\n    offwind_ds.to_netcdf(outp_path)\n</code></pre>"},{"location":"reference/build_renewable_potential/#build_renewable_potential.make_onshore_wind_profile","title":"<code>make_onshore_wind_profile(onwind_config, cutout, outp_path)</code>","text":"<p>Make the onwind geographical potentials and per unit availability time series for each raster cell ! Somewhat compute intensive !</p> <p>Parameters:</p> Name Type Description Default <code>onwind_config</code> <code>dict</code> <p>the onshore wind config (from the yaml config read by snakemake)</p> required <code>cutout</code> <code>Cutout</code> <p>the atlite cutout</p> required <code>outp_path</code> <code>PathLike</code> <p>the output path for the raster data</p> required Source code in <code>workflow/scripts/build_renewable_potential.py</code> <pre><code>def make_onshore_wind_profile(onwind_config: dict, cutout: atlite.Cutout, outp_path: PathLike):\n    \"\"\"Make the onwind geographical potentials and per unit availability time series for\n    each raster cell\n    ! Somewhat compute intensive !\n\n    Args:\n        onwind_config (dict): the onshore wind config (from the yaml config read by snakemake)\n        cutout (atlite.Cutout): the atlite cutout\n        outp_path (PathLike): the output path for the raster data\n    \"\"\"\n\n    logger.info(\"Making onshore wind profile \")\n\n    onwind_resource = onwind_config[\"resource\"]\n    onwind_correction_factor = onwind_config.get(\"correction_factor\", 1.0)\n    onwind_capacity_per_sqkm = onwind_config[\"capacity_per_sqkm\"]\n    if onwind_correction_factor != 1.0:\n        logger.info(f\"onwind_correction_factor is set as {onwind_correction_factor}\")\n\n    excluder_onwind = ExclusionContainer(crs=3035, res=500)\n\n    excluder_onwind.add_raster(grass, invert=True, crs=4326)\n    excluder_onwind.add_raster(bare, invert=True, crs=4326)\n    excluder_onwind.add_raster(shrubland, invert=True, crs=4326)\n\n    kwargs = dict(nprocesses=nprocesses, disable_progressbar=noprogress)\n    if noprogress:\n        logger.info(\"Calculate onwind landuse availabilities...\")\n        start = time.time()\n        onwind_matrix = cutout.availabilitymatrix(provinces_shp, excluder_onwind, **kwargs)\n        duration = time.time() - start\n        logger.info(f\"Completed onwind availability calculation ({duration:2.2f}s)\")\n    else:\n        onwind_matrix = cutout.availabilitymatrix(provinces_shp, excluder_onwind, **kwargs)\n\n    onwind_potential = onwind_capacity_per_sqkm * onwind_matrix.sum(\"bus\") * area\n\n    onwind_func = getattr(cutout, onwind_resource.pop(\"method\"))\n    onwind_resource[\"dask_kwargs\"] = {\"num_workers\": nprocesses}  # ?\n    onwind_capacity_factor = onwind_correction_factor * onwind_func(\n        capacity_factor=True, **onwind_resource\n    )\n    onwind_layout = onwind_capacity_factor * area * onwind_capacity_per_sqkm\n    onwind_profile, onwind_capacities = onwind_func(\n        matrix=onwind_matrix.stack(spatial=[\"y\", \"x\"]),\n        layout=onwind_layout,\n        index=buses,\n        per_unit=True,\n        return_capacity=True,\n        **onwind_resource,\n    )\n\n    logger.info(\"Calculating onwind maximal capacity per bus (method 'simple')\")\n\n    onwind_p_nom_max = onwind_capacity_per_sqkm * onwind_matrix @ area\n\n    onwind_ds = xr.merge(\n        [\n            (onwind_correction_factor * onwind_profile).rename(\"profile\"),\n            onwind_capacities.rename(\"weight\"),\n            onwind_p_nom_max.rename(\"p_nom_max\"),\n            onwind_potential.rename(\"potential\"),\n        ]\n    )\n\n    onwind_ds = onwind_ds.sel(\n        bus=(\n            (onwind_ds[\"profile\"].mean(\"time\") &gt; onwind_config.get(\"min_p_max_pu\", 0.0))\n            &amp; (onwind_ds[\"p_nom_max\"] &gt; onwind_config.get(\"min_p_nom_max\", 0.0))\n        )\n    )\n\n    if \"clip_p_max_pu\" in onwind_config:\n        min_p_max_pu = onwind_config[\"clip_p_max_pu\"]\n        onwind_ds[\"profile\"] = onwind_ds[\"profile\"].where(onwind_ds[\"profile\"] &gt;= min_p_max_pu, 0)\n\n    # shift back from UTC to network time\n    onwind_ds[\"time\"] = (\n        pd.DatetimeIndex(onwind_ds[\"time\"], tz=\"UTC\").tz_convert(TIMEZONE).tz_localize(None).values\n    )\n    onwind_ds.to_netcdf(outp_path)\n</code></pre>"},{"location":"reference/build_renewable_potential/#build_renewable_potential.make_solar_profile","title":"<code>make_solar_profile(solar_config, cutout, outp_path)</code>","text":"<p>Make the solar geographical potentials and per unit availability time series for each raster cell ! Somewhat compute intensive !</p> <p>Parameters:</p> Name Type Description Default <code>solar_config</code> <code>dict</code> <p>the solar configuration (from the yaml config read by snakemake)</p> required <code>cutout</code> <code>Cutout</code> <p>the atlite cutout</p> required <code>outp_path</code> <code>PathLike</code> <p>the output path for the raster data</p> required Source code in <code>workflow/scripts/build_renewable_potential.py</code> <pre><code>def make_solar_profile(\n    solar_config: dict,\n    cutout: atlite.Cutout,\n    outp_path: PathLike,\n):\n    \"\"\"Make the solar geographical potentials and per unit availability time series for each\n    raster cell\n    ! Somewhat compute intensive !\n\n    Args:\n        solar_config (dict): the solar configuration (from the yaml config read by snakemake)\n        cutout (atlite.Cutout): the atlite cutout\n        outp_path (PathLike): the output path for the raster data\n    \"\"\"\n\n    logger.info(\"Making solar profile \")\n    solar_config = snakemake.config[\"renewable\"][\"solar\"]\n    solar_resource = solar_config[\"resource\"]\n    solar_correction_factor = solar_config.get(\"correction_factor\", 1.0)\n    solar_capacity_per_sqkm = solar_config[\"capacity_per_sqkm\"]\n    if solar_correction_factor != 1.0:\n        logger.info(f\"solar_correction_factor is set as {solar_correction_factor}\")\n\n    # TODO not hardcoded res\n    excluder_solar = ExclusionContainer(crs=3035, res=500)\n    excluder_build_up = ExclusionContainer(crs=3035, res=500)\n\n    build_up = snakemake.input[\"Build_up_raster\"]\n\n    excluder_build_up.add_raster(build_up, invert=True, crs=CRS)\n    excluder_solar.add_raster(grass, invert=True, crs=CRS)\n    excluder_solar.add_raster(bare, invert=True, crs=CRS)\n    excluder_solar.add_raster(shrubland, invert=True, crs=CRS)\n\n    kwargs = dict(nprocesses=nprocesses, disable_progressbar=noprogress)\n    # TODO remove if else?\n    if noprogress:\n        logger.info(\"Calculate solar landuse availabilities...\")\n        start = time.time()\n        solar_matrix = cutout.availabilitymatrix(provinces_shp, excluder_solar, **kwargs)\n        buildup_matrix = cutout.availabilitymatrix(provinces_shp, excluder_build_up, **kwargs)\n        duration = time.time() - start\n        logger.info(f\"Completed solar availability calculation ({duration:2.2f}s)\")\n    else:\n        solar_matrix = cutout.availabilitymatrix(\n            shapes=provinces_shp, excluder=excluder_solar, **kwargs\n        )\n        buildup_matrix = cutout.availabilitymatrix(provinces_shp, excluder_build_up, **kwargs)\n\n    solar_potential = (\n        solar_capacity_per_sqkm * solar_matrix.sum(\"bus\") * area\n        + solar_capacity_per_sqkm * buildup_matrix.sum(\"bus\") * area\n    )\n\n    solar_func = getattr(cutout, solar_resource.pop(\"method\"))\n    solar_resource[\"dask_kwargs\"] = {\"num_workers\": nprocesses}  # ?\n    solar_capacity_factor = solar_correction_factor * solar_func(\n        capacity_factor=True, **solar_resource\n    )\n    solar_layout = solar_capacity_factor * area * solar_capacity_per_sqkm\n    solar_profile, solar_capacities = solar_func(\n        matrix=solar_matrix.stack(spatial=[\"y\", \"x\"]),\n        layout=solar_layout,\n        index=buses,\n        per_unit=True,\n        return_capacity=True,\n        **solar_resource,\n    )\n\n    logger.info(\"Calculating solar maximal capacity per bus (method 'simple')\")\n\n    solar_p_nom_max = solar_capacity_per_sqkm * solar_matrix @ area\n\n    solar_ds = xr.merge(\n        [\n            (solar_correction_factor * solar_profile).rename(\"profile\"),\n            solar_capacities.rename(\"weight\"),\n            solar_p_nom_max.rename(\"p_nom_max\"),\n            solar_potential.rename(\"potential\"),\n        ]\n    )\n\n    solar_ds = solar_ds.sel(\n        bus=(\n            (solar_ds[\"profile\"].mean(\"time\") &gt; solar_config.get(\"min_p_max_pu\", 0.0))\n            &amp; (solar_ds[\"p_nom_max\"] &gt; solar_config.get(\"min_p_nom_max\", 0.0))\n        )\n    )\n\n    if \"clip_p_max_pu\" in solar_config:\n        min_p_max_pu = solar_config[\"clip_p_max_pu\"]\n        solar_ds[\"profile\"] = solar_ds[\"profile\"].where(solar_ds[\"profile\"] &gt;= min_p_max_pu, 0)\n\n    # shift back from UTC to network time\n    solar_ds[\"time\"] = (\n        pd.DatetimeIndex(solar_ds[\"time\"], tz=\"UTC\").tz_convert(TIMEZONE).tz_localize(None).values\n    )\n\n    solar_ds.to_netcdf(outp_path)\n</code></pre>"},{"location":"reference/build_renewable_profiles/","title":"Build renewable profiles","text":"<p>Adapted from pypsa-EUR by the pypsa China-PIK authors</p> <p>Calculates for each clustered region the (i) installable capacity (based on land-use from :mod:<code>determine_availability_matrix</code>) (ii) the available generation time series (based on weather data) (iii) the average distanc from the node for onshore wind, AC-connected offshore wind, DC-connected offshore wind and solar PV generators.</p>"},{"location":"reference/build_renewable_profiles/#build_renewable_profiles--outputs","title":"Outputs","text":"<ul> <li> <p><code>resources/profile_{technology}.nc</code> with the following structure</p> <p>===================  ====================  ===================================================== Field                Dimensions            Description ===================  ====================  ===================================================== profile              year, bus, bin, time  the per unit hourly availability factors for each bus</p> <p>p_nom_max            bus, bin              maximal installable capacity at the bus (in MW)</p> <p>average_distance     bus, bin              average distance of units in the region to the                                            grid bus for onshore techs and to the shoreline                                            for offshore technologies (in km) ===================  ====================  =====================================================</p> </li> </ul>"},{"location":"reference/build_renewable_profiles/#build_renewable_profiles--description","title":"Description","text":"<p>This script functions at two main spatial resolutions: the resolution of the clustered network regions, and the resolution of the cutout grid cells for the weather data. Typically the weather data grid is finer than the network regions, so we have to work out the distribution of generators across the grid cells within each region. This is done by taking account of a combination of the available land at each grid cell (computed in :mod:<code>determine_availability_matrix</code>) and the capacity factor there.</p> <p>Based on the availability matrix, the script first computes how much of the technology can be installed at each cutout grid cell. To compute the layout of generators in each clustered region, the installable potential in each grid cell is multiplied with the capacity factor at each grid cell. This is done since we assume more generators are installed at cells with a higher capacity factor.</p> <p>Based on the average capacity factor, the potentials are further divided into a configurable number of resource classes (bins).</p> <p>This layout is then used to compute the generation availability time series from the weather data cutout from <code>atlite</code>.</p> <p>The maximal installable potential for the node (<code>p_nom_max</code>) is computed by adding up the installable potentials of the individual grid cells.</p>"},{"location":"reference/build_renewable_profiles/#build_renewable_profiles.build_resource_classes","title":"<code>build_resource_classes(cutout, nbins, regions, capacity_factor, params)</code>","text":"<p>Bin resources based on their capacity factor The number of bins can be dynamically reduced based on a min delta cf</p> <p>Parameters:</p> Name Type Description Default <code>cutout</code> <code>Cutout</code> <p>the atlite cutout</p> required <code>nbins</code> <code>int</code> <p>the number of bins</p> required <code>regions</code> <code>GeoSeries</code> <p>the regions</p> required <code>capacity_factor</code> <code>(DataArray,)</code> <p>the capacity factor</p> required <code>params</code> <code>dict</code> <p>the config for VREs</p> required <p>Returns:</p> Type Description <code>DataArray</code> <p>xr.DataArray: the mask for the resource classes</p> <code>GeoSeries</code> <p>gpd.GeoSeries: multi-indexed series [bus, bin]: geometry</p> Source code in <code>workflow/scripts/build_renewable_profiles.py</code> <pre><code>def build_resource_classes(\n    cutout: Cutout,\n    nbins: int,\n    regions: gpd.GeoSeries,\n    capacity_factor: xr.DataArray,\n    params: dict,\n) -&gt; tuple[xr.DataArray, gpd.GeoSeries]:\n    \"\"\"Bin resources based on their capacity factor\n    The number of bins can be dynamically reduced based on a min delta cf\n\n    Args:\n        cutout (Cutout): the atlite cutout\n        nbins (int): the number of bins\n        regions (gpd.GeoSeries): the regions\n        capacity_factor (xr.DataArray,): the capacity factor\n        params (dict): the config for VREs\n\n    Returns:\n        xr.DataArray: the mask for the resource classes\n        gpd.GeoSeries: multi-indexed series [bus, bin]: geometry\n    \"\"\"\n    resource_classes = params.get(\"resource_classes\", {})\n    nbins = resource_classes.get(\"n\", 1)\n    min_cf_delta = resource_classes.get(\"min_cf_delta\", 0.0)\n    buses = regions.index\n\n    # indicator matrix for which cells touch which regions\n    IndMat = np.ceil(cutout.availabilitymatrix(regions, ExclusionContainer()))\n    cf_by_bus = capacity_factor * IndMat.where(IndMat &gt; 0)\n\n    epsilon = 1e-3\n    cf_min, cf_max = (\n        cf_by_bus.min(dim=[\"x\", \"y\"]) - epsilon,\n        cf_by_bus.max(dim=[\"x\", \"y\"]) + epsilon,\n    )\n\n    # avoid binning resources that are very similar\n    nbins_per_bus = [int(min(nbins, x)) for x in (cf_max - cf_min) // min_cf_delta]\n    normed_bins = xr.DataArray(\n        np.vstack(\n            [np.hstack([[0] * (nbins - n), np.linspace(0, 1, n + 1)]) for n in nbins_per_bus]\n        ),\n        dims=[\"bus\", \"bin\"],\n        coords={\"bus\": regions.index},\n    )\n    bins = cf_min + (cf_max - cf_min) * normed_bins\n\n    cf_by_bus_bin = cf_by_bus.expand_dims(bin=range(nbins))\n    lower_edges = bins[:, :-1]\n    upper_edges = bins[:, 1:]\n    class_masks = (cf_by_bus_bin &gt;= lower_edges) &amp; (cf_by_bus_bin &lt; upper_edges)\n\n    if nbins == 1:\n        bus_bin_mi = pd.MultiIndex.from_product([regions.index, [0]], names=[\"bus\", \"bin\"])\n        class_regions = regions.set_axis(bus_bin_mi)\n        class_regions[\"cf\"] = bins.to_series()\n    else:\n        grid = cutout.grid.set_index([\"y\", \"x\"])\n        class_regions = {}\n        for bus, bin_id in product(buses, range(nbins)):\n            bus_bin_mask = (\n                class_masks.sel(bus=bus, bin=bin_id).stack(spatial=[\"y\", \"x\"]).to_pandas()\n            )\n            grid_cells = grid.loc[bus_bin_mask]\n            geometry = grid_cells.intersection(regions.loc[bus, \"geometry\"]).union_all().buffer(0)\n            class_regions[(bus, bin_id)] = geometry\n\n        class_regions = gpd.GeoDataFrame(\n            {\"geometry\": class_regions.values()},\n            index=pd.MultiIndex.from_tuples(class_regions.keys(), names=[\"bus\", \"bin\"]),\n        )\n        class_regions[\"cf\"] = bins.to_series()\n\n    return class_masks, class_regions\n</code></pre>"},{"location":"reference/build_renewable_profiles/#build_renewable_profiles.localize_cutout_time","title":"<code>localize_cutout_time(cutout, drop_leap=True)</code>","text":"<p>localize the time to the local timezone</p> <p>Parameters:</p> Name Type Description Default <code>cutout</code> <code>Cutout</code> <p>the atlite cutout object</p> required <code>drop_leap</code> <code>bool</code> <p>drop 29th Feb. Defaults to True.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>Cutout</code> <code>Cutout</code> <p>the updated cutout</p> Source code in <code>workflow/scripts/build_renewable_profiles.py</code> <pre><code>def localize_cutout_time(cutout: Cutout, drop_leap=True) -&gt; Cutout:\n    \"\"\"localize the time to the local timezone\n\n    Args:\n        cutout (Cutout): the atlite cutout object\n        drop_leap (bool, optional): drop 29th Feb. Defaults to True.\n\n    Returns:\n        Cutout: the updated cutout\n    \"\"\"\n\n    data = cutout.data\n\n    timestamps = pd.DatetimeIndex(data.time)\n    # go from ECMWF/atlite UTC to local time\n    ts_naive = timestamps.tz_localize(\"UTC\").tz_convert(TIMEZONE).tz_localize(None)\n    cutout.data = cutout.data.assign_coords(time=ts_naive)\n\n    if drop_leap:\n        data = cutout.data\n        cutout.data = data.sel(time=~((data.time.dt.month == 2) &amp; (data.time.dt.day == 29)))\n\n    return cutout\n</code></pre>"},{"location":"reference/build_renewable_profiles/#build_renewable_profiles.prepare_resource_config","title":"<code>prepare_resource_config(params, nprocesses, noprogress=True)</code>","text":"<p>Parse the resource config (atlite calc config)</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>dict</code> <p>the renewable options</p> required <code>nprocesses</code> <code>int</code> <p>the number or processes</p> required <code>noprogress</code> <code>bool</code> <p>whether to show progress bars</p> <code>True</code> <p>Returns:</p> Type Description <code>(dict, dict)</code> <p>the resource config for the atlite calcs, the turbine/panel models</p> Source code in <code>workflow/scripts/build_renewable_profiles.py</code> <pre><code>def prepare_resource_config(params: dict, nprocesses: int, noprogress=True) -&gt; tuple[dict]:\n    \"\"\"Parse the resource config (atlite calc config)\n\n    Args:\n        params (dict): the renewable options\n        nprocesses (int): the number or processes\n        noprogress (bool): whether to show progress bars\n\n    Returns:\n        (dict, dict): the resource config for the atlite calcs, the turbine/panel models\n    \"\"\"\n\n    resource = params[\"resource\"]  # pv panel params / wind turbine params\n    resource[\"show_progress\"] = not noprogress\n    tech = \"panel\" if \"panel\" in resource else \"turbine\"\n\n    # in case of multiple years\n    models = resource[tech]\n    if not isinstance(models, dict):\n        models = {0: models}\n\n    if nprocesses &gt; 1:\n        client = Client(n_workers=nprocesses, threads_per_worker=1)\n        resource[\"dask_kwargs\"] = {\"scheduler\": client}\n\n    return resource, models\n</code></pre>"},{"location":"reference/build_solar_thermal_profiles/","title":"Build solar thermal profiles","text":""},{"location":"reference/build_solar_thermal_profiles/#build_solar_thermal_profiles.build_solar_thermal_profiles","title":"<code>build_solar_thermal_profiles(pop_map, cutout, outp_path)</code>","text":"<p>build per unit solar thermal time availability profiles and save them to a file</p> <p>Parameters:</p> Name Type Description Default <code>pop_map</code> <code>DataFrame</code> <p>DataFrame with the population map</p> required <code>cutout</code> <code>Cutout</code> <p>atlite cutout object with the weather data</p> required <code>outp_path</code> <code>PathLike</code> <p>Path to the output file</p> required Source code in <code>workflow/scripts/build_solar_thermal_profiles.py</code> <pre><code>def build_solar_thermal_profiles(\n    pop_map: pd.DataFrame, cutout: atlite.Cutout, outp_path: os.PathLike\n) -&gt; None:\n    \"\"\"build per unit solar thermal time availability profiles and save them to a file\n\n    Args:\n        pop_map (pd.DataFrame): DataFrame with the population map\n        cutout (atlite.Cutout): atlite cutout object with the weather data\n        outp_path (os.PathLike): Path to the output file\n    \"\"\"\n    pop_matrix = sp.sparse.csr_matrix(pop_map.T)\n    index = pop_map.columns\n    index.name = \"provinces\"\n\n    st = cutout.solar_thermal(\n        orientation={\n            \"slope\": float(snakemake.config[\"solar_thermal_angle\"]),\n            \"azimuth\": 180.0,\n        },\n        matrix=pop_matrix,\n        index=index,\n    )\n\n    st[\"time\"] = (\n        pd.DatetimeIndex(st[\"time\"], tz=\"UTC\").tz_convert(TIMEZONE).tz_localize(None).values\n    )\n\n    with pd.HDFStore(outp_path, mode=\"w\", complevel=4) as store:\n        store[\"solar_thermal_profiles\"] = st.to_pandas().divide(pop_map.sum())\n</code></pre>"},{"location":"reference/build_temperature_profiles/","title":"Build temperature profiles","text":"<p>Functions associated with the build_temperature_profiles rule.</p>"},{"location":"reference/build_temperature_profiles/#build_temperature_profiles.build_temp_profiles","title":"<code>build_temp_profiles(pop_map, cutout, temperature_out)</code>","text":"<p>build the temperature profiles in the cutout, this converts the atlite temperature &amp; weights the node building process by the population map</p> <p>Note that atlite only supports a single time zone shift</p> <p>Parameters:</p> Name Type Description Default <code>pop_map</code> <code>DataFrame</code> <p>the map to the pop density grid cell data (hdf5)</p> required <code>cutout</code> <code>Cutout</code> <p>the weather data cutout (atlite cutout)</p> required <code>temperature_out</code> <code>PathLike</code> <p>the output path (hdf5)</p> required Source code in <code>workflow/scripts/build_temperature_profiles.py</code> <pre><code>def build_temp_profiles(pop_map: pd.DataFrame, cutout: atlite.Cutout, temperature_out: PathLike):\n    \"\"\"build the temperature profiles in the cutout, this converts the atlite temperature &amp; weights\n    the node building process by the population map\n\n    Note that atlite only supports a single time zone shift\n\n    Args:\n        pop_map (pd.DataFrame): the map to the pop density grid cell data (hdf5)\n        cutout (atlite.Cutout): the weather data cutout (atlite cutout)\n        temperature_out (PathLike): the output path (hdf5)\n    \"\"\"\n    # build a sparse matrix of BUSxCUTOUT_gridcells to weigh the cutout-&gt;bus aggregation process\n    pop_matrix = sp.sparse.csr_matrix(pop_map.T)\n    index = pop_map.columns\n    index.name = \"provinces\"\n\n    temperature = cutout.temperature(matrix=pop_matrix, index=index)\n    # convert the cutout UTC time to local time\n    temperature[\"time\"] = (\n        pd.DatetimeIndex(temperature[\"time\"], tz=\"UTC\")\n        .tz_convert(TIMEZONE)\n        .tz_localize(None)\n        .values\n    )\n\n    with pd.HDFStore(temperature_out, mode=\"w\", complevel=4) as store:\n        store[\"temperature\"] = temperature.to_pandas().divide(pop_map.sum())\n</code></pre>"},{"location":"reference/constants/","title":"Constants","text":"<p>Soft coded centalized <code>constants</code></p>"},{"location":"reference/constants/#constants.get_province_names","title":"<code>get_province_names()</code>","text":"<p>HACK to make it possible for pytest to generate a smaller network</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>if the PROV_NAMES is not a list or str</p> <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>the province node names to build the network</p> Source code in <code>workflow/scripts/constants.py</code> <pre><code>def get_province_names() -&gt; list:\n    \"\"\"HACK to make it possible for pytest to generate a smaller network\n\n    Raises:\n        ValueError: if the PROV_NAMES is not a list or str\n\n    Returns:\n        list: the province node names to build the network\n    \"\"\"\n    default_prov_names = list(REGIONAL_GEO_TIMEZONES_DEFAULT)\n    _provs = os.getenv(\"PROV_NAMES\", default_prov_names)\n    if isinstance(_provs, str):\n        _provs = re.findall(r\"[\\w']+\", _provs)\n        if not _provs:\n            xpected = '[\"region1\", ...]'\n            err = f\"Environment var PROV_NAMES {_provs} for tests did not have expected format: \"\n            raise ValueError(err + xpected)\n    elif not isinstance(_provs, list):\n        raise ValueError(\"PROV_NAMES must be a list or str\")\n    return _provs\n</code></pre>"},{"location":"reference/determine_availability_matrix/","title":"Determine availability matrix","text":"<p>The script performs a land eligibility analysis of what share of land is availability for developing the selected technology at each cutout grid cell. The script uses the <code>atlite &lt;https://github.com/pypsa/atlite&gt;</code>_ library and several GIS datasets like the Copernicus land use data, Natura2000 nature reserves, GEBCO bathymetry data</p> <p>The copernicus land monitoring data is/can be fetched by the pipeline. The GEBCO data must currently be manually downloaded   https://en.wikipedia.org/wiki/Bathymetry<code>_ data set with a global terrain   model for ocean and land at 15 arc-second intervals by the</code>General   Bathymetric Chart of the Oceans (GEBCO)   https://www.gebco.net/data_and_products/gridded_bathymetry_data/<code>_. &lt;https://www.gebco.net/data_and_products/images/gebco_2019_grid_image.jpg&gt;</code>_ The natura data must be manually downloaded for now or retrieved with the data bundle</p>"},{"location":"reference/fetch_rasters/","title":"Fetch rasters","text":"<p>Methods to fetch raster data from copernicus archive. Requires sentinelHub API. </p> <p>EXAMPLE SCRIPT</p>"},{"location":"reference/fetch_rasters/#fetch_rasters.authenticate","title":"<code>authenticate()</code>","text":"<p>authenticae with the copernicus API</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>the access token dict</p> Source code in <code>workflow/scripts/fetch_rasters.py</code> <pre><code>def authenticate() -&gt; dict:\n    \"\"\"authenticae with the copernicus API\n\n    Returns:\n        dict: the access token dict\n    \"\"\"\n    service_key = json.load(open(TOKEN_PATH, \"rb\"))\n\n    private_key = service_key[\"private_key\"].encode(\"utf-8\")\n\n    claim_set = {\n        \"iss\": service_key[\"client_id\"],\n        \"sub\": service_key[\"user_id\"],\n        \"aud\": service_key[\"token_uri\"],\n        \"iat\": int(time.time()),\n        \"exp\": int(time.time() + (60 * 60)),\n    }\n    grant = jwt.encode(claim_set, private_key, algorithm=\"RS256\")\n    base_url = claim_set[\"aud\"]\n    token_request = requests.post(\n        base_url,\n        headers={\n            \"Accept\": \"application/json\",\n            \"Content-Type\": \"application/x-www-form-urlencoded\",\n        },\n        data=f\"grant_type=urn:ietf:params:oauth:grant-type:jwt-bearer&amp;assertion={grant}\",\n    )\n    if token_request.status_code != 200:\n        print(token_request.text)\n        exit(1)\n    token = token_request.json()\n    return token\n</code></pre>"},{"location":"reference/fetch_rasters/#fetch_rasters.find_dataset_ids","title":"<code>find_dataset_ids(name='global-dynamic-land-cover')</code>","text":"<p>find the catalogue ID of the dataset</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>dataset product name. Defaults to \"global-dynamic-land-cover\".</p> <code>'global-dynamic-land-cover'</code> <p>Returns:</p> Name Type Description <code>tuple</code> <code>tuple[str]</code> <p>the results (download_info_id, download_id)</p> Source code in <code>workflow/scripts/fetch_rasters.py</code> <pre><code>def find_dataset_ids(name=\"global-dynamic-land-cover\") -&gt; tuple[str]:\n    \"\"\"find the catalogue ID of the dataset\n\n    Args:\n        name (str, optional): dataset product name. Defaults to \"global-dynamic-land-cover\".\n\n    Returns:\n        tuple: the results (download_info_id, download_id)\n    \"\"\"\n    bid = 0\n    # batches of 25\n    while True:\n        batch = \"\" if bid == 0 else f\"b_start={bid}&amp;\"\n        search_req = requests.get(\n            f\"https://land.copernicus.eu/api/@search?{batch}portal_type=DataSet&amp;metadata_fields=UID&amp;metadata_fields=dataset_full_format&amp;&amp;metadata_fields=dataset_download_information\",\n            headers={\"Accept\": \"application/json\"},\n        )\n        if search_req.status_code != 200:\n            logger.error(f\"failed request: {search_req.text}\")\n            exit(1)\n        search_results = search_req.json()\n        res = search_items(search_results, target_name=name)\n        if res == []:\n            bid += 25\n        else:\n            break\n\n    res_list = [r for r in search_results[\"items\"] if r[\"@id\"].find(\"2019\") != -1]\n    download_info_id = res_list[0][\"dataset_download_information\"][\"items\"][0][\"@id\"]\n    download_id = res_list[0][\"UID\"]\n    # dataset_url = res_list[0][\"@id\"]\n    return download_info_id, download_id\n</code></pre>"},{"location":"reference/fetch_rasters/#fetch_rasters.post_data_request","title":"<code>post_data_request(token, download_info_id, download_id)</code>","text":"<p>post the request to the copernicus API</p> <p>Parameters:</p> Name Type Description Default <code>token</code> <code>dict</code> <p>the access token</p> required <code>download_info_id</code> <code>str</code> <p>output of find_dataset_ids</p> required <code>download_id</code> <code>str</code> <p>output of find_dataset_ids</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <code>tuple[dict, str]</code> <p>the data request (json dict) and task id (string?)</p> Source code in <code>workflow/scripts/fetch_rasters.py</code> <pre><code>def post_data_request(token: dict, download_info_id: str, download_id: str) -&gt; tuple[dict, str]:\n    \"\"\"post the request to the copernicus API\n\n    Args:\n        token (dict): the access token\n        download_info_id (str): output of find_dataset_ids\n        download_id (str): output of find_dataset_ids\n\n    Returns:\n        tuple: the data request (json dict) and task id (string?)\n    \"\"\"\n    base_url = \"https://land.copernicus.eu\"\n    data_req = requests.post(\n        f\"{base_url}/api/@datarequest_post\",\n        headers={\n            \"Accept\": \"application/json\",\n            \"Content-Type\": \"application/json\",\n            \"Authorization\": f\"Bearer {token['access_token']}\",\n        },\n        json={\n            \"Datasets\": [\n                {\n                    \"DatasetID\": download_id,\n                    \"DatasetDownloadInformationID\": download_info_id,\n                    \"Layer\": \"Cover Fraction: Built-up \",\n                    \"NUTS\": \"CN\",\n                    \"OutputFormat\": \"Geotiff\",\n                    \"OutputGCS\": \"EPSG:4326\",\n                }\n            ]\n        },\n    )\n\n    return data_req.json(), data_req.json()[\"TaskIds\"][0][\"TaskID\"]\n</code></pre>"},{"location":"reference/fetch_rasters/#fetch_rasters.search_items","title":"<code>search_items(json_items, target_name='global-dynamic-land-cover')</code>","text":"<p>filter items for target name</p> <p>Parameters:</p> Name Type Description Default <code>json_items</code> <code>dict</code> <p>the json response to the seach query</p> required <code>target_name</code> <code>str</code> <p>The items to find. Defaults to \"global-dynamic-land-cover\".</p> <code>'global-dynamic-land-cover'</code> <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>the items list</p> Source code in <code>workflow/scripts/fetch_rasters.py</code> <pre><code>def search_items(json_items: dict, target_name=\"global-dynamic-land-cover\") -&gt; list:\n    \"\"\"filter items for target name\n\n    Args:\n        json_items (dict): the json response to the seach query\n        target_name (str, optional): The items to find. Defaults to \"global-dynamic-land-cover\".\n\n    Returns:\n        list: the items list\n    \"\"\"\n\n    return [itm[\"@id\"] for itm in json_items[\"items\"] if itm[\"@id\"].find(target_name) != -1]\n</code></pre>"},{"location":"reference/fetch_shapes/","title":"Fetch shapes","text":"<p>Data fetch operation for region/province/country shapes</p>"},{"location":"reference/fetch_shapes/#fetch_shapes.build_nodes","title":"<code>build_nodes(prefectures, nodes_cfg)</code>","text":"<p>Build the nodes, either directly at provincial (admin1) level or from adminlvk2 subregions</p> <p>Parameters:</p> Name Type Description Default <code>prefectures</code> <code>GeoDataFrame</code> required Source code in <code>workflow/scripts/fetch_shapes.py</code> <pre><code>def build_nodes(\n    prefectures: gpd.GeoDataFrame,\n    nodes_cfg: dict,\n) -&gt; gpd.GeoSeries:\n    \"\"\" Build the nodes, either directly at provincial (admin1) level or from adminlvk2 subregions\n\n    Args:\n      prefectures:  \"\"\"\n    gdf = prefectures.copy()\n    if nodes_cfg.get(\"split_provinces\", False):\n        validate_split_cfg(nodes_cfg[\"splits\"], gdf)\n        return split_provinces(gdf, nodes_cfg)\n    else:\n        provs = provs = gdf.dissolve(GDAM_LV1)\n        provs = provs.drop([nodes_cfg[\"exclude_provinces\"]])\n        return provs.rename_axis(\"node\")[\"geometry\"]\n</code></pre>"},{"location":"reference/fetch_shapes/#fetch_shapes.cut_smaller_from_larger","title":"<code>cut_smaller_from_larger(row, gdf, overlaps)</code>","text":"<p>automatically assign overlapping area to the smaller region</p> Example <p>areas_gdf.apply(cut_smaller_from_larger, args=(areas_gdf, overlaps), axis=1)</p> <p>Parameters:</p> Name Type Description Default <code>row</code> <code>GeoSeries</code> <p>the row from pandas apply</p> required <code>gdf</code> <code>GeoDataFrame</code> <p>the geodataframe on which the operation is performed</p> required <code>overlaps</code> <code>DataFrame</code> <p>the boolean overlap table</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>in case areas are exactly equal</p> <p>Returns:</p> Type Description <code>GeoSeries</code> <p>gpd.GeoSeries: the row with overlaps removed or not</p> Source code in <code>workflow/scripts/fetch_shapes.py</code> <pre><code>def cut_smaller_from_larger(\n    row: gpd.GeoSeries, gdf: gpd.GeoDataFrame, overlaps: DataFrame\n) -&gt; gpd.GeoSeries:\n    \"\"\"automatically assign overlapping area to the smaller region\n\n    Example:\n        areas_gdf.apply(cut_smaller_from_larger, args=(areas_gdf, overlaps), axis=1)\n\n    Args:\n        row (gpd.GeoSeries): the row from pandas apply\n        gdf (gpd.GeoDataFrame): the geodataframe on which the operation is performed\n        overlaps (DataFrame): the boolean overlap table\n\n    Raises:\n        ValueError: in case areas are exactly equal\n\n    Returns:\n        gpd.GeoSeries: the row with overlaps removed or not\n    \"\"\"\n    ovrlap_idx = np.where(overlaps.loc[row.name].values == True)[0].tolist()\n    for idx in ovrlap_idx:\n        geom = gdf.iloc[idx].geometry\n        if row.geometry.area &gt; geom.area:\n            row[\"geometry\"] = row[\"geometry\"].difference(geom)\n        elif row.geometry.area == geom.area:\n            raise ValueError(f\"Equal area overlap between {row.name} and {idx} - unhandled\")\n    return row\n</code></pre>"},{"location":"reference/fetch_shapes/#fetch_shapes.eez_by_region","title":"<code>eez_by_region(eez, province_shapes, prov_key='region', simplify_tol=0.5)</code>","text":"<p>break up the eez by admin1 regions based on voronoi polygons of the centroids</p> <p>Parameters:</p> Name Type Description Default <code>eez</code> <code>GeoDataFrame</code> <p>description</p> required <code>province_shapes</code> <code>GeoDataFrame</code> <p>description</p> required <code>prov_key</code> <code>str</code> <p>name of the provinces col in province_shapes. Defaults to \"region\".</p> <code>'region'</code> <code>simplify_tol</code> <code>float</code> <p>tolerance for simplifying the voronoi polygons. Defaults to 0.5.</p> <code>0.5</code> <p>Returns:</p> Type Description <code>GeoDataFrame</code> <p>gpd.GeoDataFrame: description</p> Source code in <code>workflow/scripts/fetch_shapes.py</code> <pre><code>def eez_by_region(\n    eez: gpd.GeoDataFrame,\n    province_shapes: gpd.GeoDataFrame,\n    prov_key=\"region\",\n    simplify_tol=0.5,\n) -&gt; gpd.GeoDataFrame:\n    \"\"\"break up the eez by admin1 regions based on voronoi polygons of the centroids\n\n    Args:\n        eez (gpd.GeoDataFrame): _description_\n        province_shapes (gpd.GeoDataFrame): _description_\n        prov_key (str, optional): name of the provinces col in province_shapes. Defaults to \"region\".\n        simplify_tol (float, optional): tolerance for simplifying the voronoi polygons. Defaults to 0.5.\n\n    Returns:\n        gpd.GeoDataFrame: _description_\n    \"\"\"\n    # generate voronoi cells (more than one per province &amp; can overlap)\n    voronois_simple = gpd.GeoDataFrame(\n        geometry=province_shapes.simplify(tolerance=simplify_tol).voronoi_polygons(),\n        crs=province_shapes.crs,\n    )\n    # assign region\n    prov_voronoi = (\n        voronois_simple.sjoin(province_shapes, predicate=\"intersects\")\n        .groupby(prov_key)\n        .apply(lambda x: x.union_all(\"unary\"))\n    )\n    prov_voronoi = gpd.GeoDataFrame(\n        geometry=prov_voronoi.values,\n        crs=province_shapes.crs,\n        data={prov_key: prov_voronoi.index},\n    )\n\n    # remove overlaps\n    gdf_ = remove_overlaps(prov_voronoi.set_index(prov_key))\n\n    eez_prov = (\n        gdf_.reset_index()\n        .overlay(eez, how=\"intersection\")[[prov_key, \"geometry\"]]\n        .groupby(prov_key)\n        .apply(lambda x: x.union_all(\"unary\"))\n    )\n    eez_prov = gpd.GeoDataFrame(\n        geometry=eez_prov.values,\n        crs=province_shapes.crs,\n        data={prov_key: eez_prov.index},\n    )\n\n    return eez_prov[eez_prov[prov_key].isin(OFFSHORE_WIND_NODES)].set_index(prov_key)\n</code></pre>"},{"location":"reference/fetch_shapes/#fetch_shapes.fetch_country_shape","title":"<code>fetch_country_shape(outp_path)</code>","text":"<p>fetch the country shape from natural earth and save it to the outpath</p> <p>Parameters:</p> Name Type Description Default <code>outp_path</code> <code>PathLike</code> <p>the path to save the country shape (geojson)</p> required Source code in <code>workflow/scripts/fetch_shapes.py</code> <pre><code>def fetch_country_shape(outp_path: PathLike):\n    \"\"\"fetch the country shape from natural earth and save it to the outpath\n\n    Args:\n        outp_path (PathLike): the path to save the country shape (geojson)\n    \"\"\"\n\n    country_shape = fetch_natural_earth_shape(\"admin_0_countries\", \"ADMIN\", COUNTRY_NAME)\n    country_shape.set_index(\"region\", inplace=True)\n    country_shape.to_file(outp_path, driver=\"GeoJSON\")\n</code></pre>"},{"location":"reference/fetch_shapes/#fetch_shapes.fetch_gadm","title":"<code>fetch_gadm(country_code='CHN', level=2)</code>","text":"<p>fetch GADM shapefile for a given country and administrative level. https://gadm.org/download_country.html</p> <p>Parameters:</p> Name Type Description Default <code>country_code</code> <code>str</code> <p>ISO3 country code (e.g., 'CHN', 'USA').</p> <code>'CHN'</code> <code>level</code> <code>int</code> <p>Administrative level (0=country, 1=region, etc.).</p> <code>2</code> <p>Returns:</p> Type Description <p>geopandas.GeoDataFrame: Loaded shapefile as GeoDataFrame.</p> Source code in <code>workflow/scripts/fetch_shapes.py</code> <pre><code>def fetch_gadm(country_code=\"CHN\", level=2):\n    \"\"\"\n    fetch GADM shapefile for a given country and administrative level.\n    https://gadm.org/download_country.html\n\n    Parameters:\n        country_code (str): ISO3 country code (e.g., 'CHN', 'USA').\n        level (int): Administrative level (0=country, 1=region, etc.).\n\n    Returns:\n        geopandas.GeoDataFrame: Loaded shapefile as GeoDataFrame.\n    \"\"\"\n    # Construct the URL\n    url = f\"https://geodata.ucdavis.edu/gadm/gadm4.1/shp/gadm41_{country_code}_shp.zip\"\n\n    # Download the zip file\n    response = requests.get(url)\n    if response.status_code != 200:\n        raise ValueError(\n            f\"Failed to download data for {country_code} - Status code: {response.status_code}\"\n        )\n\n    # Extract the zip file in memory\n    with zipfile.ZipFile(io.BytesIO(response.content)) as z:\n        # Filter to the desired level shapefile\n        level_filename = f\"gadm41_{country_code}_{level}.shp\"\n        if level_filename not in z.namelist():\n            raise ValueError(f\"Level {level} shapefile not found for {country_code}.\")\n\n        shp_dir = \"resources/data/province_shapes\"\n        z.extractall(shp_dir)\n        gdf = gpd.read_file(f\"{shp_dir}/{level_filename}\")\n        return gdf\n</code></pre>"},{"location":"reference/fetch_shapes/#fetch_shapes.fetch_maritime_eez","title":"<code>fetch_maritime_eez(zone_name)</code>","text":"<p>fetch maritime data for a country from Maritime Gazette API# (Royal marine institute of Flanders data base)</p> <p>Parameters:</p> Name Type Description Default <code>zone_name</code> <code>str</code> <p>the country's zone name, e.g \"Chinese\" for china</p> required <p>Raises:</p> Type Description <code>HTTPError</code> <p>if the request fails</p> <p>Returns:     dict: the maritime data</p> Source code in <code>workflow/scripts/fetch_shapes.py</code> <pre><code>def fetch_maritime_eez(zone_name: str) -&gt; gpd.GeoDataFrame:\n    \"\"\"fetch maritime data for a country from Maritime Gazette API#\n    (Royal marine institute of Flanders data base)\n\n    Args:\n        zone_name (str): the country's zone name, e.g \"Chinese\" for china\n\n    Raises:\n        requests.HTTPError: if the request fails\n    Returns:\n        dict: the maritime data\n    \"\"\"\n\n    def find_record_id(zone_name: str) -&gt; int:\n        # get Maritime Gazette record ID for the country\n        # eez ID is 70: see https://www.marineregions.org/gazetteer.php?p=webservices&amp;type=rest#/\n        url = f\"https://www.marineregions.org/rest/getGazetteerRecordsByName.json/{zone_name}/?like=true&amp;fuzzy=false&amp;typeID=70&amp;offset=0&amp;count=100\"\n        response = requests.get(url)\n        if response.status_code != 200:\n            raise requests.HTTPError(\n                f\"Failed to retrieve Maritime Gazette ID. Status code: {response.status_code}\"\n            )\n        record_data = response.json()\n        logger.debug(record_data)\n        return [\n            data\n            for data in record_data\n            if (data[\"status\"] == \"standard\")\n            and (data[\"preferredGazetteerName\"].lower().find(zone_name.lower()) != -1)\n        ][0][\"MRGID\"]\n\n    mgrid = find_record_id(zone_name)\n    logger.debug(f\"Found Maritime Gazette ID for {zone_name}: {mgrid}\")\n    #  URL of the WFS service\n    url = \"https://geo.vliz.be/geoserver/wfs\"\n    # WFS request parameters + record ID filter\n    base_filter_ = \"&lt;Filter&gt;&lt;PropertyIsEqualTo&gt;&lt;PropertyName&gt;mrgid_eez&lt;/PropertyName&gt;&lt;Literal&gt;\"\n    filter_ = base_filter_ + f\"{mgrid}&lt;/Literal&gt;&lt;/PropertyIsEqualTo&gt;&lt;/Filter&gt;\"\n    params = dict(\n        service=\"WFS\",\n        version=\"1.1.0\",\n        request=\"GetFeature\",\n        typeName=\"MarineRegions:eez\",\n        outputFormat=\"json\",\n        filter=filter_,\n    )\n\n    # Fetch data from WFS using requests\n    response_eez = requests.get(url, params=params)\n\n    # Check for successful request\n    if response_eez.status_code == 200:\n        data = response_eez.json()\n    else:\n        logger.error(f\"Error: {response_eez.status_code}\")\n        raise requests.HTTPError(\n            f\"Failed to retrieve Maritime Gazette data. Status code: {response_eez.status_code}\"\n        )\n    if data[\"totalFeatures\"] != 1:\n        raise ValueError(f\"Expected 1 feature, got {data['totalFeatures']}\\n: {data}\")\n    crs = data[\"crs\"][\"properties\"][\"name\"].split(\"EPSG::\")[-1]\n    eez = gpd.GeoDataFrame.from_features(data[\"features\"])\n    return eez.set_crs(epsg=crs)\n</code></pre>"},{"location":"reference/fetch_shapes/#fetch_shapes.fetch_natural_earth_shape","title":"<code>fetch_natural_earth_shape(dataset_name, filter_key, filter_value='China', region_key=None)</code>","text":"<p>fetch region or country shape from natural earth dataset and filter</p> <p>Parameters:</p> Name Type Description Default <code>dataset_name</code> <code>str</code> <p>the name of the natural earth dataset to fetch</p> required <code>filter_key</code> <code>str</code> <p>key to filter the records by</p> required <code>filter_value</code> <code>str | list</code> <p>filter pass value. Defaults to \"China\".</p> <code>'China'</code> Example <p>china country: build_natural_earth_shape(\"admin_0_countries\", \"ADMIN\", \"China\") china provinces: build_natural_earth_shape(\"admin_1_states_provinces\",      \"iso_a2\", \"CN\", region_key=\"name_en\")</p> <p>Returns:</p> Type Description <code>GeoDataFrame</code> <p>gpd.GeoDataFrame: the filtered records</p> Source code in <code>workflow/scripts/fetch_shapes.py</code> <pre><code>def fetch_natural_earth_shape(\n    dataset_name: str, filter_key: str, filter_value=\"China\", region_key=None\n) -&gt; gpd.GeoDataFrame:\n    \"\"\"fetch region or country shape from natural earth dataset and filter\n\n    Args:\n        dataset_name (str): the name of the natural earth dataset to fetch\n        filter_key (str): key to filter the records by\n        filter_value (str|list, optional): filter pass value. Defaults to \"China\".\n\n    Example:\n        china country: build_natural_earth_shape(\"admin_0_countries\", \"ADMIN\", \"China\")\n        china provinces: build_natural_earth_shape(\"admin_1_states_provinces\", \n            \"iso_a2\", \"CN\", region_key=\"name_en\")\n\n    Returns:\n        gpd.GeoDataFrame: the filtered records\n    \"\"\"\n    shpfilename = shpreader.natural_earth(\n        resolution=NATURAL_EARTH_RESOLUTION, category=\"cultural\", name=dataset_name\n    )\n    reader = shpreader.Reader(shpfilename)\n    records = list(reader.records())\n    if not region_key:\n        region_key = filter_key\n    if isinstance(filter_value, list):\n        gdf = gpd.GeoDataFrame(\n            [\n                {\"region\": c.attributes[region_key], \"geometry\": c.geometry}\n                for c in records\n                if c.attributes[filter_key] in filter_value\n            ]\n        )\n    else:\n        gdf = gpd.GeoDataFrame(\n            [\n                {\"region\": c.attributes[region_key], \"geometry\": c.geometry}\n                for c in records\n                if c.attributes[filter_key] == filter_value\n            ]\n        )\n    gdf.set_crs(epsg=CRS, inplace=True)\n    return gdf\n</code></pre>"},{"location":"reference/fetch_shapes/#fetch_shapes.fetch_prefecture_shapes","title":"<code>fetch_prefecture_shapes(fixes={GDAM_LV1: {'Nei Mongol': 'InnerMongolia', 'Xinjiang Uygur': 'Xinjiang', 'Hong Kong': 'HongKong', 'Ningxia Hui': 'Ningxia'}})</code>","text":"<p>Fetch county-level shapefiles for China.</p> <p>Parameters:</p> Name Type Description Default <code>fixes</code> <code>(dict, Optional)</code> <p>Dictionary mapping old names to new names for specific columns.</p> <code>{GDAM_LV1: {'Nei Mongol': 'InnerMongolia', 'Xinjiang Uygur': 'Xinjiang', 'Hong Kong': 'HongKong', 'Ningxia Hui': 'Ningxia'}}</code> Source code in <code>workflow/scripts/fetch_shapes.py</code> <pre><code>def fetch_prefecture_shapes(\n    fixes={\n        GDAM_LV1: {\n            \"Nei Mongol\": \"InnerMongolia\",\n            \"Xinjiang Uygur\": \"Xinjiang\",\n            \"Hong Kong\": \"HongKong\",\n            \"Ningxia Hui\": \"Ningxia\",\n        }\n    }\n):\n    \"\"\"\n    Fetch county-level shapefiles for China.\n\n    Args:\n        fixes (dict, Optional): Dictionary mapping old names to new names for specific columns.\n    \"\"\"\n    gdf = fetch_gadm(country_code=\"CHN\", level=2)\n    for col, fix_dict in fixes.items():\n        for old_name, new_name in fix_dict.items():\n            mask = gdf.query(f\"{col} == '{old_name}'\").index\n            gdf.loc[mask, col] = new_name\n    return gdf[[\"COUNTRY\", \"NAME_1\", \"NAME_2\", \"geometry\"]]\n</code></pre>"},{"location":"reference/fetch_shapes/#fetch_shapes.fetch_province_shapes","title":"<code>fetch_province_shapes()</code>","text":"<p>fetch the province shapes from natural earth and save it to the outpath</p> <p>Returns:</p> Type Description <code>GeoDataFrame</code> <p>gpd.GeoDataFrame: the province shapes</p> Source code in <code>workflow/scripts/fetch_shapes.py</code> <pre><code>def fetch_province_shapes() -&gt; gpd.GeoDataFrame:\n    \"\"\"fetch the province shapes from natural earth and save it to the outpath\n\n    Returns:\n        gpd.GeoDataFrame: the province shapes\n    \"\"\"\n\n    province_shapes = fetch_natural_earth_shape(\n        \"admin_1_states_provinces\", \"iso_a2\", COUNTRY_ISO, region_key=\"name_en\"\n    )\n    province_shapes.rename(columns={\"region\": \"province\"}, inplace=True)\n    province_shapes.province = province_shapes.province.str.replace(\" \", \"\")\n    province_shapes.sort_values(\"province\", inplace=True)\n    logger.debug(\"province shapes:\\n\", province_shapes)\n\n    filtered = province_shapes[province_shapes[\"province\"].isin(PROV_NAMES)]\n    if (filtered[\"province\"].unique() != sorted(PROV_NAMES)).all():\n        logger.warning(\n            f\"Missing provinces: {set(PROV_NAMES) - set(province_shapes['province'].unique())}\"\n        )\n    filtered.set_index(\"province\", inplace=True)\n\n    return filtered.sort_index()\n</code></pre>"},{"location":"reference/fetch_shapes/#fetch_shapes.has_overlap","title":"<code>has_overlap(gdf)</code>","text":"<p>Check for spatial overlaps across rows</p> <p>Parameters:</p> Name Type Description Default <code>gdf</code> <code>GeoDataFrame</code> <p>the geodataframe to check</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>Index x Index boolean dataframe</p> Source code in <code>workflow/scripts/fetch_shapes.py</code> <pre><code>def has_overlap(gdf: gpd.GeoDataFrame) -&gt; DataFrame:\n    \"\"\"Check for spatial overlaps across rows\n\n    Args:\n        gdf (gpd.GeoDataFrame): the geodataframe to check\n\n    Returns:\n        DataFrame: Index x Index boolean dataframe\n    \"\"\"\n    return gdf.apply(\n        lambda row: gdf[gdf.index != row.name].geometry.apply(\n            lambda geom: row.geometry.intersects(geom)\n        ),\n        axis=1,\n    )\n</code></pre>"},{"location":"reference/fetch_shapes/#fetch_shapes.remove_overlaps","title":"<code>remove_overlaps(gdf)</code>","text":"<p>remove inter row overlaps from a GeoDataFrame, cutting out the smaller region from the larger one</p> <p>Parameters:</p> Name Type Description Default <code>gdf</code> <code>GeoDataFrame</code> <p>the geodataframe to be treated</p> required <p>Returns:</p> Type Description <code>GeoDataFrame</code> <p>gpd.GeoDataFrame: the treated geodataframe</p> Source code in <code>workflow/scripts/fetch_shapes.py</code> <pre><code>def remove_overlaps(gdf: gpd.GeoDataFrame) -&gt; gpd.GeoDataFrame:\n    \"\"\"remove inter row overlaps from a GeoDataFrame, cutting out the smaller region from the larger one\n\n    Args:\n        gdf (gpd.GeoDataFrame): the geodataframe to be treated\n\n    Returns:\n        gpd.GeoDataFrame: the treated geodataframe\n    \"\"\"\n    overlaps = has_overlap(gdf)\n    return gdf.apply(cut_smaller_from_larger, args=(gdf, overlaps), axis=1)\n</code></pre>"},{"location":"reference/fetch_shapes/#fetch_shapes.split_provinces","title":"<code>split_provinces(prefectures, node_config)</code>","text":"<p>Split Inner Mongolia into East and West regions based on prefectures.</p> <p>Parameters:</p> Name Type Description Default <code>prefectures</code> <code>GeoDataFrame</code> <p>Gall chinese prefectures.</p> required <code>node_config</code> <code>dict</code> <p>the configuration for node build</p> required <p>Returns:     gpd.GeoDataFrame: Updated GeoDataFrame with Inner Mongolia split EAST/WEST.</p> Source code in <code>workflow/scripts/fetch_shapes.py</code> <pre><code>def split_provinces(\n    prefectures: gpd.GeoDataFrame,\n    node_config: dict\n) -&gt; gpd.GeoSeries:\n    \"\"\"\n    Split Inner Mongolia into East and West regions based on prefectures.\n\n    Args:\n        prefectures (gpd.GeoDataFrame): Gall chinese prefectures.\n        node_config (dict): the configuration for node build\n    Returns:\n        gpd.GeoDataFrame: Updated GeoDataFrame with Inner Mongolia split EAST/WEST.\n    \"\"\"\n    gdf = prefectures.copy()\n    for admin1, splits in node_config[\"splits\"].items():\n        mask = gdf.query(f\"{GDAM_LV1} == '{admin1}'\").index\n        splits_inv = {vv: admin1 + \"_\" + k for k, v in splits.items() for vv in v}\n        gdf.loc[mask, GDAM_LV1] = gdf.loc[mask, \"NAME_2\"].map(splits_inv)\n\n    # merge geometries by node\n    gdf.rename(columns = {GDAM_LV1: \"node\"}, inplace=True)\n    return gdf[[\"node\", \"geometry\"]].dissolve(by=\"node\", aggfunc=\"sum\")\n</code></pre>"},{"location":"reference/fetch_shapes/#fetch_shapes.validate_split_cfg","title":"<code>validate_split_cfg(split_cfg, gdf)</code>","text":"<p>validate the province split configuration.  The province (admin level 1) is split by admin level 2 {subregion: [prefecture names],..}. The prefecture names must be unique and cover all admin2 in the admin1 level.</p> <p>Parameters:</p> Name Type Description Default <code>split_cfg</code> <code>dict</code> <p>the configuration for the prefecture split</p> required <code>gdf</code> <code>GeoDataFrame</code> <p>the geodataframe with prefecture shapes</p> required <p>Raises:     ValueError: if the prefectures are not unique or do not cover all admin2 in the admin1 level</p> Source code in <code>workflow/scripts/fetch_shapes.py</code> <pre><code>def validate_split_cfg(split_cfg: dict, gdf: gpd.GeoDataFrame):\n    \"\"\"validate the province split configuration. \n    The province (admin level 1) is split by admin level 2 {subregion: [prefecture names],..}.\n    The prefecture names must be unique and cover all admin2 in the admin1 level.\n\n    Args:\n        split_cfg (dict): the configuration for the prefecture split\n        gdf (gpd.GeoDataFrame): the geodataframe with prefecture shapes\n    Raises:\n        ValueError: if the prefectures are not unique or do not cover all admin2 in the admin1 level\n    \"\"\"\n    # validate_settings\n    for admin1 in split_cfg:\n        if admin1 not in gdf[GDAM_LV1].unique():\n            err_ = f\"Invalid admin1 entry {admin1} not found in provinces {gdf[GDAM_LV1].unique()}\"\n            raise ValueError(err_)\n\n        # flatten values\n        admin2 = []\n        for names, v in split_cfg[admin1].items():\n            admin2 += v\n\n        # check completeness\n        all_admin2 = gdf.query(f'{GDAM_LV1} == \"{admin1}\"')[GDAM_LV2].unique().tolist()\n        if not sorted(admin2) == sorted(all_admin2):\n            raise ValueError(\n                f\"{admin1} prefectures do not match expected:\\ngot {admin2}\\nvs\\n {all_admin2}\"\n            )\n\n        # check uniqueness (pop -&gt; must be after completeness check)\n        duplicated = any([admin2.pop() in admin2 for i in range(len(admin2))])\n        if duplicated:\n            raise ValueError(f\"Duplicated prefecture names in {admin1}: {admin2}\")\n</code></pre>"},{"location":"reference/functions/","title":"Functions","text":"<p>Maths calculations used in the PyPSA-China workflow.</p>"},{"location":"reference/functions/#functions.HVAC_cost_curve","title":"<code>HVAC_cost_curve(distance)</code>","text":"<p>Calculate the cost of HVAC lines based on distance. Args:     distance (float): distance in km Returns:     float: cost in currency</p> Source code in <code>workflow/scripts/functions.py</code> <pre><code>def HVAC_cost_curve(distance):\n    \"\"\"Calculate the cost of HVAC lines based on distance.\n    Args:\n        distance (float): distance in km\n    Returns:\n        float: cost in currency\n    \"\"\"\n    d = np.array([608, 656, 730, 780, 903, 920, 1300])\n    c = 1000 / 7.5 * np.array([5.5, 4.71, 5.5, 5.57, 5.5, 5.5, 5.51])\n\n    c_func = interpolate.interp1d(d, c, fill_value=\"extrapolate\")\n    c_results = c_func(distance)\n\n    return c_results\n</code></pre>"},{"location":"reference/functions/#functions.area_from_lon_lat_poly","title":"<code>area_from_lon_lat_poly(geometry)</code>","text":"<p>For shapely geometry in lon-lat coordinates, returns area in km^2.</p> Source code in <code>workflow/scripts/functions.py</code> <pre><code>def area_from_lon_lat_poly(geometry):\n    \"\"\"For shapely geometry in lon-lat coordinates,\n    returns area in km^2.\"\"\"\n\n    project = partial(\n        pyproj.transform,\n        pyproj.Proj(init=\"epsg:4326\"),\n        pyproj.Proj(proj=\"aea\"),  # Source: Lon-Lat\n    )  # Target: Albers Equal Area Conical https://en.wikipedia.org/wiki/Albers_projection\n    # TODO fix\n    new_geometry = transform(project, geometry)\n\n    # default area is in m^2\n    return new_geometry.area / 1e6\n</code></pre>"},{"location":"reference/functions/#functions.cartesian","title":"<code>cartesian(s1, s2)</code>","text":"<p>Compute the Cartesian product of two pandas Series.</p> <p>Parameters:</p> Name Type Description Default <code>s1</code> <code>Series</code> <p>first series</p> required <code>s2</code> <code>Series</code> <p>second series</p> required <p>Returns:     pd.DataFrame: A DataFrame representing the Cartesian product of s1 and s2.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; s1 = pd.Series([1, 2, 3], index=[\"a\", \"b\", \"c\"])\n&gt;&gt;&gt; s2 = pd.Series([4, 5, 6], index=[\"d\", \"e\", \"f\"])\n&gt;&gt;&gt; cartesian(s1, s2)\nd  e   f\na  4  5   6\nb  8 10  12\nc 12 15  18\n</code></pre> Source code in <code>workflow/scripts/functions.py</code> <pre><code>def cartesian(s1: pd.Series, s2: pd.Series) -&gt; pd.DataFrame:\n    \"\"\"\n    Compute the Cartesian product of two pandas Series.\n\n    Args:\n        s1 (pd.Series): first series\n        s2 (pd.Series): second series\n    Returns:\n        pd.DataFrame: A DataFrame representing the Cartesian product of s1 and s2.\n\n    Examples:\n        &gt;&gt;&gt; s1 = pd.Series([1, 2, 3], index=[\"a\", \"b\", \"c\"])\n        &gt;&gt;&gt; s2 = pd.Series([4, 5, 6], index=[\"d\", \"e\", \"f\"])\n        &gt;&gt;&gt; cartesian(s1, s2)\n        d  e   f\n        a  4  5   6\n        b  8 10  12\n        c 12 15  18\n    \"\"\"\n    return pd.DataFrame(np.outer(s1, s2), index=s1.index, columns=s2.index)\n</code></pre>"},{"location":"reference/functions/#functions.haversine","title":"<code>haversine(p1, p2)</code>","text":"<p>Calculate the great circle distance in km between two points on the earth (specified in decimal degrees)</p> <p>Parameters:</p> Name Type Description Default <code>p1</code> <code>Point</code> <p>location 1 in decimal deg</p> required <code>p2</code> <code>Point</code> <p>location 2 in decimal deg</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>great circle distance in [km]</p> Source code in <code>workflow/scripts/functions.py</code> <pre><code>def haversine(p1, p2) -&gt; float:\n    \"\"\"Calculate the great circle distance in km between two points on\n    the earth (specified in decimal degrees)\n\n    Args:\n        p1 (shapely.Point): location 1 in decimal deg\n        p2 (shapely.Point): location 2 in decimal deg\n\n    Returns:\n        float: great circle distance in [km]\n    \"\"\"\n\n    # convert decimal degrees to radians\n    lon1, lat1, lon2, lat2 = map(radians, [p1[0], p1[1], p2[0], p2[1]])\n\n    # haversine formula\n    dlon = lon2 - lon1\n    dlat = lat2 - lat1\n    a = sin(dlat / 2) ** 2 + cos(lat1) * cos(lat2) * sin(dlon / 2) ** 2\n    c = 2 * asin(sqrt(a))\n    r = 6371  # Radius of earth in kilometers. Use 3956 for miles\n    return c * r\n</code></pre>"},{"location":"reference/make_summary/","title":"Make summary","text":"<p>Create summary CSV files for all scenario runs including costs, capacities, capacity factors, curtailment, energy balances, prices and other metrics.</p>"},{"location":"reference/make_summary/#make_summary.assign_carriers","title":"<code>assign_carriers(n)</code>","text":"<p>Assign AC where missing Args:     n (pypsa.Network): the network object to fix</p> Source code in <code>workflow/scripts/make_summary.py</code> <pre><code>def assign_carriers(n: pypsa.Network):\n    \"\"\"Assign AC where missing\n    Args:\n        n (pypsa.Network): the network object to fix\"\"\"\n    if \"carrier\" not in n.lines:\n        n.lines[\"carrier\"] = \"AC\"\n</code></pre>"},{"location":"reference/make_summary/#make_summary.calculate_capacities","title":"<code>calculate_capacities(n, label, capacities, adjust_link_capacities=None)</code>","text":"<p>Calculate the optimal capacities by carrier and bus carrier</p> <p>For links that connect to AC buses (bus1=AC), the capacity can be multiplied by efficiency to report the actual capacity available at the AC side rather than the input side. This ensures consistent capacity reporting across the network.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>Network</code> <p>the network object</p> required <code>label</code> <code>str</code> <p>the label used by make summaries</p> required <code>capacities</code> <code>DataFrame</code> <p>the dataframe to fill/update</p> required <code>adjust_link_capacities</code> <code>bool</code> <p>Whether to adjust link capacities by efficiency. If None, reads from config. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: updated capacities</p> Source code in <code>workflow/scripts/make_summary.py</code> <pre><code>def calculate_capacities(\n    n: pypsa.Network, label: str, capacities: pd.DataFrame, adjust_link_capacities=None\n) -&gt; pd.DataFrame:\n    \"\"\"Calculate the optimal capacities by carrier and bus carrier\n\n    For links that connect to AC buses (bus1=AC), the capacity can be multiplied by efficiency\n    to report the actual capacity available at the AC side rather than the input side.\n    This ensures consistent capacity reporting across the network.\n\n    Args:\n        n (pypsa.Network): the network object\n        label (str): the label used by make summaries\n        capacities (pd.DataFrame): the dataframe to fill/update\n        adjust_link_capacities (bool, optional): Whether to adjust link capacities by efficiency.\n            If None, reads from config. Defaults to None.\n\n    Returns:\n        pd.DataFrame: updated capacities\n    \"\"\"\n\n    # Temporarily save original link capacities\n    original_p_nom_opt = n.links.p_nom_opt.copy()\n\n    # Drop reversed links &amp; report AC capacities for links from X to AC\n    if adjust_link_capacities:\n\n        # For links where bus1 is AC, multiply capacity by efficiency coefficient to get AC side capacity\n        ac_links = n.links[n.links.bus1.map(n.buses.carrier) == \"AC\"].index\n        n.links.loc[ac_links, \"p_nom_opt\"] *= n.links.loc[ac_links, \"efficiency\"]\n\n        # ignore lossy link dummies\n        pseudo_links = n.links.query(\"Link.str.contains('reversed') &amp; capital_cost ==0 \").index\n        n.links.loc[pseudo_links, \"p_nom_opt\"] = 0\n    # Calculate optimal capacity using default grouper\n    caps = n.statistics.optimal_capacity(\n        groupby=pypsa.statistics.get_carrier_and_bus_carrier, nice_names=False\n    )\n\n    # Restore original link capacities to avoid modifying the network object\n    n.links.p_nom_opt = original_p_nom_opt\n\n    if \"load shedding\" in caps.index.get_level_values(1):\n        caps.drop(\"load shedding\", level=1, inplace=True)\n    caps.rename(index={\"AC\": \"Transmission Lines\"}, inplace=True, level=1)\n\n    # track links that feed into AC\n    mask = (n.links.bus1.map(n.buses.carrier) == \"AC\") &amp; (n.links.carrier != \"stations\")\n    to_ac = n.links.loc[mask, \"carrier\"].unique()\n\n    caps_df = caps.reset_index()\n    ac_mask = caps_df[\"carrier\"].isin(to_ac)\n    caps_df.loc[ac_mask, \"end_carrier\"] = \"AC\"\n    caps = caps_df.fillna(\"-\").set_index([\"component\", \"carrier\", \"bus_carrier\", \"end_carrier\"])[0]\n\n    capacities[label] = caps.sort_index(level=0)\n    return capacities\n</code></pre>"},{"location":"reference/make_summary/#make_summary.calculate_cfs","title":"<code>calculate_cfs(n, label, cfs)</code>","text":"<p>Calculate the capacity factors by carrier</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>Network</code> <p>the network object</p> required <code>label</code> <code>str</code> <p>the label used by make summaries</p> required <code>cfs</code> <code>DataFrame</code> <p>the dataframe to fill/update</p> required <p>Returns:     pd.DataFrame: updated cfs</p> Source code in <code>workflow/scripts/make_summary.py</code> <pre><code>def calculate_cfs(n: pypsa.Network, label: str, cfs: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Calculate the capacity factors by carrier\n\n    Args:\n        n (pypsa.Network): the network object\n        label (str): the label used by make summaries\n        cfs (pd.DataFrame): the dataframe to fill/update\n    Returns:\n        pd.DataFrame: updated cfs\n    \"\"\"\n    for c in n.iterate_components(\n        n.branch_components | n.controllable_one_port_components ^ {\"Load\", \"StorageUnit\"}\n    ):\n        capacities_c = c.df[opt_name.get(c.name, \"p\") + \"_nom_opt\"].groupby(c.df.carrier).sum()\n\n        if c.name in [\"Link\", \"Line\", \"Transformer\"]:\n            p = c.pnl.p0.abs().mean()\n        elif c.name == \"Store\":\n            p = c.pnl.e.abs().mean()\n        else:\n            p = c.pnl.p.abs().mean()\n\n        p_c = p.groupby(c.df.carrier).sum()\n        cf_c = p_c / capacities_c\n        cf_c = pd.concat([cf_c], keys=[c.list_name])\n        cfs = cfs.reindex(cf_c.index.union(cfs.index))\n        cfs.loc[cf_c.index, label] = cf_c\n\n    return cfs\n</code></pre>"},{"location":"reference/make_summary/#make_summary.calculate_co2_balance","title":"<code>calculate_co2_balance(n, label, co2_balance, withdrawal_stores=['CO2 capture'])</code>","text":"<p>calc the co2 balance [DOES NOT INCLUDE EMISSION GENERATING LINKSs] Args:     n (pypsa.Network): the network object     withdrawal_stores (list, optional): names of stores. Defaults to [\"CO2 capture\"].     label (str): the label for the column     co2_balance (pd.DataFrame): the df to update</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: updated co2_balance (bad style)</p> Source code in <code>workflow/scripts/make_summary.py</code> <pre><code>def calculate_co2_balance(\n    n: pypsa.Network,\n    label: str,\n    co2_balance: pd.DataFrame,\n    withdrawal_stores=[\"CO2 capture\"],\n) -&gt; pd.DataFrame:\n    \"\"\"calc the co2 balance [DOES NOT INCLUDE EMISSION GENERATING LINKSs]\n    Args:\n        n (pypsa.Network): the network object\n        withdrawal_stores (list, optional): names of stores. Defaults to [\"CO2 capture\"].\n        label (str): the label for the column\n        co2_balance (pd.DataFrame): the df to update\n\n    Returns:\n       pd.DataFrame: updated co2_balance (bad style)\n    \"\"\"\n\n    # year *(assumes one planning year intended),\n    year = int(np.round(n.snapshots.year.values.mean(), 0))\n\n    # emissions from generators (from fneumann course)\n    emissions = (\n        n.generators_t.p\n        / n.generators.efficiency\n        * n.generators.carrier.map(n.carriers.co2_emissions)\n    )  # t/h\n    emissions_carrier = (\n        (n.snapshot_weightings.generators @ emissions).groupby(n.generators.carrier).sum()\n    )\n\n    # format and drop 0 values\n    emissions_carrier = emissions_carrier.where(emissions_carrier &gt; 0).dropna()\n    emissions_carrier.rename(year, inplace=True)\n    emissions_carrier = emissions_carrier.to_frame()\n    # CO2 withdrawal\n    stores = n.stores_t.e.T.groupby(n.stores.carrier).sum()\n    co2_stores = stores.index.intersection(withdrawal_stores)\n    co2_withdrawal = stores.iloc[:, -1].loc[co2_stores] * -1\n    co2_withdrawal.rename(year, inplace=True)\n    co2_withdrawal = co2_withdrawal.to_frame()\n    year_balance = pd.concat([emissions_carrier, co2_withdrawal])\n\n    #  combine with previous\n    co2_balance = co2_balance.reindex(year_balance.index.union(co2_balance.index))\n    co2_balance.loc[year_balance.index, label] = year_balance[year]\n\n    return co2_balance\n</code></pre>"},{"location":"reference/make_summary/#make_summary.calculate_costs","title":"<code>calculate_costs(n, label, costs)</code>","text":"<p>Calculate the costs by carrier Args:     n (pypsa.Network): the network object     label (str): the label used by make summaries     costs (pd.DataFrame): the dataframe to fill/update Returns:     pd.DataFrame: updated costs</p> Source code in <code>workflow/scripts/make_summary.py</code> <pre><code>def calculate_costs(n: pypsa.Network, label: str, costs: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Calculate the costs by carrier\n    Args:\n        n (pypsa.Network): the network object\n        label (str): the label used by make summaries\n        costs (pd.DataFrame): the dataframe to fill/update\n    Returns:\n        pd.DataFrame: updated costs\n    \"\"\"\n\n    for c in n.iterate_components(\n        n.branch_components | n.controllable_one_port_components ^ {\"Load\"}\n    ):\n        capital_costs = c.df.capital_cost * c.df[opt_name.get(c.name, \"p\") + \"_nom_opt\"]\n        capital_costs_grouped = capital_costs.groupby(c.df.carrier).sum()\n\n        capital_costs_grouped = pd.concat([capital_costs_grouped], keys=[\"capital\"])\n        capital_costs_grouped = pd.concat([capital_costs_grouped], keys=[c.list_name])\n\n        costs = costs.reindex(capital_costs_grouped.index.union(costs.index))\n\n        costs.loc[capital_costs_grouped.index, label] = capital_costs_grouped\n\n        if c.name == \"Link\":\n            p = c.pnl.p0.multiply(n.snapshot_weightings.generators, axis=0).sum()\n        elif c.name == \"Line\":\n            continue\n        elif c.name == \"StorageUnit\":\n            p_all = c.pnl.p.multiply(n.snapshot_weightings.generators, axis=0)\n            p_all[p_all &lt; 0.0] = 0.0\n            p = p_all.sum()\n        else:\n            p = c.pnl.p.multiply(n.snapshot_weightings.generators, axis=0).sum()\n\n        # correct sequestration cost\n        if c.name == \"Store\":\n            items = c.df.index[(c.df.carrier == \"co2 stored\") &amp; (c.df.marginal_cost &lt;= -100.0)]\n            c.df.loc[items, \"marginal_cost\"] = -20.0\n\n        marginal_costs = p * c.df.marginal_cost\n\n        marginal_costs_grouped = marginal_costs.groupby(c.df.carrier).sum()\n\n        marginal_costs_grouped = pd.concat([marginal_costs_grouped], keys=[\"marginal\"])\n        marginal_costs_grouped = pd.concat([marginal_costs_grouped], keys=[c.list_name])\n\n        costs = costs.reindex(marginal_costs_grouped.index.union(costs.index))\n\n        costs.loc[marginal_costs_grouped.index, label] = marginal_costs_grouped\n\n    # TODO remove/see if needed, and if yes soft-code\n    # add back in all hydro\n    # costs.loc[(\"storage_units\", \"capital\", \"hydro\"),label] = (0.01)*2e6*n.storage_units.loc[n.storage_units.group==\"hydro\", \"p_nom\"].sum()\n    # costs.loc[(\"storage_units\", \"capital\", \"PHS\"),label] = (0.01)*2e6*n.storage_units.loc[n.storage_units.group==\"PHS\", \"p_nom\"].sum()\n    # costs.loc[(\"generators\", \"capital\", \"ror\"),label] = (0.02)*3e6*n.generators.loc[n.generators.group==\"ror\", \"p_nom\"].sum()\n\n    return costs\n</code></pre>"},{"location":"reference/make_summary/#make_summary.calculate_curtailment","title":"<code>calculate_curtailment(n, label, curtailment)</code>","text":"<p>Calculate curtailed energy by carrier</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>Network</code> <p>the network object</p> required <code>label</code> <code>str</code> <p>the label used by make summaries</p> required <code>curtailment</code> <code>DataFrame</code> <p>the dataframe to fill/update</p> required <p>Returns:     pd.DataFrame: updated curtailment</p> Source code in <code>workflow/scripts/make_summary.py</code> <pre><code>def calculate_curtailment(n: pypsa.Network, label: str, curtailment: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Calculate curtailed energy by carrier\n\n    Args:\n        n (pypsa.Network): the network object\n        label (str): the label used by make summaries\n        curtailment (pd.DataFrame): the dataframe to fill/update\n    Returns:\n        pd.DataFrame: updated curtailment\n    \"\"\"\n    p_avail_by_carr = (\n        n.generators_t.p_max_pu.multiply(n.generators.p_nom_opt)\n        .sum()\n        .groupby(n.generators.carrier)\n        .sum()\n    )\n    used = n.generators_t.p.sum().groupby(n.generators.carrier).sum()\n\n    curtailment[label] = (\n        ((p_avail_by_carr - used).clip(0) / p_avail_by_carr).fillna(0) * 100\n    ).round(3)\n\n    return curtailment\n</code></pre>"},{"location":"reference/make_summary/#make_summary.calculate_expanded_capacities","title":"<code>calculate_expanded_capacities(n, label, capacities)</code>","text":"<p>calculate the capacities by carrier</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>Network</code> <p>the network object</p> required <code>label</code> <code>str</code> <p>the label used by make summaries</p> required <code>capacities</code> <code>DataFrame</code> <p>the dataframe to fill</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.Dataframe: updated capacities (bad style)</p> Source code in <code>workflow/scripts/make_summary.py</code> <pre><code>def calculate_expanded_capacities(\n    n: pypsa.Network, label: str, capacities: pd.DataFrame\n) -&gt; pd.DataFrame:\n    \"\"\"calculate the capacities by carrier\n\n    Args:\n        n (pypsa.Network): the network object\n        label (str): the label used by make summaries\n        capacities (pd.DataFrame): the dataframe to fill\n\n    Returns:\n        pd.Dataframe: updated capacities (bad style)\n    \"\"\"\n    caps = n.statistics.expanded_capacity(\n        groupby=pypsa.statistics.get_carrier_and_bus_carrier, nice_names=False\n    )\n\n    if \"load shedding\" in caps.index.get_level_values(1):\n        caps.drop(\"load shedding\", level=1, inplace=True)\n\n    caps.rename(index={\"AC\": \"Transmission Lines\"}, inplace=True, level=1)\n\n    # track links that feed into AC\n    mask = (n.links.bus1.map(n.buses.carrier) == \"AC\") &amp; (n.links.carrier != \"stations\")\n    to_ac = n.links.loc[mask, \"carrier\"].unique()\n\n    caps_df = caps.reset_index()\n    ac_mask = caps_df[\"carrier\"].isin(to_ac)\n    caps_df.loc[ac_mask, \"end_carrier\"] = \"AC\"\n    caps = caps_df.fillna(\"-\").set_index([\"component\", \"carrier\", \"bus_carrier\", \"end_carrier\"])[0]\n\n    capacities[label] = caps.sort_index(level=0)\n    return capacities\n</code></pre>"},{"location":"reference/make_summary/#make_summary.calculate_market_values","title":"<code>calculate_market_values(n, label, market_values)</code>","text":"<p>Calculate the market value of the generators and links Args:     n (pypsa.Network): the network object     label (str): the label representing the pathway     market_values (pd.DataFrame): the dataframe to write to (not needed, refactor) Returns:     pd.DataFrame: updated market_values</p> Source code in <code>workflow/scripts/make_summary.py</code> <pre><code>def calculate_market_values(\n    n: pypsa.Network, label: str, market_values: pd.DataFrame\n) -&gt; pd.DataFrame:\n    \"\"\"Calculate the market value of the generators and links\n    Args:\n        n (pypsa.Network): the network object\n        label (str): the label representing the pathway\n        market_values (pd.DataFrame): the dataframe to write to (not needed, refactor)\n    Returns:\n        pd.DataFrame: updated market_values\n    \"\"\"\n    # Warning: doesn't include storage units\n\n    carrier = \"AC\"\n\n    buses = n.buses.index[n.buses.carrier == carrier]\n\n    # === First do market value of generators  ===\n    # === First do market value of generators  ===\n\n    generators = n.generators.index[n.buses.loc[n.generators.bus, \"carrier\"] == carrier]\n\n    techs = n.generators.loc[generators, \"carrier\"].value_counts().index\n\n    market_values = market_values.reindex(market_values.index.union(techs))\n\n    for tech in techs:\n        gens = generators[n.generators.loc[generators, \"carrier\"] == tech]\n\n        dispatch = (\n            n.generators_t.p[gens]\n            .groupby(n.generators.loc[gens, \"bus\"], axis=1)\n            .sum()\n            .reindex(columns=buses, fill_value=0.0)\n        )\n\n        revenue = dispatch * n.buses_t.marginal_price[buses]\n\n        market_values.at[tech, label] = revenue.sum().sum() / dispatch.sum().sum()\n\n    # === Now do market value of links  ===\n    # === Now do market value of links  ===\n\n    for i in [\"0\", \"1\"]:\n        carrier_links = n.links[n.links[\"bus\" + i].isin(buses)].index\n\n        techs = n.links.loc[carrier_links, \"carrier\"].value_counts().index\n\n        market_values = market_values.reindex(market_values.index.union(techs))\n\n        for tech in techs:\n            links = carrier_links[n.links.loc[carrier_links, \"carrier\"] == tech]\n\n            dispatch = (\n                n.links_t[\"p\" + i][links]\n                .groupby(n.links.loc[links, \"bus\" + i], axis=1)\n                .sum()\n                .reindex(columns=buses, fill_value=0.0)\n            )\n\n            revenue = dispatch * n.buses_t.marginal_price[buses]\n\n            market_values.at[tech, label] = revenue.sum().sum() / dispatch.sum().sum()\n\n    return market_values\n</code></pre>"},{"location":"reference/make_summary/#make_summary.calculate_metrics","title":"<code>calculate_metrics(n, label, metrics)</code>","text":"<p>LEGACY calculate a set of metrics for lines and co2 Args:     n (pypsa.Network): the network object     label (str): the label to update the table row with     metrics (pd.DataFrame): the dataframe to write to (not needed, refactor) Returns:     pd.DataFrame: updated metrics</p> Source code in <code>workflow/scripts/make_summary.py</code> <pre><code>def calculate_metrics(n: pypsa.Network, label: str, metrics: pd.DataFrame):\n    \"\"\"LEGACY calculate a set of metrics for lines and co2\n    Args:\n        n (pypsa.Network): the network object\n        label (str): the label to update the table row with\n        metrics (pd.DataFrame): the dataframe to write to (not needed, refactor)\n    Returns:\n        pd.DataFrame: updated metrics\"\"\"\n\n    metrics_list = [\n        \"line_volume\",\n        \"line_volume_limit\",\n        \"line_volume_AC\",\n        \"line_volume_DC\",\n        \"line_volume_shadow\",\n        \"co2_shadow\",\n        \"co2_budget\",\n    ]\n\n    metrics = metrics.reindex(pd.Index(metrics_list).union(metrics.index))\n\n    metrics.at[\"line_volume_DC\", label] = (n.links.length * n.links.p_nom_opt)[\n        n.links.carrier == \"DC\"\n    ].sum()\n    metrics.at[\"line_volume_AC\", label] = (n.lines.length * n.lines.s_nom_opt).sum()\n    metrics.at[\"line_volume\", label] = metrics.loc[\n        [\"line_volume_AC\", \"line_volume_DC\"], label\n    ].sum()\n\n    if \"lv_limit\" in n.global_constraints.index:\n        metrics.at[\"line_volume_limit\", label] = n.global_constraints.at[\"lv_limit\", \"constant\"]\n        metrics.at[\"line_volume_shadow\", label] = n.global_constraints.at[\"lv_limit\", \"mu\"]\n\n    if \"co2_limit\" in n.global_constraints.index:\n        metrics.at[\"co2_shadow\", label] = n.global_constraints.at[\"co2_limit\", \"mu\"]\n        metrics.at[\"co2_budget\", label] = n.global_constraints.at[\"co2_limit\", \"constant\"]\n    return metrics\n</code></pre>"},{"location":"reference/make_summary/#make_summary.calculate_nodal_capacities","title":"<code>calculate_nodal_capacities(n, label, nodal_capacities)</code>","text":"<p>Calculate the capacities by carrier and node</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>Network</code> <p>the network object</p> required <code>label</code> <code>str</code> <p>the label used by make summaries</p> required <code>nodal_capacities</code> <code>DataFrame</code> <p>the dataframe to fill/update</p> required <p>Returns:     pd.DataFrame: updated nodal_capacities</p> Source code in <code>workflow/scripts/make_summary.py</code> <pre><code>def calculate_nodal_capacities(\n    n: pypsa.Network, label: str, nodal_capacities: pd.DataFrame\n) -&gt; pd.DataFrame:\n    \"\"\"Calculate the capacities by carrier and node\n\n    Args:\n        n (pypsa.Network): the network object\n        label (str): the label used by make summaries\n        nodal_capacities (pd.DataFrame): the dataframe to fill/update\n    Returns:\n        pd.DataFrame: updated nodal_capacities\"\"\"\n    # Beware this also has extraneous locations for country (e.g. biomass) or continent-wide\n    #  (e.g. fossil gas/oil) stuff\n\n    # Filter out reversed links to avoid double-counting transmission capacity\n    # Only include positive links since positive and reversed links have the same capacity\n    positive_links_mask = n.links.index.str.contains(\"positive\")\n\n    # Create a temporary network with only positive links for capacity calculation\n    n_temp = n.copy()\n    reversed_links = n.links.index[~positive_links_mask]\n    n_temp.links = n_temp.links.drop(reversed_links)\n\n    nodal_cap = n_temp.statistics.optimal_capacity(groupby=pypsa.statistics.get_bus_and_carrier)\n    nodal_capacities[label] = nodal_cap.sort_index(level=0)\n    return nodal_capacities\n</code></pre>"},{"location":"reference/make_summary/#make_summary.calculate_nodal_cfs","title":"<code>calculate_nodal_cfs(n, label, nodal_cfs)</code>","text":"<p>Calculate the capacity factors by for each node and genertor Args:     n (pypsa.Network): the network object     label (str): the label used by make summaries     nodal_cfs (pd.DataFrame): the cap fac dataframe to fill/update Returns:     pd.DataFrame: updated nodal_cfs</p> Source code in <code>workflow/scripts/make_summary.py</code> <pre><code>def calculate_nodal_cfs(n: pypsa.Network, label: str, nodal_cfs: pd.DataFrame):\n    \"\"\"Calculate the capacity factors by for each node and genertor\n    Args:\n        n (pypsa.Network): the network object\n        label (str): the label used by make summaries\n        nodal_cfs (pd.DataFrame): the cap fac dataframe to fill/update\n    Returns:\n        pd.DataFrame: updated nodal_cfs\n    \"\"\"\n    # Beware this also has extraneous locations for country (e.g. biomass)\n    # or continent-wide (e.g. fossil gas/oil) stuff\n    for c in n.iterate_components(\n        (n.branch_components ^ {\"Line\", \"Transformer\"})\n        | n.controllable_one_port_components ^ {\"Load\", \"StorageUnit\"}\n    ):\n        capacities_c = c.df.groupby([\"location\", \"carrier\"])[\n            opt_name.get(c.name, \"p\") + \"_nom_opt\"\n        ].sum()\n\n        if c.name == \"Link\":\n            p = c.pnl.p0.abs().mean()\n        elif c.name == \"Generator\":\n            p = c.pnl.p.abs().mean()\n        elif c.name == \"Store\":\n            p = c.pnl.e.abs().mean()\n        else:\n            sys.exit()\n\n        c.df[\"p\"] = p\n        p_c = c.df.groupby([\"location\", \"carrier\"])[\"p\"].sum()\n        cf_c = p_c / capacities_c\n\n        index = pd.MultiIndex.from_tuples([(c.list_name,) + t for t in cf_c.index.to_list()])\n        nodal_cfs = nodal_cfs.reindex(index.union(nodal_cfs.index))\n        nodal_cfs.loc[index, label] = cf_c.values\n\n    return nodal_cfs\n</code></pre>"},{"location":"reference/make_summary/#make_summary.calculate_nodal_costs","title":"<code>calculate_nodal_costs(n, label, nodal_costs)</code>","text":"<p>Calculate the costs by carrier and location Args:     n (pypsa.Network): the network object     label (str): the label used by make summaries     nodal_costs (pd.DataFrame): the dataframe to fill/update Returns:     pd.DataFrame: updated nodal_costs</p> Source code in <code>workflow/scripts/make_summary.py</code> <pre><code>def calculate_nodal_costs(n: pypsa.Network, label: str, nodal_costs: pd.DataFrame):\n    \"\"\"Calculate the costs by carrier and location\n    Args:\n        n (pypsa.Network): the network object\n        label (str): the label used by make summaries\n        nodal_costs (pd.DataFrame): the dataframe to fill/update\n    Returns:\n        pd.DataFrame: updated nodal_costs\n    \"\"\"\n    # Beware this also has extraneous locations for country (e.g. biomass)\n    #  or continent-wide (e.g. fossil gas/oil) stuff\n    for c in n.iterate_components(\n        n.branch_components | n.controllable_one_port_components ^ {\"Load\"}\n    ):\n        c.df[\"capital_costs\"] = c.df.capital_cost * c.df[opt_name.get(c.name, \"p\") + \"_nom_opt\"]\n        capital_costs = c.df.groupby([\"location\", \"carrier\"])[\"capital_costs\"].sum()\n        index = pd.MultiIndex.from_tuples(\n            [(c.list_name, \"capital\") + t for t in capital_costs.index.to_list()]\n        )\n        nodal_costs = nodal_costs.reindex(index.union(nodal_costs.index))\n        nodal_costs.loc[index, label] = capital_costs.values\n\n        if c.name == \"Link\":\n            p = c.pnl.p0.multiply(n.snapshot_weightings.generators, axis=0).sum()\n        elif c.name == \"Line\":\n            continue\n        elif c.name == \"StorageUnit\":\n            p_all = c.pnl.p.multiply(n.snapshot_weightings.generators, axis=0)\n            p_all[p_all &lt; 0.0] = 0.0\n            p = p_all.sum()\n        else:\n            p = c.pnl.p.multiply(n.snapshot_weightings.generators, axis=0).sum()\n\n        # correct sequestration cost\n        if c.name == \"Store\":\n            items = c.df.index[(c.df.carrier == \"co2 stored\") &amp; (c.df.marginal_cost &lt;= -100.0)]\n            c.df.loc[items, \"marginal_cost\"] = -20.0\n\n        c.df[\"marginal_costs\"] = p * c.df.marginal_cost\n        marginal_costs = c.df.groupby([\"location\", \"carrier\"])[\"marginal_costs\"].sum()\n        index = pd.MultiIndex.from_tuples(\n            [(c.list_name, \"marginal\") + t for t in marginal_costs.index.to_list()]\n        )\n        nodal_costs = nodal_costs.reindex(index.union(nodal_costs.index))\n        nodal_costs.loc[index, label] = marginal_costs.values\n\n    return nodal_costs\n</code></pre>"},{"location":"reference/make_summary/#make_summary.calculate_nodal_lcoe","title":"<code>calculate_nodal_lcoe(n, label, nodal_lcoe)</code>","text":"<p>Calculate LCOE by province and technology</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>Network</code> <p>the network object</p> required <code>label</code> <code>str</code> <p>the label used by make summaries</p> required <code>nodal_lcoe</code> <code>DataFrame</code> <p>the dataframe to fill/update</p> required <p>Returns:     pd.DataFrame: updated nodal_lcoe</p> Source code in <code>workflow/scripts/make_summary.py</code> <pre><code>def calculate_nodal_lcoe(n: pypsa.Network, label: str, nodal_lcoe: pd.DataFrame):\n    \"\"\"Calculate LCOE by province and technology\n\n    Args:\n        n (pypsa.Network): the network object\n        label (str): the label used by make summaries\n        nodal_lcoe (pd.DataFrame): the dataframe to fill/update\n    Returns:\n        pd.DataFrame: updated nodal_lcoe\n    \"\"\"\n\n    lcoe_data = calc_lcoe(n, groupby=[\"location\", \"carrier\"])\n\n    lcoe_series = lcoe_data[\"LCOE\"]\n\n    nodal_lcoe[label] = lcoe_series\n\n    return nodal_lcoe\n</code></pre>"},{"location":"reference/make_summary/#make_summary.calculate_peak_dispatch","title":"<code>calculate_peak_dispatch(n, label, supply)</code>","text":"<p>Calculate the MAX dispatch of each component at the buses aggregated by carrier.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>Network</code> <p>the network object</p> required <code>label</code> <code>str</code> <p>the labe representing the pathway</p> required <code>supply</code> <code>DataFrame</code> <p>supply energy balance (empty df)</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: updated supply DF</p> Source code in <code>workflow/scripts/make_summary.py</code> <pre><code>def calculate_peak_dispatch(n: pypsa.Network, label: str, supply: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Calculate the MAX dispatch of each component at the buses aggregated by\n    carrier.\n\n    Args:\n        n (pypsa.Network): the network object\n        label (str): the labe representing the pathway\n        supply (pd.DataFrame): supply energy balance (empty df)\n\n    Returns:\n        pd.DataFrame: updated supply DF\n    \"\"\"\n\n    sup_ = n.statistics.supply(\n        groupby=pypsa.statistics.get_carrier_and_bus_carrier, aggregate_time=\"max\"\n    )\n    supply_reordered = sup_.reorder_levels([2, 0, 1])\n    supply_reordered.sort_index(inplace=True)\n    supply[label] = supply_reordered\n\n    return supply\n</code></pre>"},{"location":"reference/make_summary/#make_summary.calculate_supply_energy","title":"<code>calculate_supply_energy(n, label, supply_energy)</code>","text":"<p>Calculate the total energy supply/consuption of each component at the buses aggregated by carrier.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>Network</code> <p>the network object</p> required <code>label</code> <code>str</code> <p>the labe representing the pathway</p> required <code>supply_energy</code> <code>DataFrame</code> <p>supply energy balance (empty df)</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: updated supply energy balance</p> Source code in <code>workflow/scripts/make_summary.py</code> <pre><code>def calculate_supply_energy(\n    n: pypsa.Network, label: str, supply_energy: pd.DataFrame\n) -&gt; pd.DataFrame:\n    \"\"\"Calculate the total energy supply/consuption of each component at the buses\n    aggregated by carrier.\n\n    Args:\n        n (pypsa.Network): the network object\n        label (str): the labe representing the pathway\n        supply_energy (pd.DataFrame): supply energy balance (empty df)\n\n    Returns:\n        pd.DataFrame: updated supply energy balance\n    \"\"\"\n\n    eb = n.statistics.energy_balance(groupby=pypsa.statistics.get_carrier_and_bus_carrier)\n    # fragile\n    eb_reordered = eb.reorder_levels([2, 0, 1])\n    eb_reordered.sort_index(inplace=True)\n    eb_reordered.rename(index={\"AC\": \"transmission losses\"}, level=2, inplace=True)\n\n    supply_energy[label] = eb_reordered\n\n    return supply_energy\n</code></pre>"},{"location":"reference/make_summary/#make_summary.calculate_t_avgd_prices","title":"<code>calculate_t_avgd_prices(n, label, prices)</code>","text":"<p>Time averaged prices for nodes averaged over carrier (bit silly?)</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>Network</code> <p>the network object</p> required <code>label</code> <code>str</code> <p>the label representing the pathway (not needed, refactor)</p> required <code>prices</code> <code>DataFrame</code> <p>the dataframe to write to (not needed, refactor)</p> required <p>Returns:     pd.DataFrame: updated prices</p> Source code in <code>workflow/scripts/make_summary.py</code> <pre><code>def calculate_t_avgd_prices(n: pypsa.Network, label: str, prices: pd.DataFrame):\n    \"\"\"Time averaged prices for nodes averaged over carrier (bit silly?)\n\n    Args:\n        n (pypsa.Network): the network object\n        label (str): the label representing the pathway (not needed, refactor)\n        prices (pd.DataFrame): the dataframe to write to (not needed, refactor)\n    Returns:\n        pd.DataFrame: updated prices\n    \"\"\"\n    prices = prices.reindex(prices.index.union(n.buses.carrier.unique()))\n\n    # WARNING: this is time-averaged, see weighted_prices for load-weighted average\n    prices[label] = n.buses_t.marginal_price.mean().groupby(n.buses.carrier).mean()\n\n    return prices\n</code></pre>"},{"location":"reference/make_summary/#make_summary.calculate_weighted_prices","title":"<code>calculate_weighted_prices(n, label, weighted_prices)</code>","text":"<p>Demand-weighed prices for stores and loads.     For stores if withdrawal is zero, use supply instead. Args:     n (pypsa.Network): the network object     label (str): the label representing the pathway (not needed, refactor)     weighted_prices (pd.DataFrame): the dataframe to write to (not needed, refactor)</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: updated weighted_prices</p> Source code in <code>workflow/scripts/make_summary.py</code> <pre><code>def calculate_weighted_prices(\n    n: pypsa.Network, label: str, weighted_prices: pd.DataFrame\n) -&gt; pd.DataFrame:\n    \"\"\"Demand-weighed prices for stores and loads.\n        For stores if withdrawal is zero, use supply instead.\n    Args:\n        n (pypsa.Network): the network object\n        label (str): the label representing the pathway (not needed, refactor)\n        weighted_prices (pd.DataFrame): the dataframe to write to (not needed, refactor)\n\n    Returns:\n        pd.DataFrame: updated weighted_prices\n    \"\"\"\n    entries = pd.Index([\"electricity\", \"heat\", \"H2\", \"CO2 capture\", \"gas\", \"biomass\"])\n    weighted_prices = weighted_prices.reindex(entries)\n\n    # loads\n    load_rev = -1 * n.statistics.revenue(comps=\"Load\", groupby=pypsa.statistics.get_bus_carrier)\n    prices = load_rev / n.statistics.withdrawal(\n        comps=\"Load\", groupby=pypsa.statistics.get_bus_carrier\n    )\n    prices.rename(index={\"AC\": \"electricity\"}, inplace=True)\n\n    # stores\n    w = n.statistics.withdrawal(comps=\"Store\")\n    # biomass stores have no withdrawal for some reason\n    if not w[w == 0].empty:\n        w[w == 0] = n.statistics.supply(comps=\"Store\")[w == 0]\n\n    store_rev = n.statistics.revenue(comps=\"Store\")\n    mask = store_rev &gt; load_rev.sum() / 400  # remove small\n    wp_stores = store_rev[mask] / w[mask]\n    weighted_prices[label] = pd.concat([prices, wp_stores.rename({\"stations\": \"reservoir inflow\"})])\n    return weighted_prices\n</code></pre>"},{"location":"reference/make_summary/#make_summary.expand_from_wildcard","title":"<code>expand_from_wildcard(key, config)</code>","text":"<p>return a list of values for the given key in the config file Args:     key (str): the key to look for in the config file     config (dict): the config file Returns:     list: a list of values for the given key</p> Source code in <code>workflow/scripts/make_summary.py</code> <pre><code>def expand_from_wildcard(key, config) -&gt; list:\n    \"\"\"return a list of values for the given key in the config file\n    Args:\n        key (str): the key to look for in the config file\n        config (dict): the config file\n    Returns:\n        list: a list of values for the given key\n    \"\"\"\n    w = getattr(wildcards, key)\n    return config[\"scenario\"][key] if w == \"all\" else [w]\n</code></pre>"},{"location":"reference/make_summary/#make_summary.make_summaries","title":"<code>make_summaries(networks_dict, opts=None)</code>","text":"<p>Make summary tables for the given network Args:     networks_dict (dict): a dictionary of (pathway, time):network_path used in the run     opts (dict): options for each summary function Returns:     dict: a dictionary of dataframes with the summary tables</p> Source code in <code>workflow/scripts/make_summary.py</code> <pre><code>def make_summaries(\n    networks_dict: dict[tuple, os.PathLike], opts: dict = None\n) -&gt; dict[str, pd.DataFrame]:\n    \"\"\"Make summary tables for the given network\n    Args:\n        networks_dict (dict): a dictionary of (pathway, time):network_path used in the run\n        opts (dict): options for each summary function\n    Returns:\n        dict: a dictionary of dataframes with the summary tables\n\n    \"\"\"\n    if opts is None:\n        opts = {}\n\n    output_funcs = {\n        \"nodal_costs\": calculate_nodal_costs,\n        \"nodal_capacities\": calculate_nodal_capacities,\n        \"nodal_cfs\": calculate_nodal_cfs,\n        \"nodal_lcoe\": calculate_nodal_lcoe,\n        \"cfs\": calculate_cfs,\n        \"costs\": calculate_costs,\n        \"co2_balance\": calculate_co2_balance,\n        \"capacities\": calculate_capacities,\n        \"capacities_expanded\": calculate_expanded_capacities,\n        \"curtailment_pc\": calculate_curtailment,\n        \"peak_dispatch\": calculate_peak_dispatch,\n        # \"energy\": calculate_energy,\n        \"supply_energy\": calculate_supply_energy,\n        \"time_averaged_prices\": calculate_t_avgd_prices,\n        \"weighted_prices\": calculate_weighted_prices,\n        # \"price_statistics\": calculate_price_statistics,\n        \"market_values\": calculate_market_values,\n        \"metrics\": calculate_metrics,\n    }\n\n    columns = pd.MultiIndex.from_tuples(\n        networks_dict.keys(), names=[\"co2_pathway\", \"planning_horizons\"]\n    )\n    dataframes_dict = {}\n\n    # TO DO: not needed, could be made by the functions\n    for output in output_funcs.keys():\n        dataframes_dict[output] = pd.DataFrame(columns=columns, dtype=float)\n\n    for label, filename in networks_dict.items():\n        logger.info(f\"Make summary for scenario {label}, using {filename}\")\n\n        n = pypsa.Network(filename)\n        assign_carriers(n)\n        assign_locations(n)\n\n        for output, output_fn in output_funcs.items():\n            if output in opts:\n                dataframes_dict[output] = output_fn(\n                    n, label, dataframes_dict[output], **opts[output]\n                )\n            else:\n                dataframes_dict[output] = output_fn(n, label, dataframes_dict[output])\n\n    return dataframes_dict\n</code></pre>"},{"location":"reference/plot_heatmap/","title":"Plot heatmap","text":""},{"location":"reference/plot_heatmap/#plot_heatmap.set_plot_style","title":"<code>set_plot_style()</code>","text":"<p>apply plotting style to all figures</p> Source code in <code>workflow/scripts/plot_heatmap.py</code> <pre><code>def set_plot_style():\n    \"\"\"apply plotting style to all figures\"\"\"\n    plt.style.use(\n        [\n            \"classic\",\n            \"seaborn-v0_8-white\",\n            {\n                \"axes.grid\": False,\n                \"grid.linestyle\": \"--\",\n                \"grid.color\": \"0.6\",\n                \"hatch.color\": \"white\",\n                \"patch.linewidth\": 0.5,\n                \"font.size\": 12,\n                \"legend.fontsize\": \"medium\",\n                \"lines.linewidth\": 1.5,\n                \"pdf.fonttype\": 42,\n            },\n        ]\n    )\n</code></pre>"},{"location":"reference/plot_input_costs/","title":"Plot input costs","text":""},{"location":"reference/plot_input_costs/#plot_input_costs.apply_conversion","title":"<code>apply_conversion(df, target_unit_installation, target_unit_storage)</code>","text":"<p>Apply unit conversion to each row in the DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input DataFrame containing cost data</p> required <code>target_unit_installation</code> <code>str</code> <p>Target unit for installation costs (e.g. \"eur/kW\")</p> required <code>target_unit_storage</code> <code>str</code> <p>Target unit for storage costs (e.g. \"eur/kWh\")</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: DataFrame with converted units - Preserves original unit in 'original_unit' column - Updates 'unit' column with new unit - Converts all numeric values to target units</p> Source code in <code>workflow/scripts/plot_input_costs.py</code> <pre><code>def apply_conversion(df: pd.DataFrame, \n                    target_unit_installation: str, \n                    target_unit_storage: str) -&gt; pd.DataFrame:\n    \"\"\"Apply unit conversion to each row in the DataFrame.\n\n    Args:\n        df (pd.DataFrame): Input DataFrame containing cost data\n        target_unit_installation (str): Target unit for installation costs (e.g. \"eur/kW\")\n        target_unit_storage (str): Target unit for storage costs (e.g. \"eur/kWh\")\n\n    Returns:\n        pd.DataFrame: DataFrame with converted units\n            - Preserves original unit in 'original_unit' column\n            - Updates 'unit' column with new unit\n            - Converts all numeric values to target units\n    \"\"\"\n    # Ensure essential columns exist\n    for col in [\"technology\", \"reference\", \"unit\"]:\n        if col not in df.columns:\n            df[col] = f\"Default_{col}\"\n\n    # Keep the original unit in a separate column if not present\n    if \"original_unit\" not in df.columns:\n        df[\"original_unit\"] = df[\"unit\"].copy()\n\n    # Convert row by row\n    return df.apply(\n        convert_row_units,\n        axis=1,\n        args=(target_unit_installation, target_unit_storage)\n    )\n</code></pre>"},{"location":"reference/plot_input_costs/#plot_input_costs.convert_row_units","title":"<code>convert_row_units(row, target_unit_installation, target_unit_storage)</code>","text":"<p>Convert row's numeric columns to target units.</p> <p>Parameters:</p> Name Type Description Default <code>row</code> <code>Series</code> <p>Input row containing cost data</p> required <code>target_unit_installation</code> <code>str</code> <p>Target unit for installation costs (e.g. \"eur/kW\")</p> required <code>target_unit_storage</code> <code>str</code> <p>Target unit for storage costs (e.g. \"eur/kWh\")</p> required <p>Returns:</p> Type Description <code>Series</code> <p>pd.Series: Row with converted values</p> Source code in <code>workflow/scripts/plot_input_costs.py</code> <pre><code>def convert_row_units(row: pd.Series, \n                     target_unit_installation: str,\n                     target_unit_storage: str) -&gt; pd.Series:\n    \"\"\"Convert row's numeric columns to target units.\n\n    Args:\n        row (pd.Series): Input row containing cost data\n        target_unit_installation (str): Target unit for installation costs (e.g. \"eur/kW\")\n        target_unit_storage (str): Target unit for storage costs (e.g. \"eur/kWh\")\n\n    Returns:\n        pd.Series: Row with converted values\n    \"\"\"\n    if \"unit\" not in row or pd.isna(row[\"unit\"]):\n        return row\n\n    # Parse unit string\n    currency_part, capacity_part = parse_unit_string(row[\"unit\"])\n    if not currency_part or not capacity_part:\n        return row\n\n    # Determine if storage unit and get target unit\n    is_storage = is_storage_unit(capacity_part)\n    target_unit = target_unit_storage if is_storage else target_unit_installation\n    tgt_currency, tgt_capacity = parse_unit_string(target_unit)\n\n    # Get currency conversion factor\n    orig_currency = CURRENCY_ALIASES.get(currency_part.lower(), \"unknown\")\n    currency_factor = EXCHANGE_RATES.get((orig_currency, tgt_currency), 1.0)\n\n    # Get capacity conversion factor\n    norm_capacity = normalize_capacity_unit(capacity_part)\n    norm_target = normalize_capacity_unit(tgt_capacity)\n\n    if norm_capacity in [\"kw\", \"mw\", \"kwh\", \"mwh\"] and norm_target in [\"kw\", \"mw\", \"kwh\", \"mwh\"]:\n        cap_factor = get_capacity_factor(norm_capacity, norm_target)\n        new_capacity_part = norm_target\n    else:\n        cap_factor = 1.0\n        new_capacity_part = capacity_part\n\n    # Calculate total conversion factor\n    factor = currency_factor * cap_factor\n\n    # Convert numeric columns\n    numeric_cols = get_numeric_columns(row)\n    for col in numeric_cols:\n        val = row[col]\n        if pd.notna(val):\n            try:\n                row[col] = float(val) * factor\n            except ValueError:\n                pass\n\n    # Update unit string\n    row[\"unit\"] = f\"{tgt_currency.upper()}/{new_capacity_part}\"\n\n    return row\n</code></pre>"},{"location":"reference/plot_input_costs/#plot_input_costs.detect_file_encoding","title":"<code>detect_file_encoding(file_path)</code>","text":"<p>Detect the encoding of a file.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>Path to the file whose encoding needs to be detected.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The detected encoding of the file.</p> Source code in <code>workflow/scripts/plot_input_costs.py</code> <pre><code>def detect_file_encoding(file_path: str) -&gt; str:\n    \"\"\"Detect the encoding of a file.\n\n    Args:\n        file_path: Path to the file whose encoding needs to be detected.\n\n    Returns:\n        str: The detected encoding of the file.\n    \"\"\"\n    with open(file_path, 'rb') as f:\n        result = chardet.detect(f.read())\n    encoding = result['encoding']\n    return encoding\n</code></pre>"},{"location":"reference/plot_input_costs/#plot_input_costs.filter_investment_parameter","title":"<code>filter_investment_parameter(df)</code>","text":"<p>Filter DataFrame to keep only rows where parameter is 'investment'.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input DataFrame containing cost data with a 'parameter' column.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: Filtered DataFrame containing only investment parameter rows. If 'parameter' column does not exist, returns the original DataFrame unchanged.</p> Source code in <code>workflow/scripts/plot_input_costs.py</code> <pre><code>def filter_investment_parameter(df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Filter DataFrame to keep only rows where parameter is 'investment'.\n\n    Args:\n        df (pd.DataFrame): Input DataFrame containing cost data with a 'parameter' column.\n\n    Returns:\n        pd.DataFrame: Filtered DataFrame containing only investment parameter rows.\n            If 'parameter' column does not exist, returns the original DataFrame unchanged.\n    \"\"\"\n    if \"parameter\" not in df.columns:\n        logging.warning(\"Warning: 'parameter' column not found.\")\n        return df\n    # Filter for rows that have parameter == \"investment\"\n    mask = df[\"parameter\"].str.lower() == \"investment\"\n    df_investment = df[mask].copy()\n    return df_investment\n</code></pre>"},{"location":"reference/plot_input_costs/#plot_input_costs.filter_technologies_by_config","title":"<code>filter_technologies_by_config(df, techs_dict)</code>","text":"<p>Filter and categorize technologies based on the provided technology dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input DataFrame containing technology data</p> required <code>techs_dict</code> <code>Dict[str, List[str]]</code> <p>Dictionary containing technology lists with keys: - 'vre_techs': List of variable renewable energy technologies - 'conv_techs': List of conventional technologies - 'store_techs': List of storage technologies</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: Filtered DataFrame with mapped technologies and categories - 'mapped_technology': Standardized technology names - 'category': Technology category (VRE, Conventional, Storage, Solar Thermal)</p> Source code in <code>workflow/scripts/plot_input_costs.py</code> <pre><code>def filter_technologies_by_config(df: pd.DataFrame, \n                                techs_dict: Dict[str, List[str]]) -&gt; pd.DataFrame:\n    \"\"\"Filter and categorize technologies based on the provided technology dictionary.\n\n    Args:\n        df (pd.DataFrame): Input DataFrame containing technology data\n        techs_dict (Dict[str, List[str]]): Dictionary containing technology lists with keys:\n            - 'vre_techs': List of variable renewable energy technologies\n            - 'conv_techs': List of conventional technologies\n            - 'store_techs': List of storage technologies\n\n    Returns:\n        pd.DataFrame: Filtered DataFrame with mapped technologies and categories\n            - 'mapped_technology': Standardized technology names\n            - 'category': Technology category (VRE, Conventional, Storage, Solar Thermal)\n    \"\"\"\n    try:\n        # Validate input DataFrame\n        if df.empty:\n            logging.warning(\"Warning: Input DataFrame is empty\")\n            return pd.DataFrame()\n\n        if \"technology\" not in df.columns:\n            logging.error(\"Error: 'technology' column not found in DataFrame\")\n            return pd.DataFrame()\n\n        # Example aliases\n        alias_map: Dict[str, List[str]] = {\n            \"solar thermal\": [\"central solar thermal\", \"decentral solar thermal\"],\n            \"hydroelectricity\": [\"hydro\"],\n            \"heat pump\": [\"central air heat pump\", \"decentral air heat pump\"],\n            \"resistive heater\": [\"central resistive heater\", \"decentral resistive heater\"],\n            \"Sabatier\": [\"methanation\"],\n            \"H2 CHP\": [\"central hydrogen CHP\"],\n            \"OCGT gas\": [\"OCGT\"],\n            \"CHP gas\": [\"central gas CHP\", \"decentral CHP\"],\n            \"gas boiler\": [\"central gas boiler\", \"decentral gas boiler\"],\n            \"coal boiler\": [\"central coal boiler\", \"decentral coal boiler\"],\n            \"coal power plant\": [\"coal\"],\n            \"CHP coal\": [\"central coal CHP\"],\n            \"H2\": [\"H2 pipeline\", \"hydrogen storage tank type 1\"],\n            \"battery\": [\"battery storage\"],\n            \"water tanks\": [\"central water tank storage\", \"decentral water tank storage\"]\n        }\n\n        # Reverse alias map for easier lookup\n        tech_aliases: Dict[str, str] = {}\n        for main_tech, aliases in alias_map.items():\n            for alias in aliases:\n                tech_aliases[alias.lower()] = main_tech\n\n        # get all techs\n        vre_techs = techs_dict.get(\"vre_techs\", [])\n        conv_techs = techs_dict.get(\"conv_techs\", [])\n        store_techs = techs_dict.get(\"store_techs\", [])\n        solar_thermal = [\"solar thermal\"]  # special case for solar thermal\n\n        # combine all techs\n        all_techs = vre_techs + conv_techs + store_techs + solar_thermal\n\n        # create tech to category mapping\n        tech_categories: Dict[str, str] = {}\n        for tech in vre_techs:\n            if tech != \"solar thermal\":  # exclude solar thermal\n                tech_categories[tech] = \"VRE Technologies\"\n        for tech in conv_techs:\n            tech_categories[tech] = \"Conventional Technologies\"\n        for tech in store_techs:\n            tech_categories[tech] = \"Storage Technologies\"\n        tech_categories[\"solar thermal\"] = \"Solar Thermal\"\n\n        # Create a copy of the DataFrame\n        df_filtered = df.copy()\n\n        # Direct mapping\n        df_filtered[\"mapped_technology\"] = df_filtered[\"technology\"].where(\n            df_filtered[\"technology\"].isin(all_techs)\n        )\n\n        # Alias mapping\n        df_filtered[\"mapped_technology\"].fillna(\n            df_filtered[\"technology\"].str.lower().map(tech_aliases),\n            inplace=True\n        )\n\n        # Log unmapped technologies\n        unmapped_techs = df_filtered[df_filtered[\"mapped_technology\"].isna()][\"technology\"].unique()\n        if len(unmapped_techs) &gt; 0:\n            logging.warning(\n                f\"Warning: The following technologies could not be mapped: {unmapped_techs}\"\n            )\n\n        # Drop rows where no match was found\n        df_filtered = df_filtered.dropna(subset=[\"mapped_technology\"])\n\n        if df_filtered.empty:\n            logging.warning(\"Warning: No matching technologies found!\")\n            return pd.DataFrame()\n\n        # Add category based on mapped technology\n        df_filtered[\"category\"] = df_filtered[\"mapped_technology\"].map(tech_categories)\n\n        return df_filtered\n\n    except KeyError as e:\n        logging.error(f\"Error: Missing key in technology dictionary - {str(e)}\")\n        return pd.DataFrame()\n    except AttributeError as e:\n        logging.error(f\"Error: Invalid DataFrame structure - {str(e)}\")\n        return pd.DataFrame()\n    except Exception as e:\n        logging.error(f\"Unexpected error in technology filtering: {str(e)}\")\n        return pd.DataFrame()\n</code></pre>"},{"location":"reference/plot_input_costs/#plot_input_costs.get_capacity_factor","title":"<code>get_capacity_factor(orig_cap, target_cap)</code>","text":"<p>Calculate the capacity conversion factor.</p> <p>Parameters:</p> Name Type Description Default <code>orig_cap</code> <code>str</code> <p>Original capacity unit</p> required <code>target_cap</code> <code>str</code> <p>Target capacity unit</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>Conversion factor between capacity units</p> Source code in <code>workflow/scripts/plot_input_costs.py</code> <pre><code>def get_capacity_factor(orig_cap: str, target_cap: str) -&gt; float:\n    \"\"\"Calculate the capacity conversion factor.\n\n    Args:\n        orig_cap (str): Original capacity unit\n        target_cap (str): Target capacity unit\n\n    Returns:\n        float: Conversion factor between capacity units\n    \"\"\"\n    capacity_factors: Dict[Tuple[str, str], float] = {\n        (\"kw\", \"kw\"): 1.0,\n        (\"mw\", \"kw\"): 1/1000.0,\n        (\"kwh\", \"kwh\"): 1.0,\n        (\"mwh\", \"kwh\"): 1/1000.0,\n        (\"kw\", \"mw\"): 1000.0,\n        (\"mw\", \"mw\"): 1.0,\n        (\"kwh\", \"mwh\"): 1000.0,\n        (\"mwh\", \"mwh\"): 1.0\n    }\n    return capacity_factors.get((orig_cap.lower(), target_cap.lower()), 1.0)\n</code></pre>"},{"location":"reference/plot_input_costs/#plot_input_costs.get_numeric_columns","title":"<code>get_numeric_columns(row)</code>","text":"<p>Get list of numeric columns in the row.</p> <p>Parameters:</p> Name Type Description Default <code>row</code> <code>Series</code> <p>Input row</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: List of column names containing numeric values - Year columns (e.g., \"2020\", \"2025\") - Cost columns (e.g., \"cost_2020\", \"cost_2025\")</p> Source code in <code>workflow/scripts/plot_input_costs.py</code> <pre><code>def get_numeric_columns(row: pd.Series) -&gt; List[str]:\n    \"\"\"Get list of numeric columns in the row.\n\n    Args:\n        row (pd.Series): Input row\n\n    Returns:\n        List[str]: List of column names containing numeric values\n            - Year columns (e.g., \"2020\", \"2025\")\n            - Cost columns (e.g., \"cost_2020\", \"cost_2025\")\n    \"\"\"\n    numeric_cols = []\n\n    # Check for year columns\n    for col in row.index:\n        if isinstance(col, str) and col.isdigit():\n            numeric_cols.append(col)\n\n    # Check for cost columns\n    if not numeric_cols:\n        for col in row.index:\n            if isinstance(col, str) and \"cost_\" in col and col.split(\"_\")[-1].isdigit():\n                numeric_cols.append(col)\n\n    # Check for specific years\n    possible_years = [\"2020\", \"2025\", \"2030\", \"2035\", \"2040\", \"2045\", \"2050\", \"2055\", \"2060\"]\n    for year in possible_years:\n        if year in row.index and year not in numeric_cols:\n            numeric_cols.append(year)\n\n    return numeric_cols\n</code></pre>"},{"location":"reference/plot_input_costs/#plot_input_costs.is_storage_unit","title":"<code>is_storage_unit(capacity_part)</code>","text":"<p>Check if the capacity part indicates a storage unit.</p> <p>Parameters:</p> Name Type Description Default <code>capacity_part</code> <code>str</code> <p>Capacity part of the unit string</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the unit is for storage (kWh or MWh)</p> Source code in <code>workflow/scripts/plot_input_costs.py</code> <pre><code>def is_storage_unit(capacity_part: str) -&gt; bool:\n    \"\"\"Check if the capacity part indicates a storage unit.\n\n    Args:\n        capacity_part (str): Capacity part of the unit string\n\n    Returns:\n        bool: True if the unit is for storage (kWh or MWh)\n    \"\"\"\n    return \"kwh\" in capacity_part.lower() or \"mwh\" in capacity_part.lower()\n</code></pre>"},{"location":"reference/plot_input_costs/#plot_input_costs.load_and_clean_data","title":"<code>load_and_clean_data(file_path)</code>","text":"<p>Load and clean data from a CSV file.</p> <p>load a CSV file, detects its encoding, replaces '-' with NaN, and drops the 'link' column if it exists.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>Path to the CSV file to be loaded.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: Cleaned DataFrame with standardized data.</p> Source code in <code>workflow/scripts/plot_input_costs.py</code> <pre><code>def load_and_clean_data(file_path: str) -&gt; pd.DataFrame:\n    \"\"\"Load and clean data from a CSV file.\n\n    load a CSV file, detects its encoding, replaces '-' with NaN,\n    and drops the 'link' column if it exists.\n\n    Args:\n        file_path: Path to the CSV file to be loaded.\n\n    Returns:\n        pd.DataFrame: Cleaned DataFrame with standardized data.\n    \"\"\"\n    encoding = detect_file_encoding(file_path)\n    try:\n        df = pd.read_csv(file_path, encoding=encoding)\n        df.replace(\"-\", np.nan, inplace=True)\n        if \"link\" in df.columns:\n            df.drop(columns=[\"link\"], inplace=True)\n        return df\n    except Exception as e:\n        logging.error(f\"Error loading file {file_path}: {e}\")\n        return pd.DataFrame()\n</code></pre>"},{"location":"reference/plot_input_costs/#plot_input_costs.load_reference_data","title":"<code>load_reference_data(file_path)</code>","text":"<p>Load and process reference data for technology cost comparison.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>Path to the reference data CSV file.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: Processed reference data with standardized units.</p> Source code in <code>workflow/scripts/plot_input_costs.py</code> <pre><code>def load_reference_data(file_path: str) -&gt; pd.DataFrame:\n    \"\"\"Load and process reference data for technology cost comparison.\n\n    Args:\n        file_path: Path to the reference data CSV file.\n\n    Returns:\n        pd.DataFrame: Processed reference data with standardized units.\n    \"\"\"\n    try:\n        ref_df = load_and_clean_data(file_path)\n        ref_df = ref_df[ref_df[\"reference\"] != \"PyPSA-China\"]\n\n        if \"parameter\" in ref_df.columns:\n            ref_df = filter_investment_parameter(ref_df)\n        ref_df = apply_conversion(ref_df, \"eur/kW\", \"eur/kWh\")\n        return ref_df\n    except Exception as e:\n        logging.error(f\"Error loading reference data: {e}\")\n        return pd.DataFrame()\n</code></pre>"},{"location":"reference/plot_input_costs/#plot_input_costs.normalize_capacity_unit","title":"<code>normalize_capacity_unit(cap)</code>","text":"<p>Normalize capacity unit string.</p> <p>Parameters:</p> Name Type Description Default <code>cap</code> <code>str</code> <p>Capacity unit string</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Normalized capacity unit - Converts to lowercase - Removes spaces - Standardizes square notation</p> Source code in <code>workflow/scripts/plot_input_costs.py</code> <pre><code>def normalize_capacity_unit(cap: str) -&gt; str:\n    \"\"\"Normalize capacity unit string.\n\n    Args:\n        cap (str): Capacity unit string\n\n    Returns:\n        str: Normalized capacity unit\n            - Converts to lowercase\n            - Removes spaces\n            - Standardizes square notation\n    \"\"\"\n    if not cap:\n        return \"\"\n    cap = cap.lower()\n    cap = cap.replace(\" \", \"\")\n    cap = cap.replace(\"\u00b2\", \"2\")\n    return cap\n</code></pre>"},{"location":"reference/plot_input_costs/#plot_input_costs.parse_unit_string","title":"<code>parse_unit_string(unit_str)</code>","text":"<p>Parse a unit string into currency and capacity parts.</p> <p>Parameters:</p> Name Type Description Default <code>unit_str</code> <code>str</code> <p>Unit string in format \"currency/capacity\"</p> required <p>Returns:</p> Type Description <code>Tuple[str | None, str | None]</code> <p>Tuple[str | None, str | None]:  - First element: currency part (e.g., \"eur\", \"usd\", \"cny\") - Second element: capacity part (e.g., \"kw\", \"kwh\") Returns (None, None) if invalid format</p> Source code in <code>workflow/scripts/plot_input_costs.py</code> <pre><code>def parse_unit_string(unit_str: str) -&gt; Tuple[str | None, str | None]:\n    \"\"\"Parse a unit string into currency and capacity parts.\n\n    Args:\n        unit_str (str): Unit string in format \"currency/capacity\"\n\n    Returns:\n        Tuple[str | None, str | None]: \n            - First element: currency part (e.g., \"eur\", \"usd\", \"cny\")\n            - Second element: capacity part (e.g., \"kw\", \"kwh\")\n            Returns (None, None) if invalid format\n    \"\"\"\n    if pd.isna(unit_str) or \"/\" not in unit_str:\n        return None, None\n    currency_part, capacity_part = unit_str.split(\"/\", 1)\n    return currency_part.strip().lower(), capacity_part.strip().lower()\n</code></pre>"},{"location":"reference/plot_input_costs/#plot_input_costs.plot_technologies_by_category","title":"<code>plot_technologies_by_category(costs_df, ref_df=None, tech_colors=None, font_size=14, plot_reference=True)</code>","text":"<p>Plot technology cost trends with literature comparison.</p> <p>Parameters:</p> Name Type Description Default <code>costs_df</code> <code>DataFrame</code> <p>DataFrame containing the main cost data</p> required <code>ref_df</code> <code>DataFrame | None</code> <p>DataFrame containing reference data for comparison</p> <code>None</code> <code>tech_colors</code> <code>Dict[str, str] | None</code> <p>Dictionary mapping technology names to colors</p> <code>None</code> <code>font_size</code> <code>int</code> <p>Font size for plot elements</p> <code>14</code> <code>plot_reference</code> <code>bool</code> <p>Whether to plot reference data</p> <code>True</code> <p>Returns:</p> Type Description <code>Figure</code> <p>plt.Figure: The generated plot figure</p> Source code in <code>workflow/scripts/plot_input_costs.py</code> <pre><code>def plot_technologies_by_category(\n    costs_df: pd.DataFrame,\n    ref_df: pd.DataFrame | None = None,\n    tech_colors: Dict[str, str] | None = None,\n    font_size: int = 14,\n    plot_reference: bool = True\n) -&gt; plt.Figure:\n    \"\"\"Plot technology cost trends with literature comparison.\n\n    Args:\n        costs_df (pd.DataFrame): DataFrame containing the main cost data\n        ref_df (pd.DataFrame | None): DataFrame containing reference data for comparison\n        tech_colors (Dict[str, str] | None): Dictionary mapping technology names to colors\n        font_size (int): Font size for plot elements\n        plot_reference (bool): Whether to plot reference data\n\n    Returns:\n        plt.Figure: The generated plot figure\n    \"\"\"\n    if costs_df.empty:\n        logging.error(\"Error: The costs DataFrame is empty; cannot plot.\")\n        return None\n\n    if tech_colors is None:\n        tech_colors = {}\n\n    # Get year columns\n    year_cols = [col for col in costs_df.columns if col.isdigit()]\n    if not year_cols:\n        year_cols = [col.split(\"_\")[-1] for col in costs_df.columns \n                    if \"cost_\" in col and col.split(\"_\")[-1].isdigit()]\n    if not year_cols:\n        possible_years = [\"2020\", \"2025\", \"2030\", \"2035\", \"2040\", \n                         \"2045\", \"2050\", \"2055\", \"2060\"]\n        year_cols = [year for year in possible_years if year in costs_df.columns]\n    if not year_cols:\n        logging.error(\"Error: Unable to identify year columns in costs data.\")\n        return None\n    year_cols = sorted(year_cols)\n\n    ref_year_cols = [col for col in ref_df.columns if col.isdigit()] if ref_df is not None else []\n    if plot_reference and ref_df is not None and not ref_year_cols:\n        logging.error(\"Error: Unable to identify year columns in reference data.\")\n        return None\n    ref_year_cols = sorted(ref_year_cols)\n\n    # Create subplots\n    technologies = costs_df[\"technology\"].unique()\n    num_techs = len(technologies)\n    num_cols = 6\n    num_rows = (num_techs + num_cols - 1) // num_cols\n\n    fig, axs = uplt.subplots(\n        nrows=num_rows,\n        ncols=num_cols,\n        figwidth=6 * num_cols,\n        sharex=True,\n        sharey=False\n    )\n    if not isinstance(axs, np.ndarray):\n        axs = np.array([axs])\n    axs = axs.flatten()\n\n    dash_styles = ['--', '-.', ':', (0, (3, 1, 1, 1))]\n\n    # Plot each technology\n    for i, tech in enumerate(technologies):\n        if i &gt;= len(axs):\n            logging.warning(f\"Warning: Exceeded subplot limit, skipping {tech}\")\n            continue\n\n        ax = axs[i]\n        tech_df = costs_df[costs_df[\"technology\"] == tech]\n        ref_tech_df = ref_df[ref_df[\"technology\"] == tech] if ref_df is not None else None\n        color = tech_colors.get(tech, \"#999999\")\n\n        # Plot main data\n        tech_years, tech_values = [], []\n        for year in year_cols:\n            values = pd.to_numeric(tech_df[year], errors='coerce').dropna().values\n            if values.size &gt; 0:\n                tech_years.append(int(year))\n                tech_values.append(np.median(values))\n\n        legend_handles = []\n        legend_labels = []\n        if tech_years:\n            line, = ax.plot(\n                tech_years,\n                tech_values,\n                linewidth=2.5,\n                color=color,\n                linestyle='-',\n                label=tech\n            )\n            legend_handles.append(line)\n            legend_labels.append(tech)\n\n        # Plot reference data\n        if plot_reference and ref_tech_df is not None and ref_tech_df.shape[0] &gt; 0:\n            for j, (ref_name, ref_group) in enumerate(ref_tech_df.groupby(\"reference\")):\n                ref_years, ref_values = [], []\n                for year in ref_year_cols:\n                    vals = pd.to_numeric(ref_group[year], errors='coerce').dropna().values\n                    if vals.size &gt; 0:\n                        ref_years.append(int(year))\n                        ref_values.append(np.median(vals))\n\n                if ref_years:\n                    dash_style = dash_styles[j % len(dash_styles)]\n                    ref_line, = ax.plot(\n                        ref_years,\n                        ref_values,\n                        linewidth=2,\n                        color=color,\n                        linestyle=dash_style,\n                        label=ref_name\n                    )\n                    legend_handles.append(ref_line)\n                    legend_labels.append(ref_name)\n\n        # Set labels and formatting\n        if \"unit\" in tech_df.columns and not tech_df[\"unit\"].isna().all():\n            unit = tech_df[\"unit\"].iloc[0]\n            unit_parts = unit.split('/')\n            if len(unit_parts) == 2:\n                currency, capacity = unit_parts\n                if 'eur' in currency.lower():\n                    ax.set_ylabel(f\"EUR/{capacity.upper()}\", fontsize=font_size)\n                else:\n                    ax.set_ylabel(f\"{unit}\", fontsize=font_size)\n            else:\n                ax.set_ylabel(f\"{unit}\", fontsize=font_size)\n        else:\n            ax.set_ylabel(\"Cost (EUR)\", fontsize=font_size)\n\n        ax2 = ax.twinx()\n        if \"unit\" in tech_df.columns and not tech_df[\"unit\"].isna().all():\n            unit = tech_df[\"unit\"].iloc[0]\n            unit_parts = unit.split('/')\n            if len(unit_parts) == 2:\n                _, capacity = unit_parts\n                ax2.set_ylabel(f\"CNY/{capacity.upper()}\", fontsize=font_size)\n            else:\n                ax2.set_ylabel(\"Cost (CNY)\", fontsize=font_size)\n        else:\n            ax2.set_ylabel(\"Cost (CNY)\", fontsize=font_size)\n\n        y_min, y_max = ax.get_ylim()\n        ax2.set_ylim(y_min * 7.8, y_max * 7.8)\n\n        ax.set_title(f\"{tech}\", fontsize=font_size)\n        ax.grid(True, linestyle='--', alpha=0.7)\n        ax.tick_params(axis='y', labelsize=font_size)\n        ax2.tick_params(axis='y', labelsize=font_size)\n        ax.legend(\n            legend_handles,\n            legend_labels,\n            loc='upper right',\n            fontsize=font_size-2,\n            ncol=1\n        )\n\n    # Hide unused subplots\n    total_plots = num_rows * num_cols\n    for i in range(num_techs, total_plots):\n        if i &lt; len(axs):\n            axs[i].set_visible(False)\n\n    axs[0].figure.format(\n        suptitle=\"Technology Cost Trends with Literature Comparison\",\n        abc=True,\n        abcloc=\"ul\",\n        xlabel=\"Year\",\n        fontsize=font_size\n    )\n\n    return fig\n</code></pre>"},{"location":"reference/plot_inputs_visualisation/","title":"Plot inputs visualisation","text":""},{"location":"reference/plot_inputs_visualisation/#plot_inputs_visualisation.plot_average_distances","title":"<code>plot_average_distances(distances, ax=None)</code>","text":"<p>Plot the average distances to the node (region com/repr point) for each vre class Args:     distances (xr.DataArray): the average distances for each class to the node     ax (plt.Axes, optional): the axes to plot on. Defaults to None. Returns:     tuple[plt.Figure, plt.Axes]: the figure and axes</p> Source code in <code>workflow/scripts/plot_inputs_visualisation.py</code> <pre><code>def plot_average_distances(\n    distances: xr.DataArray, ax: plt.Axes = None\n) -&gt; tuple[plt.Figure, plt.Axes]:\n    \"\"\"Plot the average distances to the node (region com/repr point) for each vre class\n    Args:\n        distances (xr.DataArray): the average distances for each class to the node\n        ax (plt.Axes, optional): the axes to plot on. Defaults to None.\n    Returns:\n        tuple[plt.Figure, plt.Axes]: the figure and axes\n    \"\"\"\n    if not ax:\n        fig, ax = plt.subplots(figsize=(12, 6))\n    else:\n        fig = ax.get_figure()\n\n    distances.to_series().plot.hist(\n        bins=30,\n        figsize=(10, 6),\n        title=\"Frequency Distribution of Average Distance\",\n        xlabel=\"Average Distance (km)\",\n        ylabel=\"Frequency\",\n        # color=\"skyblue\",\n        edgecolor=\"black\",\n    )\n    fig.tight_layout()\n    return fig, ax\n</code></pre>"},{"location":"reference/plot_inputs_visualisation/#plot_inputs_visualisation.plot_resource_class_bins","title":"<code>plot_resource_class_bins(resource_classes, regions, technology, ax=None)</code>","text":"<p>Map of VRE grades (by grade/ bin number) for each node Args:     resource_classes (gpd.GeoDataFrame): the resource classes     regions (gpd.GeoDataFrame): the regions/node regions     technology (str): the technology name     ax (plt.Axes, optional): the axes to plot on. Defaults to None. Returns:     tuple[plt.Figure, plt.Axes]: the figure and axes</p> Source code in <code>workflow/scripts/plot_inputs_visualisation.py</code> <pre><code>def plot_resource_class_bins(\n    resource_classes: gpd.GeoDataFrame,\n    regions: gpd.GeoDataFrame,\n    technology: str,\n    ax: plt.Axes = None,\n) -&gt; tuple[plt.Figure, plt.Axes]:\n    \"\"\"Map of VRE grades (by grade/ bin number) for each node\n    Args:\n        resource_classes (gpd.GeoDataFrame): the resource classes\n        regions (gpd.GeoDataFrame): the regions/node regions\n        technology (str): the technology name\n        ax (plt.Axes, optional): the axes to plot on. Defaults to None.\n    Returns:\n        tuple[plt.Figure, plt.Axes]: the figure and axes\n    \"\"\"\n\n    if not ax:\n        fig, ax = plt.subplots(figsize=(12, 6))\n    else:\n        fig = ax.get_figure()\n\n    nbins = resource_classes.bin.nunique()\n\n    cmap = plt.cm.get_cmap(\"magma_r\", nbins)  # Inverted discrete colormap with `nbins` colors\n    cmap.set_bad(color=\"lightgrey\")  # Set color for missing values\n    norm = mcolors.BoundaryNorm(boundaries=range(nbins + 1), ncolors=nbins)\n\n    # Plot the class regions\n    resource_classes.reset_index().plot(\"bin\", cmap=cmap, legend=True, ax=ax, norm=norm)\n    ax.set_title(f\"Resource Classes for {technology.capitalize()} simple\")\n\n    # Add administrative region borders\n    regions.boundary.plot(ax=ax, color=\"black\", linewidth=0.5, linestyle=\"--\")\n\n    # coords.plot(ax=ax, color=\"black\", markersize=1)\n    fig.tight_layout()\n\n    return fig, ax\n</code></pre>"},{"location":"reference/plot_inputs_visualisation/#plot_inputs_visualisation.plot_resource_class_cfs","title":"<code>plot_resource_class_cfs(resource_classes, regions, technology, ax=None)</code>","text":"<p>Map of VRE capacity factors for each node and vre grade Args:     resource_classes (gpd.GeoDataFrame): the resource classes     regions (gpd.GeoDataFrame): the regions/node regions     technology (str): the technology name     ax (plt.Axes, optional): the axes to plot on. Defaults to None. Returns:     tuple[plt.Figure, plt.Axes]: the figure and axes</p> Source code in <code>workflow/scripts/plot_inputs_visualisation.py</code> <pre><code>def plot_resource_class_cfs(\n    resource_classes: gpd.GeoDataFrame,\n    regions: gpd.GeoDataFrame,\n    technology: str,\n    ax: plt.Axes = None,\n):\n    \"\"\"Map of VRE capacity factors for each node and vre grade\n    Args:\n        resource_classes (gpd.GeoDataFrame): the resource classes\n        regions (gpd.GeoDataFrame): the regions/node regions\n        technology (str): the technology name\n        ax (plt.Axes, optional): the axes to plot on. Defaults to None.\n    Returns:\n        tuple[plt.Figure, plt.Axes]: the figure and axes\n    \"\"\"\n\n    if not ax:\n        fig, ax = plt.subplots(figsize=(12, 6))\n    else:\n        fig = ax.get_figure()\n\n    # Plot the class regions\n    resource_classes.plot(\"cf\", cmap=\"magma_r\", legend=True, ax=ax)\n    ax.set_title(f\"Resource Classes for {technology.capitalize()} simple\")\n\n    # Add administrative region borders\n    regions.boundary.plot(ax=ax, color=\"black\", linewidth=0.5, linestyle=\"--\")\n\n    # coords.plot(ax=ax, color=\"black\", markersize=1)\n    fig.tight_layout()\n\n    return fig, ax\n</code></pre>"},{"location":"reference/plot_network/","title":"Plot network","text":""},{"location":"reference/plot_network/#plot_network.add_cost_pannel","title":"<code>add_cost_pannel(df, fig, preferred_order, tech_colors, plot_additions, ax_loc=[-0.09, 0.28, 0.09, 0.45], **kwargs)</code>","text":"<p>Add a cost pannel to the figure</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>the cost data to plot</p> required <code>fig</code> <code>Figure</code> <p>the figure object to which the cost pannel will be added</p> required <code>preferred_order</code> <code>Index</code> <p>index, the order in whiich to plot</p> required <code>tech_colors</code> <code>dict</code> <p>the tech colors</p> required <code>plot_additions</code> <code>bool</code> <p>plot the additions</p> required <code>ax_loc</code> <code>list</code> <p>the location of the cost pannel. Defaults to [-0.09, 0.28, 0.09, 0.45].</p> <code>[-0.09, 0.28, 0.09, 0.45]</code> Source code in <code>workflow/scripts/plot_network.py</code> <pre><code>def add_cost_pannel(\n    df: pd.DataFrame,\n    fig: plt.Figure,\n    preferred_order: pd.Index,\n    tech_colors: dict,\n    plot_additions: bool,\n    ax_loc=[-0.09, 0.28, 0.09, 0.45],\n    **kwargs: dict,\n) -&gt; None:\n    \"\"\"Add a cost pannel to the figure\n\n    Args:\n        df (pd.DataFrame): the cost data to plot\n        fig (plt.Figure): the figure object to which the cost pannel will be added\n        preferred_order (pd.Index): index, the order in whiich to plot\n        tech_colors (dict): the tech colors\n        plot_additions (bool): plot the additions\n        ax_loc (list, optional): the location of the cost pannel.\n            Defaults to [-0.09, 0.28, 0.09, 0.45].\n    \"\"\"\n    ax3 = fig.add_axes(ax_loc)\n    reordered = preferred_order.intersection(df.index).append(df.index.difference(preferred_order))\n    colors = {k.lower(): v for k, v in tech_colors.items()}\n\n    # Create color list with default color for missing carriers\n    color_list = []\n    for k in reordered:\n        if k.lower() in colors:\n            color_list.append(colors[k.lower()])\n        else:\n            color_list.append(\"lightgrey\")  # Default color for missing carriers\n\n    df.loc[reordered, df.columns].T.plot(\n        kind=\"bar\",\n        ax=ax3,\n        stacked=True,\n        color=color_list,\n        **kwargs,\n    )\n    ax3.legend().remove()\n    ax3.set_ylabel(\"annualized system cost bEUR/a\")\n    ax3.set_xticklabels(ax3.get_xticklabels(), rotation=\"horizontal\")\n    ax3.grid(axis=\"y\")\n    ax3.set_ylim([0, df.sum().max() * 1.1])\n    if plot_additions:\n        # add label\n        percent = np.round((df.sum()[\"added\"] / df.sum()[\"total\"]) * 100)\n        ax3.text(0.85, (df.sum()[\"added\"] + 15), str(percent) + \"%\", color=\"black\")\n\n    fig.tight_layout()\n    return ax3\n</code></pre>"},{"location":"reference/plot_network/#plot_network.add_energy_pannel","title":"<code>add_energy_pannel(df, fig, preferred_order, colors, ax_loc=[-0.09, 0.28, 0.09, 0.45])</code>","text":"<p>Add a cost pannel to the figure</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>the statistics supply output by carrier (from plot_energy map)</p> required <code>fig</code> <code>Figure</code> <p>the figure object to which the cost pannel will be added</p> required <code>preferred_order</code> <code>Index</code> <p>index, the order in whiich to plot</p> required <code>colors</code> <code>Series</code> <p>the colors for the techs, with the correct index and no extra techs</p> required <code>ax_loc</code> <code>list</code> <p>the pannel location. Defaults to [-0.09, 0.28, 0.09, 0.45].</p> <code>[-0.09, 0.28, 0.09, 0.45]</code> Source code in <code>workflow/scripts/plot_network.py</code> <pre><code>def add_energy_pannel(\n    df: pd.DataFrame,\n    fig: plt.Figure,\n    preferred_order: pd.Index,\n    colors: pd.Series,\n    ax_loc=[-0.09, 0.28, 0.09, 0.45],\n) -&gt; None:\n    \"\"\"Add a cost pannel to the figure\n\n    Args:\n        df (pd.DataFrame): the statistics supply output by carrier (from plot_energy map)\n        fig (plt.Figure): the figure object to which the cost pannel will be added\n        preferred_order (pd.Index): index, the order in whiich to plot\n        colors (pd.Series): the colors for the techs, with the correct index and no extra techs\n        ax_loc (list, optional): the pannel location. Defaults to [-0.09, 0.28, 0.09, 0.45].\n    \"\"\"\n    ax3 = fig.add_axes(ax_loc)\n    reordered = preferred_order.intersection(df.index).append(df.index.difference(preferred_order))\n    df = df / PLOT_SUPPLY_UNITS\n    # only works if colors has correct index\n    df.loc[reordered, df.columns].T.plot(\n        kind=\"bar\",\n        ax=ax3,\n        stacked=True,\n        color=colors[reordered],\n    )\n\n    ax3.legend().remove()\n    ax3.set_ylabel(\"Electricity supply [TWh]\")\n    ax3.set_xticklabels(ax3.get_xticklabels(), rotation=\"horizontal\")\n    ax3.grid(axis=\"y\")\n    ax3.set_ylim([0, df.sum().max() * 1.1])\n\n    fig.tight_layout()\n</code></pre>"},{"location":"reference/plot_network/#plot_network.plot_cost_map","title":"<code>plot_cost_map(network, opts, base_year=2020, plot_additions=True, capex_only=False, cost_pannel=True, save_path=None)</code>","text":"<p>Plot the cost of each node on a map as well as the line capacities</p> <p>Parameters:</p> Name Type Description Default <code>network</code> <code>Network</code> <p>the network object</p> required <code>opts</code> <code>dict</code> <p>the plotting config (snakemake.config[\"plotting\"])</p> required <code>base_year</code> <code>int</code> <p>the base year (for cost delta). Defaults to 2020.</p> <code>2020</code> <code>capex_only</code> <code>bool</code> <p>do not plot VOM (FOM is in CAPEX). Defaults to False.</p> <code>False</code> <code>plot_additions</code> <code>bool</code> <p>plot a map of investments (p_nom_opt vs p_nom).   Defaults to True.</p> <code>True</code> <code>cost_pannel</code> <code>bool</code> <p>add a bar graph with costs. Defaults to True.</p> <code>True</code> <code>save_path</code> <code>PathLike</code> <p>save figure to path (or not if None). Defaults to None.</p> <code>None</code> <p>raises:     ValueError: if plot_additions and not capex_only</p> Source code in <code>workflow/scripts/plot_network.py</code> <pre><code>def plot_cost_map(\n    network: pypsa.Network,\n    opts: dict,\n    base_year=2020,\n    plot_additions=True,\n    capex_only=False,\n    cost_pannel=True,\n    save_path: os.PathLike = None,\n):\n    \"\"\"Plot the cost of each node on a map as well as the line capacities\n\n    Args:\n        network (pypsa.Network): the network object\n        opts (dict): the plotting config (snakemake.config[\"plotting\"])\n        base_year (int, optional): the base year (for cost delta). Defaults to 2020.\n        capex_only (bool, optional): do not plot VOM (FOM is in CAPEX). Defaults to False.\n        plot_additions (bool, optional): plot a map of investments (p_nom_opt vs p_nom).\n              Defaults to True.\n        cost_pannel (bool, optional): add a bar graph with costs. Defaults to True.\n        save_path (os.PathLike, optional): save figure to path (or not if None). Defaults to None.\n    raises:\n        ValueError: if plot_additions and not capex_only\n    \"\"\"\n\n    if plot_additions and not capex_only:\n        raise ValueError(\"Cannot plot additions without capex only\")\n\n    tech_colors = make_nice_tech_colors(opts[\"tech_colors\"], opts[\"nice_names\"])\n\n    # TODO scale edges by cost from capex summary\n    def calc_link_plot_width(row, carrier=\"AC\", additions=False):\n        if row.length == 0 or row.carrier != carrier or not row.plottable:\n            return 0\n        elif additions:\n            return row.p_nom\n        else:\n            return row.p_nom_opt\n\n    # ============ === Stats by bus ===\n    # calc costs &amp; sum over component types to keep bus &amp; carrier (remove no loc)\n    costs = network.statistics.capex(groupby=[\"location\", \"carrier\"])\n    costs = costs.groupby(level=[1, 2]).sum()\n    if \"\" in costs.index:\n        costs.drop(\"\", inplace=True)\n    # we miss some buses by grouping epr location, fill w 0s\n    bus_idx = pd.MultiIndex.from_product([network.buses.index, [\"AC\"]])\n    costs = costs.reindex(bus_idx.union(costs.index), fill_value=0)\n    # add marginal (excluding quasi fixed) to costs if desired\n    if not capex_only:\n        opex = network.statistics.opex(groupby=[\"location\", \"carrier\"])\n        opex = opex.groupby(level=[1, 2]).sum()\n        cost_pies = costs + opex.reindex(costs.index, fill_value=0)\n\n    # === make map components: pies and edges\n    cost_pies = costs.fillna(0)\n    cost_pies.index.names = [\"bus\", \"carrier\"]\n    carriers = cost_pies.index.get_level_values(1).unique()\n    # map edges\n    link_plot_w = network.links.apply(lambda row: calc_link_plot_width(row, carrier=\"AC\"), axis=1)\n    edges = pd.concat([network.lines.s_nom_opt, link_plot_w]).groupby(level=0).sum()\n    line_lower_threshold = opts.get(\"min_edge_capacity\", 0)\n    edge_widths = edges.clip(line_lower_threshold, edges.max()).replace(line_lower_threshold, 0)\n\n    # === Additions ===\n    # for pathways sometimes interested in additions from last time step\n    if plot_additions:\n        installed = (\n            network.statistics.installed_capex(groupby=[\"location\", \"carrier\"])\n            .groupby(level=[1, 2])\n            .sum()\n        )\n        costs_additional = costs - installed.reindex(costs.index, fill_value=0)\n        cost_pies_additional = costs_additional.fillna(0)\n        cost_pies_additional.index.names = [\"bus\", \"carrier\"]\n\n        link_additions = network.links.apply(\n            lambda row: calc_link_plot_width(row, carrier=\"AC\", additions=True), axis=1\n        )\n        added_links = link_plot_w - link_additions.reindex(link_plot_w.index, fill_value=0)\n        added_lines = network.lines.s_nom_opt - network.lines.s_nom.reindex(\n            network.lines.index, fill_value=0\n        )\n        edge_widths_added = pd.concat([added_links, added_lines]).groupby(level=0).sum()\n\n        # add to carrier types\n        carriers = carriers.union(cost_pies_additional.index.get_level_values(1).unique())\n\n    preferred_order = pd.Index(opts[\"preferred_order\"])\n    carriers = carriers.tolist()\n\n    # Make figure with right number of pannels\n    if plot_additions:\n        fig, (ax1, ax2) = plt.subplots(1, 2, subplot_kw={\"projection\": ccrs.PlateCarree()})\n        fig.set_size_inches(opts[\"cost_map\"][\"figsize_w_additions\"])\n    else:\n        fig, ax1 = plt.subplots(subplot_kw={\"projection\": ccrs.PlateCarree()})\n        fig.set_size_inches(opts[\"cost_map\"][\"figsize\"])\n\n    # Add the total costs\n    bus_size_factor = opts[\"cost_map\"][\"bus_size_factor\"]\n    linewidth_factor = opts[\"cost_map\"][\"linewidth_factor\"]\n\n    ax = plot_map(\n        network,\n        tech_colors=tech_colors,\n        edge_widths=edge_widths / linewidth_factor,\n        bus_colors=tech_colors,\n        bus_sizes=cost_pies / bus_size_factor,\n        edge_colors=opts[\"cost_map\"][\"edge_color\"],\n        ax=ax1,\n        add_legend=not plot_additions,\n        bus_ref_title=f\"System costs{' (CAPEX)'if capex_only else ''}\",\n        **opts[\"cost_map\"],\n    )\n\n    # TODO check edges is working\n    # Add the added pathway costs\n    if plot_additions:\n        ax2 = plot_map(\n            network,\n            tech_colors=tech_colors,\n            edge_widths=edge_widths_added / linewidth_factor,\n            bus_colors=tech_colors,\n            bus_sizes=cost_pies_additional / bus_size_factor,\n            edge_colors=\"rosybrown\",\n            ax=ax2,\n            bus_ref_title=f\"Added costs{' (CAPEX)' if capex_only else ''}\",\n            add_legend=True,\n            **opts[\"cost_map\"],\n        )\n\n    # Add the optional cost pannel\n    if cost_pannel:\n        df = pd.DataFrame(columns=[\"total\"])\n        df[\"total\"] = network.statistics.capex(nice_names=False).groupby(level=1).sum()\n        if not capex_only:\n            df[\"opex\"] = network.statistics.opex(nice_names=False).groupby(level=1).sum()\n            df.rename(columns={\"total\": \"capex\"}, inplace=True)\n        elif plot_additions:\n            df[\"added\"] = (\n                df[\"total\"]\n                - network.statistics.installed_capex(nice_names=False).groupby(level=1).sum()\n            )\n\n        df.fillna(0, inplace=True)\n        df = df / PLOT_COST_UNITS\n        # TODO decide discount\n        # df = df / (1 + discount_rate) ** (int(planning_horizon) - base_year)\n        ax3 = add_cost_pannel(\n            df,\n            fig,\n            preferred_order,\n            tech_colors,\n            plot_additions,\n            ax_loc=[-0.09, 0.28, 0.09, 0.45],\n        )\n        # Set x-label angle to 45 degrees for better readability\n        for label in ax3.get_xticklabels():\n            label.set_rotation(45)\n\n    fig.set_size_inches(opts[\"cost_map\"][f\"figsize{'_w_additions' if plot_additions else ''}\"])\n    fig.tight_layout()\n    if save_path:\n        fig.savefig(save_path, transparent=opts[\"transparent\"], bbox_inches=\"tight\")\n</code></pre>"},{"location":"reference/plot_network/#plot_network.plot_energy_map","title":"<code>plot_energy_map(network, opts, energy_pannel=True, save_path=None, carrier='AC', plot_ac_imports=False, exclude_batteries=True, components=['Generator', 'Link'])</code>","text":"<p>A map plot of energy, either AC or heat</p> <p>Parameters:</p> Name Type Description Default <code>network</code> <code>Network</code> <p>the pyPSA network object</p> required <code>opts</code> <code>dict</code> <p>the plotting options (snakemake.config[\"plotting\"])</p> required <code>energy_pannel</code> <code>bool</code> <p>add an anergy pie to the left. Defaults to True.</p> <code>True</code> <code>save_path</code> <code>PathLike</code> <p>Fig outp path. Defaults to None (no save).</p> <code>None</code> <code>carrier</code> <code>str</code> <p>the energy carrier. Defaults to \"AC\".</p> <code>'AC'</code> <code>plot_ac_imports</code> <code>bool</code> <p>plot electricity imports. Defaults to False.</p> <code>False</code> <code>exclude_batteries</code> <code>bool</code> <p>exclude battery dischargers from the supply pie.</p> <code>True</code> <code>components</code> <code>list</code> <p>the components to plot. Defaults to [\"Generator\", \"Link\"].</p> <code>['Generator', 'Link']</code> <p>raises:     ValueError: if carrier is not AC or heat</p> Source code in <code>workflow/scripts/plot_network.py</code> <pre><code>def plot_energy_map(\n    network: pypsa.Network,\n    opts: dict,\n    energy_pannel=True,\n    save_path: os.PathLike = None,\n    carrier=\"AC\",\n    plot_ac_imports=False,\n    exclude_batteries=True,\n    components=[\"Generator\", \"Link\"],\n):\n    \"\"\"A map plot of energy, either AC or heat\n\n    Args:\n        network (pypsa.Network): the pyPSA network object\n        opts (dict): the plotting options (snakemake.config[\"plotting\"])\n        energy_pannel (bool, optional): add an anergy pie to the left. Defaults to True.\n        save_path (os.PathLike, optional): Fig outp path. Defaults to None (no save).\n        carrier (str, optional): the energy carrier. Defaults to \"AC\".\n        plot_ac_imports (bool, optional): plot electricity imports. Defaults to False.\n        exclude_batteries (bool, optional): exclude battery dischargers from the supply pie.\n        components (list, optional): the components to plot. Defaults to [\"Generator\", \"Link\"].\n    raises:\n        ValueError: if carrier is not AC or heat\n    \"\"\"\n    if carrier not in [\"AC\", \"heat\"]:\n        raise ValueError(\"Carrier must be either 'AC' or 'heat'\")\n\n    # make the statistics. Buses not assigned to a region will be included\n    # if they are linked to a region (e.g. turbine link w carrier = hydroelectricity)\n    energy_supply = network.statistics.supply(\n        groupby=get_location_and_carrier,\n        bus_carrier=carrier,\n        comps=components,\n    )\n    # get rid of components\n    supply_pies = energy_supply.groupby(level=[1, 2]).sum()\n\n    # TODO fix  this for heat\n    # # calc costs &amp; sum over component types to keep bus &amp; carrier (remove no loc)\n    # energy_supply = network.statistics.capex(groupby=[\"location\", \"carrier\"])\n    # energy_supply = energy_supply.groupby(level=[1, 2]).sum().drop(\"\")\n    # # we miss some buses by grouping epr location, fill w 0s\n    # bus_idx = pd.MultiIndex.from_product([network.buses.index, [\"AC\"]])\n    # supply_pies = energy_supply.reindex(bus_idx.union(energy_supply.index), fill_value=0)\n\n    # remove imports from supply pies\n    if carrier == \"AC\" and not plot_ac_imports:\n        supply_pies = supply_pies.loc[supply_pies.index.get_level_values(1) != \"AC\"]\n\n    # TODO aggregate costs below threshold into \"other\" -&gt; requires messing with network\n    # network.add(\"Carrier\", \"Other\")\n\n    # get all carrier types\n    carriers_list = supply_pies.index.get_level_values(1).unique()\n    carriers_list = carriers_list.tolist()\n\n    # TODO make line handling nicer\n    line_lower_threshold = opts.get(\"min_edge_capacity\", 500)\n    # Make figur\n    fig, ax = plt.subplots(subplot_kw={\"projection\": ccrs.PlateCarree()})\n    fig.set_size_inches(opts[\"energy_map\"][\"figsize\"])\n    # get colors\n    bus_colors = network.carriers.loc[network.carriers.nice_name.isin(carriers_list), \"color\"]\n    bus_colors.rename(opts[\"nice_names\"], inplace=True)\n\n    preferred_order = pd.Index(opts[\"preferred_order\"])\n    reordered = preferred_order.intersection(bus_colors.index).append(\n        bus_colors.index.difference(preferred_order)\n    )\n    # TODO there'sa  problem with network colors when using heat, pies aren't grouped by location\n    colors = network.carriers.color.copy()\n    colors.index = colors.index.map(opts[\"nice_names\"])\n    tech_colors = make_nice_tech_colors(opts[\"tech_colors\"], opts[\"nice_names\"])\n\n    # make sure plot isnt overpopulated\n    def calc_link_plot_width(row, carrier=\"AC\"):\n        if row.length == 0 or row.carrier != carrier or not row.plottable:\n            return 0\n        else:\n            return row.p_nom_opt\n\n    edge_carrier = \"H2 pipeline\" if carrier == \"heat\" else \"AC\"\n    link_plot_w = network.links.apply(lambda row: calc_link_plot_width(row, edge_carrier), axis=1)\n    edges = pd.concat([network.lines.s_nom_opt, link_plot_w])\n    edge_widths = edges.clip(line_lower_threshold, edges.max()).replace(line_lower_threshold, 0)\n\n    opts_plot = opts[\"energy_map\"].copy()\n    if carrier == \"heat\":\n        opts_plot[\"ref_bus_sizes\"] = opts_plot[\"ref_bus_sizes_heat\"]\n        opts_plot[\"ref_edge_sizes\"] = opts_plot[\"ref_edge_sizes_heat\"]\n        opts_plot[\"linewidth_factor\"] = opts_plot[\"linewidth_factor_heat\"]\n        opts_plot[\"bus_size_factor\"] = opts_plot[\"bus_size_factor_heat\"]\n    # exclude battery dischargers from bus sizes\n    if exclude_batteries:\n        bus_sizes = (\n            supply_pies.loc[~supply_pies.index.get_level_values(1).str.contains(\"battery\")]\n            / opts_plot[\"bus_size_factor\"]\n        )\n    else:\n        bus_sizes = supply_pies / opts_plot[\"bus_size_factor\"]\n    ax = plot_map(\n        network,\n        tech_colors=tech_colors,  # colors.to_dict(),\n        edge_widths=edge_widths / opts_plot[\"linewidth_factor\"],\n        bus_colors=bus_colors.loc[reordered],\n        bus_sizes=bus_sizes,\n        edge_colors=opts_plot[\"edge_color\"],\n        ax=ax,\n        edge_unit_conv=PLOT_CAP_UNITS,\n        bus_unit_conv=PLOT_SUPPLY_UNITS,\n        add_legend=True,\n        **opts_plot,\n    )\n    # # Add the optional cost pannel\n    if energy_pannel:\n        df = supply_pies.groupby(level=1).sum().to_frame()\n        df = df.fillna(0)\n        df.rename(columns={0: \"\"}, inplace=True)\n        add_energy_pannel(df, fig, preferred_order, bus_colors, ax_loc=[-0.09, 0.28, 0.09, 0.45])\n\n    handles, labels = ax.get_legend_handles_labels()\n    ax.legend(handles, labels, ncol=1, bbox_to_anchor=[1, 1], loc=\"upper left\")\n    fig.tight_layout()\n\n    if save_path:\n        fig.savefig(save_path, transparent=opts[\"transparent\"], bbox_inches=\"tight\")\n</code></pre>"},{"location":"reference/plot_network/#plot_network.plot_map","title":"<code>plot_map(network, tech_colors, edge_widths, bus_colors, bus_sizes, edge_colors='black', add_ref_edge_sizes=True, add_ref_bus_sizes=True, add_legend=True, bus_unit_conv=PLOT_COST_UNITS, edge_unit_conv=PLOT_CAP_UNITS, ax=None, **kwargs)</code>","text":"<p>Plot the network on a map</p> <p>Parameters:</p> Name Type Description Default <code>network</code> <code>Network</code> <p>the pypsa network (filtered to contain only relevant buses &amp; links)</p> required <code>tech_colors</code> <code>dict</code> <p>config mapping</p> required <code>edge_colors</code> <code>Series | str</code> <p>the series of edge colors</p> <code>'black'</code> <code>edge_widths</code> <code>Series</code> <p>the edge widths</p> required <code>bus_colors</code> <code>Series</code> <p>the series of bus colors</p> required <code>bus_sizes</code> <code>Series</code> <p>the series of bus sizes</p> required <code>add_ref_edge_sizes</code> <code>bool</code> <p>add reference line sizes in legend (requires edge_colors=True). Defaults to True.</p> <code>True</code> <code>add_ref_bus_sizes</code> <code>bool</code> <p>add reference bus sizes in legend. Defaults to True.</p> <code>True</code> <code>ax</code> <code>Axes</code> <p>the plotting ax. Defaults to None (new figure).</p> <code>None</code> Source code in <code>workflow/scripts/plot_network.py</code> <pre><code>def plot_map(\n    network: pypsa.Network,\n    tech_colors: dict,\n    edge_widths: pd.Series,\n    bus_colors: dict | pd.Series,\n    bus_sizes: pd.Series,\n    edge_colors: pd.Series | str = \"black\",\n    add_ref_edge_sizes=True,\n    add_ref_bus_sizes=True,\n    add_legend=True,\n    bus_unit_conv=PLOT_COST_UNITS,\n    edge_unit_conv=PLOT_CAP_UNITS,\n    ax=None,\n    **kwargs,\n) -&gt; plt.Axes:\n    \"\"\"Plot the network on a map\n\n    Args:\n        network (pypsa.Network): the pypsa network (filtered to contain only relevant buses &amp; links)\n        tech_colors (dict): config mapping\n        edge_colors (pd.Series|str): the series of edge colors\n        edge_widths (pd.Series): the edge widths\n        bus_colors (pd.Series): the series of bus colors\n        bus_sizes (pd.Series): the series of bus sizes\n        add_ref_edge_sizes (bool, optional): add reference line sizes in legend\n            (requires edge_colors=True). Defaults to True.\n        add_ref_bus_sizes (bool, optional): add reference bus sizes in legend.\n            Defaults to True.\n        ax (plt.Axes, optional): the plotting ax. Defaults to None (new figure).\n    \"\"\"\n\n    if not ax:\n        fig, ax = plt.subplots()\n    else:\n        fig = ax.get_figure()\n\n    network.plot(\n        bus_sizes=bus_sizes,\n        bus_colors=bus_colors,\n        line_colors=edge_colors,\n        link_colors=edge_colors,\n        line_widths=edge_widths,\n        link_widths=edge_widths,\n        ax=ax,\n        color_geomap=True,\n        boundaries=kwargs.get(\"boundaries\", None),\n    )\n\n    ax.add_feature(cfeature.BORDERS, linewidth=0.5, edgecolor=\"gray\")\n    states_provinces = cfeature.NaturalEarthFeature(\n        category=\"cultural\",\n        name=\"admin_1_states_provinces_lines\",\n        scale=\"50m\",\n        facecolor=\"none\",\n    )\n    # Add our states feature.\n    ax.add_feature(states_provinces, edgecolor=\"lightgray\", alpha=0.7)\n\n    if add_legend:\n        carriers = bus_sizes.index.get_level_values(1).unique()\n        # Ensure all carriers have colors, use default color for missing ones\n        colors = []\n        for carrier in carriers:\n            if carrier in tech_colors:\n                colors.append(tech_colors[carrier])\n            else:\n                colors.append(\"lightgrey\")  # Default color for missing carriers\n\n        if isinstance(edge_colors, str):\n            colors += [edge_colors]\n            labels = carriers.to_list() + [\"HVDC or HVAC link\"]\n        else:\n            colors += edge_colors.values.to_list()\n            labels = carriers.to_list() + edge_colors.index.to_list()\n        leg_opt = {\"bbox_to_anchor\": (1.42, 1.04), \"frameon\": False}\n        add_legend_patches(ax, colors, labels, legend_kw=leg_opt)\n\n    if add_ref_edge_sizes &amp; isinstance(edge_colors, str):\n        ref_unit = kwargs.get(\"ref_edge_unit\", \"GW\")\n        size_factor = float(kwargs.get(\"linewidth_factor\", 1e5))\n        ref_sizes = kwargs.get(\"ref_edge_sizes\", [1e5, 5e5])\n\n        labels = [f\"{float(s)/edge_unit_conv} {ref_unit}\" for s in ref_sizes]\n        ref_sizes = list(map(lambda x: float(x) / size_factor, ref_sizes))\n        legend_kw = dict(\n            loc=\"upper left\",\n            bbox_to_anchor=(0.26, 1.0),\n            frameon=False,\n            labelspacing=0.8,\n            handletextpad=2,\n            title=kwargs.get(\"edge_ref_title\", \"Grid cap.\"),\n        )\n        add_legend_lines(\n            ax, ref_sizes, labels, patch_kw=dict(color=edge_colors), legend_kw=legend_kw\n        )\n\n    # add reference bus sizes ferom the units\n    if add_ref_bus_sizes:\n        ref_unit = kwargs.get(\"ref_bus_unit\", \"bEUR/a\")\n        size_factor = float(kwargs.get(\"bus_size_factor\", 1e10))\n        ref_sizes = kwargs.get(\"ref_bus_sizes\", [2e10, 1e10, 5e10])\n        labels = [f\"{float(s)/bus_unit_conv:.0f} {ref_unit}\" for s in ref_sizes]\n        ref_sizes = list(map(lambda x: float(x) / size_factor, ref_sizes))\n\n        legend_kw = {\n            \"loc\": \"upper left\",\n            \"bbox_to_anchor\": (0.0, 1.0),\n            \"labelspacing\": 0.8,\n            \"frameon\": False,\n            \"handletextpad\": 0,\n            \"title\": kwargs.get(\"bus_ref_title\", \"UNDEFINED TITLE\"),\n        }\n\n        add_legend_circles(\n            ax,\n            ref_sizes,\n            labels,\n            srid=network.srid,\n            patch_kw=dict(facecolor=\"lightgrey\"),\n            legend_kw=legend_kw,\n        )\n\n    fig.tight_layout()\n\n    return ax\n</code></pre>"},{"location":"reference/plot_network/#plot_network.plot_nodal_prices","title":"<code>plot_nodal_prices(network, opts, carrier='AC', save_path=None)</code>","text":"<p>A map plot of energy, either AC or heat</p> <p>Parameters:</p> Name Type Description Default <code>network</code> <code>Network</code> <p>the pyPSA network object</p> required <code>opts</code> <code>dict</code> <p>the plotting options (snakemake.config[\"plotting\"])</p> required <code>save_path</code> <code>PathLike</code> <p>Fig outp path. Defaults to None (no save).</p> <code>None</code> <code>carrier</code> <code>str</code> <p>the energy carrier. Defaults to \"AC\".</p> <code>'AC'</code> <p>raises:     ValueError: if carrier is not AC or heat</p> Source code in <code>workflow/scripts/plot_network.py</code> <pre><code>def plot_nodal_prices(\n    network: pypsa.Network,\n    opts: dict,\n    carrier=\"AC\",\n    save_path: os.PathLike = None,\n):\n    \"\"\"A map plot of energy, either AC or heat\n\n    Args:\n        network (pypsa.Network): the pyPSA network object\n        opts (dict): the plotting options (snakemake.config[\"plotting\"])\n        save_path (os.PathLike, optional): Fig outp path. Defaults to None (no save).\n        carrier (str, optional): the energy carrier. Defaults to \"AC\".\n    raises:\n        ValueError: if carrier is not AC or heat\n    \"\"\"\n    if carrier not in [\"AC\", \"heat\"]:\n        raise ValueError(\"Carrier must be either 'AC' or 'heat'\")\n\n    # demand weighed prices per node\n    nodal_prices = (\n        network.statistics.revenue(\n            groupby=pypsa.statistics.get_bus_and_carrier_and_bus_carrier,\n            comps=\"Load\",\n            bus_carrier=carrier,\n        )\n        / network.statistics.withdrawal(\n            comps=\"Load\",\n            groupby=pypsa.statistics.get_bus_and_carrier_and_bus_carrier,\n            bus_carrier=carrier,\n        )\n        * -1\n    )\n    # drop the carrier and bus_carrier, map to colors\n    nodal_prices = nodal_prices.droplevel(1).droplevel(1)\n    norm = plt.Normalize(vmin=nodal_prices.min(), vmax=nodal_prices.max())\n    cmap = plt.get_cmap(\"plasma\")\n    bus_colors = nodal_prices.map(lambda x: cmap(norm(x)))\n\n    energy_consum = network.statistics.withdrawal(\n        groupby=pypsa.statistics.get_bus_and_carrier,\n        bus_carrier=carrier,\n        comps=[\"Load\"],\n    )\n    consum_pies = energy_consum.groupby(level=1).sum()\n\n    # Make figure\n    fig, ax = plt.subplots(subplot_kw={\"projection\": ccrs.PlateCarree()})\n    fig.set_size_inches(opts[\"price_map\"][\"figsize\"])\n    # get colors\n\n    # TODO make line handling nicer\n    # make sure plot isnt overpopulated\n    def calc_plot_width(row, carrier=\"AC\"):\n        if row.length == 0:\n            return 0\n        elif row.carrier != carrier:\n            return 0\n        else:\n            return row.p_nom_opt\n\n    line_lower_threshold = opts.get(\"min_edge_capacity\", 500)\n    edge_carrier = \"H2\" if carrier == \"heat\" else \"AC\"\n    link_plot_w = network.links.apply(lambda row: calc_plot_width(row, edge_carrier), axis=1)\n    edges = pd.concat([network.lines.s_nom_opt, link_plot_w])\n    edge_widths = edges.clip(line_lower_threshold, edges.max()).replace(line_lower_threshold, 0)\n\n    bus_size_factor = opts[\"price_map\"][\"bus_size_factor\"]\n    linewidth_factor = opts[\"price_map\"][f\"linewidth_factor{\"_heat\" if carrier == 'heat' else ''}\"]\n    plot_map(\n        network,\n        tech_colors=None,\n        edge_widths=edge_widths / linewidth_factor,\n        bus_colors=bus_colors,\n        bus_sizes=consum_pies / bus_size_factor,\n        edge_colors=opts[\"price_map\"][\"edge_color\"],\n        ax=ax,\n        edge_unit_conv=PLOT_CAP_UNITS,\n        bus_unit_conv=PLOT_SUPPLY_UNITS,\n        add_legend=False,\n        **opts[\"price_map\"],\n    )\n\n    # Add colorbar based on bus_colors\n    # fig.tight_layout()\n    fig.subplots_adjust(right=0.85)\n    cax = fig.add_axes([0.87, ax.get_position().y0, 0.02, ax.get_position().height])\n    sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n    sm.set_array([])\n    cbar = plt.colorbar(sm, cax=cax, orientation=\"vertical\")\n    cbar.set_label(f\"Nodal Prices ${CURRENCY}/MWh\")\n\n    if save_path:\n        fig.savefig(save_path, transparent=opts[\"transparent\"], bbox_inches=\"tight\")\n</code></pre>"},{"location":"reference/plot_network_heat/","title":"Plot network heat","text":"<p>Legacy functions that are not currently included in the workflow</p>"},{"location":"reference/plot_statistics/","title":"Plot statistics","text":""},{"location":"reference/plot_statistics/#plot_statistics.add_second_xaxis","title":"<code>add_second_xaxis(data, ax, label, **kwargs)</code>","text":"<p>Add a secondary X-axis to the plot. Args:     data (pd.Series): The data to plot. Its values will be plotted on the secondary X-axis.     ax (matplotlib.axes.Axes): The main matplotlib Axes object.     label (str): The label for the secondary X-axis.     **kwargs: Optional keyword arguments for plot styling.</p> Source code in <code>workflow/scripts/plot_statistics.py</code> <pre><code>def add_second_xaxis(data: pd.Series, ax, label, **kwargs):\n    \"\"\"\n    Add a secondary X-axis to the plot.\n    Args:\n        data (pd.Series): The data to plot. Its values will be plotted on the secondary X-axis.\n        ax (matplotlib.axes.Axes): The main matplotlib Axes object.\n        label (str): The label for the secondary X-axis.\n        **kwargs: Optional keyword arguments for plot styling.\n    \"\"\"\n    defaults = {\"color\": \"red\", \"text_offset\": 0.5, \"markersize\": 8, \"fontsize\": 9}\n    kwargs.update(defaults)\n\n    ax2 = ax.twiny()\n    # # y_pos creates a sequence of integers (e.g., [0, 1, 2, 3]) to serve as distinct vertical positions\n    # for each data point on the shared Y-axis. This is necessary because data.values are plotted\n    # horizontally on the secondary X-axis (ax2), requiring vertical separation for clarity.\n    y_pos = range(len(data))\n\n    ax2.plot(\n        data.values,\n        y_pos,\n        marker=\"o\",\n        linestyle=\"\",\n        color=kwargs[\"color\"],\n        markersize=kwargs[\"markersize\"],\n        label=\"Generation Share (%)\",\n    )\n\n    for i, val in enumerate(data.values):\n        ax2.text(\n            val + kwargs[\"text_offset\"],\n            i,\n            f\"{val:.1f}%\",\n            color=kwargs[\"color\"],\n            va=\"center\",\n            ha=\"left\",\n            fontsize=kwargs[\"fontsize\"],\n        )\n\n    ax2.set_xlim(left=0)\n    ax2.set_xlabel(label)\n    ax2.grid(False)\n    ax2.tick_params(axis=\"x\", labelsize=kwargs[\"fontsize\"])  # Remove color setting for ticks\n\n    return ax2\n</code></pre>"},{"location":"reference/plot_statistics/#plot_statistics.plot_capacity_factor","title":"<code>plot_capacity_factor(cf_filtered, theo_cf_filtered, ax, colors, **kwargs)</code>","text":"<p>Plot actual and theoretical capacity factors for each technology.</p> <p>Parameters:</p> Name Type Description Default <code>cf_filtered</code> <code>Series</code> <p>Actual capacity factors indexed by technology.</p> required <code>theo_cf_filtered</code> <code>Series</code> <p>Theoretical capacity factors indexed by technology.</p> required <code>ax</code> <code>Axes</code> <p>The axis to plot on.</p> required <code>colors</code> <code>dict</code> <p>Color mapping for technologies.</p> required <p>Returns:</p> Type Description <p>matplotlib.axes.Axes: The axis with the plot.</p> Source code in <code>workflow/scripts/plot_statistics.py</code> <pre><code>def plot_capacity_factor(\n    cf_filtered: pd.Series, theo_cf_filtered: pd.Series, ax: axes.Axes, colors: dict, **kwargs\n):\n    \"\"\"\n    Plot actual and theoretical capacity factors for each technology.\n\n    Args:\n        cf_filtered (pd.Series): Actual capacity factors indexed by technology.\n        theo_cf_filtered (pd.Series): Theoretical capacity factors indexed by technology.\n        ax (matplotlib.axes.Axes): The axis to plot on.\n        colors (dict): Color mapping for technologies.\n\n    Returns:\n        matplotlib.axes.Axes: The axis with the plot.\n    \"\"\"\n    x_pos = range(len(cf_filtered))\n    width = 0.35\n\n    ax.barh(\n        [i - width / 2 for i in x_pos],\n        cf_filtered.values,\n        width,\n        color=[colors.get(tech, \"lightgrey\") for tech in cf_filtered.index],\n        alpha=0.8,\n        label=\"Actual CF\",\n    )\n    ax.barh(\n        [i + width / 2 for i in x_pos],\n        theo_cf_filtered.values,\n        width,\n        color=[colors.get(tech, \"lightgrey\") for tech in theo_cf_filtered.index],\n        alpha=0.4,\n        label=\"Theoretical CF\",\n    )\n\n    for i, (tech, cf_val) in enumerate(cf_filtered.items()):\n        ax.text(\n            cf_val + 0.01,\n            i - width / 2,\n            f\"{cf_val:.2f}\",\n            va=\"center\",\n            ha=\"left\",\n            fontsize=8,\n            bbox=dict(boxstyle=\"round,pad=0.2\", facecolor=\"white\", alpha=0.8),\n        )\n        theo_val = theo_cf_filtered.get(tech, 0)\n        ax.text(\n            theo_val + 0.01,\n            i + width / 2,\n            f\"{theo_val:.2f}\",\n            va=\"center\",\n            ha=\"left\",\n            fontsize=8,\n            bbox=dict(boxstyle=\"round,pad=0.2\", facecolor=\"white\", alpha=0.5),\n        )\n\n    ax.set_yticks(list(x_pos))\n    ax.set_yticklabels(cf_filtered.index)\n    ax.set_xlabel(\"Capacity Factor\")\n    ax.set_xlim(0, max(cf_filtered.max(), theo_cf_filtered.max()) * 1.1)\n    ax.grid(False)\n    ax.legend()\n\n    return ax\n</code></pre>"},{"location":"reference/plot_statistics/#plot_statistics.plot_province_peakload_capacity","title":"<code>plot_province_peakload_capacity(df_plot, bar_cols, color_list, outp_dir)</code>","text":"<p>Plot province peak load vs installed capacity by technology. Args:     df_plot: DataFrame with provinces as index, columns as technologies and 'Peak Load'.     bar_cols: List of technology columns to plot as bars.     color_list: List of colors for each technology.     outp_dir: Output directory for saving the figure.</p> Source code in <code>workflow/scripts/plot_statistics.py</code> <pre><code>def plot_province_peakload_capacity(df_plot, bar_cols, color_list, outp_dir):\n    \"\"\"\n    Plot province peak load vs installed capacity by technology.\n    Args:\n        df_plot: DataFrame with provinces as index, columns as technologies and 'Peak Load'.\n        bar_cols: List of technology columns to plot as bars.\n        color_list: List of colors for each technology.\n        outp_dir: Output directory for saving the figure.\n    \"\"\"\n    fig, ax = plt.subplots(figsize=(14, 8))\n    df_plot[bar_cols].plot(kind=\"barh\", stacked=True, ax=ax, color=color_list, alpha=0.8)\n    # Plot peak load as red vertical line\n    for i, prov in enumerate(df_plot.index):\n        ax.plot(\n            df_plot.loc[prov, \"Peak Load\"],\n            i,\n            \"r|\",\n            markersize=18,\n            label=\"Peak Load\" if i == 0 else \"\",\n        )\n    ax.set_xlabel(\"Capacity [GW]\")\n    ax.set_ylabel(\"Province\")\n    ax.set_title(\"Peak Load vs Installed Capacity by Province\")\n    ax.grid(False)\n    # Only keep one Peak Load legend\n    handles, labels = ax.get_legend_handles_labels()\n    seen = set()\n    new_handles, new_labels = [], []\n    for h, l in zip(handles, labels):\n        if l not in seen:\n            new_handles.append(h)\n            new_labels.append(l)\n            seen.add(l)\n    ax.legend(new_handles, new_labels, loc=\"best\")\n    fig.tight_layout()\n    fig.savefig(os.path.join(outp_dir, \"province_peakload_capacity.png\"))\n</code></pre>"},{"location":"reference/plot_statistics/#plot_statistics.plot_static_per_carrier","title":"<code>plot_static_per_carrier(ds, ax, colors, drop_zero_vals=True, add_labels=True)</code>","text":"<p>Generic function to plot different statics</p> <p>Parameters:</p> Name Type Description Default <code>ds</code> <code>DataFrame</code> <p>the data to plot</p> required <code>ax</code> <code>Axes</code> <p>plotting axes</p> required <code>colors</code> <code>DataFrame</code> <p>colors for the carriers</p> required <code>drop_zero_vals</code> <code>bool</code> <p>Drop zeroes from data. Defaults to True.</p> <code>True</code> <code>add_labels</code> <code>bool</code> <p>Add value labels on bars. If None, reads from config. Defaults to None.</p> <code>True</code> Source code in <code>workflow/scripts/plot_statistics.py</code> <pre><code>def plot_static_per_carrier(\n    ds: DataFrame, ax: axes.Axes, colors: DataFrame, drop_zero_vals=True, add_labels=True\n):\n    \"\"\"Generic function to plot different statics\n\n    Args:\n        ds (DataFrame): the data to plot\n        ax (matplotlib.axes.Axes): plotting axes\n        colors (DataFrame): colors for the carriers\n        drop_zero_vals (bool, optional): Drop zeroes from data. Defaults to True.\n        add_labels (bool, optional): Add value labels on bars. If None, reads from config. Defaults to None.\n    \"\"\"\n    if drop_zero_vals:\n        ds = ds[ds != 0]\n    ds = ds.dropna()\n    c = colors[ds.index.get_level_values(\"carrier\")]\n    ds = ds.pipe(rename_index)\n    label = f\"{ds.attrs['name']} [{ds.attrs['unit']}]\"\n    ds.plot.barh(color=c.values, xlabel=label, ax=ax)\n    if add_labels:\n        for i, (index, value) in enumerate(ds.items()):\n            ax.text(value, i, f\"{value:.1f}\", va=\"center\", ha=\"left\", fontsize=8)\n    ax.grid(axis=\"y\")\n</code></pre>"},{"location":"reference/plot_statistics/#plot_statistics.prepare_capacity_factor_data","title":"<code>prepare_capacity_factor_data(n, carrier)</code>","text":"<p>Prepare Series for actual and theoretical capacity factors per technology. Returns:     cf_filtered: Series of actual capacity factors (index: nice_name)     theo_cf_filtered: Series of theoretical capacity factors (index: nice_name)</p> Source code in <code>workflow/scripts/plot_statistics.py</code> <pre><code>def prepare_capacity_factor_data(n, carrier):\n    \"\"\"\n    Prepare Series for actual and theoretical capacity factors per technology.\n    Returns:\n        cf_filtered: Series of actual capacity factors (index: nice_name)\n        theo_cf_filtered: Series of theoretical capacity factors (index: nice_name)\n    \"\"\"\n    cf_data = n.statistics.capacity_factor(groupby=[\"carrier\"]).dropna()\n    if (\"Link\", \"battery\") in cf_data.index:\n        cf_data.loc[(\"Link\", \"battery charger\")] = cf_data.loc[(\"Link\", \"battery\")]\n        cf_data.drop(index=(\"Link\", \"battery\"), inplace=True)\n    cf_data = cf_data.groupby(level=1).mean()\n\n    # Theoretical capacity factor\n    gen = n.generators.copy()\n    p_max_pu = n.generators_t.p_max_pu\n    gen[\"p_nom_used\"] = gen[\"p_nom_opt\"].fillna(gen[\"p_nom\"])\n    weighted_energy_per_gen = (p_max_pu * gen[\"p_nom_used\"]).sum()\n    gen[\"weighted_energy\"] = weighted_energy_per_gen\n\n    gen[\"nice_name\"] = gen[\"carrier\"].map(\n        lambda x: n.carriers.loc[x, \"nice_name\"] if x in n.carriers.index else x\n    )\n    grouped_energy = gen.groupby(\"nice_name\")[\"weighted_energy\"].sum()\n    grouped_capacity = gen.groupby(\"nice_name\")[\"p_nom_used\"].sum()\n    theoretical_cf_weighted = grouped_energy / grouped_capacity / len(n.snapshots)\n\n    # Only keep technologies present in both actual and theoretical CF\n    common_techs = cf_data.index.intersection(theoretical_cf_weighted.index)\n    cf_filtered = cf_data.loc[common_techs]\n    theo_cf_filtered = theoretical_cf_weighted.loc[cf_filtered.index]\n    # Todo: use config nondispatchable_techs\n    non_zero_mask = (cf_filtered != 0) &amp; (theo_cf_filtered != 0)\n    cf_filtered = cf_filtered[non_zero_mask]\n    theo_cf_filtered = theo_cf_filtered[non_zero_mask]\n    cf_filtered = cf_filtered.sort_values(ascending=True)\n    theo_cf_filtered = theo_cf_filtered.loc[cf_filtered.index]\n\n    return cf_filtered, theo_cf_filtered\n</code></pre>"},{"location":"reference/plot_statistics/#plot_statistics.prepare_province_peakload_capacity_data","title":"<code>prepare_province_peakload_capacity_data(n, attached_carriers=None)</code>","text":"<p>Prepare DataFrame for province peak load and installed capacity by technology. Returns:     df_plot: DataFrame with provinces as index, columns as technologies and 'Peak Load'.     bar_cols: List of technology columns to plot as bars.     color_list: List of colors for each technology.</p> Source code in <code>workflow/scripts/plot_statistics.py</code> <pre><code>def prepare_province_peakload_capacity_data(n, attached_carriers=None):\n    \"\"\"\n    Prepare DataFrame for province peak load and installed capacity by technology.\n    Returns:\n        df_plot: DataFrame with provinces as index, columns as technologies and 'Peak Load'.\n        bar_cols: List of technology columns to plot as bars.\n        color_list: List of colors for each technology.\n    \"\"\"\n    # Calculate peak load per province\n    load = n.loads.copy()\n    load[\"province\"] = load[\"bus\"].map(n.buses[\"location\"])\n    peak_load = n.loads_t.p_set.groupby(load[\"province\"], axis=1).sum().max()\n    peak_load = peak_load / PLOT_CAP_UNITS  # ensure peak load is in GW\n\n    # Calculate installed capacity per province and technology using optimal_capacity\n    ds = n.statistics.optimal_capacity(groupby=[\"location\", \"carrier\"]).dropna()\n    valid_components = [\"Generator\", \"StorageUnit\", \"Link\"]\n    ds = ds.loc[ds.index.get_level_values(0).isin(valid_components)]\n    if (\"Link\", \"battery\") in ds.index:\n        ds.loc[(\"Link\", \"battery charger\")] = ds.loc[(\"Link\", \"battery\")]\n        ds = ds.drop(index=(\"Link\", \"battery\"))\n    if \"stations\" in ds.index.get_level_values(2):\n        ds = ds.drop(\"stations\", level=2)\n    if \"load shedding\" in ds.index.get_level_values(2):\n        ds = ds.drop(\"load shedding\", level=2)\n    ds = ds.groupby(level=[1, 2]).sum()\n    ds.index = pd.MultiIndex.from_tuples(\n        [\n            (prov, n.carriers.loc[carrier, \"nice_name\"] if carrier in n.carriers.index else carrier)\n            for prov, carrier in ds.index\n        ],\n        names=[\"province\", \"nice_name\"],\n    )\n    cap_by_prov_tech = ds.unstack(level=-1).fillna(0)\n    cap_by_prov_tech = cap_by_prov_tech.abs() / PLOT_CAP_UNITS\n\n    if \"Battery Discharger\" in cap_by_prov_tech.columns:\n        cap_by_prov_tech = cap_by_prov_tech.drop(columns=\"Battery Discharger\")\n    if \"AC\" in cap_by_prov_tech.columns:\n        cap_by_prov_tech = cap_by_prov_tech.drop(columns=\"AC\")\n    # Only keep columns in attached_carriers if provided\n    if attached_carriers is not None:\n        # Ensure nice_name mapping for attached_carriers\n        attached_nice_names = [\n            n.carriers.loc[c, \"nice_name\"] if c in n.carriers.index else c\n            for c in attached_carriers\n        ]\n        cap_by_prov_tech = cap_by_prov_tech[\n            [c for c in cap_by_prov_tech.columns if c in attached_nice_names]\n        ]\n\n    # Merge peak load and capacity\n    df_plot = cap_by_prov_tech.copy()\n    df_plot[\"Peak Load\"] = peak_load\n\n    # Bar columns: exclude Peak Load, only keep nonzero\n    bar_cols = [c for c in df_plot.columns if c != \"Peak Load\"]\n    bar_cols = [c for c in bar_cols if df_plot[c].sum() &gt; 0]\n    color_list = [\n        n.carriers.set_index(\"nice_name\").color.get(tech, \"lightgrey\") for tech in bar_cols\n    ]\n    return df_plot, bar_cols, color_list\n</code></pre>"},{"location":"reference/plot_summary_all/","title":"Plot summary all","text":"<p>Plots energy and cost summaries for solved networks. This script collects functions that plot across planning horizons.</p>"},{"location":"reference/plot_summary_all/#plot_summary_all.plot_capacity_factors","title":"<code>plot_capacity_factors(file_list, config, techs, fig_name=None, ax=None)</code>","text":"<p>Plot evolution of capacity factors for the given technologies</p> <p>Parameters:</p> Name Type Description Default <code>file_list</code> <code>list</code> <p>the input csvs from make_summary</p> required <code>config</code> <code>dict</code> <p>the configuration for plotting (snakemake.config[\"plotting\"])</p> required <code>techs</code> <code>list</code> <p>the technologies to plot</p> required <code>fig_name</code> <code>PathLike</code> <p>the figure name. Defaults to None.</p> <code>None</code> <code>ax</code> <code>Axes</code> <p>the axes to plot on. Defaults to None.</p> <code>None</code> Source code in <code>workflow/scripts/plot_summary_all.py</code> <pre><code>def plot_capacity_factors(\n    file_list: list, config: dict, techs: list, fig_name=None, ax: object = None\n):\n    \"\"\"Plot evolution of capacity factors for the given technologies\n\n    Args:\n        file_list (list): the input csvs from make_summary\n        config (dict): the configuration for plotting (snakemake.config[\"plotting\"])\n        techs (list): the technologies to plot\n        fig_name (os.PathLike, optional): the figure name. Defaults to None.\n        ax (matplotlib.axes.Axes, optional): the axes to plot on. Defaults to None.\n    \"\"\"\n\n    capfacs_df = pd.DataFrame()\n    for results_file in file_list:\n        df_year = pd.read_csv(results_file, index_col=list(range(2)), header=[1]).T\n        capfacs_df = pd.concat([df_year, capfacs_df])\n\n    if (\"links\", \"battery\") in capfacs_df.columns:\n        capfacs_df.loc[:, (\"links\", \"battery charger\")] = capfacs_df.loc[:, (\"links\", \"battery\")]\n        capfacs_df.drop(columns=(\"links\", \"battery\"), inplace=True)\n\n    capfacs_df = capfacs_df.droplevel(0, axis=1).fillna(0)\n    capfacs_df.sort_index(axis=0, inplace=True)\n\n    invalid = [t for t in techs if t not in capfacs_df.columns]\n    logger.warning(f\"Technologies {invalid} not found in capacity factors data. Skipping them.\")\n    valid_techs = [t for t in techs if t in capfacs_df.columns]\n    capfacs_df = capfacs_df[valid_techs]\n\n    if not ax:\n        fig, ax = plt.subplots()\n    else:\n        fig = ax.get_figure()\n    fig.set_size_inches((12, 8))\n\n    colors = pd.Series(config[\"tech_colors\"], index=capfacs_df.columns)\n    # missing color may have had nice name, else NAN default\n    nice_name_colors = pd.Series(\n        config[\"tech_colors\"], index=capfacs_df.columns.map(config[\"nice_names\"])\n    ).dropna()\n    colors = colors.fillna(nice_name_colors).fillna(NAN_COLOR)\n\n    capfacs_df.plot(\n        ax=ax,\n        kind=\"line\",\n        color=colors,\n        linewidth=3,\n        marker=\"o\",\n    )\n    ax.set_ylim([0, capfacs_df.max().max() * 1.1])\n    ax.set_ylabel(\"capacity factor\")\n    ax.set_xlabel(\"\")\n    ax.grid(axis=\"y\")\n\n    handles, labels = ax.get_legend_handles_labels()\n    handles.reverse()\n    labels.reverse()\n    ax.legend(handles, labels, ncol=1, bbox_to_anchor=[1, 1], loc=\"upper left\")\n    fig.tight_layout()\n\n    if fig_name is not None:\n        fig.savefig(fig_name, transparent=False)\n</code></pre>"},{"location":"reference/plot_summary_all/#plot_summary_all.plot_co2_prices","title":"<code>plot_co2_prices(co2_prices, config, fig_name=None)</code>","text":"<p>Plot the CO2 prices Args:     co2_prices (dict): the CO2 prices per year (from the config)     config (dict): the plotting configuration     fig_name (os.PathLike, optional): the figure name. Defaults to None.</p> Source code in <code>workflow/scripts/plot_summary_all.py</code> <pre><code>def plot_co2_prices(co2_prices: dict, config: dict, fig_name=None):\n    \"\"\"Plot the CO2 prices\n    Args:\n        co2_prices (dict): the CO2 prices per year (from the config)\n        config (dict): the plotting configuration\n        fig_name (os.PathLike, optional): the figure name. Defaults to None.\n    \"\"\"\n    fig, ax = plt.subplots()\n    fig.set_size_inches((12, 8))\n\n    ax.plot(\n        co2_prices.keys(),\n        np.abs(list(co2_prices.values())),\n        marker=\"o\",\n        color=\"black\",\n        lw=2,\n    )\n    ax.set_ylabel(\"CO2 price\")\n    ax.set_xlabel(\"Year\")\n    ax.plot(co2_prices.keys(), co2_prices.values(), marker=\"o\", color=\"black\", lw=2)\n\n    fig.tight_layout()\n    if fig_name is not None:\n        fig.savefig(fig_name, transparent=config[\"transparent\"])\n</code></pre>"},{"location":"reference/plot_summary_all/#plot_summary_all.plot_co2_shadow_price","title":"<code>plot_co2_shadow_price(file_list, config, fig_name=None)</code>","text":"<p>plot the co2 price</p> <p>Parameters:</p> Name Type Description Default <code>file_list</code> <code>list</code> <p>the input csvs from make_summaries</p> required <code>config</code> <code>dict</code> <p>the snakemake configuration</p> required <code>fig_name</code> <code>PathLike</code> <p>the figure name. Defaults to None.</p> <code>None</code> Source code in <code>workflow/scripts/plot_summary_all.py</code> <pre><code>def plot_co2_shadow_price(file_list: list, config: dict, fig_name=None):\n    \"\"\"plot the co2 price\n\n    Args:\n        file_list (list): the input csvs from make_summaries\n        config (dict): the snakemake configuration\n        fig_name (os.PathLike, optional): the figure name. Defaults to None.\n    \"\"\"\n    co2_prices = {}\n    co2_budget = {}\n    for i, results_file in enumerate(file_list):\n        df_metrics = pd.read_csv(results_file, index_col=list(range(1)), header=[1])\n        co2_prices.update(dict(df_metrics.loc[\"co2_shadow\"]))\n        co2_budget.update(dict(df_metrics.loc[\"co2_budget\"]))\n\n    fig, ax = plt.subplots()\n    fig.set_size_inches((12, 8))\n\n    ax.plot(\n        co2_prices.keys(),\n        np.abs(list(co2_prices.values())),\n        marker=\"o\",\n        color=\"black\",\n        lw=2,\n    )\n    ax.set_ylabel(\"CO2 Shadow price\")\n    ax.set_xlabel(\"Year\")\n\n    ax2 = ax.twinx()\n    ax2.plot(\n        co2_budget.keys(),\n        [v / PLOT_CO2_UNITS for v in co2_budget.values()],\n        marker=\"D\",\n        color=\"blue\",\n        lw=2,\n    )\n    ax2.set_ylabel(f\"CO2 Budget [{PLOT_CO2_LABEL}]\", color=\"blue\")\n    ax2.tick_params(axis=\"y\", colors=\"blue\")\n\n    fig.tight_layout()\n\n    if fig_name is not None:\n        fig.savefig(fig_name, transparent=config[\"transparent\"])\n</code></pre>"},{"location":"reference/plot_summary_all/#plot_summary_all.plot_electricty_heat_balance","title":"<code>plot_electricty_heat_balance(file_list, config, fig_dir=None, plot_heat=True)</code>","text":"<p>plot the energy production and consumption</p> <p>Parameters:</p> Name Type Description Default <code>file_list</code> <code>list</code> <p>the input csvs  from make_dirs([year/supply_energy.csv])</p> required <code>config</code> <code>dict</code> <p>the configuration for plotting (snamkemake.config[\"plotting\"])</p> required <code>fig_dir</code> <code>PathLike</code> <p>the figure name. Defaults to None.</p> <code>None</code> <code>plot_heat</code> <code>bool</code> <p>plot heat balances. Defaults to True.</p> <code>True</code> Source code in <code>workflow/scripts/plot_summary_all.py</code> <pre><code>def plot_electricty_heat_balance(\n    file_list: list[os.PathLike], config: dict, fig_dir=None, plot_heat=True\n):\n    \"\"\"plot the energy production and consumption\n\n    Args:\n        file_list (list): the input csvs  from make_dirs([year/supply_energy.csv])\n        config (dict): the configuration for plotting (snamkemake.config[\"plotting\"])\n        fig_dir (os.PathLike, optional): the figure name. Defaults to None.\n        plot_heat (bool, optional): plot heat balances. Defaults to True.\n    \"\"\"\n    elec_df = pd.DataFrame()\n    heat_df = pd.DataFrame()\n\n    for results_file in file_list:\n        balance_df = pd.read_csv(results_file, index_col=list(range(2)), header=[1])\n        elec = balance_df.loc[\"AC\"].copy()\n        elec.set_index(elec.columns[0], inplace=True)\n        elec.rename(index={\"-\": \"electric load\"}, inplace=True)\n        elec.index.rename(\"carrier\", inplace=True)\n        # this groups subgroups of the same carrier. For example, baseyar hydro = link from dams\n        # but new hydro is generator from province\n        elec = elec.groupby(elec.index).sum()\n        to_drop = elec.index[\n            elec.max(axis=1).abs() &lt; config[\"energy_threshold\"] / PLOT_SUPPLY_UNITS\n        ]\n        elec.loc[\"Other\"] = elec.loc[to_drop].sum(axis=0)\n        elec.drop(to_drop, inplace=True)\n        elec_df = pd.concat([elec, elec_df], axis=1)\n\n        if plot_heat:\n            heat = balance_df.loc[\"heat\"].copy()\n            heat.set_index(heat.columns[0], inplace=True)\n            heat.rename(index={\"-\": \"heat load\"}, inplace=True)\n            heat.index.rename(\"carrier\", inplace=True)\n            heat = heat.groupby(heat.index).sum()\n            to_drop = heat.index[\n                heat.max(axis=1).abs() &lt; config[\"energy_threshold\"] / PLOT_SUPPLY_UNITS\n            ]\n            heat.loc[\"Other\"] = heat.loc[to_drop].sum(axis=0)\n            heat.drop(to_drop, inplace=True)\n            heat_df = pd.concat([heat, heat_df], axis=1)\n        else:\n            heat_df = pd.DataFrame()\n\n    elec_df.fillna(0, inplace=True)\n    elec_df.sort_index(axis=1, inplace=True, ascending=True)\n    elec_df = elec_df / PLOT_SUPPLY_UNITS\n\n    heat_df.fillna(0, inplace=True)\n    heat_df.sort_index(axis=1, inplace=True, ascending=True)\n    heat_df = heat_df / PLOT_SUPPLY_UNITS\n\n    # # split into consumption and generation\n    el_gen = elec_df.where(elec_df &gt;= 0).dropna(axis=0, how=\"all\").fillna(0)\n    el_con = elec_df.where(elec_df &lt; 0).dropna(axis=0, how=\"all\").fillna(0)\n    heat_gen = heat_df.where(heat_df &gt; 0).dropna(axis=0, how=\"all\").fillna(0)\n    heat_con = heat_df.where(heat_df &lt; 0).dropna(axis=0, how=\"all\").fillna(0)\n\n    # group identical values\n    el_con = el_con.groupby(el_con.index).sum()\n    el_gen = el_gen.groupby(el_gen.index).sum()\n    heat_con = heat_con.groupby(heat_con.index).sum()\n    heat_gen = heat_gen.groupby(heat_gen.index).sum()\n\n    logger.info(f\"Total energy of {round(elec_df.sum()[0])} TWh/a\")\n\n    # ===========        electricity =================\n    fig, ax = plt.subplots()\n    fig.set_size_inches((12, 8))\n\n    preferred_order = pd.Index(config[\"preferred_order\"])\n    for df in [el_gen, el_con]:\n        new_index = preferred_order.intersection(df.index).append(\n            df.index.difference(preferred_order)\n        )\n        logger.info(\n            f\"Missing technologies in preferred order: {df.index.difference(preferred_order)}\"\n        )\n\n        colors = pd.DataFrame(\n            new_index.map(config[\"tech_colors\"]), index=new_index, columns=[\"color\"]\n        )\n        colors.fillna(NAN_COLOR, inplace=True)\n        df.loc[new_index].T.plot(\n            kind=\"bar\",\n            ax=ax,\n            stacked=True,\n            color=colors[\"color\"],\n        )\n\n        handles, labels = ax.get_legend_handles_labels()\n        handles.reverse()\n        labels.reverse()\n\n    ax.set_ylim([el_con.sum(axis=0).min() * 1.1, el_gen.sum(axis=0).max() * 1.1])\n    ax.set_ylabel(\"Energy [TWh/a]\")\n    ax.set_xlabel(\"\")\n    ax.grid(axis=\"y\")\n    ax.legend(\n        handles,\n        [l.title() for l in labels],\n        ncol=1,\n        bbox_to_anchor=[1, 1],\n        loc=\"upper left\",\n    )\n\n    if config.get(\"add_bar_labels\", False):\n        label_stacked_bars(ax, len(el_gen.columns))\n\n    fig.tight_layout()\n\n    if fig_dir is not None:\n        fig.savefig(os.path.join(fig_dir, \"elec_balance.png\"), transparent=config[\"transparent\"])\n\n    # =================     heat     =================\n    fig, ax = plt.subplots()\n    fig.set_size_inches((12, 8))\n\n    for df in [heat_gen, heat_con]:\n        if not plot_heat:\n            break\n\n        preferred_order = pd.Index(config[\"preferred_order\"])\n        new_index = preferred_order.intersection(df.index).append(\n            df.index.difference(preferred_order)\n        )\n        colors = pd.DataFrame(\n            new_index.map(config[\"tech_colors\"]), index=new_index, columns=[\"color\"]\n        )\n        colors.fillna(NAN_COLOR, inplace=True)\n        df.loc[new_index].T.plot(\n            kind=\"bar\",\n            ax=ax,\n            stacked=True,\n            color=colors[\"color\"],\n        )\n\n        handles, labels = ax.get_legend_handles_labels()\n        handles.reverse()\n        labels.reverse()\n\n    if plot_heat:\n        ax.set_ylim([heat_con.sum(axis=0).min() * 1.1, heat_gen.sum(axis=0).max() * 1.1])\n        ax.set_ylabel(\"Energy [TWh/a]\")\n        ax.set_xlabel(\"\")\n        ax.grid(axis=\"y\")\n        ax.legend(handles, labels, ncol=1, bbox_to_anchor=[1, 1], loc=\"upper left\")\n        fig.tight_layout()\n\n        if fig_dir is not None:\n            fig.savefig(\n                os.path.join(fig_dir, \"heat_balance.png\"),\n                transparent=config[\"transparent\"],\n            )\n</code></pre>"},{"location":"reference/plot_summary_all/#plot_summary_all.plot_energy","title":"<code>plot_energy(file_list, config, fig_name=None)</code>","text":"<p>plot the energy production and consumption</p> <p>Parameters:</p> Name Type Description Default <code>file_list</code> <code>list</code> <p>the input csvs</p> required <code>config</code> <code>dict</code> <p>the configuration for plotting (snamkemake.config[\"plotting\"])</p> required <code>fig_name</code> <code>PathLike</code> <p>the figure name. Defaults to None.</p> <code>None</code> Source code in <code>workflow/scripts/plot_summary_all.py</code> <pre><code>def plot_energy(file_list: list, config: dict, fig_name=None):\n    \"\"\"plot the energy production and consumption\n\n    Args:\n        file_list (list): the input csvs\n        config (dict): the configuration for plotting (snamkemake.config[\"plotting\"])\n        fig_name (os.PathLike, optional): the figure name. Defaults to None.\n    \"\"\"\n    energy_df = pd.DataFrame()\n    for results_file in file_list:\n        en_df = pd.read_csv(results_file, index_col=list(range(2)), header=[1])\n        df_ = en_df.groupby(en_df.index.get_level_values(1)).sum()\n        # do this here so aggregate costs of small items only for that year\n        # convert MWh to TWh\n        df_ = df_ / PLOT_SUPPLY_UNITS\n        df_ = df_.groupby(df_.index.map(rename_techs)).sum()\n        to_drop = df_.index[df_.max(axis=1) &lt; config[\"energy_threshold\"] / PLOT_SUPPLY_UNITS]\n        df_.loc[\"Other\"] = df_.loc[to_drop].sum(axis=0)\n        df_ = df_.drop(to_drop)\n\n        energy_df = pd.concat([df_, energy_df], axis=1)\n    energy_df.fillna(0, inplace=True)\n    energy_df.sort_index(axis=1, inplace=True)\n\n    logger.info(f\"Total energy of {round(energy_df.sum()[0])} {PLOT_SUPPLY_LABEL}/a\")\n    preferred_order = pd.Index(config[\"preferred_order\"])\n    new_index = preferred_order.intersection(energy_df.index).append(\n        energy_df.index.difference(preferred_order)\n    )\n    new_columns = energy_df.columns.sort_values()\n\n    fig, ax = plt.subplots()\n    fig.set_size_inches((12, 8))\n\n    logger.debug(energy_df.loc[new_index, new_columns])\n\n    energy_df.loc[new_index, new_columns].T.plot(\n        kind=\"bar\",\n        ax=ax,\n        stacked=True,\n        color=[config[\"tech_colors\"][i] for i in new_index],\n    )\n\n    handles, labels = ax.get_legend_handles_labels()\n\n    handles.reverse()\n    labels.reverse()\n\n    ax.set_ylim([0, energy_df.sum(axis=0).max() * 1.1])\n    ax.set_ylabel(f\"Energy [{PLOT_SUPPLY_LABEL}/a]\")\n    ax.set_xlabel(\"\")\n    ax.grid(axis=\"y\")\n    ax.legend(handles, labels, ncol=1, bbox_to_anchor=[1, 1], loc=\"upper left\")\n    fig.tight_layout()\n\n    if fig_name is not None:\n        fig.savefig(fig_name, transparent=config[\"transparent\"])\n</code></pre>"},{"location":"reference/plot_summary_all/#plot_summary_all.plot_expanded_capacities","title":"<code>plot_expanded_capacities(file_list, config, plot_heat=False, plot_h2=True, fig_name=None)</code>","text":"<p>plot the expanded capacities</p> <p>Parameters:</p> Name Type Description Default <code>file_list</code> <code>list</code> <p>the input csvs from make_summary</p> required <code>config</code> <code>dict</code> <p>the configuration for plotting (snakemake.config[\"plotting\"])</p> required <code>fig_name</code> <code>PathLike</code> <p>the figure name. Defaults to None.</p> <code>None</code> <code>plot_heat</code> <code>bool</code> <p>plot heat capacities. Defaults to True.</p> <code>False</code> <code>plot_h2</code> <code>bool</code> <p>plot hydrogen capacities. Defaults to True.</p> <code>True</code> Source code in <code>workflow/scripts/plot_summary_all.py</code> <pre><code>def plot_expanded_capacities(\n    file_list: list, config: dict, plot_heat=False, plot_h2=True, fig_name=None\n):\n    \"\"\"plot the expanded capacities\n\n    Args:\n        file_list (list): the input csvs from make_summary\n        config (dict): the configuration for plotting (snakemake.config[\"plotting\"])\n        fig_name (os.PathLike, optional): the figure name. Defaults to None.\n        plot_heat (bool, optional): plot heat capacities. Defaults to True.\n        plot_h2 (bool, optional): plot hydrogen capacities. Defaults to True.\n    \"\"\"\n\n    fig, axes = plot_pathway_capacities(file_list, config, plot_heat, plot_h2, fig_name=None)\n    for i, ax in enumerate(axes.flat):\n        ylabel = ax.get_ylabel()\n        if \"Installed\" in ylabel:\n            ax.set_ylabel(ylabel.replace(\"Installed\", \"Additional\"))\n\n    if fig_name is not None:\n        fig.savefig(fig_name, transparent=config[\"transparent\"])\n</code></pre>"},{"location":"reference/plot_summary_all/#plot_summary_all.plot_pathway_capacities","title":"<code>plot_pathway_capacities(file_list, config, plot_heat=True, plot_h2=True, fig_name=None)</code>","text":"<p>plot the capacities</p> <p>Parameters:</p> Name Type Description Default <code>file_list</code> <code>list</code> <p>the input csvs from make_summary</p> required <code>config</code> <code>dict</code> <p>the configuration for plotting (snakemake.config[\"plotting\"])</p> required <code>fig_name</code> <code>PathLike</code> <p>the figure name. Defaults to None.</p> <code>None</code> <code>plot_heat</code> <code>bool</code> <p>plot heat capacities. Defaults to True.</p> <code>True</code> <code>plot_h2</code> <code>bool</code> <p>plot hydrogen capacities. Defaults to True.</p> <code>True</code> Source code in <code>workflow/scripts/plot_summary_all.py</code> <pre><code>def plot_pathway_capacities(\n    file_list: list, config: dict, plot_heat=True, plot_h2=True, fig_name=None\n):\n    \"\"\"plot the capacities\n\n    Args:\n        file_list (list): the input csvs from make_summary\n        config (dict): the configuration for plotting (snakemake.config[\"plotting\"])\n        fig_name (os.PathLike, optional): the figure name. Defaults to None.\n        plot_heat (bool, optional): plot heat capacities. Defaults to True.\n        plot_h2 (bool, optional): plot hydrogen capacities. Defaults to True.\n    \"\"\"\n\n    caps_heat, caps_h2, caps_ac, caps_stores = (\n        pd.DataFrame(),\n        pd.DataFrame(),\n        pd.DataFrame(),\n        pd.DataFrame(),\n    )\n    # loop over each year result\n    for results_file in file_list:\n        cap_df = pd.read_csv(results_file, index_col=list(range(4)), header=[1, 2])\n        # format table\n        cap_df.index.names = [\"component\", \"carrier\", \"bus_carrier\", \"end_carrier\"]\n        year = cap_df.columns.get_level_values(0)[0]\n        cap_df = cap_df.droplevel(0, axis=1).rename(columns={\"Unnamed: 4_level_1\": year})\n        cap_df /= PLOT_CAP_UNITS\n        if \"Load Shedding\" in cap_df.index.get_level_values(\"carrier\"):\n            cap_df.drop(\"Load Shedding\", level=\"carrier\", inplace=True)\n\n        # get stores relevant for reporting according to config, use later\n        stores = (\n            cap_df[\n                (cap_df.index.get_level_values(0) == \"Store\")\n                &amp; (cap_df.index.get_level_values(1).isin(config[\"capacity_tracking\"][\"stores\"]))\n            ]\n            .groupby(level=1)\n            .sum()\n        )\n\n        # drop stores from cap df\n        cap_df.drop(cap_df[cap_df.index.get_level_values(0) == \"Store\"].index, inplace=True)\n        # drop charger/dischargers for stores\n        cap_df.drop(\n            cap_df[\n                (cap_df.index.get_level_values(0) == \"Link\")\n                &amp; (cap_df.index.get_level_values(1).isin(config[\"capacity_tracking\"][\"drop_links\"]))\n            ].index,\n            inplace=True,\n        )\n\n        # select AC (important for links) and group\n        cap_ac = cap_df.reset_index().query(\n            \"bus_carrier == 'AC' | carrier =='AC' | end_carrier =='AC'\"\n        )\n        cap_ac = cap_ac.groupby(\"carrier\").sum()[year]\n\n        cap_h2 = pd.DataFrame()\n        if plot_h2:\n            cap_h2 = cap_df.reset_index().query(\n                \"bus_carrier == 'H2' | carrier =='H2' | end_carrier =='H2'\"\n            )\n            cap_h2 = cap_h2.groupby(\"carrier\").sum()[year]\n            if caps_h2.empty:\n                caps_h2 = cap_h2\n            else:\n                caps_h2 = pd.concat([caps_h2, cap_h2], axis=1).fillna(0)\n        if plot_heat:\n            # TODO issue for CHP in case of several end buses. Bus2 will not be caught\n            cap_heat = cap_df.reset_index().query(\n                \"bus_carrier == 'heat' | carrier =='heat' | end_carrier =='heat'\"\n            )\n            cap_heat = cap_heat.groupby(\"carrier\").sum()[year]\n            if caps_heat.empty:\n                caps_heat = cap_heat\n            else:\n                caps_heat = pd.concat([caps_heat, cap_h2], axis=1).fillna(0)\n\n        caps_stores = pd.concat([stores, caps_stores], axis=1).fillna(0)\n        if caps_ac.empty:\n            caps_ac = cap_ac\n        else:\n            caps_ac = pd.concat([cap_ac, caps_ac], axis=1).fillna(0)\n\n    fig, axes = plt.subplots(2, 2)\n    fig.set_size_inches((14, 15))\n\n    for i, capacity_df in enumerate([caps_ac, caps_heat, caps_stores, caps_h2]):\n        if capacity_df.empty:\n            continue\n        if isinstance(capacity_df, pd.Series):\n            capacity_df = capacity_df.to_frame()\n        k, j = divmod(i, 2)\n        ax = axes[k, j]\n        preferred_order = pd.Index(config[\"preferred_order\"])\n        new_index = preferred_order.intersection(capacity_df.index).append(\n            capacity_df.index.difference(preferred_order)\n        )\n        new_columns = capacity_df.columns.sort_values()\n\n        logger.debug(capacity_df.loc[new_index, new_columns])\n\n        capacity_df.loc[new_index, new_columns].T.plot(\n            kind=\"bar\",\n            ax=ax,\n            stacked=True,\n            color=[config[\"tech_colors\"][i] for i in new_index],\n        )\n\n        handles, labels = ax.get_legend_handles_labels()\n\n        handles.reverse()\n        labels.reverse()\n\n        if capacity_df.index.difference(caps_stores.index).empty:\n            ax.set_ylabel(f\"Installed Storage Capacity [{PLOT_CAP_LABEL}h]\")\n        else:\n            ax.set_ylabel(f\"Installed Capacity [{PLOT_CAP_LABEL}]\")\n        ax.set_ylim([0, capacity_df.sum(axis=0).max() * 1.1])\n        ax.set_xlabel(\"\")\n        ax.grid(axis=\"y\")\n        # ax.yaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: f\"{x:.1e}\"))\n        ax.legend(handles, labels, ncol=2, bbox_to_anchor=(0.5, -0.15), loc=\"upper center\")\n    fig.tight_layout()\n    fig.subplots_adjust(wspace=0.42)\n\n    if fig_name is not None:\n        fig.savefig(fig_name, transparent=config[\"transparent\"])\n\n    return fig, axes\n</code></pre>"},{"location":"reference/plot_summary_all/#plot_summary_all.plot_pathway_co2","title":"<code>plot_pathway_co2(file_list, config, fig_name=None)</code>","text":"<p>Plot the CO2 pathway balance and totals</p> <p>Parameters:</p> Name Type Description Default <code>file_list</code> <code>list</code> <p>the input csvs</p> required <code>config</code> <code>dict</code> <p>the plotting configuration</p> required <code>fig_name</code> <code>_type_</code> <p>description. Defaults to None.</p> <code>None</code> Source code in <code>workflow/scripts/plot_summary_all.py</code> <pre><code>def plot_pathway_co2(file_list: list, config: dict, fig_name=None):\n    \"\"\"Plot the CO2 pathway balance and totals\n\n    Args:\n        file_list (list): the input csvs\n        config (dict): the plotting configuration\n        fig_name (_type_, optional): _description_. Defaults to None.\n    \"\"\"\n\n    co2_balance_df = pd.DataFrame()\n    for results_file in file_list:\n        df_year = pd.read_csv(results_file, index_col=list(range(1)), header=[1]).T\n        co2_balance_df = pd.concat([df_year, co2_balance_df])\n\n    co2_balance_df.sort_index(axis=0, inplace=True)\n\n    fig, ax = plt.subplots()\n    bar_width = 0.6\n    colors = co2_balance_df.T.index.map(config[\"tech_colors\"]).values\n    co2_balance_df = co2_balance_df / PLOT_CO2_UNITS\n    co2_balance_df.plot(\n        kind=\"bar\",\n        stacked=True,\n        width=bar_width,\n        color=pd.Series(colors).fillna(NAN_COLOR),\n        ax=ax,\n    )\n    bar_centers = np.unique([patch.get_x() + bar_width / 2 for patch in ax.patches])\n    ax.plot(\n        bar_centers,\n        co2_balance_df.sum(axis=1).values,\n        color=\"black\",\n        marker=\"D\",\n        markersize=10,\n        lw=3,\n        label=\"Total\",\n    )\n    ax.set_ylabel(PLOT_CO2_LABEL)\n\n    handles, labels = ax.get_legend_handles_labels()\n    ax.legend(handles, labels, ncol=1, bbox_to_anchor=[1, 1], loc=\"upper left\")\n    ax.set_ylim([0, co2_balance_df.sum(axis=1).max() * 1.1])\n    fig.tight_layout()\n    if fig_name is not None:\n        fig.savefig(fig_name, transparent=config[\"transparent\"])\n</code></pre>"},{"location":"reference/plot_summary_all/#plot_summary_all.plot_pathway_costs","title":"<code>plot_pathway_costs(file_list, config, social_discount_rate=0.0, fig_name=None)</code>","text":"<p>plot the costs</p> <p>Parameters:</p> Name Type Description Default <code>file_list</code> <code>list</code> <p>the input csvs from make_summary</p> required <code>config</code> <code>dict</code> <p>the configuration for plotting (snakemake.config[\"plotting\"])</p> required <code>social_discount_rate</code> <code>float</code> <p>the social discount rate (0.02). Defaults to 0.0.</p> <code>0.0</code> <code>fig_name</code> <code>PathLike</code> <p>the figure name. Defaults to None.</p> <code>None</code> Source code in <code>workflow/scripts/plot_summary_all.py</code> <pre><code>def plot_pathway_costs(\n    file_list: list,\n    config: dict,\n    social_discount_rate=0.0,\n    fig_name: os.PathLike = None,\n):\n    \"\"\"plot the costs\n\n    Args:\n        file_list (list): the input csvs from make_summary\n        config (dict): the configuration for plotting (snakemake.config[\"plotting\"])\n        social_discount_rate (float, optional): the social discount rate (0.02). Defaults to 0.0.\n        fig_name (os.PathLike, optional): the figure name. Defaults to None.\n    \"\"\"\n    # all years in one df\n    df = pd.DataFrame()\n    for results_file in file_list:\n        cost_df = pd.read_csv(results_file, index_col=list(range(3)), header=[1])\n        df_ = cost_df.groupby(cost_df.index.get_level_values(2)).sum()\n        # do this here so aggregate costs of small items only for that year\n        df_ = df_ * COST_UNIT / PLOT_COST_UNITS\n        df_ = df_.groupby(df_.index.map(rename_techs)).sum()\n        to_drop = df_.index[df_.max(axis=1) &lt; config[\"costs_threshold\"] / PLOT_COST_UNITS]\n        df_.loc[\"Other\"] = df_.loc[to_drop].sum(axis=0)\n        df_ = df_.drop(to_drop)\n        df = pd.concat([df_, df], axis=1)\n\n    df.fillna(0, inplace=True)\n    df.rename(columns={int(y): y for y in df.columns}, inplace=True)\n    df.sort_index(axis=1, inplace=True, ascending=True)\n\n    # apply social discount rate\n    if social_discount_rate &gt; 0:\n        base_year = min([int(y) for y in df.columns])\n        df = df.apply(\n            lambda x: x / (1 + social_discount_rate) ** (int(x.name) - base_year),\n            axis=0,\n        )\n    elif social_discount_rate &lt; 0:\n        raise ValueError(\"Social discount rate must be positive\")\n\n    preferred_order = pd.Index(config[\"preferred_order\"])\n    new_index = preferred_order.intersection(df.index).append(df.index.difference(preferred_order))\n    logger.info(f\"Missing technologies in preferred order: {df.index.difference(preferred_order)}\")\n    fig, ax = plt.subplots()\n    fig.set_size_inches((12, 8))\n\n    df.loc[new_index].T.plot(\n        kind=\"bar\",\n        ax=ax,\n        stacked=True,\n        color=[config[\"tech_colors\"][i] for i in new_index],\n    )\n\n    handles, labels = ax.get_legend_handles_labels()\n\n    ax.set_ylim([0, df.sum(axis=0).max() * 1.1])\n    ax.set_ylabel(\"System Cost [EUR billion per year]\")\n    ax.set_xlabel(\"\")\n    ax.grid(axis=\"y\")\n    # TODO fix this - doesnt work with non-constant interval\n    ax.annotate(\n        f\"Total cost in bn Eur: {df.sum().sum()*5:.2f}\",\n        xy=(0.75, 0.9),\n        color=\"darkgray\",\n        xycoords=\"axes fraction\",\n        ha=\"right\",\n        va=\"top\",\n    )\n\n    ax.legend(\n        handles,\n        [l.title() for l in labels],\n        ncol=1,\n        bbox_to_anchor=[1, 1],\n        loc=\"upper left\",\n    )\n\n    fig.tight_layout()\n\n    if fig_name is not None:\n        fig.savefig(fig_name, transparent=config[\"transparent\"])\n</code></pre>"},{"location":"reference/plot_summary_all/#plot_summary_all.plot_prices","title":"<code>plot_prices(file_list, config, fig_name=None, absolute=False, ax=None, unit='\u20ac/MWh', **kwargs)</code>","text":"<p>plot the prices</p> <p>Parameters:</p> Name Type Description Default <code>file_list</code> <code>list</code> <p>the input csvs from make_summary</p> required <code>config</code> <code>dict</code> <p>the configuration for plotting (snakemake.config[\"plotting\"])</p> required <code>fig_name</code> <code>PathLike</code> <p>the figure name. Defaults to None.</p> <code>None</code> <code>absolute</code> <code>bool</code> <p>plot absolute prices. Defaults to False.</p> <code>False</code> <code>ax</code> <code>Axes</code> <p>the axes to plot on. Defaults to None.</p> <code>None</code> <code>unit</code> <code>str</code> <p>the unit of the prices. Defaults to \"\u20ac/MWh\".</p> <code>'\u20ac/MWh'</code> Source code in <code>workflow/scripts/plot_summary_all.py</code> <pre><code>def plot_prices(\n    file_list: list,\n    config: dict,\n    fig_name=None,\n    absolute=False,\n    ax: object = None,\n    unit=\"\u20ac/MWh\",\n    **kwargs,\n):\n    \"\"\"plot the prices\n\n    Args:\n        file_list (list): the input csvs from make_summary\n        config (dict): the configuration for plotting (snakemake.config[\"plotting\"])\n        fig_name (os.PathLike, optional): the figure name. Defaults to None.\n        absolute (bool, optional): plot absolute prices. Defaults to False.\n        ax (matplotlib.axes.Axes, optional): the axes to plot on. Defaults to None.\n        unit (str, optional): the unit of the prices. Defaults to \"\u20ac/MWh\".\n    \"\"\"\n    prices_df = pd.DataFrame()\n    for results_file in file_list:\n        df_year = pd.read_csv(results_file, index_col=list(range(1)), header=[1]).T\n\n        prices_df = pd.concat([df_year, prices_df])\n    prices_df.sort_index(axis=0, inplace=True)\n    if not ax:\n        fig, ax = plt.subplots()\n    else:\n        fig = ax.get_figure()\n    fig.set_size_inches((12, 8))\n\n    colors = config[\"tech_colors\"]\n\n    if absolute:\n        prices_df = prices_df.abs()\n\n    defaults = {\"lw\": 3, \"marker\": \"o\", \"markersize\": 5, \"alpha\": 0.8}\n    if \"linewidth\" in kwargs:\n        kwargs[\"lw\"] = kwargs.pop(\"linewidth\")\n    defaults.update(kwargs)\n    prices_df.plot(\n        ax=ax,\n        kind=\"line\",\n        color=[colors[k] if k in colors else \"k\" for k in prices_df.columns],\n        **defaults,\n    )\n    min_ = prices_df.min().min()\n    if np.sign(min_) &lt; 0:\n        min_ *= 1.1\n    else:\n        min_ *= 0.9\n    ax.set_ylim([min_, prices_df.max().max() * 1.1])\n    ax.set_ylabel(f\"Prices [{unit}]\")\n    ax.set_xlabel(\"\")\n    ax.grid(axis=\"y\")\n\n    handles, labels = ax.get_legend_handles_labels()\n\n    handles.reverse()\n    labels.reverse()\n    ax.legend(handles, labels, ncol=1, bbox_to_anchor=[1, 1], loc=\"upper left\")\n    fig.tight_layout()\n\n    if fig_name is not None:\n        fig.savefig(fig_name, transparent=False)\n</code></pre>"},{"location":"reference/plot_summary_all/#plot_summary_all.rename_techs","title":"<code>rename_techs(label)</code>","text":"<p>rename techs into grouped categories</p> <p>Parameters:</p> Name Type Description Default <code>label</code> <code>Index | iterable</code> <p>the index techs to rename</p> required <p>Returns:     pd.Index | iterable: the renamed index / iterable</p> Source code in <code>workflow/scripts/plot_summary_all.py</code> <pre><code>def rename_techs(label: pd.Index) -&gt; pd.Index:\n    \"\"\"rename techs into grouped categories\n\n    Args:\n        label (pd.Index | iterable): the index techs to rename\n    Returns:\n        pd.Index | iterable: the renamed index / iterable\"\"\"\n    prefix_to_remove = [\n        \"central \",\n        \"decentral \",\n    ]\n\n    rename_if_contains_dict = {\n        \"water tanks\": \"hot water storage\",\n        \"H2\": \"H2\",\n        \"coal cc\": \"CC\",\n    }\n    rename_if_contains = [\"gas\", \"coal\"]\n    rename = {\n        \"solar\": \"solar PV\",\n        \"Sabatier\": \"methanation\",\n        \"offwind\": \"offshore wind\",\n        \"onwind\": \"onshore wind\",\n        \"ror\": \"hydroelectricity\",\n        \"hydro\": \"hydroelectricity\",\n        \"PHS\": \"pumped hydro storage\",\n        \"hydro_inflow\": \"hydroelectricity\",\n        \"stations\": \"hydroelectricity\",\n        \"AC\": \"transmission lines\",\n        \"CO2 capture\": \"biomass carbon capture\",\n        \"CC\": \"coal carbon capture\",\n        \"battery\": \"battery\",\n    }\n\n    for ptr in prefix_to_remove:\n        if label[: len(ptr)] == ptr:\n            label = label[len(ptr) :]\n\n    for old, new in rename_if_contains_dict.items():\n        if old in label:\n            label = new\n\n    for rif in rename_if_contains:\n        if rif in label:\n            label = rif\n\n    for old, new in rename.items():\n        if old == label:\n            label = new\n    return label\n</code></pre>"},{"location":"reference/plot_summary_all/#plot_summary_all.write_data","title":"<code>write_data(data_paths, outp_dir)</code>","text":"<p>Write some selected data</p> <p>Parameters:</p> Name Type Description Default <code>data_paths</code> <code>dict</code> <p>the paths to the summary data (different per year and type)</p> required <code>outp_dir</code> <code>PathLike</code> <p>target file (summary dir)</p> required Source code in <code>workflow/scripts/plot_summary_all.py</code> <pre><code>def write_data(data_paths: dict, outp_dir: os.PathLike):\n    \"\"\"Write some selected data\n\n    Args:\n        data_paths (dict): the paths to the summary data (different per year and type)\n        outp_dir (os.PathLike): target file (summary dir)\n    \"\"\"\n    # make a summary of the co2 prices\n    co2_prices = {}\n    co2_budget = {}\n    for i, results_file in enumerate(data_paths[\"co2_price\"]):\n        df_metrics = pd.read_csv(results_file, index_col=list(range(1)), header=[1])\n        co2_prices.update(dict(df_metrics.loc[\"co2_shadow\"]))\n        co2_budget.update(dict(df_metrics.loc[\"co2_budget\"]))\n    years = list(co2_budget.keys())\n    co2_df = pd.DataFrame(\n        {\n            \"Year\": years,\n            \"CO2 Budget\": [co2_budget[year] for year in years],\n            \"CO2 Shadow Price\": [co2_prices[year] * -1 for year in years],\n        }\n    )\n    outp_p = os.path.join(outp_dir, \"co2_prices.csv\")\n    co2_df.to_csv(outp_p, index=False)\n\n    df = pd.DataFrame()\n    for results_file in data_paths[\"costs\"]:\n        cost_df = pd.read_csv(results_file, index_col=list(range(3)), header=[1])\n        df_ = cost_df.groupby(level=[1, 2]).sum()\n        df_ = df_ * COST_UNIT / PLOT_COST_UNITS\n        df = pd.concat([df_, df], axis=1)\n    df.to_csv(os.path.join(outp_dir, \"pathway_costs_not_discounted.csv\"))\n\n    prices_df = pd.DataFrame()\n    for results_file in data_paths[\"weighted_prices\"]:\n        df_year = pd.read_csv(results_file, index_col=list(range(1)), header=[1]).T\n\n        prices_df = pd.concat([df_year, prices_df])\n    prices_df.to_csv(os.path.join(outp_dir, \"weighted_prices.csv\"))\n</code></pre>"},{"location":"reference/plot_time_series/","title":"Plot time series","text":""},{"location":"reference/plot_time_series/#plot_time_series.add_reserves","title":"<code>add_reserves(n)</code>","text":"<p>plot the reserves of the network</p> Source code in <code>workflow/scripts/plot_time_series.py</code> <pre><code>def add_reserves(n: pypsa.Network):\n    \"\"\"plot the reserves of the network\"\"\"\n\n    curtailed = n.statistics.curtailment(aggregate_time=False, bus_carrier=\"AC\")\n</code></pre>"},{"location":"reference/plot_time_series/#plot_time_series.plot_energy_balance","title":"<code>plot_energy_balance(n, plot_config, bus_carrier='AC', start_date='2060-03-31 21:00', end_date='2060-04-06 12:00:00', aggregate_fossil=False, add_load_line=True, add_reserves=False, ax=None)</code>","text":"<p>plot the electricity balance of the network for the given time range</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>Network</code> <p>the network</p> required <code>plot_config</code> <code>dict</code> <p>the plotting config (snakemake.config[\"plotting\"])</p> required <code>bus_carrier</code> <code>str</code> <p>the carrier for the energy_balance op. Defaults to \"AC\".</p> <code>'AC'</code> <code>start_date</code> <code>str</code> <p>the range to plot. Defaults to \"2060-03-31 21:00\".</p> <code>'2060-03-31 21:00'</code> <code>end_date</code> <code>str</code> <p>the range to plot. Defaults to \"2060-04-06 12:00:00\".</p> <code>'2060-04-06 12:00:00'</code> <code>aggregate_fossil</code> <code>bool</code> <p>whether to aggregate fossil fuels. Defaults to False.</p> <code>False</code> <code>add_load_line</code> <code>bool</code> <p>add a dashed line for the load. Defaults to True.</p> <code>True</code> Source code in <code>workflow/scripts/plot_time_series.py</code> <pre><code>def plot_energy_balance(\n    n: pypsa.Network,\n    plot_config: dict,\n    bus_carrier=\"AC\",\n    start_date=\"2060-03-31 21:00\",\n    end_date=\"2060-04-06 12:00:00\",\n    aggregate_fossil=False,\n    add_load_line=True,\n    add_reserves=False,\n    ax: plt.Axes = None,\n):\n    \"\"\"plot the electricity balance of the network for the given time range\n\n    Args:\n        n (pypsa.Network): the network\n        plot_config (dict): the plotting config (snakemake.config[\"plotting\"])\n        bus_carrier (str, optional): the carrier for the energy_balance op. Defaults to \"AC\".\n        start_date (str, optional): the range to plot. Defaults to \"2060-03-31 21:00\".\n        end_date (str, optional): the range to plot. Defaults to \"2060-04-06 12:00:00\".\n        aggregate_fossil (bool, optional): whether to aggregate fossil fuels. Defaults to False.\n        add_load_line (bool, optional): add a dashed line for the load. Defaults to True.\n    \"\"\"\n    if not ax:\n        fig, ax = plt.subplots(figsize=(16, 8))\n    else:\n        fig = ax.get_figure()\n\n    p = (\n        n.statistics.energy_balance(aggregate_time=False, bus_carrier=bus_carrier)\n        .dropna(how=\"all\")\n        .groupby(\"carrier\")\n        .sum()\n        .div(PLOT_SUPPLY_UNITS)\n        # .drop(\"-\")\n        .T\n    )\n\n    p = p.loc[start_date:end_date]\n    p.rename(columns={\"-\": \"Load\", \"AC\": \"transmission losses\"}, inplace=True)\n\n    # aggreg fossil\n    if aggregate_fossil:\n        coal = p.filter(regex=\"[C|c]oal\")\n        p.drop(columns=coal.columns, inplace=True)\n        p[\"Coal\"] = coal.sum(axis=1)\n        gas = p.filter(regex=\"[G|g]as\")\n        p.drop(columns=gas.columns, inplace=True)\n        p[\"Gas\"] = gas.sum(axis=1)\n\n    extra_c = {\n        \"Load\": plot_config[\"tech_colors\"][\"electric load\"],\n        \"transmission losses\": plot_config[\"tech_colors\"][\"transmission losses\"],\n    }\n    nice_tech_colors = make_nice_tech_colors(plot_config[\"tech_colors\"], plot_config[\"nice_names\"])\n    color_series = get_stat_colors(n, nice_tech_colors, extra_colors=extra_c)\n    # colors &amp; names part 1\n    p.rename(plot_config[\"nice_names\"], inplace=True)\n    p.rename(columns={k: k.title() for k in p.columns}, inplace=True)\n    color_series.index = color_series.index.str.strip()\n    # split into supply and wothdrawal\n    supply = p.where(p &gt; 0).dropna(axis=1, how=\"all\")\n    charge = p.where(p &lt; 0).dropna(how=\"all\", axis=1)\n\n    # fix names and order\n\n    charge.rename(columns={\"Battery Storage\": \"Battery\"}, inplace=True)\n    supply.rename(columns={\"Battery Discharger\": \"Battery\"}, inplace=True)\n    color_series = color_series[charge.columns.union(supply.columns)]\n    color_series.rename(\n        {\"Battery Discharger\": \"Battery\", \"Battery Storage\": \"Battery\"},\n        inplace=True,\n    )\n    # Deduplicate color_series\n    color_series = color_series[~color_series.index.duplicated(keep=\"first\")]\n\n    preferred_order = plot_config[\"preferred_order\"]\n    plot_order = (\n        supply.columns.intersection(preferred_order).to_list()\n        + supply.columns.difference(preferred_order).to_list()\n    )\n\n    plot_order_charge = [name for name in preferred_order if name in charge.columns] + [\n        name for name in charge.columns if name not in preferred_order\n    ]\n\n    supply = supply.reindex(columns=plot_order)\n    charge = charge.reindex(columns=plot_order_charge)\n    if not charge.empty:\n        charge.plot.area(ax=ax, linewidth=0, color=color_series.loc[charge.columns])\n\n    supply.plot.area(\n        ax=ax,\n        linewidth=0,\n        color=color_series.loc[supply.columns].values,\n    )\n    if add_load_line:\n        charge[\"load_pos\"] = charge[\"Load\"] * -1\n        charge[\"load_pos\"].plot(linewidth=2, color=\"black\", label=\"Load\", ax=ax, linestyle=\"--\")\n        charge.drop(columns=\"load_pos\", inplace=True)\n\n    ax.legend(ncol=1, loc=\"center left\", bbox_to_anchor=(1, 0.5), frameon=False, fontsize=16)\n    ax.set_ylabel(PLOT_SUPPLY_LABEL)\n    ax.set_ylim(charge.sum(axis=1).min() * 1.07, supply.sum(axis=1).max() * 1.07)\n    ax.grid(axis=\"y\")\n    ax.set_xlim(supply.index.min(), supply.index.max())\n\n    fig.tight_layout()\n\n    return ax\n</code></pre>"},{"location":"reference/plot_time_series/#plot_time_series.plot_load_duration_curve","title":"<code>plot_load_duration_curve(network, carrier='AC', ax=None)</code>","text":"<p>plot the load duration curve for the given carrier</p> <p>Parameters:</p> Name Type Description Default <code>network</code> <code>Network</code> <p>the pypasa network object</p> required <code>carrier</code> <code>str</code> <p>the load carrier, defaults to AC</p> <code>'AC'</code> <code>ax</code> <code>Axes</code> <p>figure axes, if none fig will be created. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Axes</code> <p>plt.Axes: the plotting axes</p> Source code in <code>workflow/scripts/plot_time_series.py</code> <pre><code>def plot_load_duration_curve(\n    network: pypsa.Network, carrier: str = \"AC\", ax: plt.Axes = None\n) -&gt; plt.Axes:\n    \"\"\"plot the load duration curve for the given carrier\n\n    Args:\n        network (pypsa.Network): the pypasa network object\n        carrier (str, optional): the load carrier, defaults to AC\n        ax (plt.Axes, optional): figure axes, if none fig will be created. Defaults to None.\n\n    Returns:\n        plt.Axes: the plotting axes\n    \"\"\"\n\n    if not ax:\n        fig, ax = plt.subplots(figsize=(16, 8))\n    else:\n        fig = ax.get_figure()\n\n    load = network.statistics.withdrawal(\n        groupby=get_location_and_carrier,\n        aggregate_time=False,\n        bus_carrier=carrier,\n        comps=\"Load\",\n    ).sum()\n    load_curve = load.sort_values(ascending=False) / PLOT_CAP_UNITS\n    load_curve.reset_index(drop=True).plot(ax=ax, lw=3)\n    ax.set_ylabel(f\"Load [{PLOT_CAP_LABEL}]\")\n    ax.set_xlabel(\"Hours\")\n\n    fig.tight_layout()\n    return ax\n</code></pre>"},{"location":"reference/plot_time_series/#plot_time_series.plot_price_duration_by_node","title":"<code>plot_price_duration_by_node(network, carrier='AC', logy=True, y_lower=0.001, fig_shape=(8, 4))</code>","text":"<p>Plot the price duration curve for the given carrier by node Args:     network (pypsa.Network): the pypsa network object     carrier (str, optional): the load carrier, defaults to AC (bus suffix)     logy (bool, optional): use log scale for y axis, defaults to True     y_lower (float, optional): lower limit for y axis, defaults to 1e-3     fig_shape (tuple, optional): shape of the figure, defaults to (8, 4) Returns:     plt.Axes: the plotting axes Raises:     ValueError: if the figure shape is too small for the number of regions</p> Source code in <code>workflow/scripts/plot_time_series.py</code> <pre><code>def plot_price_duration_by_node(\n    network: pypsa.Network,\n    carrier: str = \"AC\",\n    logy=True,\n    y_lower=1e-3,\n    fig_shape=(8, 4),\n) -&gt; plt.Axes:\n    \"\"\"Plot the price duration curve for the given carrier by node\n    Args:\n        network (pypsa.Network): the pypsa network object\n        carrier (str, optional): the load carrier, defaults to AC (bus suffix)\n        logy (bool, optional): use log scale for y axis, defaults to True\n        y_lower (float, optional): lower limit for y axis, defaults to 1e-3\n        fig_shape (tuple, optional): shape of the figure, defaults to (8, 4)\n    Returns:\n        plt.Axes: the plotting axes\n    Raises:\n        ValueError: if the figure shape is too small for the number of regions\"\"\"\n\n    if carrier == \"AC\":\n        suffix = \"\"\n    else:\n        suffix = f\" {carrier}\"\n\n    nodal_prices = network.buses_t.marginal_price[pd.Index(PROV_NAMES) + suffix]\n\n    if fig_shape[0] * fig_shape[1] &lt; len(nodal_prices.columns):\n        raise ValueError(\n            f\"Figure shape {fig_shape} is too small for {len(nodal_prices.columns)} regions. \"\n            + \"Please increase the number of subplots.\"\n        )\n    fig, axes = plt.subplots(fig_shape[0], fig_shape[1], sharex=True, sharey=True, figsize=(12, 12))\n\n    # region by region sorting of prices\n    for i, region in enumerate(nodal_prices.columns):\n        reg_pr = nodal_prices[region]\n        reg_pr.sort_values(ascending=False).reset_index(drop=True).plot(\n            ax=axes[i // 4, i % fig_shape[1]], label=region\n        )\n        axes[i // 4, i % fig_shape[1]].set_title(region, fontsize=10)\n        if logy:\n            axes[i // 4, i % fig_shape[1]].semilogy()\n        if y_lower:\n            axes[i // 4, i % fig_shape[1]].set_ylim(y_lower, reg_pr.max() * 1.2)\n        elif reg_pr.min() &gt; 1e-5 and not logy:\n            axes[i // 4, i % fig_shape[1]].set_ylim(0, reg_pr.max() * 1.2)\n    fig.tight_layout(h_pad=0.2, w_pad=0.2)\n    for ax in axes.flat:\n        # Remove all x-tick labels except the largest value\n        xticks = ax.get_xticks()\n        if len(xticks) &gt; 0:\n            ax.set_xticks([xticks[0], xticks[-1]])\n            ax.set_xticklabels([f\"{xticks[0]:.0f}\", f\"{xticks[-1]:.0f}\"])\n\n    return ax\n</code></pre>"},{"location":"reference/plot_time_series/#plot_time_series.plot_price_duration_curve","title":"<code>plot_price_duration_curve(network, carrier='AC', ax=None, figsize=(8, 8))</code>","text":"<p>plot the price duration curve for the given carrier</p> <p>Parameters:</p> Name Type Description Default <code>network</code> <code>Network</code> <p>the pypasa network object</p> required <code>carrier</code> <code>str</code> <p>the load carrier, defaults to AC</p> <code>'AC'</code> <code>ax</code> <code>Axes</code> <p>Axes to plot on, if none fig will be created. Defaults to None.</p> <code>None</code> <code>figsize</code> <code>tuple</code> <p>size of the figure (if no ax given), defaults to (8, 8)</p> <code>(8, 8)</code> <p>Returns:     plt.Axes: the plotting axes</p> Source code in <code>workflow/scripts/plot_time_series.py</code> <pre><code>def plot_price_duration_curve(\n    network: pypsa.Network, carrier=\"AC\", ax: plt.Axes = None, figsize=(8, 8)\n) -&gt; plt.Axes:\n    \"\"\"plot the price duration curve for the given carrier\n\n    Args:\n        network (pypsa.Network): the pypasa network object\n        carrier (str, optional): the load carrier, defaults to AC\n        ax (plt.Axes, optional): Axes to plot on, if none fig will be created. Defaults to None.\n        figsize (tuple, optional): size of the figure (if no ax given), defaults to (8, 8)\n    Returns:\n        plt.Axes: the plotting axes\n    \"\"\"\n    if not ax:\n        fig, ax = plt.subplots(figsize=figsize)\n    else:\n        fig = ax.get_figure()\n\n    ntwk_el_price = (\n        -1\n        * network.statistics.revenue(bus_carrier=carrier, aggregate_time=False, comps=\"Load\")\n        / network.statistics.withdrawal(bus_carrier=carrier, aggregate_time=False, comps=\"Load\")\n    ).T\n    ntwk_el_price.rename(columns={\"-\": \"Load\"}, inplace=True)\n    ntwk_el_price.Load.sort_values(ascending=False).reset_index(drop=True).plot(\n        title=\"Price Duration Curve\", ax=ax, lw=2\n    )\n    fig.tight_layout()\n\n    return ax\n</code></pre>"},{"location":"reference/plot_time_series/#plot_time_series.plot_price_heatmap","title":"<code>plot_price_heatmap(network, carrier='AC', log_values=False, color_map='viridis', time_range=None, ax=None)</code>","text":"<p>plot the price heat map (region vs time) for the given carrier</p> <p>Parameters:</p> Name Type Description Default <code>network</code> <code>Network</code> <p>the pypsa network object</p> required <code>carrier</code> <code>str</code> <p>the carrier for which to get the price. Defaults to \"AC\".</p> <code>'AC'</code> <code>log_values</code> <code>bool</code> <p>whether to use log scale for the prices. Defaults to False.</p> <code>False</code> <code>color_map</code> <code>str</code> <p>the color map to use. Defaults to \"viridis\".</p> <code>'viridis'</code> <code>time_range</code> <code>Index</code> <p>the time range to plot. Defaults to None (all times).</p> <code>None</code> <code>ax</code> <code>Axes</code> <p>the plotting axis. Defaults to None (new fig).</p> <code>None</code> <p>Returns:</p> Type Description <code>Axes</code> <p>plt.Axes: the axes for plotting</p> Source code in <code>workflow/scripts/plot_time_series.py</code> <pre><code>def plot_price_heatmap(\n    network: pypsa.Network,\n    carrier=\"AC\",\n    log_values=False,\n    color_map=\"viridis\",\n    time_range: pd.Index = None,\n    ax: plt.Axes = None,\n) -&gt; plt.Axes:\n    \"\"\"plot the price heat map (region vs time) for the given carrier\n\n    Args:\n        network (pypsa.Network): the pypsa network object\n        carrier (str, optional): the carrier for which to get the price. Defaults to \"AC\".\n        log_values (bool, optional): whether to use log scale for the prices. Defaults to False.\n        color_map (str, optional): the color map to use. Defaults to \"viridis\".\n        time_range (pd.Index, optional): the time range to plot. Defaults to None (all times).\n        ax (plt.Axes, optional): the plotting axis. Defaults to None (new fig).\n\n    Returns:\n        plt.Axes: the axes for plotting\n    \"\"\"\n\n    if not ax:\n        fig, ax = plt.subplots(figsize=(20, 8))\n    else:\n        fig = ax.get_figure()\n\n    carrier_buses = network.buses.carrier[network.buses.carrier == carrier].index.values\n    nodal_prices = network.buses_t.marginal_price[carrier_buses]\n\n    if time_range is not None:\n        # Filter nodal_prices by the given time range\n        nodal_prices = nodal_prices.loc[time_range]\n    # Normalize nodal_prices with log transformation\n    if log_values:\n        # Avoid log(0) by clipping values to a minimum of 0.1\n        normalized_prices = np.log(nodal_prices.clip(lower=0.1))\n        label = \"Log-Transformed Price [\u20ac/MWh]\"\n    else:\n        normalized_prices = nodal_prices\n        label = \"Price [\u20ac/MWh]\"\n    # Create a heatmap of normalized nodal_prices\n    plot_index = normalized_prices.index.strftime(\"%m-%d %H:%M\").to_list()\n    normalized_prices.index = plot_index\n    sns.heatmap(\n        normalized_prices.T,\n        cmap=color_map,\n        cbar_kws={\"label\": label},\n        ax=ax,\n    )\n\n    # Customize the plot\n    if log_values:\n        ax.set_title(\"Heatmap of Log-Transformed Nodal Prices\")\n    else:\n        ax.set_title(\"Heatmap of Nodal Prices\")\n\n    ax.set_xlabel(\"Time\")\n    ax.set_ylabel(\"Nodes\")\n    fig.tight_layout()\n\n    return ax\n</code></pre>"},{"location":"reference/plot_time_series/#plot_time_series.plot_regional_load_durations","title":"<code>plot_regional_load_durations(network, carrier='AC', ax=None, cmap='plasma')</code>","text":"<p>plot the load duration curve for the given carrier stacked by region</p> <p>Parameters:</p> Name Type Description Default <code>network</code> <code>Network</code> <p>the pypasa network object</p> required <code>carrier</code> <code>str</code> <p>the load carrier, defaults to AC</p> <code>'AC'</code> <code>ax</code> <code>Axes</code> <p>axes to plot on, if none fig will be created. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Axes</code> <p>plt.Axes: the plotting axes</p> Source code in <code>workflow/scripts/plot_time_series.py</code> <pre><code>def plot_regional_load_durations(\n    network: pypsa.Network, carrier=\"AC\", ax=None, cmap=\"plasma\"\n) -&gt; plt.Axes:\n    \"\"\"plot the load duration curve for the given carrier stacked by region\n\n    Args:\n        network (pypsa.Network): the pypasa network object\n        carrier (str, optional): the load carrier, defaults to AC\n        ax (plt.Axes, optional): axes to plot on, if none fig will be created. Defaults to None.\n\n    Returns:\n        plt.Axes: the plotting axes\n    \"\"\"\n    if not ax:\n        fig, ax = plt.subplots(figsize=(10, 8))\n    else:\n        fig = ax.get_figure()\n\n    loads_all = network.statistics.withdrawal(\n        groupby=get_location_and_carrier,\n        aggregate_time=False,\n        bus_carrier=carrier,\n        comps=\"Load\",\n    ).sum()\n    load_curve_all = loads_all.sort_values(ascending=False) / PLOT_CAP_UNITS\n    regio = network.statistics.withdrawal(\n        groupby=get_location_and_carrier,\n        aggregate_time=False,\n        bus_carrier=carrier,\n        comps=\"Load\",\n    )\n    regio = regio.droplevel(1).T\n    load_curve_regio = regio.loc[load_curve_all.index] / PLOT_CAP_UNITS\n    load_curve_regio.reset_index(drop=True).plot.area(\n        ax=ax, stacked=True, cmap=cmap, legend=True, lw=3\n    )\n    ax.set_ylabel(f\"Load [{PLOT_CAP_LABEL}]\")\n    ax.set_xlabel(\"Hours\")\n    ax.legend(\n        ncol=3,\n        loc=\"upper center\",\n        bbox_to_anchor=(0.5, -0.15),\n        fontsize=\"small\",\n        title_fontsize=\"small\",\n        fancybox=True,\n        shadow=True,\n    )\n\n    fig.tight_layout()\n\n    return ax\n</code></pre>"},{"location":"reference/plot_time_series/#plot_time_series.plot_residual_load_duration_curve","title":"<code>plot_residual_load_duration_curve(network, ax=None, vre_techs=['Onshore Wind', 'Offshore Wind', 'Solar'])</code>","text":"<p>plot the residual load duration curve for the given carrier</p> <p>Parameters:</p> Name Type Description Default <code>network</code> <code>Network</code> <p>the pypasa network object</p> required <code>ax</code> <code>Axes</code> <p>Axes to plot on, if none fig will be created. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Axes</code> <p>plt.Axes: the plotting axes</p> Source code in <code>workflow/scripts/plot_time_series.py</code> <pre><code>def plot_residual_load_duration_curve(\n    network, ax: plt.Axes = None, vre_techs=[\"Onshore Wind\", \"Offshore Wind\", \"Solar\"]\n) -&gt; plt.Axes:\n    \"\"\"plot the residual load duration curve for the given carrier\n\n    Args:\n        network (pypsa.Network): the pypasa network object\n        ax (plt.Axes, optional): Axes to plot on, if none fig will be created. Defaults to None.\n\n    Returns:\n        plt.Axes: the plotting axes\n    \"\"\"\n    CARRIER = \"AC\"\n    if not ax:\n        fig, ax = plt.subplots(figsize=(16, 8))\n    load = network.statistics.withdrawal(\n        groupby=get_location_and_carrier,\n        aggregate_time=False,\n        bus_carrier=CARRIER,\n        comps=\"Load\",\n    ).sum()\n\n    vre_supply = (\n        network.statistics.supply(\n            groupby=get_location_and_carrier,\n            aggregate_time=False,\n            bus_carrier=CARRIER,\n            comps=\"Generator\",\n        )\n        .groupby(level=1)\n        .sum()\n        .loc[vre_techs]\n        .sum()\n    )\n\n    residual = (load - vre_supply).sort_values(ascending=False) / PLOT_CAP_UNITS\n    residual.reset_index(drop=True).plot(ax=ax, lw=3)\n    ax.set_ylabel(f\"Residual Load [{PLOT_CAP_LABEL}]\")\n    ax.set_xlabel(\"Hours\")\n\n    return ax\n</code></pre>"},{"location":"reference/plot_time_series/#plot_time_series.plot_vre_heatmap","title":"<code>plot_vre_heatmap(n, config, color_map='magma', log_values=True, time_range=None)</code>","text":"<p>plot the VRE generation per hour and day as a heatmap</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>Network</code> <p>the pypsa network object</p> required <code>time_range</code> <code>Index</code> <p>the time range to plot. Defaults to None (all times).</p> <code>None</code> <code>log_values</code> <code>bool</code> <p>whether to use log scale for the values. Defaults to True.</p> <code>True</code> <code>config</code> <code>dict</code> <p>the run config (snakemake.config).</p> required Source code in <code>workflow/scripts/plot_time_series.py</code> <pre><code>def plot_vre_heatmap(\n    n: pypsa.Network, config: dict, color_map=\"magma\", log_values=True, time_range: pd.Index = None,\n):\n    \"\"\"plot the VRE generation per hour and day as a heatmap\n\n    Args:\n        n (pypsa.Network): the pypsa network object\n        time_range (pd.Index, optional): the time range to plot. Defaults to None (all times).\n        log_values (bool, optional): whether to use log scale for the values. Defaults to True.\n        config (dict, optional): the run config (snakemake.config).\n\n    \"\"\"\n\n    vres = config[\"Techs\"].get(\"non_dispatchable\", ['Offshore Wind', 'Onshore Wind', 'Solar', 'Solar Residential'])\n    vre_avail = (\n        n.statistics.supply(\n            comps=\"Generator\",\n            aggregate_time=False,\n            bus_carrier=\"AC\",\n            nice_names=False,\n            groupby=[\"location\", \"carrier\"],\n        )\n        .query(\"carrier in @vres\")\n        .T.fillna(0)\n    )\n\n    if time_range is not None:\n        vre_avail = vre_avail.loc[time_range]\n\n    for tech in vres[::-1]:\n        tech_avail = vre_avail.T.query(\"carrier == @tech\")\n        tech_avail.index = tech_avail.index.droplevel(1)\n        tech_avail = tech_avail.T\n        tech_avail.index = tech_avail.index.strftime(\"%m-%d %H:%M\")\n        if log_values:\n            # Avoid log(0) by clipping values to a minimum of 10\n            tech_avail = np.log(tech_avail.clip(lower=10))\n        fig, ax = plt.subplots()\n        sns.heatmap(tech_avail.T, ax=ax, cmap=color_map)\n        ax.set_title(f\"{tech} generation by province\")\n</code></pre>"},{"location":"reference/plot_time_series/#plot_time_series.plot_vre_timemap","title":"<code>plot_vre_timemap(network, color_map='viridis', time_range=None)</code>","text":"<p>plot the VRE generation per hour and day as a heatmap</p> <p>Parameters:</p> Name Type Description Default <code>network</code> <code>Network</code> <p>the pypsa network object</p> required <code>color_map</code> <code>str</code> <p>the color map to use. Defaults to \"viridis\".</p> <code>'viridis'</code> <code>time_range</code> <code>Index</code> <p>the time range to plot. Defaults to None (all times).</p> <code>None</code> Source code in <code>workflow/scripts/plot_time_series.py</code> <pre><code>def plot_vre_timemap(\n    network: pypsa.Network,\n    color_map=\"viridis\",\n    time_range: pd.Index = None,\n):\n    \"\"\"plot the VRE generation per hour and day as a heatmap\n\n    Args:\n        network (pypsa.Network): the pypsa network object\n        color_map (str, optional): the color map to use. Defaults to \"viridis\".\n        time_range (pd.Index, optional): the time range to plot. Defaults to None (all times).\n    \"\"\"\n\n    vres = [\"offwind\", \"onwind\", \"solar\"]\n    vre_avail = (\n        network.statistics.supply(\n            comps=\"Generator\", aggregate_time=False, bus_carrier=\"AC\", nice_names=False\n        )\n        .query(\"carrier in @vres\")\n        .T.fillna(0)\n    )\n    if time_range is not None:\n        vre_avail = vre_avail.loc[time_range]\n\n    vre_avail[\"day\"] = vre_avail.index.strftime(\"%d-%m\")\n    vre_avail[\"hour\"] = vre_avail.index.hour\n\n    for tech in vres:\n        pivot_ = vre_avail.pivot_table(index=\"hour\", columns=\"day\", values=tech)\n        fig, ax = plt.subplots(figsize=(12, 6))\n        sns.heatmap(pivot_.sort_index(ascending=False), cmap=color_map, ax=ax)\n        ax.set_title(f\"{tech} generation by hour and day\")\n\n        fig.tight_layout()\n</code></pre>"},{"location":"reference/prepare_base_network/","title":"Prepare base network","text":""},{"location":"reference/prepare_base_network/#prepare_base_network.add_buses","title":"<code>add_buses(network, nodes, suffix, carrier, prov_centroids)</code>","text":"<p>Add buses</p> <p>Parameters:</p> Name Type Description Default <code>network</code> <code>Network</code> <p>description</p> required <code>nodes</code> <code>list | Index</code> <p>description</p> required <code>suffix</code> <code>str</code> <p>description</p> required <code>carrier</code> <code>str</code> <p>description</p> required <code>prov_centroids</code> <code>GeoSeries</code> <p>description</p> required Source code in <code>workflow/scripts/prepare_base_network.py</code> <pre><code>def add_buses(\n    network: pypsa.Network,\n    nodes: list | pd.Index,\n    suffix: str,\n    carrier: str,\n    prov_centroids: gpd.GeoSeries,\n):\n    \"\"\"Add buses\n\n    Args:\n        network (pypsa.Network): _description_\n        nodes (list | pd.Index): _description_\n        suffix (str): _description_\n        carrier (str): _description_\n        prov_centroids (gpd.GeoSeries): _description_\n    \"\"\"\n\n    network.add(\n        \"Bus\",\n        nodes,\n        suffix=suffix,\n        x=prov_centroids.x,\n        y=prov_centroids.y,\n        carrier=carrier,\n        location=nodes,\n    )\n</code></pre>"},{"location":"reference/prepare_base_network/#prepare_base_network.add_carriers","title":"<code>add_carriers(network, config, costs)</code>","text":"<p>ad the various carriers to the network based on the config file</p> <p>Parameters:</p> Name Type Description Default <code>network</code> <code>Network</code> <p>the pypsa network</p> required <code>config</code> <code>dict</code> <p>the config file</p> required <code>costs</code> <code>DataFrame</code> <p>the costs dataframe</p> required Source code in <code>workflow/scripts/prepare_base_network.py</code> <pre><code>def add_carriers(network: pypsa.Network, config: dict, costs: pd.DataFrame):\n    \"\"\"ad the various carriers to the network based on the config file\n\n    Args:\n        network (pypsa.Network): the pypsa network\n        config (dict): the config file\n        costs (pd.DataFrame): the costs dataframe\n    \"\"\"\n\n    network.add(\"Carrier\", \"AC\")\n    if config[\"heat_coupling\"]:\n        network.add(\"Carrier\", \"heat\")\n    for carrier in config[\"Techs\"][\"vre_techs\"]:\n        network.add(\"Carrier\", carrier)\n        if carrier == \"hydroelectricity\":\n            network.add(\"Carrier\", \"hydro_inflow\")\n    for carrier in config[\"Techs\"][\"store_techs\"]:\n        network.add(\"Carrier\", carrier)\n        if carrier == \"battery\":\n            network.add(\"Carrier\", \"battery discharger\")\n\n    if \"coal power plant\" in config[\"Techs\"][\"conv_techs\"] and config[\"Techs\"][\"coal_ccs_retrofit\"]:\n        network.add(\"Carrier\", \"coal cc\", co2_emissions=0.034)\n</code></pre>"},{"location":"reference/prepare_base_network/#prepare_base_network.add_co2_constraints_prices","title":"<code>add_co2_constraints_prices(network, co2_control)</code>","text":"<p>Add co2 constraints or prices</p> <p>Parameters:</p> Name Type Description Default <code>network</code> <code>Network</code> <p>the network to which prices or constraints are to be added</p> required <code>co2_control</code> <code>dict</code> <p>the config</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>unrecognised co2 control option</p> Source code in <code>workflow/scripts/prepare_base_network.py</code> <pre><code>def add_co2_constraints_prices(network: pypsa.Network, co2_control: dict):\n    \"\"\"Add co2 constraints or prices\n\n    Args:\n        network (pypsa.Network): the network to which prices or constraints are to be added\n        co2_control (dict): the config\n\n\n    Raises:\n        ValueError: unrecognised co2 control option\n    \"\"\"\n\n    if co2_control[\"control\"] is None:\n        pass\n    elif co2_control[\"control\"].startswith(\"budget\"):\n        co2_limit = co2_control[\"co2_pr_or_limit\"]\n        logger.info(\"Adding CO2 constraint based on scenario {co2_limit}\")\n        network.add(\n            \"GlobalConstraint\",\n            \"co2_limit\",\n            type=\"primary_energy\",\n            carrier_attribute=\"co2_emissions\",\n            sense=\"&lt;=\",\n            constant=co2_limit,\n        )\n    else:\n        logger.error(f\"Unhandled CO2 control config {co2_control} due to unknown control.\")\n        raise ValueError(f\"Unhandled CO2 config {config['scenario']['co2_reduction']}\")\n</code></pre>"},{"location":"reference/prepare_base_network/#prepare_base_network.add_wind_and_solar","title":"<code>add_wind_and_solar(network, techs, paths, year, costs)</code>","text":"<p>Adds wind and solar generators for each grade of renewable energy technology</p> <p>Parameters:</p> Name Type Description Default <code>network</code> <code>Network</code> <p>The PyPSA network to which the generators will be added</p> required <code>techs</code> <code>list</code> <p>A list of renewable energy technologies to add (e.g., [\"solar\", \"onwind\", \"offwind\"])</p> required <code>paths</code> <code>PathLike</code> <p>file paths containing renewable profiles (snakemake.input)</p> required <code>year</code> <code>int</code> <p>planning year</p> required <code>costs</code> <code>DataFrame</code> <p>cost parameters for each technology</p> required <p>Raises:     ValueError: for unsupported technologies or missing paths.</p> Source code in <code>workflow/scripts/prepare_base_network.py</code> <pre><code>def add_wind_and_solar(\n    network: pypsa.Network,\n    techs: list,\n    paths: os.PathLike,\n    year: int,\n    costs: pd.DataFrame,\n):\n    \"\"\"\n    Adds wind and solar generators for each grade of renewable energy technology\n\n    Args:\n        network (pypsa.Network): The PyPSA network to which the generators will be added\n        techs (list): A list of renewable energy technologies to add (e.g., [\"solar\", \"onwind\", \"offwind\"])\n        paths (os.PathLike): file paths containing renewable profiles (snakemake.input)\n        year (int): planning year\n        costs (pd.DataFrame): cost parameters for each technology\n    Raises:\n        ValueError: for unsupported technologies or missing paths.\n    \"\"\"\n\n    unsupported = set(techs).difference({\"solar\", \"onwind\", \"offwind\"})\n    if unsupported:\n        raise ValueError(f\"Carrier(s) {unsupported} not wind or solar pv\")\n    prof_paths = {f\"profile_{tech}\": paths[f\"profile_{tech}\"] for tech in techs}\n    if len(prof_paths) != len(techs):\n        raise ValueError(f\"Paths do not correspond to techs  ({prof_paths} vs {techs})\")\n\n    for tech in techs:\n        # load the renewable profiles\n        logger.info(f\"Attaching {tech} to network\")\n        with xr.open_dataset(prof_paths[f\"profile_{tech}\"]) as ds:\n            if ds.indexes[\"bus\"].empty:\n                continue\n            if \"year\" in ds.indexes:\n                ds = ds.sel(year=ds.year.min(), drop=True)\n\n            timestamps = pd.DatetimeIndex(ds.time)\n            # shift weather year to planning year\n            timestamps = timestamps.map(lambda t: t.replace(year=int(year)))\n            ds = ds.assign_coords(time=timestamps)\n\n            mask = ds.time.isin(network.snapshots)\n            ds = ds.sel(time=mask)\n\n            if not len(ds.time) == len(network.snapshots):\n                raise ValueError(\n                    f\"Mismatch in profile and network timestamps {len(ds.time)} and {len(network.snapshots)}\"\n                )\n            ds = ds.stack(bus_bin=[\"bus\", \"bin\"])\n\n        # bins represent renewable generation grades\n        flatten = lambda t: \" grade\".join(map(str, t))\n        buses = ds.indexes[\"bus_bin\"].get_level_values(\"bus\")\n        bus_bins = ds.indexes[\"bus_bin\"].map(flatten)\n\n        p_nom_max = ds[\"p_nom_max\"].to_pandas()\n        p_nom_max.index = p_nom_max.index.map(flatten)\n\n        p_max_pu = ds[\"profile\"].to_pandas()\n        p_max_pu.columns = p_max_pu.columns.map(flatten)\n\n        # add renewables\n        network.add(\n            \"Generator\",\n            bus_bins,\n            suffix=f\" {tech}\",\n            bus=buses,\n            carrier=tech,\n            p_nom_extendable=True,\n            p_nom_max=p_nom_max,\n            capital_cost=costs.at[tech, \"capital_cost\"],\n            marginal_cost=costs.at[tech, \"marginal_cost\"],\n            p_max_pu=p_max_pu,\n            lifetime=costs.at[tech, \"lifetime\"],\n        )\n</code></pre>"},{"location":"reference/prepare_base_network/#prepare_base_network.prepare_network","title":"<code>prepare_network(config, costs, paths)</code>","text":"<p>Prepares/makes the network object for myopic mode according to config &amp; at 1 node per region/province</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict</code> <p>the snakemake config</p> required <code>costs</code> <code>DataFrame</code> <p>the costs dataframe (anualised capex and marginal costs)</p> required <code>paths</code> <code>dict</code> <p>dictionary of paths to input data</p> required <p>Returns:</p> Type Description <code>Network</code> <p>pypsa.Network: the pypsa network object</p> Source code in <code>workflow/scripts/prepare_base_network.py</code> <pre><code>def prepare_network(config: dict, costs: pd.DataFrame, paths: dict) -&gt; pypsa.Network:\n    \"\"\"Prepares/makes the network object for myopic mode according to config &amp;\n    at 1 node per region/province\n\n    Args:\n        config (dict): the snakemake config\n        costs (pd.DataFrame): the costs dataframe (anualised capex and marginal costs)\n        paths (dict): dictionary of paths to input data\n\n    Returns:\n        pypsa.Network: the pypsa network object\n    \"\"\"\n\n    # derive the config\n    config[\"add_gas\"] = (\n        True if [tech for tech in config[\"Techs\"][\"conv_techs\"] if \"gas\" in tech] else False\n    )\n    config[\"add_coal\"] = (\n        True if [tech for tech in config[\"Techs\"][\"conv_techs\"] if \"coal\" in tech] else False\n    )\n\n    planning_horizons = snakemake.wildcards[\"planning_horizons\"]\n    # empty network object\n    network = pypsa.Network()\n    # load graph\n    nodes = pd.Index(PROV_NAMES)\n\n    # set times\n    # make snapshots (drop leap days)\n    snapshot_cfg = config[\"snapshots\"]\n    snapshots = make_periodic_snapshots(\n        year=snakemake.wildcards.planning_horizons,\n        freq=snapshot_cfg[\"freq\"],\n        start_day_hour=snapshot_cfg[\"start\"],\n        end_day_hour=snapshot_cfg[\"end\"],\n        bounds=snapshot_cfg[\"bounds\"],\n        # naive local tz\n        tz=None,\n        end_year=(\n            None\n            if not snapshot_cfg[\"end_year_plus1\"]\n            else snakemake.wildcards.planning_horizons + 1\n        ),\n    )\n    network.set_snapshots(snapshots.values)\n\n    network.snapshot_weightings[:] = config[\"snapshots\"][\"frequency\"]\n    represented_hours = network.snapshot_weightings.sum()[0]\n    # TODO: what about leap years?\n    n_years = represented_hours / YEAR_HRS\n\n    prov_shapes = read_province_shapes(snakemake.input.province_shape)\n    prov_centroids = prov_shapes.to_crs(\"+proj=cea\").centroid.to_crs(CRS)\n\n    # TODO split by carrier, make transparent\n    # add buses\n    for suffix in config[\"bus_suffix\"]:\n        carrier = config[\"bus_carrier\"][suffix]\n        add_buses(network, nodes, suffix, carrier, prov_centroids)\n\n    add_carriers(network, config, costs)\n\n    # ===== add load demand data =======\n    demand_path = snakemake.input.elec_load.replace(\"{planning_horizons}\", cost_year)\n    with pd.HDFStore(demand_path, mode=\"r\") as store:\n        load = store[\"load\"].loc[network.snapshots]  # MWh !!\n\n    load.columns = PROV_NAMES\n\n    network.add(\"Load\", nodes, bus=nodes, p_set=load[nodes])\n\n    ws_carriers = [c for c in config[\"Techs\"][\"vre_techs\"] if c.find(\"wind\") &gt;= 0 or c == \"solar\"]\n    add_wind_and_solar(network, ws_carriers, paths, planning_horizons, costs)\n\n    if config[\"heat_coupling\"]:\n\n        central_fraction = pd.read_hdf(snakemake.input.central_fraction)\n        with pd.HDFStore(snakemake.input.heat_demand_profile, mode=\"r\") as store:\n            heat_demand = store[\"heat_demand_profiles\"]\n            # TODO fix this possilby not working\n            heat_demand.index = heat_demand.index.tz_localize(None)\n            heat_demand = heat_demand.loc[network.snapshots]\n\n        network.add(\n            \"Load\",\n            nodes,\n            suffix=\" decentral heat\",\n            bus=nodes + \" decentral heat\",\n            p_set=heat_demand[nodes].multiply(1 - central_fraction),\n        )\n\n        network.add(\n            \"Load\",\n            nodes,\n            suffix=\" central heat\",\n            bus=nodes + \" central heat\",\n            p_set=heat_demand[nodes].multiply(central_fraction),\n        )\n\n    # ====== add gas techs ======\n    if [tech for tech in config[\"Techs\"][\"conv_techs\"] if \"gas\" in tech]:\n\n        # add converter from fuel source\n        network.add(\n            \"Generator\",\n            nodes,\n            suffix=\" gas fuel\",\n            bus=nodes + \" gas\",\n            carrier=\"gas\",\n            p_nom_extendable=False,\n            p_nom=1e8,\n            marginal_cost=costs.at[\"OCGT\", \"fuel\"],\n        )\n\n        network.add(\n            \"Store\",\n            nodes + \" gas Store\",\n            bus=nodes + \" gas\",\n            e_nom_extendable=False,\n            e_nom=1e8,\n            e_cyclic=True,\n            carrier=\"gas\",\n        )\n\n    if \"OCGT gas\" in config[\"Techs\"][\"conv_techs\"]:\n        network.add(\n            \"Link\",\n            nodes,\n            suffix=\" OCGT\",\n            bus0=nodes + \" gas\",\n            bus1=nodes,\n            carrier=\"OCGT gas\",\n            marginal_cost=costs.at[\"OCGT\", \"efficiency\"]\n            * costs.at[\"OCGT\", \"VOM\"],  # NB: VOM is per MWel\n            capital_cost=costs.at[\"OCGT\", \"efficiency\"]\n            * costs.at[\"OCGT\", \"capital_cost\"],  # NB: capital cost is per MWel\n            p_nom_extendable=True,\n            efficiency=costs.at[\"OCGT\", \"efficiency\"],\n            lifetime=costs.at[\"OCGT\", \"lifetime\"],\n        )\n\n    if \"gas boiler\" in config[\"Techs\"][\"conv_techs\"] and config[\"heat_coupling\"]:\n        for cat in [\" decentral \"]:\n            network.add(\n                \"Link\",\n                nodes + cat + \"gas boiler\",\n                p_nom_extendable=True,\n                bus0=nodes + \" gas\",\n                bus1=nodes + cat + \"heat\",\n                carrier=\"gas boiler\",\n                efficiency=costs.at[cat.lstrip() + \"gas boiler\", \"efficiency\"],\n                marginal_cost=costs.at[cat.lstrip() + \"gas boiler\", \"efficiency\"]\n                * costs.at[cat.lstrip() + \"gas boiler\", \"VOM\"],\n                capital_cost=costs.at[cat.lstrip() + \"gas boiler\", \"efficiency\"]\n                * costs.at[cat.lstrip() + \"gas boiler\", \"capital_cost\"],\n                lifetime=costs.at[cat.lstrip() + \"gas boiler\", \"lifetime\"],\n            )\n\n    # TODO missing second bus?\n    if \"CHP gas\" in config[\"Techs\"][\"conv_techs\"]:\n        network.add(\n            \"Link\",\n            nodes,\n            suffix=\" central CHP gas generator\",\n            bus0=nodes + \" gas\",\n            bus1=nodes,\n            carrier=\"CHP gas\",\n            p_nom_extendable=True,\n            marginal_cost=costs.at[\"central gas CHP\", \"efficiency\"]\n            * costs.at[\"central gas CHP\", \"VOM\"],  # NB: VOM is per MWel\n            capital_cost=costs.at[\"central gas CHP\", \"efficiency\"]\n            * costs.at[\"central gas CHP\", \"capital_cost\"],  # NB: capital cost is per MWel\n            efficiency=costs.at[\"central gas CHP\", \"efficiency\"],\n            p_nom_ratio=1.0,\n            c_b=costs.at[\"central gas CHP\", \"c_b\"],\n            lifetime=costs.at[\"central gas CHP\", \"lifetime\"],\n        )\n\n        network.add(\n            \"Link\",\n            nodes,\n            suffix=\" central CHP gas boiler\",\n            bus0=nodes + \" gas\",\n            bus1=nodes + \" central heat\",\n            carrier=\"CHP gas\",\n            p_nom_extendable=True,\n            marginal_cost=costs.at[\"central gas CHP\", \"efficiency\"]\n            * costs.at[\"central gas CHP\", \"VOM\"],  # NB: VOM is per MWel\n            efficiency=costs.at[\"central gas CHP\", \"efficiency\"]\n            / costs.at[\"central gas CHP\", \"c_v\"],\n            lifetime=costs.at[\"central gas CHP\", \"lifetime\"],\n        )\n\n    # TODO separate retrofit in config from coal power plant\n    if \"coal power plant\" in config[\"Techs\"][\"conv_techs\"] and config[\"Techs\"][\"coal_ccs_retrofit\"]:\n        network.add(\n            \"Generator\",\n            nodes,\n            suffix=\" coal cc\",\n            bus=nodes,\n            carrier=\"coal cc\",\n            p_nom_extendable=True,\n            efficiency=costs.at[\"coal\", \"efficiency\"],\n            marginal_cost=costs.at[\"coal\", \"marginal_cost\"],\n            capital_cost=costs.at[\"coal\", \"capital_cost\"]\n            + costs.at[\"retrofit\", \"capital_cost\"],  # NB: capital cost is per MWel\n            lifetime=costs.at[\"coal\", \"lifetime\"],\n        )\n        # TODO FIXME harcoded\n        for year in range(int(planning_horizons) - 25, 2021, 5):\n            network.add(\n                \"Generator\",\n                nodes,\n                suffix=\" coal-\" + str(year) + \"-retrofit\",\n                bus=nodes,\n                carrier=\"coal cc\",\n                p_nom_extendable=True,\n                capital_cost=costs.at[\"coal\", \"capital_cost\"]\n                + costs.at[\"retrofit\", \"capital_cost\"]\n                + 2021\n                - year,\n                efficiency=costs.at[\"coal\", \"efficiency\"],\n                lifetime=costs.at[\"coal\", \"lifetime\"],\n                build_year=year,\n                marginal_cost=costs.at[\"coal\", \"marginal_cost\"],\n            )\n\n    # ===== add coal techs =====\n    if [tech for tech in config[\"Techs\"][\"conv_techs\"] if \"coal\" in tech]:\n        # TODO check if this is needed (added Ivan), add for gas too, also why is it node resolved?\n        # network.add(\n        #     \"Bus\",\n        #     nodes,\n        #     suffix=\" coal fuel\",\n        #     x=prov_centroids.x,\n        #     y=prov_centroids.y,\n        #     carrier=\"coal\",\n        # )\n\n        network.add(\n            \"Generator\",\n            nodes + \" coal fuel\",\n            bus=nodes + \" coal\",\n            carrier=\"coal\",\n            p_nom_extendable=False,\n            p_nom=1e8,\n            marginal_cost=costs.at[\"coal\", \"marginal_cost\"],\n        )\n\n    if \"coal boiler\" in config[\"Techs\"][\"conv_techs\"]:\n        for cat in [\" decentral \", \" central \"]:\n            network.add(\n                \"Link\",\n                nodes + cat + \"coal boiler\",\n                p_nom_extendable=True,\n                bus0=nodes + \" coal\",\n                bus1=nodes + cat + \"heat\",\n                carrier=\"coal boiler\",\n                efficiency=costs.at[cat.lstrip() + \"coal boiler\", \"efficiency\"],\n                marginal_cost=costs.at[cat.lstrip() + \"coal boiler\", \"efficiency\"]\n                * costs.at[cat.lstrip() + \"coal boiler\", \"VOM\"],\n                capital_cost=costs.at[cat.lstrip() + \"coal boiler\", \"efficiency\"]\n                * costs.at[cat.lstrip() + \"coal boiler\", \"capital_cost\"],\n                lifetime=costs.at[cat.lstrip() + \"coal boiler\", \"lifetime\"],\n            )\n    # TODO missing second bus?\n    if \"CHP coal\" in config[\"Techs\"][\"conv_techs\"]:\n        network.add(\n            \"Link\",\n            nodes,\n            suffix=\" central CHP coal generator\",\n            bus0=nodes + \" coal\",\n            bus1=nodes,\n            carrier=\"CHP coal\",\n            p_nom_extendable=True,\n            marginal_cost=costs.at[\"central coal CHP\", \"efficiency\"]\n            * costs.at[\"central coal CHP\", \"VOM\"],  # NB: VOM is per MWel\n            capital_cost=costs.at[\"central coal CHP\", \"efficiency\"]\n            * costs.at[\"central coal CHP\", \"capital_cost\"],  # NB: capital cost is per MWel\n            efficiency=costs.at[\"central coal CHP\", \"efficiency\"],\n            p_nom_ratio=1.0,\n            c_b=costs.at[\"central coal CHP\", \"c_b\"],\n            lifetime=costs.at[\"central coal CHP\", \"lifetime\"],\n        )\n\n        network.add(\n            \"Link\",\n            nodes,\n            suffix=\" central CHP coal boiler\",\n            bus0=nodes + \" coal\",\n            bus1=nodes + \" central heat\",\n            carrier=\"CHP coal\",\n            p_nom_extendable=True,\n            marginal_cost=costs.at[\"central coal CHP\", \"efficiency\"]\n            * costs.at[\"central coal CHP\", \"VOM\"],  # NB: VOM is per MWel\n            efficiency=costs.at[\"central coal CHP\", \"efficiency\"]\n            / costs.at[\"central coal CHP\", \"c_v\"],\n            lifetime=costs.at[\"central coal CHP\", \"lifetime\"],\n        )\n\n    if config[\"add_biomass\"]:\n        network.add(\n            \"Bus\",\n            nodes,\n            suffix=\" biomass\",\n            x=prov_centroids.x,\n            y=prov_centroids.y,\n            carrier=\"biomass\",\n        )\n\n        biomass_potential = pd.read_hdf(snakemake.input.biomass_potential)\n        biomass_potential.index = biomass_potential.index + \" biomass\"\n        network.add(\n            \"Store\",\n            nodes + \" biomass\",\n            bus=nodes + \" biomass\",\n            e_nom_extendable=False,\n            e_nom=biomass_potential,\n            e_initial=biomass_potential,\n            carrier=\"biomass\",\n        )\n\n        network.add(\"Carrier\", \"CO2\", co2_emissions=0)\n        network.add(\n            \"Bus\",\n            nodes,\n            suffix=\" CO2\",\n            x=prov_centroids.x,\n            y=prov_centroids.y,\n            carrier=\"CO2\",\n        )\n\n        network.add(\"Store\", nodes + \" CO2\", bus=nodes + \" CO2\", carrier=\"CO2\")\n        # normally taking away from carrier generates CO2, but here we are\n        # adding CO2 stored, so the emissions will point the other way ?\n        network.add(\"Carrier\", \"CO2 capture\", co2_emissions=1)\n        network.add(\n            \"Bus\",\n            nodes,\n            suffix=\" CO2 capture\",\n            x=prov_centroids.x,\n            y=prov_centroids.y,\n            carrier=\"CO2 capture\",\n        )\n\n        network.add(\n            \"Store\",\n            nodes + \" CO2 capture\",\n            bus=nodes + \" CO2 capture\",\n            e_nom_extendable=True,\n            carrier=\"CO2 capture\",\n        )\n\n        network.add(\n            \"Link\",\n            nodes + \" central biomass CHP capture\",\n            bus0=nodes + \" CO2\",\n            bus1=nodes + \" CO2 capture\",\n            bus2=nodes,\n            p_nom_extendable=True,\n            carrier=\"CO2 capture\",\n            efficiency=costs.at[\"biomass CHP capture\", \"capture_rate\"],\n            efficiency2=-1\n            * costs.at[\"biomass CHP capture\", \"capture_rate\"]\n            * costs.at[\"biomass CHP capture\", \"electricity-input\"],\n            capital_cost=costs.at[\"biomass CHP capture\", \"capture_rate\"]\n            * costs.at[\"biomass CHP capture\", \"capital_cost\"],\n            lifetime=costs.at[\"biomass CHP capture\", \"lifetime\"],\n        )\n        # TODO rmemoe hard coded\n        network.add(\n            \"Link\",\n            nodes + \" central biomass CHP\",\n            bus0=nodes + \" biomass\",\n            bus1=nodes,\n            bus2=nodes + \" central heat\",\n            bus3=nodes + \" CO2\",\n            p_nom_extendable=True,\n            carrier=\"biomass\",\n            efficiency=costs.at[\"biomass CHP\", \"efficiency\"],\n            efficiency2=costs.at[\"biomass CHP\", \"efficiency-heat\"],\n            # 4187.0095385594495TWh equates to 0.79*(5.24/3.04) Gt CO2  # tCO2/MWh\n            # TODO centralise\n            efficiency3=0.32522269504651985,\n            capital_cost=costs.at[\"biomass CHP\", \"efficiency\"]\n            * costs.at[\"biomass CHP\", \"capital_cost\"],\n            marginal_cost=costs.at[\"biomass CHP\", \"efficiency\"]\n            * costs.at[\"biomass CHP\", \"marginal_cost\"]\n            + costs.at[\"solid biomass\", \"fuel\"],\n            lifetime=costs.at[\"biomass CHP\", \"lifetime\"],\n        )\n\n        network.add(\n            \"Link\",\n            nodes + \" decentral biomass boiler\",\n            bus0=nodes + \" biomass\",\n            bus1=nodes + \" decentral heat\",\n            p_nom_extendable=True,\n            carrier=\"biomass\",\n            efficiency=costs.at[\"biomass boiler\", \"efficiency\"],\n            capital_cost=costs.at[\"biomass boiler\", \"efficiency\"]\n            * costs.at[\"biomass boiler\", \"capital_cost\"],\n            marginal_cost=costs.at[\"biomass boiler\", \"efficiency\"]\n            * costs.at[\"biomass boiler\", \"marginal_cost\"]\n            + costs.at[\"biomass boiler\", \"pelletizing cost\"]\n            + costs.at[\"solid biomass\", \"fuel\"],\n            lifetime=costs.at[\"biomass boiler\", \"lifetime\"],\n        )\n\n    if config[\"add_hydro\"]:\n\n        # load dams\n        df = pd.read_csv(config[\"hydro_dams\"][\"dams_path\"], index_col=0)\n        points = df.apply(lambda row: Point(row.Lon, row.Lat), axis=1)\n        dams = gpd.GeoDataFrame(df, geometry=points, crs=CRS)\n\n        hourly_rng = pd.date_range(\n            config[\"hydro_dams\"][\"inflow_date_start\"],\n            config[\"hydro_dams\"][\"inflow_date_end\"],\n            freq=\"1h\",\n            inclusive=\"left\",\n        )\n        inflow = pd.read_pickle(config[\"hydro_dams\"][\"inflow_path\"]).reindex(\n            hourly_rng, fill_value=0\n        )\n        inflow.columns = dams.index\n        # convert to naive local timezone\n        inflow.index = inflow.index.tz_localize(\"UTC\").tz_convert(TIMEZONE).tz_localize(None)\n        inflow = inflow.loc[str(INFLOW_DATA_YR)]\n        inflow = shift_profile_to_planning_year(inflow, planning_horizons)\n        inflow = inflow.loc[network.snapshots]\n\n        water_consumption_factor = (\n            dams.loc[:, \"Water_consumption_factor_avg\"] * 1e3\n        )  # m^3/KWh -&gt; m^3/MWh\n\n        ###\n        # # Add hydro stations as buses\n        network.add(\n            \"Bus\",\n            dams.index,\n            suffix=\" station\",\n            carrier=\"stations\",\n            x=dams[\"geometry\"].to_crs(\"+proj=cea\").centroid.to_crs(CRS).x,\n            y=dams[\"geometry\"].to_crs(\"+proj=cea\").centroid.to_crs(CRS).y,\n            location=dams[\"Province\"],\n        )\n\n        dam_buses = network.buses[network.buses.carrier == \"stations\"]\n\n        # ===== add hydro reservoirs as stores ======\n        initial_capacity = pd.read_pickle(config[\"hydro_dams\"][\"reservoir_initial_capacity_path\"])\n        effective_capacity = pd.read_pickle(\n            config[\"hydro_dams\"][\"reservoir_effective_capacity_path\"]\n        )\n        initial_capacity.index = dams.index\n        effective_capacity.index = dams.index\n        initial_capacity = initial_capacity / water_consumption_factor\n        effective_capacity = effective_capacity / water_consumption_factor\n\n        network.add(\n            \"Store\",\n            dams.index,\n            suffix=\" reservoir\",\n            bus=dam_buses.index,\n            e_nom=effective_capacity,\n            e_initial=initial_capacity,\n            e_cyclic=True,\n            marginal_cost=config[\"hyro\"][\"marginal_cost\"][\"reservoir\"]  # EUR/MWh\"\n        )\n\n        # add hydro turbines to link stations to provinces\n        network.add(\n            \"Link\",\n            dams.index,\n            suffix=\" turbines\",\n            bus0=dam_buses.index,\n            bus1=dams[\"Province\"],\n            carrier=\"hydroelectricity\",\n            p_nom=10 * dams[\"installed_capacity_10MW\"],\n            capital_cost=costs.at[\"hydro\", \"capital_cost\"],\n            efficiency=1,\n        )\n\n        # add rivers to link station to station\n        bus0s = [\n            0,\n            21,\n            11,\n            19,\n            22,\n            29,\n            8,\n            40,\n            25,\n            1,\n            7,\n            4,\n            10,\n            15,\n            12,\n            20,\n            26,\n            6,\n            3,\n            39,\n        ]\n        bus1s = [\n            5,\n            11,\n            19,\n            22,\n            32,\n            8,\n            40,\n            25,\n            35,\n            2,\n            4,\n            10,\n            9,\n            12,\n            20,\n            23,\n            6,\n            17,\n            14,\n            16,\n        ]\n\n        for bus0, bus2 in list(zip(dams.index[bus0s], dam_buses.iloc[bus1s].index)):\n\n            # normal flow\n            network.links.at[bus0 + \" turbines\", \"bus2\"] = bus2\n            network.links.at[bus0 + \" turbines\", \"efficiency2\"] = 1.0\n\n        # spillage\n        for bus0, bus1 in list(zip(dam_buses.iloc[bus0s].index, dam_buses.iloc[bus1s].index)):\n            network.add(\n                \"Link\",\n                \"{}-{}\".format(bus0, bus1) + \" spillage\",\n                bus0=bus0,\n                bus1=bus1,\n                p_nom_extendable=True,\n            )\n\n        dam_ends = [\n            dam\n            for dam in range(len(dams.index))\n            if (dam in bus1s and dam not in bus0s) or (dam not in bus0s + bus1s)\n        ]\n\n        for bus0 in dam_buses.iloc[dam_ends].index:\n            network.add(\n                \"Link\",\n                bus0 + \" spillage\",\n                bus0=bus0,\n                bus1=\"Tibet\",\n                p_nom_extendable=True,\n                efficiency=0.0,\n            )\n\n        # == add inflow as generators\n        # only feed into hydro stations which are the first of a cascade\n        inflow_stations = [dam for dam in range(len(dams.index)) if dam not in bus1s]\n\n        for inflow_station in inflow_stations:\n\n            # p_nom = 1 and p_max_pu &amp; p_min_pu = p_pu, compulsory inflow\n\n            p_nom = (inflow / water_consumption_factor).iloc[:, inflow_station].max()\n            p_pu = (inflow / water_consumption_factor).iloc[:, inflow_station] / p_nom\n\n            network.add(\n                \"Generator\",\n                dams.index[inflow_station] + \" inflow\",\n                bus=dam_buses.iloc[inflow_station].name,\n                carrier=\"hydro_inflow\",\n                p_max_pu=p_pu.clip(1.0e-6),\n                p_min_pu=p_pu.clip(1.0e-6),\n                p_nom=p_nom,\n            )\n\n            # p_nom*p_pu = XXX m^3 then use turbines efficiency to convert to power\n\n        # ======= add other existing hydro power\n        hydro_p_nom = pd.read_hdf(config[\"hydro_dams\"][\"p_nom_path\"])\n        hydro_p_max_pu = pd.read_hdf(\n            config[\"hydro_dams\"][\"p_max_pu_path\"],\n            key=config[\"hydro_dams\"][\"p_max_pu_key\"],\n        ).tz_localize(None)\n\n        hydro_p_max_pu = shift_profile_to_planning_year(hydro_p_max_pu, planning_horizons)\n        # sort buses (columns) otherwise stuff will break\n        hydro_p_max_pu.sort_index(axis=1, inplace=True)\n\n        hydro_p_max_pu = hydro_p_max_pu.loc[snapshots]\n        hydro_p_max_pu.index = network.snapshots\n\n        network.add(\n            \"Generator\",\n            nodes,\n            suffix=\" hydroelectricity\",\n            bus=nodes,\n            carrier=\"hydroelectricity\",\n            p_nom=hydro_p_nom,\n            p_nom_min=hydro_p_nom,\n            p_nom_extendable=False,\n            capital_cost=costs.at[\"hydro\", \"capital_cost\"],\n            p_max_pu=hydro_p_max_pu,\n        )\n\n    if config[\"add_H2\"]:\n\n        network.add(\n            \"Bus\",\n            nodes,\n            suffix=\" H2\",\n            x=prov_centroids.x,\n            y=prov_centroids.y,\n            carrier=\"H2\",\n        )\n\n        network.add(\n            \"Link\",\n            nodes + \" H2 Electrolysis\",\n            bus0=nodes,\n            bus1=nodes + \" H2\",\n            bus2=nodes + \" central heat\",\n            p_nom_extendable=True,\n            carrier=\"H2\",\n            efficiency=costs.at[\"electrolysis\", \"efficiency\"],\n            efficiency2=costs.at[\"electrolysis\", \"efficiency-heat\"],\n            capital_cost=costs.at[\"electrolysis\", \"capital_cost\"],\n            lifetime=costs.at[\"electrolysis\", \"lifetime\"],\n        )\n\n        network.add(\n            \"Link\",\n            nodes + \" central H2 CHP\",\n            bus0=nodes + \" H2\",\n            bus1=nodes,\n            bus2=nodes + \" central heat\",\n            p_nom_extendable=True,\n            carrier=\"H2 CHP\",\n            efficiency=costs.at[\"central hydrogen CHP\", \"efficiency\"],\n            efficiency2=costs.at[\"central hydrogen CHP\", \"efficiency\"]\n            / costs.at[\"central hydrogen CHP\", \"c_b\"],\n            capital_cost=costs.at[\"central hydrogen CHP\", \"efficiency\"]\n            * costs.at[\"central hydrogen CHP\", \"capital_cost\"],\n            lifetime=costs.at[\"central hydrogen CHP\", \"lifetime\"],\n        )\n\n        # TODO fix hard coded\n        H2_under_nodes = pd.Index(\n            [\n                \"Sichuan\",\n                \"Chongqing\",\n                \"Hubei\",\n                \"Jiangxi\",\n                \"Anhui\",\n                \"Jiangsu\",\n                \"Shandong\",\n                \"Guangdong\",\n            ]\n        )\n        H2_type1_nodes = nodes.difference(H2_under_nodes)\n\n        network.add(\n            \"Store\",\n            H2_under_nodes + \" H2 Store\",\n            bus=H2_under_nodes + \" H2\",\n            e_nom_extendable=True,\n            e_cyclic=True,\n            capital_cost=costs.at[\"hydrogen storage underground\", \"capital_cost\"],\n            lifetime=costs.at[\"hydrogen storage underground\", \"lifetime\"],\n        )\n\n        network.add(\n            \"Store\",\n            H2_type1_nodes + \" H2 Store\",\n            bus=H2_type1_nodes + \" H2\",\n            e_nom_extendable=True,\n            e_cyclic=True,\n            capital_cost=costs.at[\n                \"hydrogen storage tank type 1 including compressor\", \"capital_cost\"\n            ],\n            lifetime=costs.at[\"hydrogen storage tank type 1 including compressor\", \"lifetime\"],\n        )\n\n    if config[\"add_methanation\"]:\n        network.add(\n            \"Link\",\n            nodes + \" Sabatier\",\n            bus0=nodes + \" H2\",\n            bus1=nodes + \" gas\",\n            p_nom_extendable=True,\n            carrier=\"Sabatier\",\n            efficiency=costs.at[\"methanation\", \"efficiency\"],\n            capital_cost=costs.at[\"methanation\", \"efficiency\"]\n            * costs.at[\"methanation\", \"capital_cost\"]\n            + costs.at[\"direct air capture\", \"capital_cost\"]\n            * costs.at[\"gas\", \"co2_emissions\"]\n            * costs.at[\"methanation\", \"efficiency\"],\n            # TODO fix hardcoded\n            marginal_cost=(400 - 5 * (int(cost_year) - 2020))\n            * costs.at[\"gas\", \"co2_emissions\"]\n            * costs.at[\"methanation\", \"efficiency\"],\n            lifetime=costs.at[\"methanation\", \"lifetime\"],\n        )\n\n    if \"nuclear\" in config[\"Techs\"][\"vre_techs\"]:\n        nuclear_nodes = pd.Index(NUCLEAR_EXTENDABLE)\n        network.add(\n            \"Generator\",\n            nuclear_nodes,\n            suffix=\" nuclear\",\n            p_nom_extendable=True,\n            p_min_pu=0.7,\n            bus=nuclear_nodes,\n            carrier=\"nuclear\",\n            efficiency=costs.at[\"nuclear\", \"efficiency\"],\n            capital_cost=costs.at[\"nuclear\", \"capital_cost\"],  # NB: capital cost is per MWel\n            marginal_cost=costs.at[\"nuclear\", \"marginal_cost\"],\n            lifetime=costs.at[\"nuclear\", \"lifetime\"],\n        )\n\n    if \"heat pump\" in config[\"Techs\"][\"vre_techs\"]:\n\n        with pd.HDFStore(snakemake.input.cop_name, mode=\"r\") as store:\n            ashp_cop = store[\"ashp_cop_profiles\"]\n            ashp_cop.index = ashp_cop.index.tz_localize(None)\n            ashp_cop = shift_profile_to_planning_year(ashp_cop, planning_horizons)\n            gshp_cop = store[\"gshp_cop_profiles\"]\n            gshp_cop.index = gshp_cop.index.tz_localize(None)\n            gshp_cop = shift_profile_to_planning_year(gshp_cop, planning_horizons)\n            ashp_cop = ashp_cop.loc[snapshots]\n            gshp_cop = gshp_cop.loc[snapshots]\n\n        for cat in [\" decentral \", \" central \"]:\n            network.add(\n                \"Link\",\n                nodes,\n                suffix=cat + \"heat pump\",\n                bus0=nodes,\n                bus1=nodes + cat + \"heat\",\n                carrier=\"heat pump\",\n                efficiency=(\n                    ashp_cop[nodes]\n                    if config[\"time_dep_hp_cop\"]\n                    else costs.at[cat.lstrip() + \"air-sourced heat pump\", \"efficiency\"]\n                ),\n                capital_cost=costs.at[cat.lstrip() + \"air-sourced heat pump\", \"efficiency\"]\n                * costs.at[cat.lstrip() + \"air-sourced heat pump\", \"capital_cost\"],\n                marginal_cost=costs.at[cat.lstrip() + \"air-sourced heat pump\", \"efficiency\"]\n                * costs.at[cat.lstrip() + \"air-sourced heat pump\", \"marginal_cost\"],\n                p_nom_extendable=True,\n                lifetime=costs.at[cat.lstrip() + \"air-sourced heat pump\", \"lifetime\"],\n            )\n            # TODO not valid for decentral\n            network.add(\n                \"Link\",\n                nodes,\n                suffix=cat + \" ground heat pump\",\n                bus0=nodes,\n                bus1=nodes + cat + \"heat\",\n                carrier=\"heat pump\",\n                efficiency=(\n                    gshp_cop[nodes]\n                    if config[\"time_dep_hp_cop\"]\n                    else costs.at[\"decentral ground-sourced heat pump\", \"efficiency\"]\n                ),\n                capital_cost=costs.at[cat.lstrip() + \"ground-sourced heat pump\", \"efficiency\"]\n                * costs.at[\"decentral ground-sourced heat pump\", \"capital_cost\"],\n                marginal_cost=costs.at[cat.lstrip() + \"ground-sourced heat pump\", \"efficiency\"]\n                * costs.at[cat.lstrip() + \"ground-sourced heat pump\", \"marginal_cost\"],\n                p_nom_extendable=True,\n                lifetime=costs.at[\"decentral ground-sourced heat pump\", \"lifetime\"],\n            )\n\n    if \"resistive heater\" in config[\"Techs\"][\"vre_techs\"]:\n        for cat in [\" decentral \", \" central \"]:\n            network.add(\n                \"Link\",\n                nodes + cat + \"resistive heater\",\n                bus0=nodes,\n                bus1=nodes + cat + \"heat\",\n                carrier=\"resistive heater\",\n                efficiency=costs.at[cat.lstrip() + \"resistive heater\", \"efficiency\"],\n                capital_cost=costs.at[cat.lstrip() + \"resistive heater\", \"efficiency\"]\n                * costs.at[cat.lstrip() + \"resistive heater\", \"capital_cost\"],\n                marginal_cost=costs.at[cat.lstrip() + \"resistive heater\", \"efficiency\"]\n                * costs.at[cat.lstrip() + \"resistive heater\", \"marginal_cost\"],\n                p_nom_extendable=True,\n                lifetime=costs.at[cat.lstrip() + \"resistive heater\", \"lifetime\"],\n            )\n\n    if \"solar thermal\" in config[\"Techs\"][\"vre_techs\"]:\n        # this is the amount of heat collected in W per m^2, accounting\n        # for efficiency\n        with pd.HDFStore(snakemake.input.solar_thermal_name, mode=\"r\") as store:\n            # 1e3 converts from W/m^2 to MW/(1000m^2) = kW/m^2\n            solar_thermal = config[\"solar_cf_correction\"] * store[\"solar_thermal_profiles\"] / 1e3\n\n        solar_thermal.index = solar_thermal.index.tz_localize(None)\n        solar_thermal = shift_profile_to_planning_year(solar_thermal, planning_horizons)\n        solar_thermal = solar_thermal.loc[snapshots]\n\n        for cat in [\" decentral \", \" central \"]:\n            network.add(\n                \"Generator\",\n                nodes,\n                suffix=cat + \"solar thermal\",\n                bus=nodes + cat + \"heat\",\n                carrier=\"solar thermal\",\n                p_nom_extendable=True,\n                capital_cost=costs.at[cat.lstrip() + \"solar thermal\", \"capital_cost\"],\n                p_max_pu=solar_thermal[nodes].clip(1.0e-4),\n                lifetime=costs.at[cat.lstrip() + \"solar thermal\", \"lifetime\"],\n            )\n\n    if \"water tanks\" in config[\"Techs\"][\"store_techs\"]:\n        for cat in [\" decentral \", \" central \"]:\n            network.add(\n                \"Bus\",\n                nodes,\n                suffix=cat + \"water tanks\",\n                x=prov_centroids.x,\n                y=prov_centroids.y,\n                carrier=\"water tanks\",\n            )\n\n            network.add(\n                \"Link\",\n                nodes + cat + \"water tanks charger\",\n                bus0=nodes + cat + \"heat\",\n                bus1=nodes + cat + \"water tanks\",\n                carrier=\"water tanks\",\n                efficiency=costs.at[\"water tank charger\", \"efficiency\"],\n                p_nom_extendable=True,\n            )\n\n            network.add(\n                \"Link\",\n                nodes + cat + \"water tanks discharger\",\n                bus0=nodes + cat + \"water tanks\",\n                bus1=nodes + cat + \"heat\",\n                carrier=\"water tanks\",\n                efficiency=costs.at[\"water tank discharger\", \"efficiency\"],\n                p_nom_extendable=True,\n            )\n            # [HP] 180 day time constant for centralised, 3 day for decentralised\n            tes_tau = config[\"water_tanks\"][\"tes_tau\"][cat.strip()]\n            network.add(\n                \"Store\",\n                nodes + cat + \"water tank\",\n                bus=nodes + cat + \"water tanks\",\n                carrier=\"water tanks\",\n                e_cyclic=True,\n                e_nom_extendable=True,\n                standing_loss=1 - np.exp(-1 / (24.0 * tes_tau)),\n                capital_cost=costs.at[cat.lstrip() + \"water tank storage\", \"capital_cost\"],\n                lifetime=costs.at[cat.lstrip() + \"water tank storage\", \"lifetime\"],\n            )\n\n    if \"battery\" in config[\"Techs\"][\"store_techs\"]:\n        network.add(\n            \"Bus\",\n            nodes,\n            suffix=\" battery\",\n            x=prov_centroids.x,\n            y=prov_centroids.y,\n            carrier=\"battery\",\n        )\n\n        network.add(\n            \"Store\",\n            nodes + \" battery\",\n            bus=nodes + \" battery\",\n            e_cyclic=True,\n            e_nom_extendable=True,\n            capital_cost=costs.at[\"battery storage\", \"capital_cost\"],\n            lifetime=costs.at[\"battery storage\", \"lifetime\"],\n        )\n\n        network.add(\n            \"Link\",\n            nodes + \" battery charger\",\n            bus0=nodes,\n            bus1=nodes + \" battery\",\n            efficiency=costs.at[\"battery inverter\", \"efficiency\"] ** 0.5,\n            capital_cost=costs.at[\"battery inverter\", \"capital_cost\"],\n            p_nom_extendable=True,\n            carrier=\"battery\",\n            lifetime=costs.at[\"battery inverter\", \"lifetime\"],\n        )\n\n        network.add(\n            \"Link\",\n            nodes + \" battery discharger\",\n            bus0=nodes + \" battery\",\n            bus1=nodes,\n            efficiency=costs.at[\"battery inverter\", \"efficiency\"] ** 0.5,\n            marginal_cost=0.0,\n            carrier=\"battery discharger\",\n            p_nom_extendable=True,\n        )\n\n    if \"PHS\" in config[\"Techs\"][\"store_techs\"]:\n        # pure pumped hydro storage, fixed, 6h energy by default, no inflow\n        hydrocapa_df = pd.read_csv(\"resources/data/hydro/PHS_p_nom.csv\", index_col=0)\n        phss = hydrocapa_df.index[hydrocapa_df[\"MW\"] &gt; 0].intersection(nodes)\n        if config[\"hydro\"][\"hydro_capital_cost\"]:\n            cc = costs.at[\"PHS\", \"capital_cost\"]\n        else:\n            cc = 0.0\n\n        network.add(\n            \"StorageUnit\",\n            phss,\n            suffix=\" PHS\",\n            bus=phss,\n            carrier=\"PHS\",\n            p_nom_extendable=False,\n            p_nom=hydrocapa_df.loc[phss][\"MW\"],\n            p_nom_min=hydrocapa_df.loc[phss][\"MW\"],\n            max_hours=config[\"hydro\"][\"PHS_max_hours\"],\n            efficiency_store=np.sqrt(costs.at[\"PHS\", \"efficiency\"]),\n            efficiency_dispatch=np.sqrt(costs.at[\"PHS\", \"efficiency\"]),\n            cyclic_state_of_charge=True,\n            capital_cost=cc,\n            marginal_cost=0.0,\n        )\n\n    # ============= add lines =========\n    # The lines are implemented according to the transport model (no KVL) with losses.\n    # This requires two directions\n    # see Neumann et al 10.1016/j.apenergy.2022.118859\n    # TODO make lossless optional (speed up)\n\n    if not config[\"no_lines\"]:\n        edges = pd.read_csv(snakemake.input.edges, header=None)\n\n        lengths = NON_LIN_PATH_SCALING * np.array(\n            [\n                haversine(\n                    [network.buses.at[name0, \"x\"], network.buses.at[name0, \"y\"]],\n                    [network.buses.at[name1, \"x\"], network.buses.at[name1, \"y\"]],\n                )\n                for name0, name1 in edges[[0, 1]].values\n            ]\n        )\n\n        cc = (\n            (config[\"line_cost_factor\"] * lengths * [HVAC_cost_curve(len_) for len_ in lengths])\n            * LINE_SECURITY_MARGIN\n            * FOM_LINES\n            * n_years\n            * annuity(ECON_LIFETIME_LINES, config[\"costs\"][\"discountrate\"])\n        )\n\n        network.add(\n            \"Link\",\n            edges[0] + \"-\" + edges[1],\n            bus0=edges[0].values,\n            bus1=edges[1].values,\n            suffix=\" positive\",\n            p_nom_extendable=True,\n            p_min_pu=0,\n            efficiency=config[\"transmission_efficiency\"][\"DC\"][\"efficiency_static\"]\n            * config[\"transmission_efficiency\"][\"DC\"][\"efficiency_per_1000km\"] ** (lengths / 1000),\n            length=lengths,\n            capital_cost=cc,\n        )\n\n        network.add(\n            \"Link\",\n            edges[1] + \"-\" + edges[0],\n            bus0=edges[1].values,\n            bus1=edges[0].values,\n            suffix=\" reversed\",\n            p_nom_extendable=True,\n            p_min_pu=0,\n            efficiency=config[\"transmission_efficiency\"][\"DC\"][\"efficiency_static\"]\n            * config[\"transmission_efficiency\"][\"DC\"][\"efficiency_per_1000km\"] ** (lengths / 1000),\n            length=lengths,\n            capital_cost=0,\n        )\n\n    if config[\"Techs\"][\"hydrogen_lines\"]:\n        edges = pd.read_csv(snakemake.input.edges, header=None)\n        lengths = NON_LIN_PATH_SCALING * np.array(\n            [\n                haversine(\n                    [network.buses.at[name0, \"x\"], network.buses.at[name0, \"y\"]],\n                    [network.buses.at[name1, \"x\"], network.buses.at[name1, \"y\"]],\n                )\n                for name0, name1 in edges[[0, 1]].values\n            ]\n        )\n\n        cc = costs.at[\"H2 (g) pipeline\", \"capital_cost\"] * lengths\n\n        network.add(\n            \"Link\",\n            edges[0] + \"-\" + edges[1] + \" H2 pipeline\",\n            suffix=\" positive\",\n            bus0=edges[0].values + \" H2\",\n            bus1=edges[1].values + \" H2\",\n            bus2=edges[0].values,\n            p_nom_extendable=True,\n            p_nom=0,\n            p_nom_min=0,\n            p_min_pu=0,\n            efficiency=config[\"transmission_efficiency\"][\"H2 pipeline\"][\"efficiency_static\"]\n            * config[\"transmission_efficiency\"][\"H2 pipeline\"][\"efficiency_per_1000km\"]\n            ** (lengths / 1000),\n            efficiency2=-config[\"transmission_efficiency\"][\"H2 pipeline\"][\"compression_per_1000km\"]\n            * lengths\n            / 1e3,\n            length=lengths,\n            lifetime=costs.at[\"H2 (g) pipeline\", \"lifetime\"],\n            capital_cost=cc,\n        )\n\n        network.add(\n            \"Link\",\n            edges[1] + \"-\" + edges[0] + \" H2 pipeline\",\n            suffix=\" reversed\",\n            bus0=edges[1].values + \" H2\",\n            bus1=edges[0].values + \" H2\",\n            bus2=edges[1].values,\n            p_nom_extendable=True,\n            p_nom=0,\n            p_nom_min=0,\n            p_min_pu=0,\n            efficiency=config[\"transmission_efficiency\"][\"H2 pipeline\"][\"efficiency_static\"]\n            * config[\"transmission_efficiency\"][\"H2 pipeline\"][\"efficiency_per_1000km\"]\n            ** (lengths / 1000),\n            efficiency2=-config[\"transmission_efficiency\"][\"H2 pipeline\"][\"compression_per_1000km\"]\n            * lengths\n            / 1e3,\n            length=lengths,\n            lifetime=costs.at[\"H2 (g) pipeline\", \"lifetime\"],\n            capital_cost=0,\n        )\n    return network\n</code></pre>"},{"location":"reference/prepare_base_network_2020/","title":"Prepare base network 2020","text":""},{"location":"reference/prepare_existing_capacities/","title":"Prepare existing capacities","text":"<p>Functions to prepare existing assets for the network</p> <p>SHORT TERM FIX until PowerPlantMatching is implemented - required as split from add_existing_baseyear for remind compat</p>"},{"location":"reference/prepare_existing_capacities/#prepare_existing_capacities.assign_year_bins","title":"<code>assign_year_bins(df, year_bins)</code>","text":"<p>Assign a year bin to the existing capacities according to the config</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame with existing capacities and build years (DateIn)</p> required <code>year_bins</code> <code>list</code> <p>years to bin the existing capacities to</p> required <p>Returns:     pd.DataFrame: DataFrame regridded to the year bins</p> Source code in <code>workflow/scripts/prepare_existing_capacities.py</code> <pre><code>def assign_year_bins(df: pd.DataFrame, year_bins: list) -&gt; pd.DataFrame:\n    \"\"\"\n    Assign a year bin to the existing capacities according to the config\n\n    Args:\n        df (pd.DataFrame): DataFrame with existing capacities and build years (DateIn)\n        year_bins (list): years to bin the existing capacities to\n    Returns:\n        pd.DataFrame: DataFrame regridded to the year bins\n    \"\"\"\n\n    df_ = df.copy()\n    # bin by years (np.digitize)\n    df_[\"grouping_year\"] = np.take(year_bins, np.digitize(df.DateIn, year_bins, right=True))\n    return df_.fillna(0)\n</code></pre>"},{"location":"reference/prepare_existing_capacities/#prepare_existing_capacities.convert_CHP_to_poweronly","title":"<code>convert_CHP_to_poweronly(capacities)</code>","text":"<p>Convert CHP capacities to power-only capacities by removing the heat part</p> <p>Parameters:</p> Name Type Description Default <code>capacities</code> <code>DataFrame</code> <p>DataFrame with existing capacities</p> required <p>Returns:     pd.DataFrame: DataFrame with converted capacities</p> Source code in <code>workflow/scripts/prepare_existing_capacities.py</code> <pre><code>def convert_CHP_to_poweronly(capacities: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Convert CHP capacities to power-only capacities by removing the heat part\n\n    Args:\n        capacities (pd.DataFrame): DataFrame with existing capacities\n    Returns:\n        pd.DataFrame: DataFrame with converted capacities\n    \"\"\"\n    # Convert CHP to power-only by removing the heat part\n    chp_mask = capacities.Tech.str.contains(\"CHP\")\n    capacities.loc[chp_mask, \"Fueltype\"] = (\n        capacities.loc[chp_mask, \"Fueltype\"]\n        .str.replace(\"central coal CHP\", \"coal power plant\")\n        .str.replace(\"central gas CHP\", \"gas CCGT\")\n    )\n    # update the Tech field based on the converted Fueltype\n    capacities.loc[chp_mask, \"Tech\"] = (\n        capacities.loc[chp_mask, \"Fueltype\"]\n        .str.replace(\" CHP\", \"\")\n        .str.replace(\"CHP \", \" \")\n        .str.replace(\"gas \", \"\")\n        .str.replace(\"coal power plant\", \"coal\")\n    )\n    return capacities\n</code></pre>"},{"location":"reference/prepare_existing_capacities/#prepare_existing_capacities.determine_simulation_timespan","title":"<code>determine_simulation_timespan(config, year)</code>","text":"<p>Determine the simulation timespan in years (so the network object is not needed) Args:     config (dict): the snakemake config     year (int): the year to simulate Returns:     int: the simulation timespan in years</p> Source code in <code>workflow/scripts/prepare_existing_capacities.py</code> <pre><code>def determine_simulation_timespan(config: dict, year: int) -&gt; int:\n    \"\"\"Determine the simulation timespan in years (so the network object is not needed)\n    Args:\n        config (dict): the snakemake config\n        year (int): the year to simulate\n    Returns:\n        int: the simulation timespan in years\n    \"\"\"\n\n    # make snapshots (drop leap days) -&gt; possibly do all the unpacking in the function\n    snapshot_cfg = config[\"snapshots\"]\n    snapshots = make_periodic_snapshots(\n        year=year,\n        freq=snapshot_cfg[\"freq\"],\n        start_day_hour=snapshot_cfg[\"start\"],\n        end_day_hour=snapshot_cfg[\"end\"],\n        bounds=snapshot_cfg[\"bounds\"],\n        # naive local timezone\n        tz=None,\n        end_year=None if not snapshot_cfg[\"end_year_plus1\"] else year + 1,\n    )\n\n    # load costs\n    n_years = config[\"snapshots\"][\"frequency\"] * len(snapshots) / YEAR_HRS\n\n    return n_years\n</code></pre>"},{"location":"reference/prepare_existing_capacities/#prepare_existing_capacities.distribute_vre_by_grade","title":"<code>distribute_vre_by_grade(cap_by_year, grade_capacities)</code>","text":"<p>distribute vre capacities by grade potential, use up better grades first</p> <p>Parameters:</p> Name Type Description Default <code>cap_by_year</code> <code>Series</code> <p>the vre tech potential p_nom_max added per year</p> required <code>grade_capacities</code> <code>Series</code> <p>the vre grade potential for the tech and bus</p> required <p>Returns:     pd.DataFrame: DataFrame with the distributed vre capacities (shape: years x buses)</p> Source code in <code>workflow/scripts/prepare_existing_capacities.py</code> <pre><code>def distribute_vre_by_grade(cap_by_year: pd.Series, grade_capacities: pd.Series) -&gt; pd.DataFrame:\n    \"\"\"distribute vre capacities by grade potential, use up better grades first\n\n    Args:\n        cap_by_year (pd.Series): the vre tech potential p_nom_max added per year\n        grade_capacities (pd.Series): the vre grade potential for the tech and bus\n    Returns:\n        pd.DataFrame: DataFrame with the distributed vre capacities (shape: years x buses)\n    \"\"\"\n\n    availability = cap_by_year.sort_index(ascending=False)\n    to_distribute = grade_capacities.fillna(0).sort_index()\n    n_years = len(to_distribute)\n    n_sources = len(availability)\n\n    # To store allocation per year per source (shape: sources x years)\n    allocation = np.zeros((n_sources, n_years), dtype=int)\n    remaining = availability.values\n\n    for j in range(n_years):\n        needed = to_distribute.values[j]\n        cumsum = np.cumsum(remaining)\n        used_up = cumsum &lt; needed\n        cutoff = np.argmax(cumsum &gt;= needed)\n\n        allocation[used_up, j] = remaining[used_up]\n\n        if needed &gt; (cumsum[cutoff - 1] if cutoff &gt; 0 else 0):\n            allocation[cutoff, j] = needed - (cumsum[cutoff - 1] if cutoff &gt; 0 else 0)\n\n        # Subtract what was used from availability\n        remaining -= allocation[:, j]\n\n    return pd.DataFrame(data=allocation, columns=grade_capacities.index, index=availability.index)\n</code></pre>"},{"location":"reference/prepare_existing_capacities/#prepare_existing_capacities.fix_existing_capacities","title":"<code>fix_existing_capacities(existing_df, costs, year_bins, baseyear)</code>","text":"<p>add/fill missing dateIn, drop expired assets, drop too new assets</p> <p>Parameters:</p> Name Type Description Default <code>existing_df</code> <code>DataFrame</code> <p>the existing capacities</p> required <code>costs</code> <code>DataFrame</code> <p>the technoeconomic data</p> required <code>year_bins</code> <code>list</code> <p>the year groups</p> required <code>baseyear</code> <code>int</code> <p>the base year (run year)</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: fixed capacities</p> Source code in <code>workflow/scripts/prepare_existing_capacities.py</code> <pre><code>def fix_existing_capacities(\n    existing_df: pd.DataFrame, costs: pd.DataFrame, year_bins: list, baseyear: int\n) -&gt; pd.DataFrame:\n    \"\"\"add/fill missing dateIn, drop expired assets, drop too new assets\n\n    Args:\n        existing_df (pd.DataFrame): the existing capacities\n        costs (pd.DataFrame): the technoeconomic data\n        year_bins (list): the year groups\n        baseyear (int): the base year (run year)\n\n    Returns:\n        pd.DataFrame: fixed capacities\n    \"\"\"\n    existing_df.DateIn = existing_df.DateIn.astype(int)\n    # add/fill missing dateIn\n    if \"DateOut\" not in existing_df.columns:\n        existing_df[\"DateOut\"] = np.nan\n    # names matching costs split across FuelType and Tech, apply to both. Fillna means no overwrite\n    lifetimes = existing_df.Fueltype.map(costs.lifetime).fillna(\n        existing_df.Tech.map(costs.lifetime)\n    )\n    if lifetimes.isna().any():\n        raise ValueError(\n            f\"Some assets have no lifetime assigned: \\n{lifetimes[lifetimes.isna()]}. \"\n            \"Please check the costs file for the missing lifetimes.\"\n        )\n    existing_df.loc[:, \"DateOut\"] = existing_df.DateOut.fillna(lifetimes) + existing_df.DateIn\n\n    # TODO go through the pypsa-EUR fuel drops for the new ppmatching style\n    # drop assets which are already phased out / decommissioned\n    phased_out = existing_df[existing_df[\"DateOut\"] &lt; baseyear].index\n    existing_df.drop(phased_out, inplace=True)\n\n    newer_assets = (existing_df.DateIn &gt; max(year_bins)).sum()\n    if newer_assets:\n        logger.warning(\n            f\"There are {newer_assets} assets with build year \"\n            f\"after last power grouping year {max(year_bins)}. \"\n            \"These assets are dropped and not considered.\"\n            \"Consider to redefine the grouping years to keep them.\"\n        )\n        to_drop = existing_df[existing_df.DateIn &gt; max(year_bins)].index\n        existing_df.drop(to_drop, inplace=True)\n\n    existing_df[\"lifetime\"] = existing_df.DateOut - existing_df[\"grouping_year\"]\n\n    existing_df.rename(columns={\"cluster_bus\": \"bus\"}, inplace=True)\n    return existing_df\n</code></pre>"},{"location":"reference/prepare_existing_capacities/#prepare_existing_capacities.read_existing_capacities","title":"<code>read_existing_capacities(paths_dict, techs)</code>","text":"<p>Read existing capacities from csv files and format them Args:     paths_dict (dict[str, os.PathLike]): dictionary with paths to the csv files     techs (list): list of technologies to read Returns:     pd.DataFrame: DataFrame with existing capacities</p> Source code in <code>workflow/scripts/prepare_existing_capacities.py</code> <pre><code>def read_existing_capacities(paths_dict: dict[str, os.PathLike], techs: list) -&gt; pd.DataFrame:\n    \"\"\"Read existing capacities from csv files and format them\n    Args:\n        paths_dict (dict[str, os.PathLike]): dictionary with paths to the csv files\n        techs (list): list of technologies to read\n    Returns:\n        pd.DataFrame: DataFrame with existing capacities\n    \"\"\"\n    # TODO fix centralise (make a dict from start?)\n    carrier = {\n        \"coal\": \"coal power plant\",\n        \"CHP coal\": \"central coal CHP\",\n        \"CHP gas\": \"central gas CHP\",\n        \"OCGT\": \"gas OCGT\",\n        \"CCGT\": \"gas CCGT\",\n        \"solar\": \"solar\",\n        \"solar thermal\": \"central solar thermal\",\n        \"onwind\": \"onwind\",\n        \"offwind\": \"offwind\",\n        \"coal boiler\": \"central coal boiler\",\n        \"ground heat pump\": \"central ground-sourced heat pump\",\n        \"nuclear\": \"nuclear\",\n    }\n    carrier = {k: v for k, v in carrier.items() if k in techs}\n\n    df_agg = pd.DataFrame()\n    for tech in carrier:\n        df = pd.read_csv(paths_dict[tech], index_col=0).fillna(0.0)\n        df.columns = df.columns.astype(int)\n        df = df.sort_index()\n\n        for year in df.columns:\n            for node in df.index:\n                name = f\"{node}-{tech}-{year}\"\n                capacity = df.loc[node, year]\n                if capacity &gt; 0.0:\n                    df_agg.at[name, \"Fueltype\"] = carrier[tech]\n                    df_agg.at[name, \"Tech\"] = tech\n                    df_agg.at[name, \"Capacity\"] = capacity\n                    df_agg.at[name, \"DateIn\"] = year\n                    df_agg.at[name, \"cluster_bus\"] = node\n\n    return df_agg\n</code></pre>"},{"location":"reference/prepare_network/","title":"Prepare network","text":"<p>Function suite and script to define the network to be solved. Network components are added here. Additional constraints require the linopy model and are added in the solve_network script.</p> <p>These functions are currently only for the overnight mode. Myopic pathway mode contains near         duplicates which need to merged in the future. Idem for solve_network.py</p>"},{"location":"reference/prepare_network/#prepare_network.add_H2","title":"<code>add_H2(network, config, nodes, costs)</code>","text":"<p>add H2 generators, storage and links to the network - currently all or nothing</p> <p>Parameters:</p> Name Type Description Default <code>network</code> <code>Network</code> <p>network object too which H2 comps will be added</p> required <code>config</code> <code>dict</code> <p>the config (snakemake config)</p> required <code>nodes</code> <code>Index</code> <p>the buses</p> required <code>costs</code> <code>DataFrame</code> <p>the cost database</p> required Source code in <code>workflow/scripts/prepare_network.py</code> <pre><code>def add_H2(network: pypsa.Network, config: dict, nodes: pd.Index, costs: pd.DataFrame):\n    \"\"\"add H2 generators, storage and links to the network - currently all or nothing\n\n    Args:\n        network (pypsa.Network): network object too which H2 comps will be added\n        config (dict): the config (snakemake config)\n        nodes (pd.Index): the buses\n        costs (pd.DataFrame): the cost database\n    \"\"\"\n    # TODO, does it make sense?\n    if config.get(\"heat_coupling\", False):\n        network.add(\n            \"Link\",\n            name=nodes + \" H2 Electrolysis\",\n            bus0=nodes,\n            bus1=nodes + \" H2\",\n            bus2=nodes + \" central heat\",\n            p_nom_extendable=True,\n            carrier=\"H2 Electrolysis\",\n            efficiency=costs.at[\"electrolysis\", \"efficiency\"],\n            efficiency2=costs.at[\"electrolysis\", \"efficiency-heat\"],\n            capital_cost=costs.at[\"electrolysis\", \"capital_cost\"],\n            lifetime=costs.at[\"electrolysis\", \"lifetime\"],\n        )\n    else:\n        network.add(\n            \"Link\",\n            name=nodes + \" H2 Electrolysis\",\n            bus0=nodes,\n            bus1=nodes + \" H2\",\n            p_nom_extendable=True,\n            carrier=\"H2 Electrolysis\",\n            efficiency=costs.at[\"electrolysis\", \"efficiency\"],\n            capital_cost=costs.at[\"electrolysis\", \"capital_cost\"],\n            lifetime=costs.at[\"electrolysis\", \"lifetime\"],\n        )\n\n    if \"fuel cell\" in config[\"Techs\"][\"vre_techs\"]:\n        network.add(\n            \"Link\",\n            name=nodes + \" H2 Fuel Cell\",\n            bus0=nodes + \" H2\",\n            bus1=nodes,\n            p_nom_extendable=True,\n            efficiency=costs.at[\"fuel cell\", \"efficiency\"],\n            capital_cost=costs.at[\"fuel cell\", \"efficiency\"]\n            * costs.at[\"fuel cell\", \"capital_cost\"],\n            lifetime=costs.at[\"fuel cell\", \"lifetime\"],\n            carrier=\"H2 fuel cell\",\n        )\n    if \"H2 turbine\" in config[\"Techs\"][\"vre_techs\"]:\n        network.add(\n            \"Link\",\n            name=nodes + \" H2 turbine\",\n            bus0=nodes + \" H2\",\n            bus1=nodes,\n            p_nom_extendable=True,\n            efficiency=costs.at[\"H2 turbine\", \"efficiency\"],\n            capital_cost=costs.at[\"H2 turbine\", \"efficiency\"]\n            * costs.at[\"H2 turbine\", \"capital_cost\"],\n            lifetime=costs.at[\"H2 turbine\", \"lifetime\"],\n            carrier=\"H2 turbine\",\n        )\n\n    H2_under_nodes_ = pd.Index(config[\"H2\"][\"geo_storage_nodes\"])\n    H2_type1_nodes_ = nodes.difference(H2_under_nodes_)\n    H2_under_nodes = H2_under_nodes_.intersection(nodes)\n    H2_type1_nodes = H2_type1_nodes_.intersection(nodes)\n    if not (\n        H2_under_nodes_.shape == H2_under_nodes.shape\n        and H2_type1_nodes_.shape == H2_type1_nodes.shape\n    ):\n        logger.warning(\"Some H2 storage nodes are not in the network buses\")\n\n    network.add(\n        \"Store\",\n        H2_under_nodes + \" H2 Store\",\n        bus=H2_under_nodes + \" H2\",\n        e_nom_extendable=True,\n        e_cyclic=True,\n        capital_cost=costs.at[\"hydrogen storage underground\", \"capital_cost\"],\n        lifetime=costs.at[\"hydrogen storage underground\", \"lifetime\"],\n    )\n\n    # TODO harmonize with remind (add if in techs)\n    network.add(\n        \"Store\",\n        H2_type1_nodes + \" H2 Store\",\n        bus=H2_type1_nodes + \" H2\",\n        e_nom_extendable=True,\n        e_cyclic=True,\n        capital_cost=costs.at[\"hydrogen storage tank type 1 including compressor\", \"capital_cost\"],\n        lifetime=costs.at[\"hydrogen storage tank type 1 including compressor\", \"lifetime\"],\n    )\n    if config[\"add_methanation\"]:\n        cost_year = snakemake.wildcards[\"planning_horizons\"]\n        network.add(\n            \"Link\",\n            nodes + \" Sabatier\",\n            bus0=nodes + \" H2\",\n            bus1=nodes + \" gas\",\n            carrier=\"Sabatier\",\n            p_nom_extendable=True,\n            efficiency=costs.at[\"methanation\", \"efficiency\"],\n            capital_cost=costs.at[\"methanation\", \"efficiency\"]\n            * costs.at[\"methanation\", \"capital_cost\"]\n            + costs.at[\"direct air capture\", \"capital_cost\"]\n            * costs.at[\"gas\", \"co2_emissions\"]\n            * costs.at[\"methanation\", \"efficiency\"],\n            # TODO fix me\n            lifetime=costs.at[\"methanation\", \"lifetime\"],\n            marginal_cost=(400 - 5 * (int(cost_year) - 2020))\n            * costs.at[\"gas\", \"co2_emissions\"]\n            * costs.at[\"methanation\", \"efficiency\"],\n        )\n\n    if config[\"Techs\"][\"hydrogen_lines\"]:\n        edge_path = config[\"edge_paths\"].get(config[\"scenario\"][\"topology\"], None)\n        if edge_path is None:\n            raise ValueError(f\"No grid found for topology {config['scenario']['topology']}\")\n        else:\n            edges_ = pd.read_csv(\n                edge_path, sep=\",\", header=None, names=[\"bus0\", \"bus1\", \"p_nom\"]\n            ).fillna(0)\n            edges = edges_[edges_[\"bus0\"].isin(nodes) &amp; edges_[\"bus1\"].isin(nodes)]\n            if edges_.shape[0] != edges.shape[0]:\n                logger.warning(\"Some edges are not in the network buses\")\n\n        # fix this to use map with x.y\n        lengths = config[\"lines\"][\"line_length_factor\"] * np.array(\n            [\n                haversine(\n                    [network.buses.at[bus0, \"x\"], network.buses.at[bus0, \"y\"]],\n                    [network.buses.at[bus1, \"x\"], network.buses.at[bus1, \"y\"]],\n                )\n                for bus0, bus1 in edges[[\"bus0\", \"bus1\"]].values\n            ]\n        )\n\n        # TODO harmonize with remind (add if in techs)\n        cc = costs.at[\"H2 (g) pipeline\", \"capital_cost\"] * lengths\n\n        # === h2 pipeline with losses ====\n        # NB this only works if there is an equalising constraint, which is hidden in solve_ntwk\n        network.add(\n            \"Link\",\n            edges[\"bus0\"] + \"-\" + edges[\"bus1\"] + \" H2 pipeline\",\n            suffix=\" positive\",\n            bus0=edges[\"bus0\"].values + \" H2\",\n            bus1=edges[\"bus1\"].values + \" H2\",\n            bus2=edges[\"bus0\"].values,\n            carrier=\"H2 pipeline\",\n            p_nom_extendable=True,\n            p_nom=0,\n            p_nom_min=0,\n            p_min_pu=0,\n            efficiency=config[\"transmission_efficiency\"][\"H2 pipeline\"][\"efficiency_static\"]\n            * config[\"transmission_efficiency\"][\"H2 pipeline\"][\"efficiency_per_1000km\"]\n            ** (lengths / 1000),\n            efficiency2=-config[\"transmission_efficiency\"][\"H2 pipeline\"][\"compression_per_1000km\"]\n            * lengths\n            / 1e3,\n            length=lengths,\n            lifetime=costs.at[\"H2 (g) pipeline\", \"lifetime\"],\n            capital_cost=cc,\n        )\n\n        network.add(\n            \"Link\",\n            edges[\"bus0\"] + \"-\" + edges[\"bus1\"] + \" H2 pipeline\",\n            suffix=\" reversed\",\n            carrier=\"H2 pipeline\",\n            bus0=edges[\"bus1\"].values + \" H2\",\n            bus1=edges[\"bus0\"].values + \" H2\",\n            bus2=edges[\"bus1\"].values,\n            p_nom_extendable=True,\n            p_nom=0,\n            p_nom_min=0,\n            p_min_pu=0,\n            efficiency=config[\"transmission_efficiency\"][\"H2 pipeline\"][\"efficiency_static\"]\n            * config[\"transmission_efficiency\"][\"H2 pipeline\"][\"efficiency_per_1000km\"]\n            ** (lengths / 1000),\n            efficiency2=-config[\"transmission_efficiency\"][\"H2 pipeline\"][\"compression_per_1000km\"]\n            * lengths\n            / 1e3,\n            length=lengths,\n            lifetime=costs.at[\"H2 (g) pipeline\", \"lifetime\"],\n            capital_cost=0,\n        )\n</code></pre>"},{"location":"reference/prepare_network/#prepare_network.add_biomass_chp","title":"<code>add_biomass_chp(network, costs, nodes, biomass_potential, prov_centroids, add_beccs=True)</code>","text":"<p>add biomass to the network. Biomass is here a new build (and not a retrofit) and is not co-fired with coal. An optional CC can be added to biomass</p> <p>NOTE THAT THE CC IS NOT CONSTRAINED TO THE BIOMASS?</p> <p>Parameters:</p> Name Type Description Default <code>network</code> <code>Network</code> <p>the pypsa network</p> required <code>costs</code> <code>DataFrame</code> <p>the costs dataframe</p> required <code>nodes</code> <code>Index</code> <p>the nodes</p> required <code>biomass_potential</code> <code>DataFrame</code> <p>the biomass potential</p> required <code>prov_centroids</code> <code>GeoDataFrame</code> <p>the x,y locations of the nodes</p> required <code>add_beccs</code> <code>bool</code> <p>whether to add BECCS. Defaults to True.</p> <code>True</code> Source code in <code>workflow/scripts/prepare_network.py</code> <pre><code>def add_biomass_chp(\n    network: pypsa.Network,\n    costs: pd.DataFrame,\n    nodes: pd.Index,\n    biomass_potential: pd.DataFrame,\n    prov_centroids: gpd.GeoDataFrame,\n    add_beccs: bool = True,\n):\n    \"\"\"add biomass to the network. Biomass is here a new build (and not a retrofit)\n    and is not co-fired with coal. An optional CC can be added to biomass\n\n    NOTE THAT THE CC IS NOT CONSTRAINED TO THE BIOMASS?\n\n    Args:\n        network (pypsa.Network): the pypsa network\n        costs (pd.DataFrame): the costs dataframe\n        nodes (pd.Index): the nodes\n        biomass_potential (pd.DataFrame): the biomass potential\n        prov_centroids (gpd.GeoDataFrame): the x,y locations of the nodes\n        add_beccs (bool, optional): whether to add BECCS. Defaults to True.\n    \"\"\"\n\n    suffix = \" biomass\"\n    biomass_potential.index = biomass_potential.index.map(\n        lambda x: x + suffix if not x.endswith(suffix) else x\n    )\n\n    network.add(\n        \"Bus\",\n        nodes,\n        suffix=suffix,\n        x=prov_centroids.x,\n        y=prov_centroids.y,\n        carrier=\"biomass\",\n    )\n    logger.info(\"Adding biomass buses\")\n    logger.info(f\"{nodes + suffix}\")\n    logger.info(\"potentials\")\n    # aggricultural residue biomass\n    # NOTE THIS CURRENTLY DOESN'T INCLUDE TRANSPORT between nodes\n    # NOTE additional emissions from treatment/remedials are missing\n    network.add(\n        \"Store\",\n        nodes + suffix,\n        bus=nodes + suffix,\n        e_nom_extendable=False,\n        e_nom=biomass_potential,\n        e_initial=biomass_potential,\n        carrier=\"biomass\",\n    )\n    biomass_co2_intsty = estimate_co2_intensity_xing()\n    network.add(\n        \"Link\",\n        nodes + \" central biomass CHP\",\n        bus0=nodes + \" biomass\",\n        bus1=nodes,\n        bus2=nodes + \" central heat\",\n        bus3=nodes + \" CO2\",\n        p_nom_extendable=True,\n        carrier=\"biomass\",\n        efficiency=costs.at[\"biomass CHP\", \"efficiency\"],\n        efficiency2=costs.at[\"biomass CHP\", \"efficiency-heat\"],\n        efficiency3=biomass_co2_intsty,\n        capital_cost=costs.at[\"biomass CHP\", \"efficiency\"]\n        * costs.at[\"biomass CHP\", \"capital_cost\"],\n        marginal_cost=costs.at[\"biomass CHP\", \"efficiency\"]\n        * costs.at[\"biomass CHP\", \"marginal_cost\"]\n        + costs.at[\"solid biomass\", \"fuel\"],\n        lifetime=costs.at[\"biomass CHP\", \"lifetime\"],\n    )\n    if add_beccs:\n        network.add(\n            \"Link\",\n            nodes + \" central biomass CHP capture\",\n            bus0=nodes + \" CO2\",\n            bus1=nodes + \" CO2 capture\",\n            bus2=nodes,\n            p_nom_extendable=True,\n            carrier=\"CO2 capture\",\n            efficiency=costs.at[\"biomass CHP capture\", \"capture_rate\"],\n            efficiency2=-1\n            * costs.at[\"biomass CHP capture\", \"capture_rate\"]\n            * costs.at[\"biomass CHP capture\", \"electricity-input\"],\n            capital_cost=costs.at[\"biomass CHP capture\", \"capture_rate\"]\n            * costs.at[\"biomass CHP capture\", \"capital_cost\"],\n            lifetime=costs.at[\"biomass CHP capture\", \"lifetime\"],\n        )\n\n    network.add(\n        \"Link\",\n        nodes + \" decentral biomass boiler\",\n        bus0=nodes + \" biomass\",\n        bus1=nodes + \" decentral heat\",\n        p_nom_extendable=True,\n        carrier=\"biomass\",\n        efficiency=costs.at[\"biomass boiler\", \"efficiency\"],\n        capital_cost=costs.at[\"biomass boiler\", \"efficiency\"]\n        * costs.at[\"biomass boiler\", \"capital_cost\"],\n        marginal_cost=costs.at[\"biomass boiler\", \"efficiency\"]\n        * costs.at[\"biomass boiler\", \"marginal_cost\"]\n        + costs.at[\"biomass boiler\", \"pelletizing cost\"]\n        + costs.at[\"solid biomass\", \"fuel\"],\n        lifetime=costs.at[\"biomass boiler\", \"lifetime\"],\n    )\n</code></pre>"},{"location":"reference/prepare_network/#prepare_network.add_carriers","title":"<code>add_carriers(network, config, costs)</code>","text":"<p>add the various carriers to the network based on the config file</p> <p>Parameters:</p> Name Type Description Default <code>network</code> <code>Network</code> <p>the pypsa network</p> required <code>config</code> <code>dict</code> <p>the config file</p> required <code>costs</code> <code>DataFrame</code> <p>the costs dataframe</p> required Source code in <code>workflow/scripts/prepare_network.py</code> <pre><code>def add_carriers(network: pypsa.Network, config: dict, costs: pd.DataFrame):\n    \"\"\"add the various carriers to the network based on the config file\n\n    Args:\n        network (pypsa.Network): the pypsa network\n        config (dict): the config file\n        costs (pd.DataFrame): the costs dataframe\n    \"\"\"\n\n    network.add(\"Carrier\", \"AC\")\n    if config.get(\"heat_coupling\", False):\n        network.add(\"Carrier\", \"heat\")\n    for carrier in config[\"Techs\"][\"vre_techs\"]:\n        network.add(\"Carrier\", carrier)\n        if carrier == \"hydroelectricity\":\n            network.add(\"Carrier\", \"hydro_inflow\")\n    for carrier in config[\"Techs\"][\"store_techs\"]:\n        network.add(\"Carrier\", carrier)\n        if carrier == \"battery\":\n            network.add(\"Carrier\", \"battery discharger\")\n\n    # add fuel carriers, emissions in # in t_CO2/MWht\n    if config[\"add_gas\"]:\n        network.add(\"Carrier\", \"gas\", co2_emissions=costs.at[\"gas\", \"co2_emissions\"])\n    if config[\"add_coal\"]:\n        network.add(\"Carrier\", \"coal\", co2_emissions=costs.at[\"coal\", \"co2_emissions\"])\n    if \"CCGT-CCS\" in config[\"Techs\"][\"conv_techs\"]:\n        network.add(\"Carrier\", \"gas ccs\", co2_emissions=costs.at[\"gas ccs\", \"co2_emissions\"])\n    if \"coal-CCS\" in config[\"Techs\"][\"conv_techs\"]:\n        network.add(\"Carrier\", \"coal ccs\", co2_emissions=costs.at[\"coal ccs\", \"co2_emissions\"])\n</code></pre>"},{"location":"reference/prepare_network/#prepare_network.add_co2_capture_support","title":"<code>add_co2_capture_support(network, nodes, prov_centroids)</code>","text":"<p>add the necessary CO2 capture carriers &amp; stores to the network Args:     network (pypsa.Network): the network object     nodes (pd.Index): the nodes     prov_centroids (gpd.GeoDataFrame): the x,y locations of the nodes</p> Source code in <code>workflow/scripts/prepare_network.py</code> <pre><code>def add_co2_capture_support(\n    network: pypsa.Network, nodes: pd.Index, prov_centroids: gpd.GeoDataFrame\n):\n    \"\"\"add the necessary CO2 capture carriers &amp; stores to the network\n    Args:\n        network (pypsa.Network): the network object\n        nodes (pd.Index): the nodes\n        prov_centroids (gpd.GeoDataFrame): the x,y locations of the nodes\n    \"\"\"\n\n    network.add(\"Carrier\", \"CO2\", co2_emissions=0)\n    network.add(\n        \"Bus\",\n        nodes,\n        suffix=\" CO2\",\n        x=prov_centroids.x,\n        y=prov_centroids.y,\n        carrier=\"CO2\",\n    )\n\n    network.add(\"Store\", nodes + \" CO2\", bus=nodes + \" CO2\", carrier=\"CO2\")\n    # normally taking away from carrier generates CO2, but here we are\n    # adding CO2 stored, so the emissions will point the other way ?\n    network.add(\"Carrier\", \"CO2 capture\", co2_emissions=1)\n    network.add(\n        \"Bus\",\n        nodes,\n        suffix=\" CO2 capture\",\n        x=prov_centroids.x,\n        y=prov_centroids.y,\n        carrier=\"CO2 capture\",\n    )\n\n    network.add(\n        \"Store\",\n        nodes + \" CO2 capture\",\n        bus=nodes + \" CO2 capture\",\n        e_nom_extendable=True,\n        carrier=\"CO2 capture\",\n    )\n</code></pre>"},{"location":"reference/prepare_network/#prepare_network.add_conventional_generators","title":"<code>add_conventional_generators(network, nodes, config, prov_centroids, costs)</code>","text":"<p>add conventional generators to the network</p> <p>Parameters:</p> Name Type Description Default <code>network</code> <code>Network</code> <p>the pypsa network object</p> required <code>nodes</code> <code>Index</code> <p>the nodes</p> required <code>config</code> <code>dict</code> <p>the snakemake config</p> required <code>prov_centroids</code> <code>GeoDataFrame</code> <p>the x,y locations of the nodes</p> required <code>costs</code> <code>DataFrame</code> <p>the costs data base</p> required Source code in <code>workflow/scripts/prepare_network.py</code> <pre><code>def add_conventional_generators(\n    network: pypsa.Network,\n    nodes: pd.Index,\n    config: dict,\n    prov_centroids: gpd.GeoDataFrame,\n    costs: pd.DataFrame,\n):\n    \"\"\"add conventional generators to the network\n\n    Args:\n        network (pypsa.Network): the pypsa network object\n        nodes (pd.Index): the nodes\n        config (dict): the snakemake config\n        prov_centroids (gpd.GeoDataFrame): the x,y locations of the nodes\n        costs (pd.DataFrame): the costs data base\n    \"\"\"\n    if config[\"add_gas\"]:\n        # add converter from fuel source\n        network.add(\n            \"Bus\",\n            nodes,\n            suffix=\" gas\",\n            x=prov_centroids.x,\n            y=prov_centroids.y,\n            carrier=\"gas\",\n            location=nodes,\n        )\n\n        network.add(\n            \"Generator\",\n            nodes,\n            suffix=\" gas fuel\",\n            bus=nodes + \" gas\",\n            carrier=\"gas\",\n            p_nom_extendable=True,\n            p_nom=1e7,\n            marginal_cost=costs.at[\"gas\", \"fuel\"],\n        )\n\n        # gas prices identical per region, pipelines ignored\n        network.add(\n            \"Store\",\n            nodes + \" gas Store\",\n            bus=nodes + \" gas\",\n            e_nom_extendable=True,\n            carrier=\"gas\",\n            e_nom=1e7,\n            e_cyclic=True,\n        )\n\n    # add gas will then be true\n    gas_techs = [\"OCGT\", \"CCGT\"]\n    for tech in gas_techs:\n        if tech in config[\"Techs\"][\"conv_techs\"]:\n            network.add(\n                \"Link\",\n                nodes,\n                suffix=f\" {tech}\",\n                bus0=nodes + \" gas\",\n                bus1=nodes,\n                marginal_cost=costs.at[tech, \"efficiency\"]\n                * costs.at[tech, \"VOM\"],  # NB: VOM is per MWel\n                capital_cost=costs.at[tech, \"efficiency\"]\n                * costs.at[tech, \"capital_cost\"],  # NB: capital cost is per MWel\n                p_nom_extendable=True,\n                efficiency=costs.at[tech, \"efficiency\"],\n                lifetime=costs.at[tech, \"lifetime\"],\n                carrier=f\"gas {tech}\",\n            )\n\n    if \"CCGT-CCS\" in config[\"Techs\"][\"conv_techs\"]:\n        network.add(\n            \"Generator\",\n            nodes,\n            suffix=\" CCGT-CCS\",\n            bus=nodes,\n            carrier=\"gas ccs\",\n            p_nom_extendable=True,\n            efficiency=costs.at[\"CCGT-CCS\", \"efficiency\"],\n            marginal_cost=costs.at[\"CCGT-CCS\", \"marginal_cost\"],\n            capital_cost=costs.at[\"CCGT-CCS\", \"efficiency\"]\n            * costs.at[\"CCGT-CCS\", \"capital_cost\"],  # NB: capital cost is per MWel\n            lifetime=costs.at[\"CCGT-CCS\", \"lifetime\"],\n            p_max_pu=0.9,  # planned and forced outages\n        )\n\n    if config[\"add_coal\"]:\n        ramps = config.get(\n            \"fossil_ramps\", {\"coal\": {\"ramp_limit_up\": np.nan, \"ramp_limit_down\": np.nan}}\n        )\n        ramps = ramps.get(\"coal\", {\"ramp_limit_up\": np.nan, \"ramp_limit_down\": np.nan})\n        ramps = {k: v * config[\"snapshots\"][\"frequency\"] for k, v in ramps.items()}\n        # this is the non sector-coupled approach\n        # for industry may have an issue in that coal feeds to chem sector\n        # no coal in Beijing - political decision\n        network.add(\n            \"Generator\",\n            nodes[nodes != \"Beijing\"],\n            suffix=\" coal power\",\n            bus=nodes[nodes != \"Beijing\"],\n            carrier=\"coal\",\n            p_nom_extendable=True,\n            efficiency=costs.at[\"coal\", \"efficiency\"],\n            marginal_cost=costs.at[\"coal\", \"marginal_cost\"],\n            capital_cost=costs.at[\"coal\", \"efficiency\"]\n            * costs.at[\"coal\", \"capital_cost\"],  # NB: capital cost is per MWel\n            lifetime=costs.at[\"coal\", \"lifetime\"],\n            ramp_limit_up=ramps[\"ramp_limit_up\"],\n            ramp_limit_down=ramps[\"ramp_limit_down\"],\n            p_max_pu=0.9,  # planned and forced outages\n        )\n\n        if \"coal-CCS\" in config[\"Techs\"][\"conv_techs\"]:\n            network.add(\n                \"Generator\",\n                nodes,\n                suffix=\" coal-CCS\",\n                bus=nodes,\n                carrier=\"coal ccs\",\n                p_nom_extendable=True,\n                efficiency=costs.at[\"coal ccs\", \"efficiency\"],\n                marginal_cost=costs.at[\"coal ccs\", \"marginal_cost\"],\n                capital_cost=costs.at[\"coal ccs\", \"efficiency\"]\n                * costs.at[\"coal ccs\", \"capital_cost\"],  # NB: capital cost is per MWel\n                lifetime=costs.at[\"coal ccs\", \"lifetime\"],\n                p_max_pu=0.9,  # planned and forced outages\n            )\n</code></pre>"},{"location":"reference/prepare_network/#prepare_network.add_heat_coupling","title":"<code>add_heat_coupling(network, config, nodes, prov_centroids, costs, planning_year, paths)</code>","text":"<p>add the heat-coupling links and generators to the network</p> <p>Parameters:</p> Name Type Description Default <code>network</code> <code>Network</code> <p>the network object</p> required <code>config</code> <code>dict</code> <p>the config</p> required <code>nodes</code> <code>Index</code> <p>the node names. Defaults to pd.Index.</p> required <code>prov_centroids</code> <code>GeoDataFrame</code> <p>the node locations.</p> required <code>costs</code> <code>DataFrame</code> <p>the costs dataframe for emissions</p> required <code>paths</code> <code>dict</code> <p>the paths to the data files</p> required Source code in <code>workflow/scripts/prepare_network.py</code> <pre><code>def add_heat_coupling(\n    network: pypsa.Network,\n    config: dict,\n    nodes: pd.Index,\n    prov_centroids: gpd.GeoDataFrame,\n    costs: pd.DataFrame,\n    planning_year: int,\n    paths: dict,\n):\n    \"\"\"add the heat-coupling links and generators to the network\n\n    Args:\n        network (pypsa.Network): the network object\n        config (dict): the config\n        nodes (pd.Index): the node names. Defaults to pd.Index.\n        prov_centroids (gpd.GeoDataFrame): the node locations.\n        costs (pd.DataFrame): the costs dataframe for emissions\n        paths (dict): the paths to the data files\n    \"\"\"\n    central_fraction = pd.read_hdf(paths[\"central_fraction\"])\n    with pd.HDFStore(paths[\"heat_demand_profile\"], mode=\"r\") as store:\n        heat_demand = store[\"heat_demand_profiles\"]\n        # TODO fix this if not working\n        heat_demand.index = heat_demand.index.tz_localize(None)\n        heat_demand = heat_demand.loc[network.snapshots]\n\n    network.add(\n        \"Bus\",\n        nodes,\n        suffix=\" decentral heat\",\n        x=prov_centroids.x,\n        y=prov_centroids.y,\n        carrier=\"heat\",\n        location=nodes,\n    )\n\n    network.add(\n        \"Bus\",\n        nodes,\n        suffix=\" central heat\",\n        x=prov_centroids.x,\n        y=prov_centroids.y,\n        carrier=\"heat\",\n        location=nodes,\n    )\n\n    network.add(\n        \"Load\",\n        nodes,\n        suffix=\" decentral heat\",\n        bus=nodes + \" decentral heat\",\n        p_set=heat_demand[nodes].multiply(1 - central_fraction[nodes]),\n    )\n\n    network.add(\n        \"Load\",\n        nodes,\n        suffix=\" central heat\",\n        bus=nodes + \" central heat\",\n        p_set=heat_demand[nodes].multiply(central_fraction[nodes]),\n    )\n\n    if \"heat pump\" in config[\"Techs\"][\"vre_techs\"]:\n        logger.info(f\"loading cop profiles from {paths['cop_name']}\")\n        with pd.HDFStore(paths[\"cop_name\"], mode=\"r\") as store:\n            ashp_cop = store[\"ashp_cop_profiles\"]\n            ashp_cop.index = ashp_cop.index.tz_localize(None)\n            ashp_cop = shift_profile_to_planning_year(\n                ashp_cop, snakemake.wildcards.planning_horizons\n            )\n            gshp_cop = store[\"gshp_cop_profiles\"]\n            gshp_cop.index = gshp_cop.index.tz_localize(None)\n            gshp_cop = shift_profile_to_planning_year(\n                gshp_cop, snakemake.wildcards.planning_horizons\n            )\n\n        for cat in [\" decentral \", \" central \"]:\n            network.add(\n                \"Link\",\n                nodes,\n                suffix=cat + \"heat pump\",\n                bus0=nodes,\n                bus1=nodes + cat + \"heat\",\n                carrier=\"heat pump\",\n                efficiency=(\n                    ashp_cop.loc[network.snapshots, nodes]\n                    if config[\"time_dep_hp_cop\"]\n                    else costs.at[cat.lstrip() + \"air-sourced heat pump\", \"efficiency\"]\n                ),\n                capital_cost=costs.at[cat.lstrip() + \"air-sourced heat pump\", \"efficiency\"]\n                * costs.at[cat.lstrip() + \"air-sourced heat pump\", \"capital_cost\"],\n                marginal_cost=costs.at[cat.lstrip() + \"air-sourced heat pump\", \"efficiency\"]\n                * costs.at[cat.lstrip() + \"air-sourced heat pump\", \"marginal_cost\"],\n                p_nom_extendable=True,\n                lifetime=costs.at[cat.lstrip() + \"air-sourced heat pump\", \"lifetime\"],\n            )\n\n        network.add(\n            \"Link\",\n            nodes,\n            suffix=\" ground heat pump\",\n            bus0=nodes,\n            bus1=nodes + \" decentral heat\",\n            carrier=\"heat pump\",\n            efficiency=(\n                gshp_cop.loc[network.snapshots, nodes]\n                if config[\"time_dep_hp_cop\"]\n                else costs.at[\"decentral ground-sourced heat pump\", \"efficiency\"]\n            ),\n            marginal_cost=costs.at[cat.lstrip() + \"ground-sourced heat pump\", \"efficiency\"]\n            * costs.at[cat.lstrip() + \"ground-sourced heat pump\", \"marginal_cost\"],\n            capital_cost=costs.at[cat.lstrip() + \"ground-sourced heat pump\", \"efficiency\"]\n            * costs.at[\"decentral ground-sourced heat pump\", \"capital_cost\"],\n            p_nom_extendable=True,\n            lifetime=costs.at[\"decentral ground-sourced heat pump\", \"lifetime\"],\n        )\n\n    if \"water tanks\" in config[\"Techs\"][\"store_techs\"]:\n        for cat in [\" decentral \", \" central \"]:\n            network.add(\n                \"Bus\",\n                nodes,\n                suffix=cat + \"water tanks\",\n                x=prov_centroids.x,\n                y=prov_centroids.y,\n                carrier=\"water tanks\",\n                location=nodes,\n            )\n\n            network.add(\n                \"Link\",\n                nodes + cat + \"water tanks charger\",\n                bus0=nodes + cat + \"heat\",\n                bus1=nodes + cat + \"water tanks\",\n                carrier=\"water tanks\",\n                efficiency=costs.at[\"water tank charger\", \"efficiency\"],\n                p_nom_extendable=True,\n            )\n\n            network.add(\n                \"Link\",\n                nodes + cat + \"water tanks discharger\",\n                bus0=nodes + cat + \"water tanks\",\n                bus1=nodes + cat + \"heat\",\n                carrier=\"water tanks\",\n                efficiency=costs.at[\"water tank discharger\", \"efficiency\"],\n                p_nom_extendable=True,\n            )\n            # [HP] 180 day time constant for centralised, 3 day for decentralised\n            tes_tau = config[\"water_tanks\"][\"tes_tau\"][cat.strip()]\n            network.add(\n                \"Store\",\n                nodes + cat + \"water tank\",\n                bus=nodes + cat + \"water tanks\",\n                carrier=\"water tanks\",\n                e_cyclic=True,\n                e_nom_extendable=True,\n                standing_loss=1 - np.exp(-1 / (24.0 * tes_tau)),\n                capital_cost=costs.at[cat.lstrip() + \"water tank storage\", \"capital_cost\"],\n                lifetime=costs.at[cat.lstrip() + \"water tank storage\", \"lifetime\"],\n            )\n\n    if \"resistive heater\" in config[\"Techs\"][\"vre_techs\"]:\n\n        for cat in [\" decentral \", \" central \"]:\n            network.add(\n                \"Link\",\n                nodes + cat + \"resistive heater\",\n                bus0=nodes,\n                bus1=nodes + cat + \"heat\",\n                carrier=\"resistive heater\",\n                efficiency=costs.at[cat.lstrip() + \"resistive heater\", \"efficiency\"],\n                capital_cost=costs.at[cat.lstrip() + \"resistive heater\", \"efficiency\"]\n                * costs.at[cat.lstrip() + \"resistive heater\", \"capital_cost\"],\n                marginal_cost=costs.at[cat.lstrip() + \"resistive heater\", \"efficiency\"]\n                * costs.at[cat.lstrip() + \"resistive heater\", \"marginal_cost\"],\n                p_nom_extendable=True,\n                lifetime=costs.at[cat.lstrip() + \"resistive heater\", \"lifetime\"],\n            )\n\n    if (\n        \"H2 CHP\" in config[\"Techs\"][\"vre_techs\"]\n        and config[\"add_H2\"]\n        and config.get(\"heat_coupling\", False)\n    ):\n        network.add(\n            \"Bus\",\n            nodes,\n            suffix=\" central H2 CHP\",\n            x=prov_centroids.x,\n            y=prov_centroids.y,\n            carrier=\"H2\",\n            location=nodes,\n        )\n        network.add(\n            \"Link\",\n            name=nodes + \" central H2 CHP\",\n            bus0=nodes + \" H2\",\n            bus1=nodes,\n            bus2=nodes + \" central heat\",\n            p_nom_extendable=True,\n            efficiency=costs.at[\"central hydrogen CHP\", \"efficiency\"],\n            efficiency2=costs.at[\"central hydrogen CHP\", \"efficiency\"]\n            / costs.at[\"central hydrogen CHP\", \"c_b\"],\n            capital_cost=costs.at[\"central hydrogen CHP\", \"efficiency\"]\n            * costs.at[\"central hydrogen CHP\", \"capital_cost\"],\n            lifetime=costs.at[\"central hydrogen CHP\", \"lifetime\"],\n            carrier=\"H2 CHP\",\n        )\n\n    if \"CHP gas\" in config[\"Techs\"][\"conv_techs\"]:\n        # TODO apply same as for coal (include Cb)\n        # OCGT CHP\n        network.add(\n            \"Link\",\n            nodes,\n            suffix=\" CHP gas\",\n            bus0=nodes + \" gas\",\n            bus1=nodes,\n            bus2=nodes + \" central heat\",\n            p_nom_extendable=True,\n            marginal_cost=costs.at[\"central gas CHP\", \"efficiency\"]\n            * costs.at[\"central gas CHP\", \"VOM\"],  # NB: VOM is per MWel\n            capital_cost=costs.at[\"central gas CHP\", \"efficiency\"]\n            * costs.at[\"central gas CHP\", \"capital_cost\"],  # NB: capital cost is per MWel\n            efficiency=costs.at[\"central gas CHP\", \"efficiency\"],\n            efficiency2=config[\"chp_parameters\"][\"eff_th\"],\n            lifetime=costs.at[\"central gas CHP\", \"lifetime\"],\n            carrier=\"CHP gas\",\n        )\n\n    if \"CHP coal\" in config[\"Techs\"][\"conv_techs\"]:\n        logger.info(\"Adding CHP coal to network\")\n\n        network.add(\n            \"Bus\",\n            nodes,\n            suffix=\" coal fuel\",\n            x=prov_centroids.x,\n            y=prov_centroids.y,\n            carrier=\"coal\",\n            location=nodes,\n        )\n\n        network.add(\n            \"Generator\",\n            nodes + \" coal fuel\",\n            bus=nodes + \" coal fuel\",\n            carrier=\"coal\",\n            p_nom_extendable=False,\n            p_nom=1e8,\n            marginal_cost=costs.at[\"coal\", \"fuel\"],\n        )\n\n        # NOTE generator | boiler is a key word for the constraint\n        network.add(\n            \"Link\",\n            name=nodes,\n            suffix=\" CHP coal generator\",\n            bus0=nodes + \" coal fuel\",\n            bus1=nodes,\n            p_nom_extendable=True,\n            marginal_cost=costs.at[\"central coal CHP\", \"efficiency\"]\n            * costs.at[\"central coal CHP\", \"VOM\"],  # NB: VOM is per MWel\n            capital_cost=costs.at[\"central coal CHP\", \"efficiency\"]\n            * costs.at[\"central coal CHP\", \"capital_cost\"],  # NB: capital cost is per MWel\n            efficiency=costs.at[\"central coal CHP\", \"efficiency\"],\n            c_b=costs.at[\"central coal CHP\", \"c_b\"],\n            p_nom_ratio=1.0,\n            lifetime=costs.at[\"central coal CHP\", \"lifetime\"],\n            carrier=\"CHP coal\",\n        )\n\n        network.add(\n            \"Link\",\n            nodes,\n            suffix=\" central CHP coal boiler\",\n            bus0=nodes + \" coal fuel\",\n            bus1=nodes + \" central heat\",\n            carrier=\"CHP coal\",\n            p_nom_extendable=True,\n            marginal_cost=costs.at[\"central coal CHP\", \"efficiency\"]\n            * costs.at[\"central coal CHP\", \"VOM\"],  # NB: VOM is per MWel\n            efficiency=costs.at[\"central coal CHP\", \"efficiency\"]\n            / costs.at[\"central coal CHP\", \"c_v\"],\n            lifetime=costs.at[\"central coal CHP\", \"lifetime\"],\n        )\n\n    if \"coal boiler\" in config[\"Techs\"][\"conv_techs\"]:\n        for cat in [\"decentral\", \"central\"]:\n            network.add(\n                \"Link\",\n                nodes + f\" {cat} coal boiler\",\n                p_nom_extendable=True,\n                bus0=nodes + \" coal fuel\",\n                bus1=nodes + f\" {cat} heat\",\n                efficiency=costs.at[f\"{cat} coal boiler\", \"efficiency\"],\n                marginal_cost=costs.at[f\"{cat} coal boiler\", \"efficiency\"]\n                * costs.at[f\"{cat} coal boiler\", \"VOM\"],\n                capital_cost=costs.at[f\"{cat} coal boiler\", \"efficiency\"]\n                * costs.at[f\"{cat} coal boiler\", \"capital_cost\"],\n                lifetime=costs.at[f\"{cat} coal boiler\", \"lifetime\"],\n                carrier=f\"coal boiler {cat}\",\n            )\n\n    if \"gas boiler\" in config[\"Techs\"][\"conv_techs\"]:\n        for cat in [\"decentral\", \"central\"]:\n            network.add(\n                \"Link\",\n                nodes + cat + \"gas boiler\",\n                p_nom_extendable=True,\n                bus0=nodes + \" gas\",\n                bus1=nodes + f\" {cat} heat\",\n                efficiency=costs.at[f\"{cat} gas boiler\", \"efficiency\"],\n                marginal_cost=costs.at[f\"{cat} gas boiler\", \"VOM\"],\n                capital_cost=costs.at[f\"{cat} gas boiler\", \"efficiency\"]\n                * costs.at[f\"{cat} gas boiler\", \"capital_cost\"],\n                lifetime=costs.at[f\"{cat} gas boiler\", \"lifetime\"],\n                carrier=f\"gas boiler {cat}\",\n            )\n\n    if \"solar thermal\" in config[\"Techs\"][\"vre_techs\"]:\n\n        # this is the amount of heat collected in W per m^2, accounting\n        # for efficiency\n        with pd.HDFStore(paths[\"solar_thermal_name\"], mode=\"r\") as store:\n            # 1e3 converts from W/m^2 to MW/(1000m^2) = kW/m^2\n            solar_thermal = config[\"solar_cf_correction\"] * store[\"solar_thermal_profiles\"] / 1e3\n\n        solar_thermal.index = solar_thermal.index.tz_localize(None)\n        solar_thermal = shift_profile_to_planning_year(solar_thermal, planning_year)\n        solar_thermal = solar_thermal.loc[network.snapshots]\n\n        for cat in [\" decentral \"]:\n            network.add(\n                \"Generator\",\n                nodes,\n                suffix=cat + \"solar thermal\",\n                bus=nodes + cat + \"heat\",\n                carrier=\"solar thermal\",\n                p_nom_extendable=True,\n                capital_cost=costs.at[cat.lstrip() + \"solar thermal\", \"capital_cost\"],\n                p_max_pu=solar_thermal[nodes].clip(1.0e-4),\n                lifetime=costs.at[cat.lstrip() + \"solar thermal\", \"lifetime\"],\n            )\n</code></pre>"},{"location":"reference/prepare_network/#prepare_network.add_hydro","title":"<code>add_hydro(network, config, nodes, prov_shapes, costs, planning_horizons)</code>","text":"<p>Add the hydropower plants (dams) to the network. Due to the spillage/basin calculations these have real locations not just nodes. WARNING: the node is assigned based on the damn province name (turbine link)         NOT future proof</p> <p>Parameters:</p> Name Type Description Default <code>network</code> <code>Network</code> <p>the network object</p> required <code>config</code> <code>dict</code> <p>the yaml config</p> required <code>nodes</code> <code>Index</code> <p>the buses</p> required <code>prov_shapes</code> <code>GeoDataFrame</code> <p>the province shapes GDF</p> required <code>costs</code> <code>DataFrame</code> <p>the costs dataframe</p> required <code>planning_horizons</code> <code>int</code> <p>the year</p> required Source code in <code>workflow/scripts/prepare_network.py</code> <pre><code>def add_hydro(\n    network: pypsa.Network,\n    config: dict,\n    nodes: pd.Index,\n    prov_shapes: gpd.GeoDataFrame,\n    costs: pd.DataFrame,\n    planning_horizons: int,\n):\n    \"\"\"Add the hydropower plants (dams) to the network.\n    Due to the spillage/basin calculations these have real locations not just nodes.\n    WARNING: the node is assigned based on the damn province name (turbine link)\n            NOT future proof\n\n    Args:\n        network (pypsa.Network): the network object\n        config (dict): the yaml config\n        nodes (pd.Index): the buses\n        prov_shapes (gpd.GeoDataFrame): the province shapes GDF\n        costs (pd.DataFrame): the costs dataframe\n        planning_horizons (int): the year\n    \"\"\"\n\n    logger.info(\"\\tAdding dam cascade\")\n\n    # load dams\n    df = pd.read_csv(config[\"hydro_dams\"][\"dams_path\"], index_col=0)\n    points = df.apply(lambda row: Point(row.Lon, row.Lat), axis=1)\n    dams = gpd.GeoDataFrame(df, geometry=points, crs=CRS)\n    # store all info, then filter by selected nodes\n    dam_provinces = dams.Province\n    all_dams = dams.index.values\n    dams = dams[dams.Province.isin(nodes)]\n\n    logger.debug(f\"Hydro dams in {nodes} provinces: {dams.index}\")\n\n    hourly_rng = pd.date_range(\n        config[\"hydro_dams\"][\"inflow_date_start\"],\n        config[\"hydro_dams\"][\"inflow_date_end\"],\n        freq=\"1h\",  # THIS IS THE INFLOW RES\n        inclusive=\"left\",\n    )\n    # TODO implement inflow calc, understand resolution (seems daily!)\n    inflow = pd.read_pickle(config[\"hydro_dams\"][\"inflow_path\"])\n    # select inflow year\n    hourly_rng = hourly_rng[hourly_rng.year == INFLOW_DATA_YR]\n    inflow = inflow.loc[inflow.index.year == INFLOW_DATA_YR]\n    inflow = inflow.reindex(hourly_rng, fill_value=0)\n    inflow.columns = all_dams  # TODO dangerous\n    # select only the dams in the network\n    inflow = inflow.loc[:, inflow.columns.map(dam_provinces).isin(nodes)]\n    inflow = shift_profile_to_planning_year(inflow, planning_horizons)\n    inflow = inflow.loc[network.snapshots]\n    # m^3/KWh -&gt; m^3/MWh\n    water_consumption_factor = dams.loc[:, \"Water_consumption_factor_avg\"] * 1e3\n\n    #######\n    # ### Add hydro stations as buses\n    network.add(\n        \"Bus\",\n        dams.index,\n        suffix=\" station\",\n        carrier=\"stations\",\n        x=dams[\"geometry\"].to_crs(\"+proj=cea\").centroid.to_crs(prov_shapes.crs).x,\n        y=dams[\"geometry\"].to_crs(\"+proj=cea\").centroid.to_crs(prov_shapes.crs).y,\n        location=dams[\"Province\"],\n    )\n\n    dam_buses = network.buses[network.buses.carrier == \"stations\"]\n\n    # ===== add hydro reservoirs as stores ======\n    initial_capacity = pd.read_pickle(config[\"hydro_dams\"][\"reservoir_initial_capacity_path\"])\n    effective_capacity = pd.read_pickle(config[\"hydro_dams\"][\"reservoir_effective_capacity_path\"])\n    initial_capacity.index = all_dams\n    effective_capacity.index = all_dams\n    initial_capacity = initial_capacity / water_consumption_factor\n    effective_capacity = effective_capacity / water_consumption_factor\n\n    # select relevant dams in nodes\n    effective_capacity = effective_capacity.loc[\n        effective_capacity.index.map(dam_provinces).isin(nodes)\n    ]\n    initial_capacity = initial_capacity.loc[initial_capacity.index.map(dam_provinces).isin(nodes)]\n\n    network.add(\n        \"Store\",\n        dams.index,\n        suffix=\" reservoir\",\n        bus=dam_buses.index,\n        e_nom=effective_capacity,\n        e_initial=initial_capacity,\n        e_cyclic=True,\n        # TODO fix all config[\"costs\"]\n        marginal_cost=config[\"hydro\"][\"marginal_cost\"][\"reservoir\"],\n    )\n\n    # add hydro turbines to link stations to provinces\n    network.add(\n        \"Link\",\n        dams.index,\n        suffix=\" turbines\",\n        bus0=dam_buses.index,\n        bus1=dams[\"Province\"],\n        carrier=\"hydroelectricity\",\n        p_nom=10 * dams[\"installed_capacity_10MW\"],\n        capital_cost=(\n            costs.at[\"hydro\", \"capital_cost\"] if config[\"hydro\"][\"hydro_capital_cost\"] else 0\n        ),\n        efficiency=1,\n        location=dams[\"Province\"],\n        p_nom_extendable=False,\n    )\n\n    # ===  add rivers to link station to station\n    dam_edges = pd.read_csv(config[\"hydro_dams\"][\"damn_flows_path\"], delimiter=\",\")\n    in_nodes = dam_edges.bus0.map(dam_provinces).isin(nodes) &amp; dam_edges.end_bus.map(\n        dam_provinces\n    ).isin(nodes)\n    dam_edges = dam_edges[in_nodes]\n\n    # === normal flow ====\n    for row in dam_edges.iterrows():\n        bus0 = row[1].bus0 + \" turbines\"\n        bus2 = row[1].end_bus + \" station\"\n        network.links.at[bus0, \"bus2\"] = bus2\n        network.links.at[bus0, \"efficiency2\"] = 1.0\n\n    # === spillage ====\n    # TODO WHY EXTENDABLE - weather year?\n    for row in dam_edges.iterrows():\n        bus0 = row[1].bus0 + \" station\"\n        bus1 = row[1].end_bus + \" station\"\n        network.add(\n            \"Link\",\n            \"{}-{}\".format(bus0, bus1) + \" spillage\",\n            bus0=bus0,\n            bus1=bus1,\n            p_nom_extendable=True,\n        )\n\n    dam_ends = [\n        dam\n        for dam in np.unique(dams.index.values)\n        if dam not in dam_edges[\"bus0\"]\n        or dam not in dam_edges[\"end_bus\"]\n        or (dam in dam_edges[\"end_bus\"].values &amp; dam not in dam_edges[\"bus0\"])\n    ]\n    # need some kind of sink to absorb spillage (e,g ocean).\n    # here hack by flowing to existing bus with 0 efficiency (lose)\n    # TODO make more transparent -&gt; generator with neg sign and 0 c0st\n    for bus0 in dam_ends:\n        network.add(\n            \"Link\",\n            bus0 + \" spillage\",\n            bus0=bus0 + \" station\",\n            bus1=bus0 + \" station\",\n            p_nom_extendable=True,\n            efficiency=0.0,\n        )\n\n    # add inflow as generators\n    # only feed into hydro stations which are the first of a cascade\n    inflow_stations = [\n        dam for dam in np.unique(dams.index.values) if dam not in dam_edges[\"end_bus\"].values\n    ]\n\n    for inflow_station in inflow_stations:\n\n        # p_nom = 1 and p_max_pu &amp; p_min_pu = p_pu, compulsory inflow\n        p_nom = (inflow / water_consumption_factor)[inflow_station].max()\n        p_pu = (inflow / water_consumption_factor)[inflow_station] / p_nom\n        p_pu.index = network.snapshots\n        network.add(\n            \"Generator\",\n            inflow_station + \" inflow\",\n            bus=inflow_station + \" station\",\n            carrier=\"hydro_inflow\",\n            p_max_pu=p_pu.clip(1.0e-6),\n            # p_min_pu=p_pu.clip(1.0e-6),\n            p_nom=p_nom,\n        )\n\n        # p_nom*p_pu = XXX m^3 then use turbines efficiency to convert to power\n\n    # ======= add other existing hydro power (not lattitude resolved) ===\n    hydro_p_nom = pd.read_hdf(config[\"hydro_dams\"][\"p_nom_path\"]).loc[nodes]\n    hydro_p_max_pu = (\n        pd.read_hdf(\n            config[\"hydro_dams\"][\"p_max_pu_path\"],\n            key=config[\"hydro_dams\"][\"p_max_pu_key\"],\n        ).tz_localize(None)\n    )[nodes]\n\n    hydro_p_max_pu = shift_profile_to_planning_year(hydro_p_max_pu, planning_horizons)\n    # sort buses (columns) otherwise stuff will break\n    hydro_p_max_pu.sort_index(axis=1, inplace=True)\n\n    hydro_p_max_pu = hydro_p_max_pu.loc[snapshots]\n    hydro_p_max_pu.index = network.snapshots\n\n    logger.info(\"\\tAdding extra hydro capacity (regionally aggregated)\")\n\n    network.add(\n        \"Generator\",\n        nodes,\n        suffix=\" hydroelectricity\",\n        bus=nodes,\n        carrier=\"hydroelectricity\",\n        p_nom=hydro_p_nom,\n        p_nom_min=hydro_p_nom,\n        p_nom_extendable=False,\n        p_max_pu=hydro_p_max_pu,\n        capital_cost=(\n            costs.at[\"hydro\", \"capital_cost\"] if config[\"hydro\"][\"hydro_capital_cost\"] else 0\n        ),\n    )\n</code></pre>"},{"location":"reference/prepare_network/#prepare_network.add_voltage_links","title":"<code>add_voltage_links(network, config)</code>","text":"<p>add HVDC/AC links (no KVL)</p> <p>Parameters:</p> Name Type Description Default <code>network</code> <code>Network</code> <p>the network object</p> required <code>config</code> <code>dict</code> <p>the snakemake config</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>Invalid Edge path in config options</p> Source code in <code>workflow/scripts/prepare_network.py</code> <pre><code>def add_voltage_links(network: pypsa.Network, config: dict):\n    \"\"\"add HVDC/AC links (no KVL)\n\n    Args:\n        network (pypsa.Network): the network object\n        config (dict): the snakemake config\n\n    Raises:\n        ValueError: Invalid Edge path in config options\n    \"\"\"\n\n    represented_hours = network.snapshot_weightings.sum()[0]\n    n_years = represented_hours / 8760.0\n\n    # determine topology\n    edge_path = config[\"edge_paths\"].get(config[\"scenario\"][\"topology\"], None)\n    if edge_path is None:\n        raise ValueError(f\"No grid found for topology {config['scenario']['topology']}\")\n    else:\n        edges_ = pd.read_csv(\n            edge_path, sep=\",\", header=None, names=[\"bus0\", \"bus1\", \"p_nom\"]\n        ).fillna(0)\n        edges = edges_[edges_[\"bus0\"].isin(PROV_NAMES) &amp; edges_[\"bus1\"].isin(PROV_NAMES)]\n        if edges_.shape[0] != edges.shape[0]:\n            logger.warning(\"Some edges are not in the network\")\n    # fix this to use map with x.y\n    lengths = config[\"lines\"][\"line_length_factor\"] * np.array(\n        [\n            haversine(\n                [network.buses.at[bus0, \"x\"], network.buses.at[bus0, \"y\"]],\n                [network.buses.at[bus1, \"x\"], network.buses.at[bus1, \"y\"]],\n            )\n            for bus0, bus1 in edges[[\"bus0\", \"bus1\"]].values\n        ]\n    )\n\n    # get for backward compatibility\n    security_config = config.get(\"security\", {\"line_security_margin\": 70})\n    line_margin = security_config.get(\"line_security_margin\", 70) / 100\n\n    line_cost = (\n        lengths * costs.at[\"HVDC overhead\", \"capital_cost\"] * FOM_LINES * n_years\n    ) + costs.at[\n        \"HVDC inverter pair\", \"capital_cost\"\n    ]  # /MW\n\n    # ==== lossy transport model (split into 2) ====\n    # NB this only works if there is an equalising constraint, which is hidden in solve_ntwk\n    if config[\"line_losses\"]:\n        network.add(\n            \"Link\",\n            edges[\"bus0\"] + \"-\" + edges[\"bus1\"],\n            bus0=edges[\"bus0\"].values,\n            bus1=edges[\"bus1\"].values,\n            suffix=\" positive\",\n            p_nom_extendable=True,\n            p_nom=edges[\"p_nom\"].values,\n            p_nom_min=edges[\"p_nom\"].values,\n            p_min_pu=0,\n            p_max_pu=line_margin,\n            efficiency=config[\"transmission_efficiency\"][\"DC\"][\"efficiency_static\"]\n            * config[\"transmission_efficiency\"][\"DC\"][\"efficiency_per_1000km\"] ** (lengths / 1000),\n            length=lengths,\n            capital_cost=line_cost,\n            carrier=\"AC\",  # Fake - actually DC\n        )\n        # 0 len for reversed in case line limits are specified in km. Limited in constraints to fwdcap\n        network.add(\n            \"Link\",\n            edges[\"bus0\"] + \"-\" + edges[\"bus1\"],\n            bus0=edges[\"bus1\"].values,\n            bus1=edges[\"bus0\"].values,\n            suffix=\" reversed\",\n            p_nom_extendable=True,\n            p_nom=edges[\"p_nom\"].values,\n            p_nom_min=edges[\"p_nom\"].values,\n            p_min_pu=0,\n            efficiency=config[\"transmission_efficiency\"][\"DC\"][\"efficiency_static\"]\n            * config[\"transmission_efficiency\"][\"DC\"][\"efficiency_per_1000km\"] ** (lengths / 1000),\n            length=0,\n            capital_cost=0,\n            carrier=\"AC\",\n        )\n    # lossless transport model\n    else:\n        network.add(\n            \"Link\",\n            edges[\"bus0\"] + \"-\" + edges[\"bus1\"],\n            p_nom=edges[\"p_nom\"].values,\n            p_nom_min=edges[\"p_nom\"].values,\n            bus0=edges[\"bus0\"].values,\n            bus1=edges[\"bus1\"].values,\n            p_nom_extendable=True,\n            p_min_pu=-1,\n            length=lengths,\n            capital_cost=line_cost,\n            carrier=\"AC\",\n        )\n</code></pre>"},{"location":"reference/prepare_network/#prepare_network.add_wind_and_solar","title":"<code>add_wind_and_solar(network, techs, paths, year, costs)</code>","text":"<p>Adds wind and solar generators for each grade of renewable energy technology</p> <p>Parameters:</p> Name Type Description Default <code>network</code> <code>Network</code> <p>The PyPSA network to which the generators will be added</p> required <code>techs</code> <code>list</code> <p>A list of renewable energy technologies to add. (e.g., [\"solar\", \"onwind\", \"offwind\"])</p> required <code>paths</code> <code>PathLike</code> <p>file paths containing renewable profiles (snakemake.input)</p> required <code>year</code> <code>int</code> <p>planning year</p> required <code>costs</code> <code>DataFrame</code> <p>cost parameters for each technology</p> required <p>Raises:     ValueError: for unsupported technologies or missing paths.</p> Source code in <code>workflow/scripts/prepare_network.py</code> <pre><code>def add_wind_and_solar(\n    network: pypsa.Network,\n    techs: list,\n    paths: os.PathLike,\n    year: int,\n    costs: pd.DataFrame,\n):\n    \"\"\"\n    Adds wind and solar generators for each grade of renewable energy technology\n\n    Args:\n        network (pypsa.Network): The PyPSA network to which the generators will be added\n        techs (list): A list of renewable energy technologies to add.\n            (e.g., [\"solar\", \"onwind\", \"offwind\"])\n        paths (os.PathLike): file paths containing renewable profiles (snakemake.input)\n        year (int): planning year\n        costs (pd.DataFrame): cost parameters for each technology\n    Raises:\n        ValueError: for unsupported technologies or missing paths.\n    \"\"\"\n\n    unsupported = set(techs).difference({\"solar\", \"onwind\", \"offwind\"})\n    if unsupported:\n        raise ValueError(f\"Carrier(s) {unsupported} not wind or solar pv\")\n    prof_paths = {f\"profile_{tech}\": paths[f\"profile_{tech}\"] for tech in techs}\n    if len(prof_paths) != len(techs):\n        raise ValueError(f\"Paths do not correspond to techs  ({prof_paths} vs {techs})\")\n\n    for tech in techs:\n        # load the renewable profiles\n        logger.info(f\"Attaching {tech} to network\")\n        with xr.open_dataset(prof_paths[f\"profile_{tech}\"]) as ds:\n            if ds.indexes[\"bus\"].empty:\n                continue\n            if \"year\" in ds.indexes:\n                ds = ds.sel(year=ds.year.min(), drop=True)\n\n            timestamps = pd.DatetimeIndex(ds.time)\n            shift_weather_to_planning_yr = lambda t: t.replace(year=int(year))\n            timestamps = timestamps.map(shift_weather_to_planning_yr)\n            ds = ds.assign_coords(time=timestamps)\n\n            mask = ds.time.isin(network.snapshots)\n            ds = ds.sel(time=mask)\n\n            if not len(ds.time) == len(network.snapshots):\n                err = f\"{len(ds.time)} and {len(network.snapshots)}\"\n                raise ValueError(\"Mismatch in profile and network timestamps \" + err)\n            ds = ds.stack(bus_bin=[\"bus\", \"bin\"])\n\n        # remove low potential bins\n        cutoff = config.get(\"renewable_potential_cutoff\", 0)\n        ds = ds.where(ds[\"p_nom_max\"] &gt; cutoff, drop=True)\n\n        # bins represent renewable generation grades\n        flatten = lambda t: \" grade\".join(map(str, t))\n        buses = ds.indexes[\"bus_bin\"].get_level_values(\"bus\")\n        bus_bins = ds.indexes[\"bus_bin\"].map(flatten)\n\n        p_nom_max = ds[\"p_nom_max\"].to_pandas()\n        p_nom_max.index = p_nom_max.index.map(flatten)\n\n        p_max_pu = ds[\"profile\"].to_pandas()\n        p_max_pu.columns = p_max_pu.columns.map(flatten)\n\n        # add renewables\n        network.add(\n            \"Generator\",\n            bus_bins,\n            suffix=f\" {tech}\",\n            bus=buses,\n            carrier=tech,\n            p_nom_extendable=True,\n            p_nom_max=p_nom_max,\n            capital_cost=costs.at[tech, \"capital_cost\"],\n            marginal_cost=costs.at[tech, \"marginal_cost\"],\n            p_max_pu=p_max_pu,\n            lifetime=costs.at[tech, \"lifetime\"],\n        )\n</code></pre>"},{"location":"reference/prepare_network/#prepare_network.generate_periodic_profiles","title":"<code>generate_periodic_profiles(dt_index=None, col_tzs=pd.Series(index=PROV_NAMES, data=(len(PROV_NAMES) * ['Shanghai'])), weekly_profile=range(24 * 7))</code>","text":"<p>Give a 24*7 long list of weekly hourly profiles, generate this for each country for the period dt_index, taking account of time zones and Summer Time.</p> Source code in <code>workflow/scripts/prepare_network.py</code> <pre><code>def generate_periodic_profiles(\n    dt_index=None,\n    col_tzs=pd.Series(index=PROV_NAMES, data=len(PROV_NAMES) * [\"Shanghai\"]),\n    weekly_profile=range(24 * 7),\n):\n    \"\"\"Give a 24*7 long list of weekly hourly profiles, generate this\n    for each country for the period dt_index, taking account of time\n    zones and Summer Time.\"\"\"\n\n    weekly_profile = pd.Series(weekly_profile, range(24 * 7))\n    # TODO fix, no longer take into accoutn summer time\n    # ALSO ADD A TODO in base_network\n    week_df = pd.DataFrame(index=dt_index, columns=col_tzs.index)\n    for ct in col_tzs.index:\n        week_df[ct] = [24 * dt.weekday() + dt.hour for dt in dt_index.tz_localize(None)]\n        week_df[ct] = week_df[ct].map(weekly_profile)\n    return week_df\n</code></pre>"},{"location":"reference/prepare_network/#prepare_network.prepare_network","title":"<code>prepare_network(config, costs, snapshots, biomass_potential=None, paths=None)</code>","text":"<p>Prepares/makes the network object for overnight mode according to config &amp; at 1 node per region/province</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict</code> <p>the snakemake config</p> required <code>costs</code> <code>DataFrame</code> <p>the costs dataframe (anualised capex and marginal costs)</p> required <code>snapshots</code> <code>date_range</code> <p>the snapshots for the network</p> required <code>biomass_potential</code> <code>(Optional, DataFrame)</code> <p>biomass potential dataframe. Defaults to None.</p> <code>None</code> <code>paths</code> <code>(Optional, dict)</code> <p>the paths to the data files. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Network</code> <p>pypsa.Network: the pypsa network object</p> Source code in <code>workflow/scripts/prepare_network.py</code> <pre><code>def prepare_network(\n    config: dict,\n    costs: pd.DataFrame,\n    snapshots: pd.date_range,\n    biomass_potential: pd.DataFrame = None,\n    paths: dict = None,\n) -&gt; pypsa.Network:\n    \"\"\"Prepares/makes the network object for overnight mode according to config &amp;\n    at 1 node per region/province\n\n    Args:\n        config (dict): the snakemake config\n        costs (pd.DataFrame): the costs dataframe (anualised capex and marginal costs)\n        snapshots (pd.date_range): the snapshots for the network\n        biomass_potential (Optional, pd.DataFrame): biomass potential dataframe. Defaults to None.\n        paths (Optional, dict): the paths to the data files. Defaults to None.\n\n    Returns:\n        pypsa.Network: the pypsa network object\n    \"\"\"\n\n    # determine whether gas/coal to be added depending on specified conv techs\n    config[\"add_gas\"] = (\n        True\n        if [tech for tech in config[\"Techs\"][\"conv_techs\"] if (\"gas\" in tech or \"CGT\" in tech)]\n        else False\n    )\n    config[\"add_coal\"] = (\n        True if [tech for tech in config[\"Techs\"][\"conv_techs\"] if \"coal\" in tech] else False\n    )\n\n    planning_horizons = snakemake.wildcards[\"planning_horizons\"]\n\n    # Build the Network object, which stores all other objects\n    network = pypsa.Network()\n    network.set_snapshots(snapshots)\n    network.snapshot_weightings[:] = config[\"snapshots\"][\"frequency\"]\n    # load graph\n    nodes = pd.Index(PROV_NAMES)\n    # toso soft code\n    countries = [\"CN\"] * len(nodes)\n\n    # TODO check crs projection correct\n    # load provinces\n    prov_shapes = read_province_shapes(paths[\"province_shape\"])\n    prov_centroids = prov_shapes.to_crs(\"+proj=cea\").centroid.to_crs(CRS)\n\n    # add AC buses\n    network.add(\n        \"Bus\", nodes, x=prov_centroids.x, y=prov_centroids.y, location=nodes, country=countries\n    )\n\n    # add carriers\n    add_carriers(network, config, costs)\n\n    # load electricity demand data\n    demand_path = paths[\"elec_load\"].replace(\"{planning_horizons}\", f\"{cost_year}\")\n    with pd.HDFStore(demand_path, mode=\"r\") as store:\n        load = store[\"load\"].loc[network.snapshots, PROV_NAMES]  # MWHr\n\n    network.add(\"Load\", nodes, bus=nodes, p_set=load[nodes])\n\n    ws_carriers = [c for c in config[\"Techs\"][\"vre_techs\"] if c.find(\"wind\") &gt;= 0 or c == \"solar\"]\n    add_wind_and_solar(network, ws_carriers, paths, planning_horizons, costs)\n\n    add_conventional_generators(network, nodes, config, prov_centroids, costs)\n\n    # nuclear is brownfield\n    if \"nuclear\" in config[\"Techs\"][\"vre_techs\"]:\n\n        nuclear_nodes = pd.Index(NUCLEAR_EXTENDABLE)\n        network.add(\n            \"Generator\",\n            nuclear_nodes,\n            suffix=\" nuclear\",\n            p_nom_extendable=True,\n            p_max_pu=config[\"nuclear_reactors\"][\"p_max_pu\"],\n            p_min_pu=config[\"nuclear_reactors\"][\"p_min_pu\"],\n            bus=nuclear_nodes,\n            carrier=\"nuclear\",\n            efficiency=costs.at[\"nuclear\", \"efficiency\"],\n            capital_cost=costs.at[\"nuclear\", \"capital_cost\"],  # NB: capital cost is per MWel\n            marginal_cost=costs.at[\"nuclear\", \"marginal_cost\"],\n            lifetime=costs.at[\"nuclear\", \"lifetime\"],\n        )\n\n    # TODO add coal CC? no retrofit option\n\n    if \"PHS\" in config[\"Techs\"][\"store_techs\"]:\n        # TODO soft-code path\n        # pure pumped hydro storage, fixed, 6h energy by default, no inflow\n        hydrocapa_df = pd.read_csv(\"resources/data/hydro/PHS_p_nom.csv\", index_col=0)\n        phss = hydrocapa_df.index[hydrocapa_df[\"MW\"] &gt; 0].intersection(nodes)\n        if config[\"hydro\"][\"hydro_capital_cost\"]:\n            cc = costs.at[\"PHS\", \"capital_cost\"]\n        else:\n            cc = 0.0\n\n        network.add(\n            \"StorageUnit\",\n            phss,\n            suffix=\" PHS\",\n            bus=phss,\n            carrier=\"PHS\",\n            p_nom_extendable=True,\n            # p_nom_max=hydrocapa_df.loc[phss][\"MW\"],\n            p_nom=hydrocapa_df.loc[phss][\"MW\"],\n            p_nom_min=hydrocapa_df.loc[phss][\"MW\"],\n            max_hours=config[\"hydro\"][\"PHS_max_hours\"],\n            efficiency_store=np.sqrt(costs.at[\"PHS\", \"efficiency\"]),\n            efficiency_dispatch=np.sqrt(costs.at[\"PHS\", \"efficiency\"]),\n            cyclic_state_of_charge=True,\n            capital_cost=cc,\n            marginal_cost=0.0,\n        )\n\n    if config[\"add_hydro\"]:\n        logger.info(\"Adding hydro to network\")\n        add_hydro(network, config, nodes, prov_centroids, costs, planning_horizons)\n\n    if config[\"add_H2\"]:\n        logger.info(\"Adding H2 buses to network\")\n        # do beore heat coupling to avoid warning\n        network.add(\n            \"Bus\",\n            nodes,\n            suffix=\" H2\",\n            x=prov_centroids.x,\n            y=prov_centroids.y,\n            carrier=\"H2\",\n            location=nodes,\n        )\n\n    if config.get(\"heat_coupling\", False):\n        logger.info(\"Adding heat and CHP to the network\")\n        add_heat_coupling(network, config, nodes, prov_centroids, costs, planning_horizons, paths)\n\n        if config[\"add_biomass\"]:\n            logger.info(\"Adding biomass to network\")\n            add_co2_capture_support(network, nodes, prov_centroids)\n            add_biomass_chp(\n                network,\n                costs,\n                nodes,\n                biomass_potential[nodes],\n                prov_centroids,\n                add_beccs=\"beccs\" in config[\"Techs\"][\"vre_techs\"],\n            )\n\n    if config[\"add_H2\"]:\n        logger.info(\"Adding H2 to network\")\n        add_H2(network, config, nodes, costs)\n\n    if \"battery\" in config[\"Techs\"][\"store_techs\"]:\n\n        network.add(\n            \"Bus\",\n            nodes,\n            suffix=\" battery\",\n            x=prov_centroids.x,\n            y=prov_centroids.y,\n            carrier=\"battery\",\n            location=nodes,\n        )\n\n        # TODO Why no standing loss?: test with\n        min_charge = config[\"electricity\"].get(\"min_charge\", {\"battery\": 0})\n        min_charge = min_charge.get(\"battery\", 0)\n        network.add(\n            \"Store\",\n            nodes + \" battery\",\n            bus=nodes + \" battery\",\n            e_cyclic=True,\n            e_nom_extendable=True,\n            e_min_pu=min_charge,\n            capital_cost=costs.at[\"battery storage\", \"capital_cost\"],\n            lifetime=costs.at[\"battery storage\", \"lifetime\"],\n        )\n\n        # TODO understand/remove sources, data should not be in code\n        # Sources:\n        # [HP]: Henning, Palzer http://www.sciencedirect.com/science/article/pii/S1364032113006710\n        # [B]: Budischak et al. http://www.sciencedirect.com/science/article/pii/S0378775312014759\n\n        network.add(\n            \"Link\",\n            nodes + \" battery charger\",\n            bus0=nodes,\n            bus1=nodes + \" battery\",\n            efficiency=costs.at[\"battery inverter\", \"efficiency\"] ** 0.5,\n            capital_cost=costs.at[\"battery inverter\", \"efficiency\"]\n            * costs.at[\"battery inverter\", \"capital_cost\"],\n            p_nom_extendable=True,\n            carrier=\"battery\",\n            lifetime=costs.at[\"battery inverter\", \"lifetime\"],\n        )\n\n        network.add(\n            \"Link\",\n            nodes + \" battery discharger\",\n            bus0=nodes + \" battery\",\n            bus1=nodes,\n            efficiency=costs.at[\"battery inverter\", \"efficiency\"] ** 0.5,\n            marginal_cost=0.0,\n            p_nom_extendable=True,\n            carrier=\"battery discharger\",\n        )\n\n    # ============= add lines =========\n    # The lines are implemented according to the transport model (no KVL) and without losses.\n    # see Neumann et al 10.1016/j.apenergy.2022.118859\n    # TODO make not lossless optional (? - increases computing cost)\n\n    if not config[\"no_lines\"]:\n        add_voltage_links(network, config)\n\n    assign_locations(network)\n    return network\n</code></pre>"},{"location":"reference/prepare_network_common/","title":"Prepare network common","text":""},{"location":"reference/prepare_network_common/#prepare_network_common.add_HV_links","title":"<code>add_HV_links(network, config, n_years)</code>","text":"<p>add high voltage connections as links in the lossy transport model (see Neumann et al)</p> <p>Parameters:</p> Name Type Description Default <code>network</code> <code>Network</code> <p>the pypsa network</p> required <code>config</code> <code>dict</code> <p>the configuration dictionary</p> required <code>n_years</code> <code>int</code> <p>the number of years for discounting</p> required Source code in <code>workflow/scripts/prepare_network_common.py</code> <pre><code>def add_HV_links(network: pypsa.Network, config: dict, n_years: int):\n    \"\"\"add high voltage connections as links in the lossy transport model (see Neumann et al)\n\n    Args:\n        network (pypsa.Network): the pypsa network\n        config (dict): the configuration dictionary\n        n_years (int): the number of years for discounting\n    \"\"\"\n\n    edge_path = config[\"edge_paths\"].get(config[\"scenario\"][\"topology\"], None)\n    if edge_path is None:\n        raise ValueError(f\"No grid found for topology {config['scenario']['topology']}\")\n    else:\n        edges = pd.read_csv(\n            edge_path, sep=\",\", header=None, names=[\"bus0\", \"bus1\", \"p_nom\"]\n        ).fillna(0)\n\n    # fix this to use map with x.y\n    lengths = NON_LIN_PATH_SCALING * np.array(\n        [\n            haversine(\n                [network.buses.at[bus0, \"x\"], network.buses.at[bus0, \"y\"]],\n                [network.buses.at[bus1, \"x\"], network.buses.at[bus1, \"y\"]],\n            )\n            for bus0, bus1 in edges[[\"bus0\", \"bus1\"]].values\n        ]\n    )\n\n    cc = (\n        (config[\"line_cost_factor\"] * lengths * [HVAC_cost_curve(len_) for len_ in lengths])\n        * LINE_SECURITY_MARGIN\n        * FOM_LINES\n        * n_years\n        * annuity(ECON_LIFETIME_LINES, config[\"costs\"][\"discountrate\"])\n    )\n\n    network.add(\n        \"Link\",\n        edges[\"bus0\"] + \"-\" + edges[\"bus1\"],\n        p_nom=edges[\"p_nom\"].values,\n        p_nom_min=edges[\"p_nom\"].values,\n        bus0=edges[\"bus0\"].values,\n        bus1=edges[\"bus1\"].values,\n        p_nom_extendable=True,\n        p_min_pu=-1,\n        length=lengths,\n        capital_cost=cc,\n    )\n</code></pre>"},{"location":"reference/prepare_network_common/#prepare_network_common.calc_renewable_pu_avail","title":"<code>calc_renewable_pu_avail(renewable_ds, planning_year, snapshots)</code>","text":"<p>calaculate the renewable per unit availability</p> <p>Parameters:</p> Name Type Description Default <code>renewable_ds</code> <code>Dataset</code> <p>the renewable dataset from build_renewable_potential</p> required <code>planning_year</code> <code>int</code> <p>the investment year</p> required <code>snapshots</code> <code>Index</code> <p>the network snapshots</p> required Source code in <code>workflow/scripts/prepare_network_common.py</code> <pre><code>def calc_renewable_pu_avail(\n    renewable_ds: xr.Dataset, planning_year: int, snapshots: pd.Index\n) -&gt; pd.DataFrame:\n    \"\"\"calaculate the renewable per unit availability\n\n    Args:\n        renewable_ds (xr.Dataset): the renewable dataset from build_renewable_potential\n        planning_year (int): the investment year\n        snapshots (pd.Index): the network snapshots\n    \"\"\"\n    rnwable_p_max_pu = renewable_ds[\"profile\"].transpose(\"time\", \"bus\").to_pandas()\n    rnwable_p_max_pu = shift_profile_to_planning_year(rnwable_p_max_pu, planning_year)\n    if not (snapshots.isin(rnwable_p_max_pu.index)).all():\n        err = \"Snapshots do not match renewable data profile data:\"\n        err += f\"\\n\\tmissing {snapshots.difference(rnwable_p_max_pu.index)}.\\n\"\n        tip = \"You may may need to regenerate your cutout or adapt the snapshots\"\n        raise ValueError(err + tip)\n    rnwable_p_max_pu = rnwable_p_max_pu.loc[snapshots]\n    return rnwable_p_max_pu.sort_index(axis=1)\n</code></pre>"},{"location":"reference/readers/","title":"Readers","text":"<p>file reading support functions</p>"},{"location":"reference/readers/#readers.read_yearly_load_projections","title":"<code>read_yearly_load_projections(yearly_projections_p='resources/data/load/Province_Load_2020_2060.csv', conversion=1)</code>","text":"<p>prepare projections for model use</p> <p>Parameters:</p> Name Type Description Default <code>yearly_projections_p</code> <code>PathLike</code> <p>the data path.     Defaults to \"resources/data/load/Province_Load_2020_2060.csv\".</p> <code>'resources/data/load/Province_Load_2020_2060.csv'</code> <code>conversion</code> <code>int</code> <p>the conversion factor to MWh. Defaults to 1.</p> <code>1</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: the formatted data, in MWh</p> Source code in <code>workflow/scripts/readers.py</code> <pre><code>def read_yearly_load_projections(\n    yearly_projections_p: os.PathLike = \"resources/data/load/Province_Load_2020_2060.csv\",\n    conversion=1,\n) -&gt; pd.DataFrame:\n    \"\"\"prepare projections for model use\n\n    Args:\n        yearly_projections_p (os.PathLike, optional): the data path.\n                Defaults to \"resources/data/load/Province_Load_2020_2060.csv\".\n        conversion (int, optional): the conversion factor to MWh. Defaults to 1.\n\n    Returns:\n        pd.DataFrame: the formatted data, in MWh\n    \"\"\"\n    yearly_proj = pd.read_csv(yearly_projections_p)\n    yearly_proj.rename(columns={\"Unnamed: 0\": \"province\", \"region\": \"province\"}, inplace=True)\n    if \"province\" not in yearly_proj.columns:\n        raise ValueError(\n            \"The province (or region or unamed) column is missing in the yearly projections data\"\n            \". Index cannot be built\"\n        )\n    yearly_proj.set_index(\"province\", inplace=True)\n    yearly_proj.rename(columns={c: int(c) for c in yearly_proj.columns}, inplace=True)\n\n    return yearly_proj * conversion\n</code></pre>"},{"location":"reference/readers_geospatial/","title":"Readers geospatial","text":"<p>File reading support functions</p>"},{"location":"reference/readers_geospatial/#readers_geospatial.read_offshore_province_shapes","title":"<code>read_offshore_province_shapes(shape_file, index_name='province')</code>","text":"<p>read the offshore province shape files (based on the eez)</p> <p>Parameters:</p> Name Type Description Default <code>shape_file</code> <code>PathLike</code> <p>the path to the .shp file &amp; co</p> required <code>index_name</code> <code>str</code> <p>the name of the index column. Defaults to \"province\".</p> <code>'province'</code> <p>Returns:</p> Type Description <code>GeoDataFrame</code> <p>gpd.GeoDataFrame: the offshore province shapes as a GeoDataFrame</p> Source code in <code>workflow/scripts/readers_geospatial.py</code> <pre><code>def read_offshore_province_shapes(\n    shape_file: os.PathLike, index_name=\"province\"\n) -&gt; gpd.GeoDataFrame:\n    \"\"\"read the offshore province shape files (based on the eez)\n\n    Args:\n        shape_file (os.PathLike): the path to the .shp file &amp; co\n        index_name (str, optional): the name of the index column. Defaults to \"province\".\n\n    Returns:\n        gpd.GeoDataFrame: the offshore province shapes as a GeoDataFrame\n    \"\"\"\n\n    offshore_regional = gpd.read_file(shape_file).set_index(index_name)\n    offshore_regional = offshore_regional.reindex(OFFSHORE_WIND_NODES).rename_axis(\"bus\")\n    if offshore_regional.geometry.isnull().any():\n        empty_geoms = offshore_regional[offshore_regional.geometry.isnull()].index.to_list()\n        raise ValueError(\n            f\"There are empty geometries in offshore_regional {empty_geoms}, offshore wind will fail\"\n        )\n\n    return offshore_regional\n</code></pre>"},{"location":"reference/readers_geospatial/#readers_geospatial.read_pop_density","title":"<code>read_pop_density(path, clip_shape=None, crs=CRS, chunks=25, var_name='pop_density')</code>","text":"<p>read raster data, clip it to a clip_shape and convert it to a GeoDataFrame</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>PathLike</code> <p>the target path for the raster data (tif)</p> required <code>clip_shape</code> <code>GeoSeries</code> <p>the shape to clip the data. Defaults to None.</p> <code>None</code> <code>crs</code> <code>int</code> <p>the coordinate system. Defaults to 4326.</p> <code>CRS</code> <code>var_name</code> <code>str</code> <p>the variable name. Defaults to \"var\".</p> <code>'pop_density'</code> <code>chunks</code> <code>int</code> <p>the chunk size for the raster data. Defaults to 25.</p> <code>25</code> <p>Returns:</p> Type Description <code>GeoDataFrame</code> <p>gpd.GeoDataFrame: the raster data for the aoi</p> Source code in <code>workflow/scripts/readers_geospatial.py</code> <pre><code>def read_pop_density(\n    path: os.PathLike,\n    clip_shape: gpd.GeoSeries = None,\n    crs=CRS,\n    chunks=25,\n    var_name=\"pop_density\",\n) -&gt; gpd.GeoDataFrame:\n    \"\"\"read raster data, clip it to a clip_shape and convert it to a GeoDataFrame\n\n    Args:\n        path (os.PathLike): the target path for the raster data (tif)\n        clip_shape (gpd.GeoSeries, optional): the shape to clip the data. Defaults to None.\n        crs (int, optional): the coordinate system. Defaults to 4326.\n        var_name (str, optional): the variable name. Defaults to \"var\".\n        chunks (int, optional): the chunk size for the raster data. Defaults to 25.\n\n    Returns:\n        gpd.GeoDataFrame: the raster data for the aoi\n    \"\"\"\n\n    ds = read_raster(path, clip_shape, var_name, plot=False)\n    ds = ds.where(ds &gt; 0)\n\n    df = ds.to_dataframe(var_name)\n    df.reset_index(inplace=True)\n\n    # Convert the DataFrame to a GeoDataFrame\n    return gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df.x, df.y), crs=crs)\n</code></pre>"},{"location":"reference/readers_geospatial/#readers_geospatial.read_province_shapes","title":"<code>read_province_shapes(shape_file)</code>","text":"<p>read the province shape files</p> <p>Parameters:</p> Name Type Description Default <code>shape_file</code> <code>PathLike</code> <p>the path to the .shp file &amp; co</p> required <p>Returns:</p> Type Description <code>GeoDataFrame</code> <p>gpd.GeoDataFrame: the province shapes as a GeoDataFrame</p> Source code in <code>workflow/scripts/readers_geospatial.py</code> <pre><code>def read_province_shapes(shape_file: os.PathLike) -&gt; gpd.GeoDataFrame:\n    \"\"\"read the province shape files\n\n    Args:\n        shape_file (os.PathLike): the path to the .shp file &amp; co\n\n    Returns:\n        gpd.GeoDataFrame: the province shapes as a GeoDataFrame\n    \"\"\"\n\n    prov_shapes = gpd.GeoDataFrame.from_file(shape_file)\n    prov_shapes = prov_shapes.to_crs(CRS)\n    prov_shapes.set_index(\"province\", inplace=True)\n    # TODO: does this make sense? reindex after?\n    if not (prov_shapes.sort_index().index == sorted(PROV_NAMES)).all():\n        missing = f\"Missing provinces: {set(PROV_NAMES) - set(prov_shapes.index)}\"\n        raise ValueError(f\"Province names do not match expected names: missing {missing}\")\n\n    return prov_shapes\n</code></pre>"},{"location":"reference/readers_geospatial/#readers_geospatial.read_raster","title":"<code>read_raster(path, clip_shape=None, var_name='var', chunks=60, plot=False)</code>","text":"<p>Read raster data and optionally clip it to a given shape.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>PathLike</code> <p>The path to the raster file.</p> required <code>clip_shape</code> <code>GeoSeries</code> <p>The shape to clip the raster data. Defaults to None.</p> <code>None</code> <code>var_name</code> <code>str</code> <p>The variable name to assign to the raster data. Defaults to \"var\".</p> <code>'var'</code> <code>chunks</code> <code>int</code> <p>The chunk size for the raster data. Defaults to 60.</p> <code>60</code> <code>plot</code> <code>bool</code> <p>Whether to plot the raster data. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>DataArray</code> <code>DataArray</code> <p>The raster data as an xarray DataArray.</p> Source code in <code>workflow/scripts/readers_geospatial.py</code> <pre><code>def read_raster(\n    path: os.PathLike,\n    clip_shape: gpd.GeoSeries = None,\n    var_name=\"var\",\n    chunks=60,\n    plot=False,\n) -&gt; DataArray:\n    \"\"\"Read raster data and optionally clip it to a given shape.\n\n    Args:\n        path (os.PathLike): The path to the raster file.\n        clip_shape (gpd.GeoSeries, optional): The shape to clip the raster data. Defaults to None.\n        var_name (str, optional): The variable name to assign to the raster data. Defaults to \"var\".\n        chunks (int, optional): The chunk size for the raster data. Defaults to 60.\n        plot (bool, optional): Whether to plot the raster data. Defaults to False.\n\n    Returns:\n        DataArray: The raster data as an xarray DataArray.\n    \"\"\"\n    ds = rioxarray.open_rasterio(path, chunks=chunks, default_name=\"pop_density\")\n    ds = ds.rename(var_name)\n\n    if clip_shape is not None:\n        ds = ds.rio.clip(clip_shape.geometry)\n\n    if plot:\n        ds.plot()\n\n    return ds\n</code></pre>"},{"location":"reference/solve_network/","title":"Solve network","text":"<p>Functions to add constraints and prepare the network for the solver. Associated with the <code>solve_networks</code> rule in the Snakefile.</p>"},{"location":"reference/solve_network/#solve_network.add_battery_constraints","title":"<code>add_battery_constraints(n)</code>","text":"<p>Add constraint ensuring that charger = discharger, i.e. 1 * charger_size - efficiency * discharger_size = 0</p> Source code in <code>workflow/scripts/solve_network.py</code> <pre><code>def add_battery_constraints(n: pypsa.Network):\n    \"\"\"\n    Add constraint ensuring that charger = discharger, i.e.\n    1 * charger_size - efficiency * discharger_size = 0\n    \"\"\"\n    if not n.links.p_nom_extendable.any():\n        return\n\n    discharger_bool = n.links.index.str.contains(\"battery discharger\")\n    charger_bool = n.links.index.str.contains(\"battery charger\")\n\n    dischargers_ext = n.links[discharger_bool].query(\"p_nom_extendable\").index\n    chargers_ext = n.links[charger_bool].query(\"p_nom_extendable\").index\n\n    eff = n.links.efficiency[dischargers_ext].values\n    lhs = n.model[\"Link-p_nom\"].loc[chargers_ext] - n.model[\"Link-p_nom\"].loc[dischargers_ext] * eff\n\n    n.model.add_constraints(lhs == 0, name=\"Link-charger_ratio\")\n</code></pre>"},{"location":"reference/solve_network/#solve_network.add_chp_constraints","title":"<code>add_chp_constraints(n)</code>","text":"<p>Add constraints to couple the heat and electricity output of CHP plants      (using the cb and cv parameter). See the DEA technology cataloge</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>Network</code> <p>the pypsa network object to which's model the constraints are added</p> required Source code in <code>workflow/scripts/solve_network.py</code> <pre><code>def add_chp_constraints(n: pypsa.Network):\n    \"\"\"Add constraints to couple the heat and electricity output of CHP plants\n         (using the cb and cv parameter). See the DEA technology cataloge\n\n    Args:\n        n (pypsa.Network): the pypsa network object to which's model the constraints are added\n    \"\"\"\n    electric = n.links.index.str.contains(\"CHP\") &amp; n.links.index.str.contains(\"generator\")\n    heat = n.links.index.str.contains(\"CHP\") &amp; n.links.index.str.contains(\"boiler\")\n\n    electric_ext = n.links[electric].query(\"p_nom_extendable\").index\n    heat_ext = n.links[heat].query(\"p_nom_extendable\").index\n\n    electric_fix = n.links[electric].query(\"~p_nom_extendable\").index\n    heat_fix = n.links[heat].query(\"~p_nom_extendable\").index\n\n    p = n.model[\"Link-p\"]  # dimension: [time, link]\n\n    # output ratio between heat and electricity and top_iso_fuel_line for extendable\n    if not electric_ext.empty:\n        p_nom = n.model[\"Link-p_nom\"]\n\n        lhs = (\n            p_nom.loc[electric_ext]\n            * (n.links.p_nom_ratio * n.links.efficiency)[electric_ext].values\n            - p_nom.loc[heat_ext] * n.links.efficiency[heat_ext].values\n        )\n        n.model.add_constraints(lhs == 0, name=\"chplink-fix_p_nom_ratio\")\n\n        rename = {\"Link-ext\": \"Link\"}\n        lhs = p.loc[:, electric_ext] + p.loc[:, heat_ext] - p_nom.rename(rename).loc[electric_ext]\n        n.model.add_constraints(lhs &lt;= 0, name=\"chplink-top_iso_fuel_line_ext\")\n\n    # top_iso_fuel_line for fixed\n    if not electric_fix.empty:\n        lhs = p.loc[:, electric_fix] + p.loc[:, heat_fix]\n        rhs = n.links.p_nom[electric_fix]\n        n.model.add_constraints(lhs &lt;= rhs, name=\"chplink-top_iso_fuel_line_fix\")\n\n    # back-pressure\n    if not n.links[electric].index.empty:\n        lhs = (\n            p.loc[:, heat] * (n.links.efficiency[heat] * n.links.c_b[electric].values)\n            - p.loc[:, electric] * n.links.efficiency[electric]\n        )\n        n.model.add_constraints(lhs &lt;= 0, name=\"chplink-backpressure\")\n</code></pre>"},{"location":"reference/solve_network/#solve_network.add_co2_constraints_prices","title":"<code>add_co2_constraints_prices(network, co2_control)</code>","text":"<p>Add co2 constraints or prices</p> <p>Parameters:</p> Name Type Description Default <code>network</code> <code>Network</code> <p>the network to which prices or constraints are to be added</p> required <code>co2_control</code> <code>dict</code> <p>the config</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>unrecognised co2 control option</p> Source code in <code>workflow/scripts/solve_network.py</code> <pre><code>def add_co2_constraints_prices(network: pypsa.Network, co2_control: dict):\n    \"\"\"Add co2 constraints or prices\n\n    Args:\n        network (pypsa.Network): the network to which prices or constraints are to be added\n        co2_control (dict): the config\n\n    Raises:\n        ValueError: unrecognised co2 control option\n    \"\"\"\n\n    if co2_control[\"control\"] is None:\n        pass\n    elif co2_control[\"control\"] == \"price\":\n        logger.info(\"Adding CO2 price to marginal costs of generators and storage units\")\n        add_emission_prices(network, emission_prices={\"co2\": co2_control[\"co2_pr_or_limit\"]})\n\n    elif co2_control[\"control\"].startswith(\"budget\"):\n        co2_limit = co2_control[\"co2_pr_or_limit\"]\n        logger.info(\"Adding CO2 constraint based on scenario {co2_limit}\")\n        network.add(\n            \"GlobalConstraint\",\n            \"co2_limit\",\n            type=\"primary_energy\",\n            carrier_attribute=\"co2_emissions\",\n            sense=\"&lt;=\",\n            constant=co2_limit,\n        )\n    else:\n        logger.error(f\"Unhandled CO2 control config {co2_control} due to unknown control.\")\n        raise ValueError(f\"Unhandled CO2 control config {co2_control} due to unknown control\")\n</code></pre>"},{"location":"reference/solve_network/#solve_network.add_emission_prices","title":"<code>add_emission_prices(n, emission_prices={'co2': 0.0}, exclude_co2=False)</code>","text":"<p>from pypsa-eur: add GHG price to marginal costs of generators and storage units</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>Network</code> <p>the pypsa network</p> required <code>emission_prices</code> <code>dict</code> <p>emission prices per GHG. Defaults to {\"co2\": 0.0}.</p> <code>{'co2': 0.0}</code> <code>exclude_co2</code> <code>bool</code> <p>do not charge for CO2 emissions. Defaults to False.</p> <code>False</code> Source code in <code>workflow/scripts/solve_network.py</code> <pre><code>def add_emission_prices(n: pypsa.Network, emission_prices={\"co2\": 0.0}, exclude_co2=False):\n    \"\"\"from pypsa-eur: add GHG price to marginal costs of generators and storage units\n\n    Args:\n        n (pypsa.Network): the pypsa network\n        emission_prices (dict, optional): emission prices per GHG. Defaults to {\"co2\": 0.0}.\n        exclude_co2 (bool, optional): do not charge for CO2 emissions. Defaults to False.\n    \"\"\"\n    if exclude_co2:\n        emission_prices.pop(\"co2\")\n    em_price = (\n        pd.Series(emission_prices).rename(lambda x: x + \"_emissions\")\n        * n.carriers.filter(like=\"_emissions\")\n    ).sum(axis=1)\n\n    n.meta.update({\"emission_prices\": emission_prices})\n\n    gen_em_price = n.generators.carrier.map(em_price) / n.generators.efficiency\n\n    n.generators[\"marginal_cost\"] += gen_em_price\n    n.generators_t[\"marginal_cost\"] += gen_em_price[n.generators_t[\"marginal_cost\"].columns]\n    # storage units su\n    su_em_price = n.storage_units.carrier.map(em_price) / n.storage_units.efficiency_dispatch\n    n.storage_units[\"marginal_cost\"] += su_em_price\n\n    logger.info(\"Added emission prices to marginal costs of generators and storage units\")\n    logger.info(f\"\\tEmission prices: {emission_prices}\")\n</code></pre>"},{"location":"reference/solve_network/#solve_network.add_land_use_constraint","title":"<code>add_land_use_constraint(n, planning_horizons)</code>","text":"<p>Add land use constraints for renewable energy potential. This ensures that the brownfield +  greenfield vre installations for each generator tech do not exceed the technical potential.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>Network</code> <p>the network object to add the constraints to</p> required <code>planning_horizons</code> <code>str | int</code> <p>the planning horizon year as string</p> required Source code in <code>workflow/scripts/solve_network.py</code> <pre><code>def add_land_use_constraint(n: pypsa.Network, planning_horizons: str | int) -&gt; None:\n    \"\"\"\n    Add land use constraints for renewable energy potential. This ensures that the brownfield +\n     greenfield vre installations for each generator tech do not exceed the technical potential.\n\n    Args:\n        n (pypsa.Network): the network object to add the constraints to\n        planning_horizons (str | int): the planning horizon year as string\n    \"\"\"\n    # warning: this will miss existing offwind which is not classed AC-DC and has carrier 'offwind'\n\n    for carrier in [\n        \"solar\",\n        \"solar thermal\",\n        \"onwind\",\n        \"offwind\",\n        \"offwind-ac\",\n        \"offwind-dc\",\n        \"offwind-float\",\n    ]:\n        ext_i = (n.generators.carrier == carrier) &amp; ~n.generators.p_nom_extendable\n        grouper = n.generators.loc[ext_i].index.str.replace(f\" {carrier}.*$\", \"\", regex=True)\n        existing = n.generators.loc[ext_i, \"p_nom\"].groupby(grouper).sum()\n        existing.index += f\" {carrier}\"\n        n.generators.loc[existing.index, \"p_nom_max\"] -= existing\n\n    # check if existing capacities are larger than technical potential\n    existing_large = n.generators[n.generators[\"p_nom_min\"] &gt; n.generators[\"p_nom_max\"]].index\n    if len(existing_large):\n        logger.warning(\n            f\"Existing capacities larger than technical potential for {existing_large},\\\n                        adjust technical potential to existing capacities\"\n        )\n        n.generators.loc[existing_large, \"p_nom_max\"] = n.generators.loc[\n            existing_large, \"p_nom_min\"\n        ]\n\n    n.generators[\"p_nom_max\"] = n.generators[\"p_nom_max\"].clip(lower=0)\n</code></pre>"},{"location":"reference/solve_network/#solve_network.add_operational_reserve_margin","title":"<code>add_operational_reserve_margin(n, config)</code>","text":"<p>Build operational reserve margin constraints based on the formulation given in https://genxproject.github.io/GenX.jl/stable/Model_Reference/core/#GenX.operational_reserves_core!-Tuple{JuMP.Model,%20Dict,%20Dict}</p> <p>The constraint is network wide and not at each node!</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>Network</code> <p>the network object to optimize</p> required <code>config</code> <code>dict</code> <p>the configuration dictionary</p> required Example <p>config.yaml requires to specify operational_reserve: operational_reserve:     activate: true     epsilon_load: 0.02 # percentage of load at each snapshot     epsilon_vres: 0.02 # percentage of VRES at each snapshot     contingency: 400000 # MW</p> Source code in <code>workflow/scripts/solve_network.py</code> <pre><code>def add_operational_reserve_margin(n: pypsa.network, config):\n    \"\"\"\n    Build operational reserve margin constraints based on the formulation given in\n    https://genxproject.github.io/GenX.jl/stable/Model_Reference/core/#GenX.operational_reserves_core!-Tuple{JuMP.Model,%20Dict,%20Dict}\n\n    The constraint is network wide and not at each node!\n\n    Args:\n        n (pypsa.Network): the network object to optimize\n        config (dict): the configuration dictionary\n\n    Example:\n        config.yaml requires to specify operational_reserve:\n        operational_reserve:\n            activate: true\n            epsilon_load: 0.02 # percentage of load at each snapshot\n            epsilon_vres: 0.02 # percentage of VRES at each snapshot\n            contingency: 400000 # MW\n    \"\"\"\n    reserve_config = config[\"operational_reserve\"]\n    VRE_TECHS = config[\"Techs\"].get(\"non_dispatchable\", [\"onwind\", \"offwind\", \"solar\"])\n    EPSILON_LOAD, EPSILON_VRES = reserve_config[\"epsilon_load\"], reserve_config[\"epsilon_vres\"]\n    CONTINGENCY = float(reserve_config[\"contingency\"])\n\n    # AC producers\n    ac_mask = n.generators.bus.map(n.buses.carrier) == \"AC\"\n    ac_buses = n.buses.query(\"carrier =='AC'\").index\n    attached_carriers = filter_carriers(n, \"AC\")\n    # conceivably a link could have a negative efficiency and flow towards bus0 - don't consider\n    prod_links = n.links.query(\"carrier in @attached_carriers &amp; not bus0 in @ac_buses\")\n    transport_links = prod_links.bus0.map(n.buses.carrier) == prod_links.bus1.map(n.buses.carrier)\n    prod_links = prod_links.loc[transport_links == False]\n    prod_gen = n.generators.loc[ac_mask]\n    producers_all = prod_links.index.append(prod_gen.index)\n    producers_all.name = \"Producers-p\"\n\n    # RSERVES\n    n.model.add_variables(0, np.inf, coords=[n.snapshots, prod_gen.index], name=\"Generator-r\")\n    n.model.add_variables(0, np.inf, coords=[n.snapshots, prod_links.index], name=\"Link-r\")\n\n    # Define Reserve and weigh VRES by their mean availability (\"capacity credit\")\n    vres_gen = prod_gen.query(\"carrier in @VRE_TECHS\")\n    non_vre = prod_gen.index.difference(vres_gen.index)\n    # full capacity credit for non-VRE producers (questionable, maybe should be weighted by availability)\n    summed_reserve = (n.model[\"Link-r\"] * prod_links.efficiency).sum(\"Link\") + n.model[\n        \"Generator-r\"\n    ].loc[:, non_vre].sum(\"Generator\")\n\n    # VRE capacity credit &amp; margin reqs\n    ext_idx = vres_gen.query(\"p_nom_extendable\").index\n    avail = n.generators_t.p_max_pu.loc[:, vres_gen.index]\n    vres_idx = avail.columns\n\n    if not vres_gen.empty:\n        # Reserve score based on actual avail (perfect foresight) not mean/expected avail\n        vre_reserve_score = (n.model[\"Generator-r\"].loc[:, vres_gen.index] * avail).sum(\"Generator\")\n        summed_reserve += vre_reserve_score\n    if not ext_idx.empty and not vres_idx.empty:\n        # reqs from brownfield VRE generators. epsilon is the margin for VRES forecast error\n        avail_factor = n.generators_t.p_max_pu[ext_idx]\n        p_nom_vres = n.model[\"Generator-p_nom\"].loc[ext_idx].rename({\"Generator-ext\": \"Generator\"})\n        vre_req_ext = (p_nom_vres * (EPSILON_VRES * xr.DataArray(avail_factor))).sum(\"Generator\")\n    else:\n        vre_req_ext = 0\n\n    if not vres_idx.empty:\n        # reqs extendable VRE generators\n        avail_factor = n.generators_t.p_max_pu[vres_idx.difference(ext_idx)]\n        renewable_capacity = n.generators.p_nom[vres_idx.difference(ext_idx)]\n        vre_req_fix = (avail_factor * renewable_capacity).sum(axis=1)\n    else:\n        vre_req_fix = 0\n\n    lhs = summed_reserve - vre_req_ext\n    # Right-hand-side\n    demand = get_as_dense(n, \"Load\", \"p_set\").sum(axis=1)\n    rhs = EPSILON_LOAD * demand + EPSILON_VRES * vre_req_fix + CONTINGENCY\n\n    n.model.add_constraints(lhs &gt;= rhs, name=\"Reserve-margin\")\n\n    # Need additional constraints (reserve + dispatch &lt;= p_nom): gen_r + gen_p &lt;= gen_p_nom (capacity)\n    to_constrain = {\"Link\": prod_links, \"Generator\": prod_gen}\n    for component, producer in to_constrain.items():\n        logger.info(f\"adding secondary reserve constraint for {component}s\")\n\n        fix_i = producer.query(\"p_nom_extendable==False\").index\n        ext_i = producer.query(\"p_nom_extendable==True\").index\n\n        dispatch = n.model[f\"{component}-p\"].loc[:, producer.index]\n        reserve = n.model[f\"{component}-r\"].loc[:, ext_i.union(fix_i)]\n\n        capacity_variable = n.model[f\"{component}-p_nom\"].loc[ext_i]\n        capacity_variable = capacity_variable.rename({f\"{component}-ext\": f\"{component}\"})\n        capacity_fixed = getattr(n, component.lower() + \"s\").p_nom[fix_i]\n\n        p_max_pu = get_as_dense(n, f\"{component}\", \"p_max_pu\")\n\n        lhs = dispatch + reserve\n        # MAY have to check what happens in case pmaxpu is not defined for all items\n        rhs = capacity_variable * p_max_pu[ext_i] + (p_max_pu[fix_i] * capacity_fixed)\n        n.model.add_constraints(\n            lhs - rhs.loc[lhs.indexes] &lt;= 0, name=f\"{component}-p-reserve-upper\"\n        )\n</code></pre>"},{"location":"reference/solve_network/#solve_network.add_remind_paid_off_constraints","title":"<code>add_remind_paid_off_constraints(n)</code>","text":"<p>Paid-off components can be placed wherever PyPSA wants but have a total limit.</p> <p>Add constraints to ensure that the paid off capacity from REMIND is not exceeded across the network &amp; that it does not exceed the technical potential.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>Network</code> <p>the network object to which's model the constraints are added</p> required Source code in <code>workflow/scripts/solve_network.py</code> <pre><code>def add_remind_paid_off_constraints(n: pypsa.Network) -&gt; None:\n    \"\"\"\n    Paid-off components can be placed wherever PyPSA wants but have a total limit.\n\n    Add constraints to ensure that the paid off capacity from REMIND is not\n    exceeded across the network &amp; that it does not exceed the technical potential.\n\n    Args:\n        n (pypsa.Network): the network object to which's model the constraints are added\n    \"\"\"\n\n    if not n.config[\"run\"].get(\"is_remind_coupled\", False):\n        logger.info(\"Skipping paid off constraints as REMIND is not coupled\")\n        return\n\n    # In coupled-mode components (Generators, Links,..) have limits p/e_nom_rcl &amp; a tech_group\n    # These columns are added by `add_existing_baseyear.add_paid_off_capacity`.\n    # p/e_nom_rcl is the availale paid-off capacity per tech group and is nan for non paid-off (usual) generators.\n    # rcl is a legacy name from Aodenweller\n    for component in [\"Generator\", \"Link\", \"Store\"]:\n\n        prefix = \"e\" if component == \"Store\" else \"p\"\n        paid_off_col = f\"{prefix}_nom_max_rcl\"\n\n        paid_off = getattr(n, component.lower() + \"s\").copy()\n        # if there are no paid_off components\n        if paid_off_col not in paid_off.columns:\n            continue\n        else:\n            paid_off.dropna(subset=[paid_off_col], inplace=True)\n\n        paid_off_totals = paid_off.set_index(\"tech_group\")[paid_off_col].drop_duplicates()\n\n        # LHS: p_nom per technology grp &lt; totals\n        groupers = [paid_off[\"tech_group\"]]\n        grouper_lhs = xr.DataArray(pd.MultiIndex.from_arrays(groupers), dims=[f\"{component}-ext\"])\n        p_nom_groups = (\n            n.model[f\"{component}-{prefix}_nom\"].loc[paid_off.index].groupby(grouper_lhs).sum()\n        )\n\n        # get indices to sort RHS. the grouper is multi-indexed (legacy from PyPSA-Eur)\n        idx = p_nom_groups.indexes[\"group\"]\n        idx = [x[0] for x in idx]\n\n        # Add constraint\n        if not p_nom_groups.empty:\n            n.model.add_constraints(\n                p_nom_groups &lt;= paid_off_totals[idx].values,\n                name=f\"paidoff_cap_totals_{component.lower()}\",\n            )\n\n    # === ensure normal e/p_nom_max is respected for (paid_off + normal) components\n    # e.g. if PV has 100MW tech potential at nodeA, paid_off+normal p_nom_opt &lt;100MW\n    for component in [\"Generator\", \"Link\", \"Store\"]:\n        paidoff_comp = getattr(n, component.lower() + \"s\").copy()\n\n        prefix = \"e\" if component == \"Store\" else \"p\"\n        paid_off_col = f\"{prefix}_nom_max_rcl\"\n        # if there are no paid_off components\n        if paid_off_col not in paidoff_comp.columns:\n            continue\n        else:\n            paidoff_comp.dropna(subset=[paid_off_col], inplace=True)\n\n        # techs that only exist as paid-off don't have usual counterparts\n        remind_only_techs = n.config[\"existing_capacities\"].get(\"remind_only_tech_groups\", [])\n        paidoff_comp = paidoff_comp.query(\"tech_group not in @remind_only_techs\")\n\n        if paidoff_comp.empty:\n            continue\n\n        # find equivalent usual (not paid-off) components\n        usual_comps_idx = paidoff_comp.index.str.replace(\"_paid_off\", \"\")\n        usual_comps = getattr(n, component.lower() + \"s\").loc[usual_comps_idx].copy()\n        usual_comps = usual_comps[~usual_comps.p_nom_max.isin([np.inf, np.nan])]\n\n        paid_off_wlimits = paidoff_comp.loc[usual_comps.index + \"_paid_off\"]\n        to_constrain = pd.concat([usual_comps, paid_off_wlimits], axis=0)\n        if to_constrain.empty:\n            continue\n        to_constrain.rename_axis(index=f\"{component}-ext\", inplace=True)\n        # otherwise n.model query will fail. This is needed in case freeze_compoents was used\n        # it is fine so long as p_nom is zero for the frozen components\n        to_constrain = to_constrain.query(\"p_nom_extendable==True\")\n        to_constrain[\"grouper\"] = to_constrain.index.str.replace(\"_paid_off\", \"\")\n\n        grouper = xr.DataArray(to_constrain.grouper, dims=[f\"{component}-ext\"])\n\n        lhs = n.model[f\"{component}-{prefix}_nom\"].loc[to_constrain.index].groupby(grouper).sum()\n        # RHS\n        idx = lhs.indexes[\"grouper\"]\n\n        if not lhs.empty:\n            n.model.add_constraints(\n                lhs &lt;= usual_comps.loc[idx].p_nom_max.values,\n                name=f\"constrain_paidoff&amp;usual_{component}_potential\",\n            )\n</code></pre>"},{"location":"reference/solve_network/#solve_network.add_transmission_constraints","title":"<code>add_transmission_constraints(n)</code>","text":"<p>Add constraint ensuring that transmission lines p_nom are the same for both directions, i.e. p_nom positive = p_nom negative</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>Network</code> <p>the network object to optimize</p> required Source code in <code>workflow/scripts/solve_network.py</code> <pre><code>def add_transmission_constraints(n: pypsa.Network):\n    \"\"\"\n    Add constraint ensuring that transmission lines p_nom are the same for both directions, i.e.\n    p_nom positive = p_nom negative\n\n    Args:\n        n (pypsa.Network): the network object to optimize\n    \"\"\"\n\n    if not n.links.p_nom_extendable.any():\n        return\n\n    positive_bool = n.links.index.str.contains(\"positive\")\n    negative_bool = n.links.index.str.contains(\"reversed\")\n\n    positive_ext = n.links[positive_bool].query(\"p_nom_extendable\").index\n    negative_ext = n.links[negative_bool].query(\"p_nom_extendable\").index\n\n    lhs = n.model[\"Link-p_nom\"].loc[positive_ext]\n    rhs = n.model[\"Link-p_nom\"].loc[negative_ext]\n\n    n.model.add_constraints(lhs == rhs, name=\"Link-transmission\")\n</code></pre>"},{"location":"reference/solve_network/#solve_network.extra_functionality","title":"<code>extra_functionality(n, _)</code>","text":"<p>Add supplementary constraints to the network model. <code>pypsa.linopf.network_lopf</code>. If you want to enforce additional custom constraints, this is a good location to add them. The arguments <code>opts</code> and <code>snakemake.config</code> are expected to be attached to the network.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>Network</code> <p>the network object to optimize</p> required <code>_</code> <p>dummy for compatibility with pypsa solve</p> required Source code in <code>workflow/scripts/solve_network.py</code> <pre><code>def extra_functionality(n: pypsa.Network, _) -&gt; None:\n    \"\"\"\n    Add supplementary constraints to the network model. ``pypsa.linopf.network_lopf``.\n    If you want to enforce additional custom constraints, this is a good location to add them.\n    The arguments ``opts`` and ``snakemake.config`` are expected to be attached to the network.\n\n    Args:\n        n (pypsa.Network): the network object to optimize\n        _: dummy for compatibility with pypsa solve\n    \"\"\"\n    config = n.config\n    add_battery_constraints(n)\n    add_transmission_constraints(n)\n    add_chp_constraints(n)\n    if config[\"run\"].get(\"is_remind_coupled\", False):\n        logger.info(\"Adding remind paid off constraints\")\n        add_remind_paid_off_constraints(n)\n\n    reserve = config.get(\"operational_reserve\", {})\n    if reserve.get(\"activate\", False):\n        logger.info(\"Adding operational reserve margin constraints\")\n        add_operational_reserve_margin(n, config)\n\n    logger.info(\"Added extra functionality to the network model\")\n</code></pre>"},{"location":"reference/solve_network/#solve_network.freeze_components","title":"<code>freeze_components(n, config, exclude=['H2 turbine'])</code>","text":"<p>Set p_nom_extendable=False for the components in the network. Applies to vre_techs and conventional technologies not in the exclude list.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>Network</code> <p>the network object</p> required <code>config</code> <code>dict</code> <p>the configuration dictionary</p> required <code>exclude</code> <code>list</code> <p>list of technologies to exclude from freezing. Defaults to [\"OCGT\"]</p> <code>['H2 turbine']</code> Source code in <code>workflow/scripts/solve_network.py</code> <pre><code>def freeze_components(n: pypsa.Network, config: dict, exclude: list = [\"H2 turbine\"]):\n    \"\"\"Set p_nom_extendable=False for the components in the network.\n    Applies to vre_techs and conventional technologies not in the exclude list.\n\n    Args:\n        n (pypsa.Network): the network object\n        config (dict): the configuration dictionary\n        exclude (list, optional): list of technologies to exclude from freezing.\n            Defaults to [\"OCGT\"]\n    \"\"\"\n\n    # Freeze VRE and conventional techs\n    freeze = config[\"Techs\"][\"vre_techs\"] + config[\"Techs\"][\"conv_techs\"]\n    freeze = [f for f in freeze if f not in exclude]\n    if \"coal boiler\" in freeze:\n        freeze += [\"coal boiler central\", \"coal boiler decentral\"]\n    if \"gas boiler\" in freeze:\n        freeze += [\"gas boiler central\", \"gas boiler decentral\"]\n\n    # very ugly -&gt; how to make more robust?\n    to_fix = {\n        \"OCGT\": \"gas OCGT\",\n        \"CCGT\": \"gas CCGT\",\n        \"CCGT-CCS\": \"gas ccs\",\n        \"coal power plant\": \"coal\",\n        \"coal-CCS\": \"coal ccs\",\n    }\n    freeze += [to_fix[k] for k in to_fix if k in freeze]\n\n    for comp in [\"generators\", \"links\"]:\n        query = \"carrier in @freeze &amp; p_nom_extendable == True\"\n        components = getattr(n, comp)\n        # p_nom_max_rcl.isna(): exclude paid_off as needed\n        if \"p_nom_max_rcl\" in components.columns:\n            query += \" &amp; p_nom_max_rcl.isna()\"\n        mask = components.query(query).index\n        components.loc[mask, \"p_nom_extendable\"] = False\n</code></pre>"},{"location":"reference/solve_network/#solve_network.prepare_network","title":"<code>prepare_network(n, solve_opts, config, plan_year, co2_pathway)</code>","text":"<p>prepare the network for the solver, Args:     n (pypsa.Network): the network object to optimize     solve_opts (dict): solving options     config (dict): the snakemake configuration dictionary     plan_year (int): planning horizon year for which network is solved     co2_pathway (str): the CO2 pathway name to use</p> <p>Returns:</p> Type Description <code>Network</code> <p>pypsa.Network: network object with additional constraints</p> Source code in <code>workflow/scripts/solve_network.py</code> <pre><code>def prepare_network(\n    n: pypsa.Network, solve_opts: dict, config: dict, plan_year: int, co2_pathway: str\n) -&gt; pypsa.Network:\n    \"\"\"prepare the network for the solver,\n    Args:\n        n (pypsa.Network): the network object to optimize\n        solve_opts (dict): solving options\n        config (dict): the snakemake configuration dictionary\n        plan_year (int): planning horizon year for which network is solved\n        co2_pathway (str): the CO2 pathway name to use\n\n    Returns:\n        pypsa.Network: network object with additional constraints\n    \"\"\"\n\n    co2_opts = ConfigManager(config).fetch_co2_restriction(co2_pathway, int(plan_year))\n    add_co2_constraints_prices(n, co2_opts)\n\n    if \"clip_p_max_pu\" in solve_opts:\n        for df in (n.generators_t.p_max_pu, n.storage_units_t.inflow):\n            df.where(df &gt; solve_opts[\"clip_p_max_pu\"], other=0.0, inplace=True)\n\n    # TODO duplicated with freeze components\n    if solve_opts.get(\"load_shedding\"):\n        n.add(\"Carrier\", \"Load Shedding\")\n        buses_i = n.buses.query(\"carrier == 'AC'\").index\n        n.add(\n            \"Generator\",\n            buses_i,\n            \" load\",\n            bus=buses_i,\n            carrier=\"Load Shedding\",\n            marginal_cost=solve_opts.get(\"voll\", 1e5),  # EUR/MWh\n            # intersect between macroeconomic and surveybased\n            # willingness to pay\n            # http://journal.frontiersin.org/article/10.3389/fenrg.2015.00055/full\n            p_nom=1e6,  # MW\n        )\n\n    if solve_opts.get(\"noisy_costs\"):\n        for t in n.iterate_components(n.one_port_components):\n            # if 'capital_cost' in t.df:\n            #    t.df['capital_cost'] += 1e1 + 2.*(np.random.random(len(t.df)) - 0.5)\n            if \"marginal_cost\" in t.df:\n                t.df[\"marginal_cost\"] += 1e-2 + 2e-3 * (np.random.random(len(t.df)) - 0.5)\n\n        for t in n.iterate_components([\"Line\", \"Link\"]):\n            t.df[\"capital_cost\"] += (1e-1 + 2e-2 * (np.random.random(len(t.df)) - 0.5)) * t.df[\n                \"length\"\n            ]\n\n    if config[\"run\"].get(\"is_remind_coupled\", False) &amp; (\n        config[\"existing_capacities\"].get(\"freeze_new\", False)\n    ):\n        freeze_components(\n            n,\n            config,\n            exclude=config[\"existing_capacities\"].get(\"never_freeze\", []),\n        )\n\n    if solve_opts.get(\"nhours\"):\n        nhours = solve_opts[\"nhours\"]\n        n.set_snapshots(n.snapshots[:nhours])\n        n.snapshot_weightings[:] = YEAR_HRS / nhours\n\n    if config[\"existing_capacities\"].get(\"add\", False):\n        add_land_use_constraint(n, plan_year)\n\n    return n\n</code></pre>"},{"location":"reference/solve_network/#solve_network.set_transmission_limit","title":"<code>set_transmission_limit(n, kind, factor, n_years=1)</code>","text":"<p>Set global transimission limit constraints - adapted from pypsa-eur</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>Network</code> <p>the network object</p> required <code>kind</code> <code>str</code> <p>the kind of limit to set, either 'c' for cost or 'v' for volume or l for length</p> required <code>factor</code> <code>float or str</code> <p>the factor to apply to the base year quantity, per year</p> required <code>n_years</code> <code>int</code> <p>the number of years to consider for the limit. Defaults to 1.</p> <code>1</code> Source code in <code>workflow/scripts/solve_network.py</code> <pre><code>def set_transmission_limit(n: pypsa.Network, kind: str, factor: float, n_years=1):\n    \"\"\"\n    Set global transimission limit constraints - adapted from pypsa-eur\n\n    Args:\n        n (pypsa.Network): the network object\n        kind (str): the kind of limit to set, either 'c' for cost or 'v' for volume or l for length\n        factor (float or str): the factor to apply to the base year quantity, per year\n        n_years (int, optional): the number of years to consider for the limit. Defaults to 1.\n    \"\"\"\n    logger.info(\n        f\"Adding global transmission limit for {kind} with factor {factor}/year &amp; {n_years} years\"\n    )\n    links_dc = n.links.query(\"carrier in ['AC','DC']\").index\n    # links_dc_rev = n.links.query(\"carrier in ['AC','DC'] &amp; Link.str.contains('reverse')\").index\n\n    _lines_s_nom = (\n        np.sqrt(3)\n        * n.lines.type.map(n.line_types.i_nom)\n        * n.lines.num_parallel\n        * n.lines.bus0.map(n.buses.v_nom)\n    )\n    lines_s_nom = n.lines.s_nom.where(n.lines.type == \"\", _lines_s_nom)\n\n    col = \"capital_cost\" if kind == \"c\" else \"length\"\n    ref = lines_s_nom @ n.lines[col] + n.links.loc[links_dc, \"p_nom\"] @ n.links.loc[links_dc, col]\n\n    if factor == \"opt\" or float(factor) ** n_years &gt; 1.0:\n        n.lines[\"s_nom_min\"] = lines_s_nom\n        n.lines[\"s_nom_extendable\"] = True\n\n        n.links.loc[links_dc, \"p_nom_extendable\"] = True\n\n    elif float(factor) ** n_years == 1.0:\n        # if factor is 1.0, then we do not need to extend\n        n.lines[\"s_nom_min\"] = lines_s_nom\n        n.lines[\"s_nom_extendable\"] = False\n        n.links.loc[links_dc, \"p_nom_extendable\"] = False\n\n        # factor = 1 + 1e-7  # to avoid numerical issues with the constraints\n\n    elif float(factor) ** n_years &lt; 1.0:\n        n.lines[\"s_nom_min\"] = 0\n        n.links.loc[links_dc, \"p_nom_min\"] = 0\n        # n.links.loc[links_dc_rev, \"p_nom_min\"] = 0\n\n    if factor != \"opt\":\n        con_type = \"expansion_cost\" if kind == \"c\" else \"volume_expansion\"\n        rhs = float(factor) ** n_years * ref\n        logger.info(\n            f\"Adding global transmission limit for {kind} to {float(factor)**n_years} current value\"\n        )\n        n.add(\n            \"GlobalConstraint\",\n            f\"l{kind}_limit\",\n            type=f\"transmission_{con_type}_limit\",\n            sense=\"&lt;=\",\n            constant=rhs,\n            carrier_attribute=\"AC, DC\",\n        )\n</code></pre>"},{"location":"reference/solve_network/#solve_network.solve_network","title":"<code>solve_network(n, config, solving, opts='', **kwargs)</code>","text":"<p>perform the optimisation Args:     n (pypsa.Network): the pypsa network object     config (dict): the configuration dictionary     solving (dict): the solving configuration dictionary     opts (str): optional wildcards such as ll (not used in pypsa-china)</p> <p>Returns:</p> Type Description <code>Network</code> <p>pypsa.Network: the optimized network</p> Source code in <code>workflow/scripts/solve_network.py</code> <pre><code>def solve_network(\n    n: pypsa.Network, config: dict, solving: dict, opts: str = \"\", **kwargs\n) -&gt; pypsa.Network:\n    \"\"\"perform the optimisation\n    Args:\n        n (pypsa.Network): the pypsa network object\n        config (dict): the configuration dictionary\n        solving (dict): the solving configuration dictionary\n        opts (str): optional wildcards such as ll (not used in pypsa-china)\n\n    Returns:\n        pypsa.Network: the optimized network\n    \"\"\"\n    set_of_options = solving[\"solver\"][\"options\"]\n    solver_options = solving[\"solver_options\"][set_of_options] if set_of_options else {}\n    solver_name = solving[\"solver\"][\"name\"]\n    cf_solving = solving[\"options\"]\n    track_iterations = cf_solving.get(\"track_iterations\", False)\n    min_iterations = cf_solving.get(\"min_iterations\", 4)\n    max_iterations = cf_solving.get(\"max_iterations\", 6)\n    transmission_losses = cf_solving.get(\"transmission_losses\", 0)\n\n    # add to network for extra_functionality\n    n.config = config\n    n.opts = opts\n\n    skip_iterations = cf_solving.get(\"skip_iterations\", False)\n    if not n.lines.s_nom_extendable.any():\n        skip_iterations = True\n        logger.info(\"No expandable lines found. Skipping iterative solving.\")\n\n    if skip_iterations:\n        status, condition = n.optimize(\n            solver_name=solver_name,\n            transmission_losses=transmission_losses,\n            extra_functionality=extra_functionality,\n            **solver_options,\n            **kwargs,\n        )\n    else:\n        status, condition = n.optimize.optimize_transmission_expansion_iteratively(\n            solver_name=solver_name,\n            track_iterations=track_iterations,\n            min_iterations=min_iterations,\n            max_iterations=max_iterations,\n            transmission_losses=transmission_losses,\n            extra_functionality=extra_functionality,\n            **solver_options,\n            **kwargs,\n        )\n\n    if status != \"ok\":\n        logger.warning(f\"Solving status '{status}' with termination condition '{condition}'\")\n    if \"infeasible\" in condition:\n        raise RuntimeError(\"Solving status 'infeasible'\")\n\n    return n\n</code></pre>"},{"location":"reference/solve_network_myopic/","title":"Solve network myopic","text":"<p>Functions to add constraints and prepare the network for the solver. Associated with the <code>solve_network_myopic</code> rule in the Snakefile. To be merged/consolidated with the <code>solve_network</code> script.</p>"},{"location":"reference/solve_network_myopic/#solve_network_myopic.add_battery_constraints","title":"<code>add_battery_constraints(n)</code>","text":"<p>Add constraint ensuring that charger = discharger, i.e. 1 * charger_size - efficiency * discharger_size = 0</p> Source code in <code>workflow/scripts/solve_network_myopic.py</code> <pre><code>def add_battery_constraints(n):\n    \"\"\"\n    Add constraint ensuring that charger = discharger, i.e.\n    1 * charger_size - efficiency * discharger_size = 0\n    \"\"\"\n    if not n.links.p_nom_extendable.any():\n        return\n\n    discharger_bool = n.links.index.str.contains(\"battery discharger\")\n    charger_bool = n.links.index.str.contains(\"battery charger\")\n\n    dischargers_ext = n.links[discharger_bool].query(\"p_nom_extendable\").index\n    chargers_ext = n.links[charger_bool].query(\"p_nom_extendable\").index\n\n    eff = n.links.efficiency[dischargers_ext].values\n    lhs = n.model[\"Link-p_nom\"].loc[chargers_ext] - n.model[\"Link-p_nom\"].loc[dischargers_ext] * eff\n\n    n.model.add_constraints(lhs == 0, name=\"Link-charger_ratio\")\n</code></pre>"},{"location":"reference/solve_network_myopic/#solve_network_myopic.add_chp_constraints","title":"<code>add_chp_constraints(n)</code>","text":"<p>Add constraints to couple the heat and electricity output of CHP plants      (using the cb and cv parameter). See the DEA technology cataloge</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>Network</code> <p>the pypsa network object to which's model the constraints are added</p> required Source code in <code>workflow/scripts/solve_network_myopic.py</code> <pre><code>def add_chp_constraints(n):\n    \"\"\"Add constraints to couple the heat and electricity output of CHP plants\n         (using the cb and cv parameter). See the DEA technology cataloge\n\n    Args:\n        n (pypsa.Network): the pypsa network object to which's model the constraints are added\n    \"\"\"\n    electric = n.links.index.str.contains(\"CHP\") &amp; n.links.index.str.contains(\"generator\")\n    heat = n.links.index.str.contains(\"CHP\") &amp; n.links.index.str.contains(\"boiler\")\n\n    electric_ext = n.links[electric].query(\"p_nom_extendable\").index\n    heat_ext = n.links[heat].query(\"p_nom_extendable\").index\n\n    electric_fix = n.links[electric].query(\"~p_nom_extendable\").index\n    heat_fix = n.links[heat].query(\"~p_nom_extendable\").index\n\n    p = n.model[\"Link-p\"]  # dimension: [time, link]\n\n    # output ratio between heat and electricity and top_iso_fuel_line for extendable\n    if not electric_ext.empty:\n        p_nom = n.model[\"Link-p_nom\"]\n\n        lhs = (\n            p_nom.loc[electric_ext]\n            * (n.links.p_nom_ratio * n.links.efficiency)[electric_ext].values\n            - p_nom.loc[heat_ext] * n.links.efficiency[heat_ext].values\n        )\n        n.model.add_constraints(lhs == 0, name=\"chplink-fix_p_nom_ratio\")\n\n        rename = {\"Link-ext\": \"Link\"}\n        lhs = p.loc[:, electric_ext] + p.loc[:, heat_ext] - p_nom.rename(rename).loc[electric_ext]\n        n.model.add_constraints(lhs &lt;= 0, name=\"chplink-top_iso_fuel_line_ext\")\n\n    # top_iso_fuel_line for fixed\n    if not electric_fix.empty:\n        lhs = p.loc[:, electric_fix] + p.loc[:, heat_fix]\n        rhs = n.links.p_nom[electric_fix]\n        n.model.add_constraints(lhs &lt;= rhs, name=\"chplink-top_iso_fuel_line_fix\")\n\n    # back-pressure\n    if not n.links[electric].index.empty:\n        lhs = (\n            p.loc[:, heat] * (n.links.efficiency[heat] * n.links.c_b[electric].values)\n            - p.loc[:, electric] * n.links.efficiency[electric]\n        )\n        n.model.add_constraints(lhs &lt;= 0, name=\"chplink-backpressure\")\n</code></pre>"},{"location":"reference/solve_network_myopic/#solve_network_myopic.add_land_use_constraint","title":"<code>add_land_use_constraint(n, planning_horizons)</code>","text":"<p>Add land use constraints for renewable energy potential. This ensures that the brownfield + greenfield  vre installations for each generator technology do not exceed the technical potential.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>Network</code> <p>the network object to add the constraints to</p> required <code>planning_horizons</code> <code>str | int</code> <p>the planning horizon year as string</p> required Source code in <code>workflow/scripts/solve_network_myopic.py</code> <pre><code>def add_land_use_constraint(n: pypsa.Network, planning_horizons: str | int) -&gt; None:\n    \"\"\"\n    Add land use constraints for renewable energy potential. This ensures that the brownfield + greenfield\n     vre installations for each generator technology do not exceed the technical potential.\n\n    Args:\n        n (pypsa.Network): the network object to add the constraints to\n        planning_horizons (str | int): the planning horizon year as string\n    \"\"\"\n    # warning: this will miss existing offwind which is not classed AC-DC and has carrier 'offwind'\n\n    for carrier in [\n        \"solar\",\n        \"solar rooftop\",\n        \"solar-hsat\",\n        \"onwind\",\n        \"offwind\",\n        \"offwind-ac\",\n        \"offwind-dc\",\n        \"offwind-float\",\n    ]:\n        ext_i = (n.generators.carrier == carrier) &amp; ~n.generators.p_nom_extendable\n        grouper = n.generators.loc[ext_i].index.str.replace(f\" {carrier}.*$\", \"\", regex=True)\n        existing = n.generators.loc[ext_i, \"p_nom\"].groupby(grouper).sum()\n        existing.index += f\" {carrier}-{planning_horizons}\"\n        n.generators.loc[existing.index, \"p_nom_max\"] -= existing\n\n    # check if existing capacities are larger than technical potential\n    existing_large = n.generators[n.generators[\"p_nom_min\"] &gt; n.generators[\"p_nom_max\"]].index\n    if len(existing_large):\n        logger.warning(\n            f\"Existing capacities larger than technical potential for {existing_large},\\\n                        adjust technical potential to existing capacities\"\n        )\n        n.generators.loc[existing_large, \"p_nom_max\"] = n.generators.loc[\n            existing_large, \"p_nom_min\"\n        ]\n\n    n.generators[\"p_nom_max\"] = n.generators[\"p_nom_max\"].clip(lower=0)\n</code></pre>"},{"location":"reference/solve_network_myopic/#solve_network_myopic.add_retrofit_constraints","title":"<code>add_retrofit_constraints(n)</code>","text":"<p>Add constraints to ensure retrofit capacity is linked to the original capacity Args:     n (pypsa.Network): the pypsa network object to which's model the constraints are added</p> Source code in <code>workflow/scripts/solve_network_myopic.py</code> <pre><code>def add_retrofit_constraints(n):\n    \"\"\"\n    Add constraints to ensure retrofit capacity is linked to the original capacity\n    Args:\n        n (pypsa.Network): the pypsa network object to which's model the constraints are added\n    \"\"\"\n    p_nom_max = pd.read_csv(\"resources/data/p_nom/p_nom_max_cc.csv\", index_col=0)\n    p_nom_max = pd.read_csv(\"resources/data/p_nom/p_nom_max_cc.csv\", index_col=0)\n    planning_horizon = snakemake.wildcards.planning_horizons\n    for year in range(int(planning_horizon) - 40, 2021, 5):\n        coal = (\n            n.generators[\n                (n.generators.carrier == \"coal power plant\") &amp; (n.generators.build_year == year)\n            ]\n            .query(\"p_nom_extendable\")\n            .index\n        )\n        Bus = (\n            n.generators[\n                (n.generators.carrier == \"coal power plant\") &amp; (n.generators.build_year == year)\n            ]\n            .query(\"p_nom_extendable\")\n            .bus.values\n        )\n        coal_retrofit = (\n            n.generators[\n                n.generators.index.str.contains(\"retrofit\")\n                &amp; (n.generators.build_year == year)\n                &amp; n.generators.bus.isin(Bus)\n            ]\n            .query(\"p_nom_extendable\")\n            .index\n        )\n        coal_retrofitted = (\n            n.generators[\n                n.generators.index.str.contains(\"retrofit\")\n                &amp; (n.generators.build_year == year)\n                &amp; n.generators.bus.isin(Bus)\n            ]\n            .query(\"~p_nom_extendable\")\n            .groupby(\"bus\")\n            .sum()\n            .p_nom_opt\n        )\n\n        lhs = (\n            n.model[\"Generator-p_nom\"].loc[coal]\n            + n.model[\"Generator-p_nom\"].loc[coal_retrofit]\n            - (\n                p_nom_max[str(year)].loc[Bus]\n                - coal_retrofitted.reindex(p_nom_max[str(year)].loc[Bus].index, fill_value=0)\n            ).values\n        )\n\n        n.model.add_constraints(lhs == 0, name=\"Generator-coal-retrofit-\" + str(year))\n</code></pre>"},{"location":"reference/solve_network_myopic/#solve_network_myopic.add_transmission_constraints","title":"<code>add_transmission_constraints(n)</code>","text":"<p>Add constraint ensuring that transmission lines p_nom are the same for both directions, i.e. p_nom positive = p_nom negative</p> Source code in <code>workflow/scripts/solve_network_myopic.py</code> <pre><code>def add_transmission_constraints(n):\n    \"\"\"\n    Add constraint ensuring that transmission lines p_nom are the same for both directions, i.e.\n    p_nom positive = p_nom negative\n    \"\"\"\n    if not n.links.p_nom_extendable.any():\n        return\n\n    positive_bool = n.links.index.str.contains(\"positive\")\n    negative_bool = n.links.index.str.contains(\"reversed\")\n\n    positive_ext = n.links[positive_bool].query(\"p_nom_extendable\").index\n    negative_ext = n.links[negative_bool].query(\"p_nom_extendable\").index\n\n    lhs = n.model[\"Link-p_nom\"].loc[positive_ext] - n.model[\"Link-p_nom\"].loc[negative_ext]\n\n    n.model.add_constraints(lhs == 0, name=\"Link-transimission\")\n</code></pre>"},{"location":"reference/solve_network_myopic/#solve_network_myopic.extra_functionality","title":"<code>extra_functionality(n, snapshots)</code>","text":"<p>Collects supplementary constraints which will be passed to <code>pypsa.linopf.network_lopf</code>. If you want to enforce additional custom constraints, this is a good location to add them. The arguments <code>opts</code> and <code>snakemake.config</code> are expected to be attached to the network.</p> Source code in <code>workflow/scripts/solve_network_myopic.py</code> <pre><code>def extra_functionality(n, snapshots):\n    \"\"\"\n    Collects supplementary constraints which will be passed to ``pypsa.linopf.network_lopf``.\n    If you want to enforce additional custom constraints, this is a good location to add them.\n    The arguments ``opts`` and ``snakemake.config`` are expected to be attached to the network.\n    \"\"\"\n\n    add_chp_constraints(n)\n    add_battery_constraints(n)\n    add_transmission_constraints(n)\n    if snakemake.wildcards.planning_horizons != \"2020\":\n        add_retrofit_constraints(n)\n</code></pre>"},{"location":"reference/solve_network_myopic/#solve_network_myopic.prepare_network","title":"<code>prepare_network(n, config, solve_opts=None)</code>","text":"<p>prepare the network for the solver Args:     n (pypsa.Network): the pypsa network object     solve_opts (dict): the solving options</p> Source code in <code>workflow/scripts/solve_network_myopic.py</code> <pre><code>def prepare_network(\n    n,\n    config: dict,\n    solve_opts=None,\n):\n    \"\"\"prepare the network for the solver\n    Args:\n        n (pypsa.Network): the pypsa network object\n        solve_opts (dict): the solving options\n    \"\"\"\n    if \"clip_p_max_pu\" in solve_opts:\n        for df in (\n            n.generators_t.p_max_pu,\n            n.generators_t.p_min_pu,\n            n.storage_units_t.inflow,\n        ):\n            df.where(df &gt; solve_opts[\"clip_p_max_pu\"], other=0.0, inplace=True)\n\n    load_shedding = solve_opts.get(\"load_shedding\")\n    if load_shedding:\n        # intersect between macroeconomic and surveybased willingness to pay\n        # http://journal.frontiersin.org/article/10.3389/fenrg.2015.00055/full\n        # TODO: retrieve color and nice name from config\n        n.add(\"Carrier\", \"load\", color=\"#dd2e23\", nice_name=\"Load shedding\")\n        buses_i = n.buses.query(\"carrier == 'AC'\").index\n        if not np.isscalar(load_shedding):\n            # TODO: do not scale via sign attribute (use Eur/MWh instead of Eur/kWh)\n            load_shedding = 1e2  # Eur/kWh\n\n        n.madd(\n            \"Generator\",\n            buses_i,\n            \" load\",\n            bus=buses_i,\n            carrier=\"load\",\n            sign=1e-3,  # Adjust sign to measure p and p_nom in kW instead of MW\n            marginal_cost=load_shedding,  # Eur/kWh\n            p_nom=1e9,  # kW\n        )\n\n    if solve_opts.get(\"noisy_costs\"):\n        for t in n.iterate_components():\n            # if 'capital_cost' in t.df:\n            #    t.df['capital_cost'] += 1e1 + 2.*(np.random.random(len(t.df)) - 0.5)\n            if \"marginal_cost\" in t.df:\n                t.df[\"marginal_cost\"] += 1e-2 + 2e-3 * (np.random.random(len(t.df)) - 0.5)\n\n        for t in n.iterate_components([\"Line\", \"Link\"]):\n            t.df[\"capital_cost\"] += (1e-1 + 2e-2 * (np.random.random(len(t.df)) - 0.5)) * t.df[\n                \"length\"\n            ]\n\n    if solve_opts.get(\"nhours\"):\n        nhours = solve_opts[\"nhours\"]\n        n.set_snapshots(n.snapshots[:nhours])\n        n.snapshot_weightings[:] = 8760.0 / nhours\n\n    if config[\"existing_capacities\"].get(\"add\", True):\n        add_land_use_constraint(n)\n\n    return n\n</code></pre>"},{"location":"reference/solve_network_myopic/#solve_network_myopic.solve_network","title":"<code>solve_network(n, config, solving, opts='', **kwargs)</code>","text":"<p>perform the optimisation Args:     n (pypsa.Network): the pypsa network object     config (dict): the configuration dictionary     solving (dict): the solving configuration dictionary     opts (str): optional wildcards such as ll (not used in pypsa-china)</p> Source code in <code>workflow/scripts/solve_network_myopic.py</code> <pre><code>def solve_network(n: pypsa.Network, config: dict, solving, opts=\"\", **kwargs) -&gt; pypsa.Network:\n    \"\"\"perform the optimisation\n    Args:\n        n (pypsa.Network): the pypsa network object\n        config (dict): the configuration dictionary\n        solving (dict): the solving configuration dictionary\n        opts (str): optional wildcards such as ll (not used in pypsa-china)\n    \"\"\"\n    set_of_options = solving[\"solver\"][\"options\"]\n    solver_options = solving[\"solver_options\"][set_of_options] if set_of_options else {}\n    solver_name = solving[\"solver\"][\"name\"]\n    cf_solving = solving[\"options\"]\n    track_iterations = cf_solving.get(\"track_iterations\", False)\n    min_iterations = cf_solving.get(\"min_iterations\", 4)\n    max_iterations = cf_solving.get(\"max_iterations\", 6)\n    transmission_losses = cf_solving.get(\"transmission_losses\", 0)\n\n    # add to network for extra_functionality\n    n.config = config\n    n.opts = opts\n\n    skip_iterations = cf_solving.get(\"skip_iterations\", False)\n    if not n.lines.s_nom_extendable.any():\n        skip_iterations = True\n        logger.info(\"No expandable lines found. Skipping iterative solving.\")\n\n    if skip_iterations:\n        status, condition = n.optimize(\n            solver_name=solver_name,\n            transmission_losses=transmission_losses,\n            extra_functionality=extra_functionality,\n            **solver_options,\n            **kwargs,\n        )\n    else:\n        status, condition = n.optimize.optimize_transmission_expansion_iteratively(\n            solver_name=solver_name,\n            track_iterations=track_iterations,\n            min_iterations=min_iterations,\n            max_iterations=max_iterations,\n            transmission_losses=transmission_losses,\n            extra_functionality=extra_functionality,\n            **solver_options,\n            **kwargs,\n        )\n\n    if status != \"ok\":\n        logger.warning(f\"Solving status '{status}' with termination condition '{condition}'\")\n    if \"infeasible\" in condition:\n        raise RuntimeError(\"Solving status 'infeasible'\")\n\n    return n\n</code></pre>"},{"location":"reference/remind_coupling/disaggregate_data/","title":"Disaggregate data","text":"<p>generic disaggregation development Split steps into:</p> <ul> <li>ETL</li> <li>disagg (also an ETL op)</li> </ul> <p>to be rebalanced with the remind_coupling package</p>"},{"location":"reference/remind_coupling/disaggregate_data/#remind_coupling.disaggregate_data.add_possible_techs_to_paidoff","title":"<code>add_possible_techs_to_paidoff(paidoff, tech_groups)</code>","text":"<p>Add possible PyPSA technologies to the paid off capacities DataFrame. The paidoff capacities are grouped in case the Remind-PyPSA tecg mapping is not 1:1 but the network needs to add PyPSA techs. A constraint is added so the paid off caps per group are not exceeded.</p> <p>Parameters:</p> Name Type Description Default <code>paidoff</code> <code>DataFrame</code> <p>DataFrame with paid off capacities</p> required <p>Returns:     pd.DataFrame: paid off techs with list of PyPSA technologies Example:     &gt;&gt; tech_groups         PyPSA_tech, group         coal CHP, coal         coal, coal     &gt;&gt; add_possible_techs_to_paidoff(paidoff, tech_groups)     &gt;&gt; paidoff         tech_group, paid_off_capacity, techs         coal, 1000, ['coal CHP', 'coal']</p> Source code in <code>workflow/scripts/remind_coupling/disaggregate_data.py</code> <pre><code>def add_possible_techs_to_paidoff(paidoff: pd.DataFrame, tech_groups: pd.Series) -&gt; pd.DataFrame:\n    \"\"\"Add possible PyPSA technologies to the paid off capacities DataFrame.\n    The paidoff capacities are grouped in case the Remind-PyPSA tecg mapping is not 1:1\n    but the network needs to add PyPSA techs.\n    A constraint is added so the paid off caps per group are not exceeded.\n\n    Args:\n        paidoff (pd.DataFrame): DataFrame with paid off capacities\n    Returns:\n        pd.DataFrame: paid off techs with list of PyPSA technologies\n    Example:\n        &gt;&gt; tech_groups\n            PyPSA_tech, group\n            coal CHP, coal\n            coal, coal\n        &gt;&gt; add_possible_techs_to_paidoff(paidoff, tech_groups)\n        &gt;&gt; paidoff\n            tech_group, paid_off_capacity, techs\n            coal, 1000, ['coal CHP', 'coal']\n    \"\"\"\n    df = tech_groups.reset_index()\n    possibilities = df.groupby(\"group\").PyPSA_tech.apply(lambda x: list(x.unique()))\n    paidoff[\"techs\"] = paidoff.tech_group.map(possibilities)\n    return paidoff\n</code></pre>"},{"location":"reference/remind_coupling/disaggregate_data/#remind_coupling.disaggregate_data.disagg_ac_using_ref","title":"<code>disagg_ac_using_ref(data, reference_data, reference_year)</code>","text":"<p>Spatially Disaggregate the load using regional/nodal reference data     (e.g. the projections from Hu2013 as in the Zhou et al PyPSA-China version)</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>DataFrame containing the load data</p> required <code>reference_data</code> <code>DataFrame</code> <p>DataFrame containing the reference data</p> required <code>reference_year</code> <code>int | str</code> <p>Year to use for disaggregation</p> required <p>Returns:     pd.DataFrame: Disaggregated load data (Region x Year)</p> Source code in <code>workflow/scripts/remind_coupling/disaggregate_data.py</code> <pre><code>@register_etl(\"disagg_acload_ref\")\ndef disagg_ac_using_ref(\n    data: pd.DataFrame,\n    reference_data: pd.DataFrame,\n    reference_year: int | str,\n) -&gt; pd.DataFrame:\n    \"\"\"Spatially Disaggregate the load using regional/nodal reference data\n        (e.g. the projections from Hu2013 as in the Zhou et al PyPSA-China version)\n\n    Args:\n        data (pd.DataFrame): DataFrame containing the load data\n        reference_data (pd.DataFrame): DataFrame containing the reference data\n        reference_year (int | str): Year to use for disaggregation\n    Returns:\n        pd.DataFrame: Disaggregated load data (Region x Year)\n    \"\"\"\n\n    regional_reference = reference_data[int(reference_year)]\n    regional_reference /= regional_reference.sum()\n    electricity_demand = data[\"loads\"].query(\"load == 'ac'\")\n    electricity_demand.set_index(\"year\", inplace=True)\n    logger.info(\"Disaggregating load according to Hu et al. demand projections\")\n    disagg_load = SpatialDisaggregator().use_static_reference(\n        electricity_demand.value, regional_reference\n    )\n\n    return disagg_load\n</code></pre>"},{"location":"reference/remind_coupling/generic_etl/","title":"Generic etl","text":"<p>generic etl development, to be rebalanced with the remind_coupling package</p> <p>The ETL operations are governed by the config file. Allowed fields are defined by the rpycpl.etl.Transformation class and are     name: str     method: Optional[str]     frames: Dict[str, Any]     params: Dict[str, Any]     filters: Dict[str, Any     kwargs: Dict[str, Any]     dependencies: Dict[str, Any]</p> <p>The sequence of operations matters: Dependencies represents previous step outputs.</p>"},{"location":"reference/remind_coupling/generic_etl/#remind_coupling.generic_etl.ETLRunner","title":"<code>ETLRunner</code>","text":"<p>Container class to execute ETL steps.</p> Source code in <code>workflow/scripts/remind_coupling/generic_etl.py</code> <pre><code>class ETLRunner:\n    \"\"\"Container class to execute ETL steps.\"\"\"\n\n    @staticmethod\n    def run(\n        step: Transformation,\n        frames: dict[str, pd.DataFrame],\n        previous_outputs: dict[str, Any] = None,\n        **kwargs,\n    ) -&gt; pd.DataFrame:\n        \"\"\"Run the ETL step using the provided frames and extra arguments.\n        Args:\n            step (Transformation): The ETL step to run.\n            frames (dict): Dictionary of loaded frames.\n            previous_outputs (dict, optional): Dictionary of outputs from previous\n                steps that can be used as inputs.\n            **kwargs: Additional arguments for the ETL method.\n        Returns:\n            pd.DataFrame: The result of the ETL step.\n        \"\"\"\n        method = step.name if not step.method else step.method\n        func = ETL_REGISTRY.get(method)\n        if not func:\n            raise ValueError(f\"ETL method '{method}' not found in registry.\")\n\n        # Handle dependencies on previous outputs if specified in the step\n        if hasattr(step, \"dependencies\") and step.dependencies and previous_outputs:\n            for output_key in step.dependencies:\n                if output_key in previous_outputs:\n                    # Add the dependency to frames with the specified key\n                    frames[output_key] = previous_outputs[output_key]\n                else:\n                    msg = f\"Dependency '{output_key}' not found in previous outputs\"\n                    msg += f\" for step '{step.name}'\"\n                    raise ValueError(msg)\n\n        kwargs.update(step.kwargs)\n        if kwargs:\n            return func(frames, **kwargs)\n        else:\n            return func(frames)\n</code></pre>"},{"location":"reference/remind_coupling/generic_etl/#remind_coupling.generic_etl.ETLRunner.run","title":"<code>run(step, frames, previous_outputs=None, **kwargs)</code>  <code>staticmethod</code>","text":"<p>Run the ETL step using the provided frames and extra arguments. Args:     step (Transformation): The ETL step to run.     frames (dict): Dictionary of loaded frames.     previous_outputs (dict, optional): Dictionary of outputs from previous         steps that can be used as inputs.     **kwargs: Additional arguments for the ETL method. Returns:     pd.DataFrame: The result of the ETL step.</p> Source code in <code>workflow/scripts/remind_coupling/generic_etl.py</code> <pre><code>@staticmethod\ndef run(\n    step: Transformation,\n    frames: dict[str, pd.DataFrame],\n    previous_outputs: dict[str, Any] = None,\n    **kwargs,\n) -&gt; pd.DataFrame:\n    \"\"\"Run the ETL step using the provided frames and extra arguments.\n    Args:\n        step (Transformation): The ETL step to run.\n        frames (dict): Dictionary of loaded frames.\n        previous_outputs (dict, optional): Dictionary of outputs from previous\n            steps that can be used as inputs.\n        **kwargs: Additional arguments for the ETL method.\n    Returns:\n        pd.DataFrame: The result of the ETL step.\n    \"\"\"\n    method = step.name if not step.method else step.method\n    func = ETL_REGISTRY.get(method)\n    if not func:\n        raise ValueError(f\"ETL method '{method}' not found in registry.\")\n\n    # Handle dependencies on previous outputs if specified in the step\n    if hasattr(step, \"dependencies\") and step.dependencies and previous_outputs:\n        for output_key in step.dependencies:\n            if output_key in previous_outputs:\n                # Add the dependency to frames with the specified key\n                frames[output_key] = previous_outputs[output_key]\n            else:\n                msg = f\"Dependency '{output_key}' not found in previous outputs\"\n                msg += f\" for step '{step.name}'\"\n                raise ValueError(msg)\n\n    kwargs.update(step.kwargs)\n    if kwargs:\n        return func(frames, **kwargs)\n    else:\n        return func(frames)\n</code></pre>"},{"location":"reference/remind_coupling/generic_etl/#remind_coupling.generic_etl.RemindLoader","title":"<code>RemindLoader</code>","text":"<p>Load Remind symbol tables from csvs or gdx</p> Source code in <code>workflow/scripts/remind_coupling/generic_etl.py</code> <pre><code>class RemindLoader:\n    \"\"\"Load Remind symbol tables from csvs or gdx\"\"\"\n\n    def __init__(self, remind_dir: PathLike, backend=\"csv\"):\n        self.remind_dir = remind_dir\n\n        supported_backends = [\"csv\", \"gdx\"]\n        if backend not in supported_backends:\n            raise ValueError(f\"Backend {backend} not supported. Available: {supported_backends}\")\n        self.backend = backend\n\n    def _group_split_frames(self, keys, pattern: str = r\"_part\\d+$\") -&gt; dict[str, list[str]]:\n        \"\"\"Chat gpt regex magic to group split frames\n        Args:\n            keys (list): list of keys\n            pattern (str, optional): regex pattern to group split frames by. Defaults to r\"_part\\\\d+$\".\"\n        Returns:\n            dict[str, list[str]]: dictionary with base name as key and list of keys as value\n        \"\"\"\n        grouped = {}\n        for k in keys:\n            base = re.sub(pattern, \"\", k)\n            grouped.setdefault(base, []).append(k)\n        return grouped\n\n    def load_frames_csv(self, frames: dict[str, str]) -&gt; dict[str, pd.DataFrame]:\n        \"\"\"Remind Frames to read\n        Args:\n            frames (dict): (param: remind_symbol_name) to read\n        Returns:\n            dict[str, pd.DataFrame]: dictionary (param: dataframe)\n        \"\"\"\n        paths = {k: os.path.join(self.remind_dir, v + \".csv\") for k, v in frames.items() if v}\n        return {k: read_remind_csv(v) for k, v in paths.items()}\n\n    def load_frames_gdx(\n        self, frames: dict[str, str], gdx_file: PathLike\n    ) -&gt; dict[str, pd.DataFrame]:\n\n        raise NotImplementedError(\"GDX loading not implemented yet\")\n\n    def merge_split_frames(self, frames: dict[str, pd.DataFrame]) -&gt; dict[str, pd.DataFrame]:\n        \"\"\"In case several REMIND parameters are needed, group them by their base name\n        Args:\n            frames (dict): Dictionary with all dataframes\n        Example:\n            frames = {eta: 'pm_dataeta', eta_part2: 'pm_eta_conv'}\n            merge_split_frames(frames)\n            &gt;&gt; {eta: pd.concat([pm_dataeta, pm_eta_conv], axis=0).drop_duplicates()}\n        \"\"\"\n\n        grouped = self._group_split_frames(frames)\n        unmerged = {k: v for k, v in grouped.items() if len(v) &gt; 1}\n        merged = {k: pd.concat([frames[v] for v in multi], axis=0) for k, multi in unmerged.items()}\n        merged = {k: v.drop_duplicates().reset_index(drop=True) for k, v in merged.items()}\n\n        to_drop = [item for sublist in unmerged.values() for item in sublist]\n        frames = {k: v for k, v in frames.items() if k not in to_drop}\n        frames.update(merged)\n        return frames\n\n    def auto_load(\n        self, frames: dict[str, str], filters: dict[str, str] = None\n    ) -&gt; dict[str, pd.DataFrame]:\n        \"\"\"Automatically load, merge, and filter frames in one step.\n\n        Args:\n            frames: Dictionary mapping parameter names to REMIND symbol names.\n            filters: Optional dictionary of filter expressions to apply to frames.\n\n        Returns:\n            Dictionary of processed DataFrames ready for transformation.\n        \"\"\"\n        # Load raw frames\n        if self.backend == \"gdx\":\n            raw_frames = self.load_frames_gdx(frames, os.path.join(self.remind_dir, \"gdx\"))\n        elif self.backend == \"csv\":\n            raw_frames = self.load_frames_csv(frames)\n\n        # Merge split frames\n        processed_frames = self.merge_split_frames(raw_frames)\n\n        # Apply filters if any\n        if filters:\n            for k, filter_expr in filters.items():\n                if k in processed_frames:\n                    processed_frames[k] = processed_frames[k].query(filter_expr)\n                else:\n                    logger.warning(f\"Filter specified for non-existent frame: {k}\")\n\n        return processed_frames\n</code></pre>"},{"location":"reference/remind_coupling/generic_etl/#remind_coupling.generic_etl.RemindLoader.auto_load","title":"<code>auto_load(frames, filters=None)</code>","text":"<p>Automatically load, merge, and filter frames in one step.</p> <p>Parameters:</p> Name Type Description Default <code>frames</code> <code>dict[str, str]</code> <p>Dictionary mapping parameter names to REMIND symbol names.</p> required <code>filters</code> <code>dict[str, str]</code> <p>Optional dictionary of filter expressions to apply to frames.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, DataFrame]</code> <p>Dictionary of processed DataFrames ready for transformation.</p> Source code in <code>workflow/scripts/remind_coupling/generic_etl.py</code> <pre><code>def auto_load(\n    self, frames: dict[str, str], filters: dict[str, str] = None\n) -&gt; dict[str, pd.DataFrame]:\n    \"\"\"Automatically load, merge, and filter frames in one step.\n\n    Args:\n        frames: Dictionary mapping parameter names to REMIND symbol names.\n        filters: Optional dictionary of filter expressions to apply to frames.\n\n    Returns:\n        Dictionary of processed DataFrames ready for transformation.\n    \"\"\"\n    # Load raw frames\n    if self.backend == \"gdx\":\n        raw_frames = self.load_frames_gdx(frames, os.path.join(self.remind_dir, \"gdx\"))\n    elif self.backend == \"csv\":\n        raw_frames = self.load_frames_csv(frames)\n\n    # Merge split frames\n    processed_frames = self.merge_split_frames(raw_frames)\n\n    # Apply filters if any\n    if filters:\n        for k, filter_expr in filters.items():\n            if k in processed_frames:\n                processed_frames[k] = processed_frames[k].query(filter_expr)\n            else:\n                logger.warning(f\"Filter specified for non-existent frame: {k}\")\n\n    return processed_frames\n</code></pre>"},{"location":"reference/remind_coupling/generic_etl/#remind_coupling.generic_etl.RemindLoader.load_frames_csv","title":"<code>load_frames_csv(frames)</code>","text":"<p>Remind Frames to read Args:     frames (dict): (param: remind_symbol_name) to read Returns:     dict[str, pd.DataFrame]: dictionary (param: dataframe)</p> Source code in <code>workflow/scripts/remind_coupling/generic_etl.py</code> <pre><code>def load_frames_csv(self, frames: dict[str, str]) -&gt; dict[str, pd.DataFrame]:\n    \"\"\"Remind Frames to read\n    Args:\n        frames (dict): (param: remind_symbol_name) to read\n    Returns:\n        dict[str, pd.DataFrame]: dictionary (param: dataframe)\n    \"\"\"\n    paths = {k: os.path.join(self.remind_dir, v + \".csv\") for k, v in frames.items() if v}\n    return {k: read_remind_csv(v) for k, v in paths.items()}\n</code></pre>"},{"location":"reference/remind_coupling/generic_etl/#remind_coupling.generic_etl.RemindLoader.merge_split_frames","title":"<code>merge_split_frames(frames)</code>","text":"<p>In case several REMIND parameters are needed, group them by their base name Args:     frames (dict): Dictionary with all dataframes Example:     frames = {eta: 'pm_dataeta', eta_part2: 'pm_eta_conv'}     merge_split_frames(frames)     &gt;&gt; {eta: pd.concat([pm_dataeta, pm_eta_conv], axis=0).drop_duplicates()}</p> Source code in <code>workflow/scripts/remind_coupling/generic_etl.py</code> <pre><code>def merge_split_frames(self, frames: dict[str, pd.DataFrame]) -&gt; dict[str, pd.DataFrame]:\n    \"\"\"In case several REMIND parameters are needed, group them by their base name\n    Args:\n        frames (dict): Dictionary with all dataframes\n    Example:\n        frames = {eta: 'pm_dataeta', eta_part2: 'pm_eta_conv'}\n        merge_split_frames(frames)\n        &gt;&gt; {eta: pd.concat([pm_dataeta, pm_eta_conv], axis=0).drop_duplicates()}\n    \"\"\"\n\n    grouped = self._group_split_frames(frames)\n    unmerged = {k: v for k, v in grouped.items() if len(v) &gt; 1}\n    merged = {k: pd.concat([frames[v] for v in multi], axis=0) for k, multi in unmerged.items()}\n    merged = {k: v.drop_duplicates().reset_index(drop=True) for k, v in merged.items()}\n\n    to_drop = [item for sublist in unmerged.values() for item in sublist]\n    frames = {k: v for k, v in frames.items() if k not in to_drop}\n    frames.update(merged)\n    return frames\n</code></pre>"},{"location":"reference/remind_coupling/make_pypsa_config/","title":"Make pypsa config","text":"<p>Script to create a PyPSA config file based on the REMIND output/config. NB: Needs to be run before the coupled PyPSA run.</p> Example"},{"location":"reference/remind_coupling/make_pypsa_config/#remind_coupling.make_pypsa_config--config-file-name-needs-to-match-the-output-of-the-snakemake-rule","title":"!! config file name needs to match the output of the snakemake rule","text":"<p><code>snakemake config_file_name -f --cores=1 # makes config_file_name</code> <code>snakemake --configfile=config_file_name</code> # the run</p>"},{"location":"reference/remind_coupling/make_pypsa_config/#remind_coupling.make_pypsa_config.get_currency_conversion","title":"<code>get_currency_conversion(template_cfg)</code>","text":"<p>Get the currency conversion factor from the template config.</p> <p>Parameters:</p> Name Type Description Default <code>template_cfg</code> <code>dict</code> <p>The template configuration dictionary.</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>The currency conversion factor.</p> Source code in <code>workflow/scripts/remind_coupling/make_pypsa_config.py</code> <pre><code>def get_currency_conversion(template_cfg: dict) -&gt; float:\n    \"\"\"Get the currency conversion factor from the template config.\n\n    Args:\n        template_cfg (dict): The template configuration dictionary.\n\n    Returns:\n        float: The currency conversion factor.\n    \"\"\"\n    # ugly stuff\n    etl_cfg = template_cfg[\"remind_etl\"][\"etl_steps\"]\n    techec_cfg = [step for step in etl_cfg if step[\"name\"] == \"technoeconomic_data\"][0]\n    return techec_cfg[\"kwargs\"][\"currency_conversion\"]\n</code></pre>"},{"location":"reference/remind_coupling/make_pypsa_config/#remind_coupling.make_pypsa_config.read_remind_em_prices","title":"<code>read_remind_em_prices(remind_outp_dir, region)</code>","text":"<p>Read relevant REMIND data from the output directory. Args:     remind_outp_dir (os.PathLike): Path to the REMIND output directory.     region (str): Remind region to filter the data by. Returns:     dict: Dictionary containing REMIND data.</p> Source code in <code>workflow/scripts/remind_coupling/make_pypsa_config.py</code> <pre><code>def read_remind_em_prices(remind_outp_dir: os.PathLike, region: str) -&gt; dict:\n    \"\"\"\n    Read relevant REMIND data from the output directory.\n    Args:\n        remind_outp_dir (os.PathLike): Path to the REMIND output directory.\n        region (str): Remind region to filter the data by.\n    Returns:\n        dict: Dictionary containing REMIND data.\n    \"\"\"\n\n    co2_p = (\n        (\n            coupl_utils.read_remind_csv(os.path.join(remind_outp_dir, \"pm_taxCO2eq.csv\"))\n            .query(\"region == @region\")\n            .drop(columns=[\"region\"])\n            .set_index(\"year\")\n        )\n        * 1000\n        / MW_CO2\n        * MW_C\n    )  # gC to ton CO2\n\n    # get remind version\n    with open(os.path.join(remind_outp_dir, \"c_model_version.csv\"), \"r\") as f:\n        remind_v = f.read().split(\"\\n\")[1].replace(\",\", \"\").replace(\" \", \"\")\n    # get remind run name\n    with open(os.path.join(remind_outp_dir, \"c_expname.csv\"), \"r\") as f:\n        remind_exp_name = f.read().split(\"\\n\")[1].replace(\",\", \"\").replace(\" \", \"\")\n\n    return {\"co2_prices\": co2_p, \"version\": remind_v, \"expname\": remind_exp_name}\n</code></pre>"},{"location":"reference/remind_coupling/setup/","title":"Setup","text":"<p>Setup scripts/remind as standalone for dev</p> <ul> <li>add paths</li> <li>mock snakemake</li> </ul>"},{"location":"reference/remind_coupling/setup/#remind_coupling.setup.setup_paths","title":"<code>setup_paths()</code>","text":"<p>Add the scripts directory to the Python path to enable direct imports from workflow/scripts subdirectories without relative imports. (for debugging) Call this at the beginning of any script that needs to import from sibling modules.</p> Source code in <code>workflow/scripts/remind_coupling/setup.py</code> <pre><code>def setup_paths():\n    \"\"\"\n    Add the scripts directory to the Python path to enable direct imports\n    from workflow/scripts subdirectories without relative imports. (for debugging)\n    Call this at the beginning of any script that needs to import from sibling modules.\n    \"\"\"\n    scripts_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n    if scripts_dir not in sys.path:\n        sys.path.insert(0, scripts_dir)\n</code></pre>"}]}